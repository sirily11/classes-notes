{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw4.ipynb","provenance":[],"authorship_tag":"ABX9TyM4s7Ss0ne1Ocpv1xGo/D+A"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"J2Q7wBFjFYja"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5wInlfsZFv74"},"source":["#Question 1\n","\n","Write code to perform the following tasks using GraphFrames:\n","\n","Find Alice's two-hop neighbors' names, regardless of the edge type."]},{"cell_type":"code","metadata":{"id":"BfGnv0d-Fz_v"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w4FnJEtHF0di"},"source":["# Question 2\n","\n","Redo the previous question, but exclude Alice's two-hop neighbors who have an edge back to Alice."]},{"cell_type":"code","metadata":{"id":"GDqM0e1GF7yp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ENjhf8kIF_0w"},"source":["# Question 3\n","\n","Find all people who follow Charlie.\n","\n","Hint: Use AND in SQL, or (..) & (..) in DataFrame boolean expressions."]},{"cell_type":"code","metadata":{"id":"fcTAtvA6GBeA"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yHgBvcRqGDHf"},"source":["# Question 4\n","\n","Find all people who are being followed by at least 2 people."]},{"cell_type":"code","metadata":{"id":"Fl2mxvc1GEl3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yl4HykdGGJtL"},"source":["# Question 5\n","\n","Create a queue of 10 RDDs using this data set and feed it into a Spark Streaming program.  Your Spark Streaming algorithm should maintain a state that keeps track of the longest noun seen so far associated with each distinct adjective. After each RDD, print any 5 adjectives and their associated longest nouns, as well as the longest noun associated with the adjective 'good'. Note that not every line in the data set contains exactly two words, so make sure to clean the data as they are fed into the streaming program.  The skeleton code is provided below:\n","\n","\n","```python\n","\n","from pyspark.streaming import StreamingContext\n","\n","ssc = StreamingContext(sc, 5)\n","# Provide a checkpointing directory. Required for stateful transformations\n","ssc.checkpoint(\"checkpoint\")\n","\n","numPartitions = 8\n","rdd = sc.textFile('../data/adj_noun_pairs.txt', numPartitions)\n","rddQueue = rdd.randomSplit([1]*10, 123)\n","lines = ssc.queueStream(rddQueue)\n","\n","# FILL IN YOUR CODE\n","\n","ssc.start()\n","ssc.awaitTermination(50)\n","ssc.stop(False)\n","print(\"Finished\")\n","```"]},{"cell_type":"code","metadata":{"id":"D9Pu9SucGNX_"},"source":[""],"execution_count":null,"outputs":[]}]}