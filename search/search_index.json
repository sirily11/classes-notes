{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"List of notes \u00b6","title":"List of notes"},{"location":"#list-of-notes","text":"","title":"List of notes"},{"location":"MSBD5001/Lecture%202/","text":"Supervised learning \u00b6 Supervised larning Unsupervised learning Reinforcement learning Whats is machine learning \u00b6 A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks. Linear Regression \u00b6 A Part of machine learning Given training set x, y Find a good approximation to f: \\(x \\to y\\) Examples: Spam detection ( Classification) Digit recognition ( Classification) House price prediction (Refression) Terminology \u00b6 Given a data point (x, y), x is called featyre vector, y is called label The dataset given for learning is training data The dataset to be tested is called testing data Machine learning 3 steps \u00b6 Collect data, extract features Determine a model Train the model with the data Loss \u00b6 Loss on traning set We measure the error using a loss function \\(L(y, \\hat{y})\\) For regression, squared error is often used \\( \\(L(y_1, f(x_i)) = (y_i - f(x_i))^2\\) \\) Loss on testing set Empirical loss is measuring the loss on the training set We assume both training set and testing set are i.i.d from the same distribution D - Minimizing loss on training set will make loss on testing set small Minimizing loss functions \u00b6 The minimizers of some loss functions have analytical solutions: an exact solution you can explicitly derive by analyzing the formula. However, most poular supervised learning models use loss functions with no analytical solution We use gradient descent to approximate the minimal value of function. Gradients: A vector, points to the direction where changing value is the fastest. Method \u00b6 For function G, randomly guess an initial value \\(x_0\\) Repeat \\(xi+1 = x_i - r \\times \\nabla G(x)\\) where \\(\\nabla\\) denotes the gradients, r denotes learning rate Until convergence from sympy import symbols , diff r = 0.1 f_i = ( 1 , 1 , 1 ) x , y , z = symbols ( 'x y z' , real = True ) f = ( y + 2 * x ) ** 2 + y + 2 * x g = ( diff ( f , x ), diff ( f , y ), diff ( f , z )) G (8*x + 4*y + 2, 4*x + 2*y + 1, 0) import numpy as np result = np . array ([ 8 , 6 , 3 ]) * r + np . array ([ 1 , 1 , 1 ]) result array([1.8, 1.6, 1.3]) Linear Classification \u00b6 Use a line to separate data points Use \\(x = (x_1, x_2)\\) , \\(w = (w_1, w_2)\\) , i.e., x, w are vectors in 2D space Doesn't work well with classification problem Label y as either 1 or -1 Find f_w(x) = w^Tx that minimizes the loss function \\( \\(L(f_w(x)) = \\frac{1}{n}\\sum_{i=1}^n(w^Tx_i-y_1)^2\\) \\) Find a line that minimizes the distance between red and blue If there is a outlier in the graph, the seperation line will miss classification some points - If the value get very large, the \\(w^TX_i\\) is correct \\(\\to\\) large loss value even if predict value is positive. \u00b6 Solution: We use sigmoid function to minimize the value between 0 and 1 \\( \\(\\sigma(a) = \\frac{1}{1+exp(-a)}\\) \\) Similar to step functions Continuous and easy to compute Some properties of sigmoid function \u00b6 \\(\\sigma(a) = \\frac{1}{1+exp(-a)} \\in (0, 1)\\) symetric Easy to compute gradients Logistic Regression \u00b6 Better approach ( cross-entropy loss function) find w that minimizes loss function If misclassfication happens on i-th data with label 1, \\(log(\\sigma(w^Tx_i))\\) is very large No analytical solution, needs gradient descent SVM \u00b6 A svm performs classification by finding the hyperplane that maximizes the margin between the two classes K-Nearest neighbor methods \u00b6 Learning algorithm: just store training examples Prediction algorithm: Regression: take the average value of k nearest neighbors Classification: assign to the most frequent class of k nearest neighbors Easy to train with high storage requirement, but high-computation cost at prediction --- Linear knn Advantages Easy to fit Strong assumtions on linear relationship Disadvantages Hard to classify the data Takes a lot of computation power Decision Tree \u00b6 Entropy is used to measure how informative is a probability distribution. The more entropy, the more uncertainty. More info Wrap up \u00b6 Collect data, extract features Determine a model Select a good model for your data","title":"Lecture 2"},{"location":"MSBD5001/Lecture%202/#supervised-learning","text":"Supervised larning Unsupervised learning Reinforcement learning","title":"Supervised learning"},{"location":"MSBD5001/Lecture%202/#whats-is-machine-learning","text":"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks.","title":"Whats is machine learning"},{"location":"MSBD5001/Lecture%202/#linear-regression","text":"A Part of machine learning Given training set x, y Find a good approximation to f: \\(x \\to y\\) Examples: Spam detection ( Classification) Digit recognition ( Classification) House price prediction (Refression)","title":"Linear Regression"},{"location":"MSBD5001/Lecture%202/#terminology","text":"Given a data point (x, y), x is called featyre vector, y is called label The dataset given for learning is training data The dataset to be tested is called testing data","title":"Terminology"},{"location":"MSBD5001/Lecture%202/#machine-learning-3-steps","text":"Collect data, extract features Determine a model Train the model with the data","title":"Machine learning 3 steps"},{"location":"MSBD5001/Lecture%202/#loss","text":"Loss on traning set We measure the error using a loss function \\(L(y, \\hat{y})\\) For regression, squared error is often used \\( \\(L(y_1, f(x_i)) = (y_i - f(x_i))^2\\) \\) Loss on testing set Empirical loss is measuring the loss on the training set We assume both training set and testing set are i.i.d from the same distribution D - Minimizing loss on training set will make loss on testing set small","title":"Loss"},{"location":"MSBD5001/Lecture%202/#minimizing-loss-functions","text":"The minimizers of some loss functions have analytical solutions: an exact solution you can explicitly derive by analyzing the formula. However, most poular supervised learning models use loss functions with no analytical solution We use gradient descent to approximate the minimal value of function. Gradients: A vector, points to the direction where changing value is the fastest.","title":"Minimizing loss functions"},{"location":"MSBD5001/Lecture%202/#method","text":"For function G, randomly guess an initial value \\(x_0\\) Repeat \\(xi+1 = x_i - r \\times \\nabla G(x)\\) where \\(\\nabla\\) denotes the gradients, r denotes learning rate Until convergence from sympy import symbols , diff r = 0.1 f_i = ( 1 , 1 , 1 ) x , y , z = symbols ( 'x y z' , real = True ) f = ( y + 2 * x ) ** 2 + y + 2 * x g = ( diff ( f , x ), diff ( f , y ), diff ( f , z )) G (8*x + 4*y + 2, 4*x + 2*y + 1, 0) import numpy as np result = np . array ([ 8 , 6 , 3 ]) * r + np . array ([ 1 , 1 , 1 ]) result array([1.8, 1.6, 1.3])","title":"Method"},{"location":"MSBD5001/Lecture%202/#linear-classification","text":"Use a line to separate data points Use \\(x = (x_1, x_2)\\) , \\(w = (w_1, w_2)\\) , i.e., x, w are vectors in 2D space Doesn't work well with classification problem Label y as either 1 or -1 Find f_w(x) = w^Tx that minimizes the loss function \\( \\(L(f_w(x)) = \\frac{1}{n}\\sum_{i=1}^n(w^Tx_i-y_1)^2\\) \\) Find a line that minimizes the distance between red and blue If there is a outlier in the graph, the seperation line will miss classification some points","title":"Linear Classification"},{"location":"MSBD5001/Lecture%202/#-if-the-value-get-very-large-the-wtx_i-is-correct-to-large-loss-value-even-if-predict-value-is-positive","text":"Solution: We use sigmoid function to minimize the value between 0 and 1 \\( \\(\\sigma(a) = \\frac{1}{1+exp(-a)}\\) \\) Similar to step functions Continuous and easy to compute","title":"- If the value get very large, the \\(w^TX_i\\) is correct \\(\\to\\) large loss value even if predict value is positive."},{"location":"MSBD5001/Lecture%202/#some-properties-of-sigmoid-function","text":"\\(\\sigma(a) = \\frac{1}{1+exp(-a)} \\in (0, 1)\\) symetric Easy to compute gradients","title":"Some properties of sigmoid function"},{"location":"MSBD5001/Lecture%202/#logistic-regression","text":"Better approach ( cross-entropy loss function) find w that minimizes loss function If misclassfication happens on i-th data with label 1, \\(log(\\sigma(w^Tx_i))\\) is very large No analytical solution, needs gradient descent","title":"Logistic Regression"},{"location":"MSBD5001/Lecture%202/#svm","text":"A svm performs classification by finding the hyperplane that maximizes the margin between the two classes","title":"SVM"},{"location":"MSBD5001/Lecture%202/#k-nearest-neighbor-methods","text":"Learning algorithm: just store training examples Prediction algorithm: Regression: take the average value of k nearest neighbors Classification: assign to the most frequent class of k nearest neighbors Easy to train with high storage requirement, but high-computation cost at prediction --- Linear knn Advantages Easy to fit Strong assumtions on linear relationship Disadvantages Hard to classify the data Takes a lot of computation power","title":"K-Nearest neighbor methods"},{"location":"MSBD5001/Lecture%202/#decision-tree","text":"Entropy is used to measure how informative is a probability distribution. The more entropy, the more uncertainty. More info","title":"Decision Tree"},{"location":"MSBD5001/Lecture%202/#wrap-up","text":"Collect data, extract features Determine a model Select a good model for your data","title":"Wrap up"},{"location":"MSBD5001/Lecture%203/","text":"Overfitting and underfitting \u00b6 Overfitting \u00b6 Even when training data and testing data are i.i.d, generalization may also fail. Is a modeling error which occurs when a function is too closely fit to a limited set of data points. Why is overfitting a problem \u00b6 Overfitting leads to low training error yet high testing error. Out goal is to make the testing error small, Not the training error. Plotting a polynomial \u00b6 Using a polynomial of degree N to fit \\(y==\\sum^N_{i=1}w_ix_i\\) Higher degree has more complex curve to fit the data. https://github.com/MSBD-5001/Lecture-Materials/blob/master/l3_simulation_lecture.ipynb Underfitting \u00b6 Occurs when the model or algorithm doesn't fit the the data well enough. Example \u00b6 import numpy as np import matplotlib.pyplot as plt import pandas as pd import statsmodels.api as sm from statsmodels import regression from scipy i0mport poly1d /usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm x = np . arange ( 10 ) y = 2 * np . random . randn ( 10 ) + x ** 2 xs = np . linspace ( - 0 . 25 , 9 . 25 , 200 ) lin = np . polyfit ( x , y , 1 ) quad = np . polyfit ( x , y , 2 ) many = np . polyfit ( x , y , 9 ) plt . scatter ( x , y ) plt . plot ( xs , poly1d ( lin )( xs )) plt . plot ( xs , poly1d ( quad )( xs )) plt . plot ( xs , poly1d ( many )( xs )) plt . ylabel ( 'Y' ) plt . xlabel ( 'X' ) plt . legend ([ 'Underfit' , 'Good fit' , 'Overfit' ]); Errors: Bias and variance \u00b6 Expect error = \\(Bias^2\\) + Variance + Noise Bias: Difference between the average prediction of our model and the correct value which we are trying to predict Variance: The variability of model prediction for a fiven data point. Our goal is to select models that are of optimal complexity. Complex models have low bias and high variance: - Low bias: Complicated models capture a lot of features - High variance: testing set many not have the same feature - Overfitting Simeple models have low variance and high bias. - Underfitting How to reduce variance and keep bias at a low value? \u00b6 Larger training dataset reduces variance Noise is unavoidable on the data Regularization and ensemble learning Selecting good models \u00b6 Validation \u00b6 Split training data into training and validation data Validation data are only used to evaluate the performance of trained model. If model generalize well on validation data, then should also generalize well on testing data. Wasting part of original training data. Cross validation \u00b6 Will make all training data for validation Partition training data into serveral groups repeat: One group as validation set, train new model Performance metric: average error on validation data. k-fold cross validation \u00b6 Equally split data into k folds Each time uses one fold as validation K fold can be used for large dataset Leave-one-out can be used when dataset is small. Use only 1 sample for validation, the rest for training. Select models with cross-validation. Use cross validation to evaluate performance of different models. Select the best model. Improving the models \u00b6 Method Train sequentially or in parallel How to generate different models Reduces bias or variance Bagging Parallel Boostrap data Variance Random Forest Parallel Bootsrap + random subset of features at splitting Variance Boosting Sequential Reweight training data Bias and variance Regularization \u00b6 Prevent overfitting by reducing flexibility of the model. Prevent parameters having too large absolute values. - Reduce variance - Prevent overfitting Ensemeble \u00b6 Standard decision trees can achieve low bias. - Training set error can be zero. You can always train to the last branch - Large variance Early stopping with fixed nodes or fixed depth may incur high bias Averaging \u00b6 For regression: Simply average the results predicted by different trees, can take weighted average For classification: just select the most predicted value. Also called voting Baging \u00b6 Short for Boostrap aggregating. Bootstrap samples B times, each with size N, with replacement. Train B classifiers each with a bootstrap sample. Bagging gets similar bias: data are from resampling. Random Forest \u00b6 Refinement of the bagged trees. Problem: We want the trees to be independent, don't want them to be similar. But bootstrapping data doesn't help that much: still drawn from same dataset with all features. At each tree split, a random sample of m features are drawn. Only these m features are consldered for splitting. Typically, m is \\(\\sqrt{p}\\) pr \\(p/3\\) where p is the total number of features. Boosting \u00b6 Random forest and bagging: trees are trained in parallel Boosting: trees should be trained sequentially - Start with original training sample - In each iteration: - Train a classifier and check wich samples are hard to train - Increase the weight of those mis-classified samples in training data - Repeat this - Final classifier: weighted classifier model.","title":"Lecture 3"},{"location":"MSBD5001/Lecture%203/#overfitting-and-underfitting","text":"","title":"Overfitting and underfitting"},{"location":"MSBD5001/Lecture%203/#overfitting","text":"Even when training data and testing data are i.i.d, generalization may also fail. Is a modeling error which occurs when a function is too closely fit to a limited set of data points.","title":"Overfitting"},{"location":"MSBD5001/Lecture%203/#why-is-overfitting-a-problem","text":"Overfitting leads to low training error yet high testing error. Out goal is to make the testing error small, Not the training error.","title":"Why is overfitting a problem"},{"location":"MSBD5001/Lecture%203/#plotting-a-polynomial","text":"Using a polynomial of degree N to fit \\(y==\\sum^N_{i=1}w_ix_i\\) Higher degree has more complex curve to fit the data. https://github.com/MSBD-5001/Lecture-Materials/blob/master/l3_simulation_lecture.ipynb","title":"Plotting a polynomial"},{"location":"MSBD5001/Lecture%203/#underfitting","text":"Occurs when the model or algorithm doesn't fit the the data well enough.","title":"Underfitting"},{"location":"MSBD5001/Lecture%203/#example","text":"import numpy as np import matplotlib.pyplot as plt import pandas as pd import statsmodels.api as sm from statsmodels import regression from scipy i0mport poly1d /usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm x = np . arange ( 10 ) y = 2 * np . random . randn ( 10 ) + x ** 2 xs = np . linspace ( - 0 . 25 , 9 . 25 , 200 ) lin = np . polyfit ( x , y , 1 ) quad = np . polyfit ( x , y , 2 ) many = np . polyfit ( x , y , 9 ) plt . scatter ( x , y ) plt . plot ( xs , poly1d ( lin )( xs )) plt . plot ( xs , poly1d ( quad )( xs )) plt . plot ( xs , poly1d ( many )( xs )) plt . ylabel ( 'Y' ) plt . xlabel ( 'X' ) plt . legend ([ 'Underfit' , 'Good fit' , 'Overfit' ]);","title":"Example"},{"location":"MSBD5001/Lecture%203/#errors-bias-and-variance","text":"Expect error = \\(Bias^2\\) + Variance + Noise Bias: Difference between the average prediction of our model and the correct value which we are trying to predict Variance: The variability of model prediction for a fiven data point. Our goal is to select models that are of optimal complexity. Complex models have low bias and high variance: - Low bias: Complicated models capture a lot of features - High variance: testing set many not have the same feature - Overfitting Simeple models have low variance and high bias. - Underfitting","title":"Errors: Bias and variance"},{"location":"MSBD5001/Lecture%203/#how-to-reduce-variance-and-keep-bias-at-a-low-value","text":"Larger training dataset reduces variance Noise is unavoidable on the data Regularization and ensemble learning","title":"How to reduce variance and keep bias at a low value?"},{"location":"MSBD5001/Lecture%203/#selecting-good-models","text":"","title":"Selecting good models"},{"location":"MSBD5001/Lecture%203/#validation","text":"Split training data into training and validation data Validation data are only used to evaluate the performance of trained model. If model generalize well on validation data, then should also generalize well on testing data. Wasting part of original training data.","title":"Validation"},{"location":"MSBD5001/Lecture%203/#cross-validation","text":"Will make all training data for validation Partition training data into serveral groups repeat: One group as validation set, train new model Performance metric: average error on validation data.","title":"Cross validation"},{"location":"MSBD5001/Lecture%203/#k-fold-cross-validation","text":"Equally split data into k folds Each time uses one fold as validation K fold can be used for large dataset Leave-one-out can be used when dataset is small. Use only 1 sample for validation, the rest for training. Select models with cross-validation. Use cross validation to evaluate performance of different models. Select the best model.","title":"k-fold cross validation"},{"location":"MSBD5001/Lecture%203/#improving-the-models","text":"Method Train sequentially or in parallel How to generate different models Reduces bias or variance Bagging Parallel Boostrap data Variance Random Forest Parallel Bootsrap + random subset of features at splitting Variance Boosting Sequential Reweight training data Bias and variance","title":"Improving the models"},{"location":"MSBD5001/Lecture%203/#regularization","text":"Prevent overfitting by reducing flexibility of the model. Prevent parameters having too large absolute values. - Reduce variance - Prevent overfitting","title":"Regularization"},{"location":"MSBD5001/Lecture%203/#ensemeble","text":"Standard decision trees can achieve low bias. - Training set error can be zero. You can always train to the last branch - Large variance Early stopping with fixed nodes or fixed depth may incur high bias","title":"Ensemeble"},{"location":"MSBD5001/Lecture%203/#averaging","text":"For regression: Simply average the results predicted by different trees, can take weighted average For classification: just select the most predicted value. Also called voting","title":"Averaging"},{"location":"MSBD5001/Lecture%203/#baging","text":"Short for Boostrap aggregating. Bootstrap samples B times, each with size N, with replacement. Train B classifiers each with a bootstrap sample. Bagging gets similar bias: data are from resampling.","title":"Baging"},{"location":"MSBD5001/Lecture%203/#random-forest","text":"Refinement of the bagged trees. Problem: We want the trees to be independent, don't want them to be similar. But bootstrapping data doesn't help that much: still drawn from same dataset with all features. At each tree split, a random sample of m features are drawn. Only these m features are consldered for splitting. Typically, m is \\(\\sqrt{p}\\) pr \\(p/3\\) where p is the total number of features.","title":"Random Forest"},{"location":"MSBD5001/Lecture%203/#boosting","text":"Random forest and bagging: trees are trained in parallel Boosting: trees should be trained sequentially - Start with original training sample - In each iteration: - Train a classifier and check wich samples are hard to train - Increase the weight of those mis-classified samples in training data - Repeat this - Final classifier: weighted classifier model.","title":"Boosting"},{"location":"MSBD5001/Lecture%209/","text":"Big Data integration: Record linkage \u00b6 Record linkage: blocking + pairwise matching + clustering - Scalability, similarity, semantics Blocking: e\u000eciently create small blocks of similar records - Ensures scalability Pairwise matching: compares all record pairs in a block - Computes similarity Clustering: groups sets of records into entities - Ensures semantics Volume : dealing with billions of records - Map-reduce based record linkage - Blocking Velocity Incremental record linkage Variety Matching structured and unstructured data Matching Web tables and catalogs Veracity Linking temporal records Data Fusion \u00b6 Data fusion: voting + source quality + copy detection - Resolves inconsistency across diversity of sources - Support di erence of opinion Data fusion: voting + source quality + copy detection - Gives more weight to knowledgeable sources - Reduces weight of copier sources Rule-based \u00b6 Using the observed value from the most recently updated source Taking the average, maximum, or minimum for numerical values Majority voting Naive voting \u00b6 Supports dierence of opinion, allows conflict resolution Works well for independent sources that have similar accuracy When sources have di erent accuracies - Need to give more weight to votes by knowledgeable sources When sources copy from other sources - Need to reduce the weight of votes by copiers Problem: the wisdom of minority fkccecia, Truth Discovery \u00b6 A important feature of turth discover is to estimate source reliabilities. To identify the trustworthy information, i.e. truths: - weighted aggregation of data based on the estimated source reliabilities Both source reliabilities and truths are unknown. - If a source provide trustworthy information frequently, it will be assigned a higher reliability. - If a piece of information is supported by soruces with high reliabilities, it will have a larger change to be selected as the truth. Iteratively until converges: - Truth computation step - Source weight estimation step Truth computation: - The truth is inferred through weighted voting. Optimization-based Methods Easier to understand and interpret - Iteration methods Prior knowledge - Optimization-based: can be formulated as extra constraints - Probabilistic graphical model: can be captured by the hyper parameters Techniques for big data \u00b6 Veracity - Using source trustworthiness - Combining source accuracy and copy detection - Multiple truth values - Erroneous numeric data - Experimental comparison on deep web data Volume: Online data fusion Velocity Truth discovery for dynamic data Variety Combining record linkage with data fusion fkccecia,","title":"Lecture 9"},{"location":"MSBD5001/Lecture%209/#big-data-integration-record-linkage","text":"Record linkage: blocking + pairwise matching + clustering - Scalability, similarity, semantics Blocking: e\u000eciently create small blocks of similar records - Ensures scalability Pairwise matching: compares all record pairs in a block - Computes similarity Clustering: groups sets of records into entities - Ensures semantics Volume : dealing with billions of records - Map-reduce based record linkage - Blocking Velocity Incremental record linkage Variety Matching structured and unstructured data Matching Web tables and catalogs Veracity Linking temporal records","title":"Big Data integration: Record linkage"},{"location":"MSBD5001/Lecture%209/#data-fusion","text":"Data fusion: voting + source quality + copy detection - Resolves inconsistency across diversity of sources - Support di erence of opinion Data fusion: voting + source quality + copy detection - Gives more weight to knowledgeable sources - Reduces weight of copier sources","title":"Data Fusion"},{"location":"MSBD5001/Lecture%209/#rule-based","text":"Using the observed value from the most recently updated source Taking the average, maximum, or minimum for numerical values Majority voting","title":"Rule-based"},{"location":"MSBD5001/Lecture%209/#naive-voting","text":"Supports dierence of opinion, allows conflict resolution Works well for independent sources that have similar accuracy When sources have di erent accuracies - Need to give more weight to votes by knowledgeable sources When sources copy from other sources - Need to reduce the weight of votes by copiers Problem: the wisdom of minority fkccecia,","title":"Naive voting"},{"location":"MSBD5001/Lecture%209/#truth-discovery","text":"A important feature of turth discover is to estimate source reliabilities. To identify the trustworthy information, i.e. truths: - weighted aggregation of data based on the estimated source reliabilities Both source reliabilities and truths are unknown. - If a source provide trustworthy information frequently, it will be assigned a higher reliability. - If a piece of information is supported by soruces with high reliabilities, it will have a larger change to be selected as the truth. Iteratively until converges: - Truth computation step - Source weight estimation step Truth computation: - The truth is inferred through weighted voting. Optimization-based Methods Easier to understand and interpret - Iteration methods Prior knowledge - Optimization-based: can be formulated as extra constraints - Probabilistic graphical model: can be captured by the hyper parameters","title":"Truth Discovery"},{"location":"MSBD5001/Lecture%209/#techniques-for-big-data","text":"Veracity - Using source trustworthiness - Combining source accuracy and copy detection - Multiple truth values - Erroneous numeric data - Experimental comparison on deep web data Volume: Online data fusion Velocity Truth discovery for dynamic data Variety Combining record linkage with data fusion fkccecia,","title":"Techniques for big data"},{"location":"MSBD5001/Lecture1/","text":"Lecture 1 \u00b6 Classification - Data to classes Regression - Predicting a numeric value Clustering Different types of problems \u00b6 Classification Problem - MNIST Dataset Regression - Predicting stock value Clustering Automatically identify the data Data integration \u00b6 Data are created independently A higher-level abstraction Statical analysis \u00b6 Collecting data \u00b6 Collecting, exploring and presenting large amounts of data to discover underlying patterns and trends Data come in two types: - Discrete - Continuous We have - barchart - piechart, Stem-and-leaf plot - Scatterplot ( it uses caresian coordinates to display values for two variables for set of data) - Form - Direction Numerical descriptive measures of data (Central tendency) - Mean - Min - Max - Median - Mode A sampling method is a procudure for selecting sample elements from a population. Relationship between variables: \u00b6 Eyeball fit: Fit two points on the plot so that the line passing through them fives a fairly good fit. Least square fit: Fit a line \\(y = a + bX\\) such that it minimaizes the error S Correlation coefficient, denoted as r, measures the degree to which two variables movements are associated. r = 1 means perfect positive relationship r = 1 means a perfect negative relationship r = 0 means no relationship Forecasting \u00b6 An experiment is an action where the result is uncertain A sample space is all the possible outomes of an experiment, denoted as \\(S\\) . A event is a subset of S Probability : is the measure of how likely an event is to occur out of the number of possible outcomes. $p = \\frac{The\\ number\\ of outcomes}{sample space} $ Parameters \u00b6 Sample can be generated by a probability model, where parameters are characteristics of the model Variance \u00b6 Variance is another parameter of probability model It is a measure of how spread out it is Statical analysis \u00b6 Collecting, exploring and presenting large amounts of data to discover underlying patterns and","title":"Lecture1"},{"location":"MSBD5001/Lecture1/#lecture-1","text":"Classification - Data to classes Regression - Predicting a numeric value Clustering","title":"Lecture 1"},{"location":"MSBD5001/Lecture1/#different-types-of-problems","text":"Classification Problem - MNIST Dataset Regression - Predicting stock value Clustering Automatically identify the data","title":"Different types of problems"},{"location":"MSBD5001/Lecture1/#data-integration","text":"Data are created independently A higher-level abstraction","title":"Data integration"},{"location":"MSBD5001/Lecture1/#statical-analysis","text":"","title":"Statical analysis"},{"location":"MSBD5001/Lecture1/#collecting-data","text":"Collecting, exploring and presenting large amounts of data to discover underlying patterns and trends Data come in two types: - Discrete - Continuous We have - barchart - piechart, Stem-and-leaf plot - Scatterplot ( it uses caresian coordinates to display values for two variables for set of data) - Form - Direction Numerical descriptive measures of data (Central tendency) - Mean - Min - Max - Median - Mode A sampling method is a procudure for selecting sample elements from a population.","title":"Collecting data"},{"location":"MSBD5001/Lecture1/#relationship-between-variables","text":"Eyeball fit: Fit two points on the plot so that the line passing through them fives a fairly good fit. Least square fit: Fit a line \\(y = a + bX\\) such that it minimaizes the error S Correlation coefficient, denoted as r, measures the degree to which two variables movements are associated. r = 1 means perfect positive relationship r = 1 means a perfect negative relationship r = 0 means no relationship","title":"Relationship between variables:"},{"location":"MSBD5001/Lecture1/#forecasting","text":"An experiment is an action where the result is uncertain A sample space is all the possible outomes of an experiment, denoted as \\(S\\) . A event is a subset of S Probability : is the measure of how likely an event is to occur out of the number of possible outcomes. $p = \\frac{The\\ number\\ of outcomes}{sample space} $","title":"Forecasting"},{"location":"MSBD5001/Lecture1/#parameters","text":"Sample can be generated by a probability model, where parameters are characteristics of the model","title":"Parameters"},{"location":"MSBD5001/Lecture1/#variance","text":"Variance is another parameter of probability model It is a measure of how spread out it is","title":"Variance"},{"location":"MSBD5001/Lecture1/#statical-analysis_1","text":"Collecting, exploring and presenting large amounts of data to discover underlying patterns and","title":"Statical analysis"},{"location":"MSBD5001/Lecture4/","text":"Unsupervised learning \u00b6 Another important class of machine learning methods Analyze the structure of the data using feature X Supervised: Use features X to predict labels Y Unsupervised: Only requires features, don\u2019t deal with labels Examples: - Clustering: divide a dataset into meaningful groups. Data points in the same group are more similar with each other, compared to those in different groups. Dimensionality Reduction: | have a dataset of extremely high dimension of features (e.g., images), can | represent them with a lower dimension? Ranking: | have a dataset represented as a graph, each data point is a node, and their relationship are edges, e.g., the World Wide Web, can | rank the importance of the data points? Clustering \u00b6 Clustering: the process of grouping a set of objects into classes of similar objects Objects within the same cluster should be more similar. Objects across the different clusters should be less similar. K-means clustering method \u00b6 Given k, the k-means algorithm is implemented in four steps: Partition objects into k nonempty subsets Compute the mean point for every cluster at current partitioning Reassign each object to the cluster with the nearest mean point Go back to Step 2, stop when the assignment does not change k means by python \u00b6 Original Graph import matplotlib.pyplot as plt import seaborn as sns ; sns . set () # for plot styling import numpy as np from sklearn.datasets.samples_generator import make_blobs X , y_true = make_blobs ( n_samples = 300 , centers = 4 , cluster_std = 0.60 , random_state = 0 ) plt . scatter ( X [:, 0 ], X [:, 1 ], s = 50 ); After grouping from sklearn.cluster import KMeans for i in range ( 1 , 10 ): kmeans = KMeans ( n_clusters = i ) kmeans . fit ( X ) y_kmeans = kmeans . predict ( X ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y_kmeans , s = 50 , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 ) plt . title ( f \"k-means cluster: {i}\" ) plt . show () Example 1 \u00b6 Suppose you have 5 points in 1-D: {1,2,4,7,10}. Use k-means to cluster these points with k=2. Start with initial partition {1} and {2,4,7,10}. The distance is difference of coordinate on the axis. Drawbacks \u00b6 Both large K and small K can lead to bad results: left K=4, right K=2. Didn't describe data well Hierarchical clustering \u00b6 A method of cluster analysis which seeks to build a hierarchy of clusters No need to specify the number of clusters, we can generate partitions at different levels of the hierarchy. A dendrogram is a tree diagram that can be used to represent the hierarchical clustering structure between data points. The height of connections in the dendrogram represents the distance between partitions: The higher the connection, the larger distance between the two connected clusters. Cut the dendrogram: Clustering is obtained by cutting the dendrogram at the desired level, then each connected component forms a cluster. Building diagram \u00b6 Two types: bottom-up (agglomerative), and top-down (divisive) Bottom-up: two groups are merged if distance between them is less than a threshold Top-down: one group is split into two if inter- group distance is more than a threshold Dimensionality Reduction \u00b6 Given data points in d dimensions, convert them to data points in r<d dimensions, with minimal loss of information. Used for statistical analysis, data compression, and data visualization Idea of Principle Component Analysis \u00b6 Reduce from n-dimension to k-dimension: Find vectors \\(u^1,u^2,...,u^k\\) onto which to project the data, so as to minimize the projection error. These vectors should represent primary information of data, we call them principle components Identity matrix \u00b6 In linear algebra, the identity matrix (sometimes ambiguously called a unit matrix) of size n is the n \u00d7 n square matrix with ones on the main diagonal and zeros elsewhere. It is denoted by In, or simply by I if the size is immaterial or can be trivially determined by the context. In some fields, such as quantum mechanics, the identity matrix is denoted by a boldface one, 1; otherwise it is identical to I. Less frequently, some mathematics books use U or E to represent the identity matrix, meaning \"unit matrix\" and the German word Einheitsmatrix respectively. Eigenvalues and eigenvectors \u00b6 https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors https://www.khanacademy.org/math/linear-algebra/alternate-bases/eigen-everything/v/linear-algebra-introduction-to-eigenvalues-and-eigenvectors Intuitive Idea of PCA \u00b6 https://dilloncamp.com/projects/pca.html What we DON\u2019T want for projection: Original data has large variance, but projected data has small variance. lt means original data is spread, but projected data is not: A lot of information loss during projection. PCA\u2018s goal: maximize the variance of projected data. In fact, the mathematical definition of principle. components (PC) is the eigenvectors of covariance matrix of data points The order of PCs follows the magnitude of. eigenvalues, e.g., the most Signi icant PC is the eigenvector corresponding to largest eigenvalue Page rating \u00b6 A method for rating the importance of web pages using the link structure of the web Simple Recursive Formulation \u00b6 Each link\u2019s vote is proportional to the importance of its source page If page P with importance x has n out-links, each link gets x/n votes Page P\u2019s own importance is the sum of the votes on its in-links Final PageRank score: Importance=sum of votes from all in-links Page rank in python \u00b6 \\[ r_a = \\sum_{j=1}^n L_{a.j}r_j\\] import numpy as np import matplotlib.pyplot as plt # set plot size plt . rcParams [ 'figure.figsize' ] = [ 20 , 5 ] def page_rank ( matrix : np . array , iter = 3 ): shape = matrix . shape [ 1 ] r = np . full (( shape , 1 ), 1 / shape ) l = matrix total_results = None for i in range ( iter ): r = l . dot ( r ) if i == 0 : total_results = r else : total_results = np . concatenate (( total_results , r ), axis = 1 ) return r , total_results m = np . array ( [[ 0 , 0.5 , 0 , 0 ], [ 1 / 3 , 0 , 0 , 1 / 2 ], [ 1 / 3 , 0 , 0 , 1 / 2 ], [ 1 / 3 , 1 / 2 , 1 , 0 ] ] ) r , total_results = page_rank ( m , 20 ) for i , p in enumerate ( total_results ): plt . plot ( p , label = f \" { i } - line\" ) plt . legend () <matplotlib.legend.Legend at 0x7fc4907065c0> Random walk interpretation \u00b6 An equivalent view of PageRank. Imagine a random web surfer At any time t, surfer is on some page P At time t+1, the surfer follows an outlink from P uniformly at random. Ends up on some page Q linked from P, process repeats indefinitely Let p(t) be a vector whose \\(i^{th}\\) component is the probability that the surfer is at page i at time t * p(t) is a probability distribution on pages Stationary Distribution \u00b6 Where is the surfer at time t+1? - Follows a link uniformly at some probability - p(t+1) = Mp(t) where M is the transition probability Suppose the random walk reaches a state such that p(t+1) = Mp(t) = p(t) Then p(t) is called a stationary distribution for the random walk The PageRank score r is the stationary distribution, can be solved by r=Mr. Normalization by scaling sum of r to 1. Stationary Distribution=PageRank Score \u00b6 Stationary distribution represents PageRank score. PageRank Score: A node\u2019s importance equals to the votes from adjacent nodes. Stationary Distribution: Probability of being at one node equals to sum of the probabilities coming from other nodes Both of them describe the stable state. Spider Traps \u00b6 Agroup of pages is a spider trap if there are no links from pages within the group to pages outside the group Random surfer gets trapped, it continuously walk in the trap. Spider traps violate the conditions needed for the random walk theorem import numpy as np def pagerank ( M , num_iterations : int = 100 , d : float = 0.85 ): \"\"\"PageRank: The trillion dollar algorithm. Parameters ---------- M : numpy array adjacency matrix where M_i,j represents the link from 'j' to 'i', such that for all 'j' sum(i, M_i,j) = 1 num_iterations : int, optional number of iterations, by default 100 d : float, optional damping factor, by default 0.85 Returns ------- numpy array a vector of ranks such that v_i is the i-th rank from [0, 1], v sums to 1 \"\"\" N = M . shape [ 1 ] v = np . random . rand ( N , 1 ) v = v / np . linalg . norm ( v , 1 ) M_hat = ( d * M + ( 1 - d ) / N ) for i in range ( num_iterations ): v = M_hat @ v return v M = np . array ([[ 0.5 , 0.5 , 0 ], [ 0.5 , 0 , 0 ], [ 0 , 0.5 , 1 ], ]) v = pagerank ( M , 100 , 0.8 ) print ( v ) [[0.21212121] [0.15151515] [0.63636364]]","title":"Lecture4"},{"location":"MSBD5001/Lecture4/#unsupervised-learning","text":"Another important class of machine learning methods Analyze the structure of the data using feature X Supervised: Use features X to predict labels Y Unsupervised: Only requires features, don\u2019t deal with labels Examples: - Clustering: divide a dataset into meaningful groups. Data points in the same group are more similar with each other, compared to those in different groups. Dimensionality Reduction: | have a dataset of extremely high dimension of features (e.g., images), can | represent them with a lower dimension? Ranking: | have a dataset represented as a graph, each data point is a node, and their relationship are edges, e.g., the World Wide Web, can | rank the importance of the data points?","title":"Unsupervised learning"},{"location":"MSBD5001/Lecture4/#clustering","text":"Clustering: the process of grouping a set of objects into classes of similar objects Objects within the same cluster should be more similar. Objects across the different clusters should be less similar.","title":"Clustering"},{"location":"MSBD5001/Lecture4/#k-means-clustering-method","text":"Given k, the k-means algorithm is implemented in four steps: Partition objects into k nonempty subsets Compute the mean point for every cluster at current partitioning Reassign each object to the cluster with the nearest mean point Go back to Step 2, stop when the assignment does not change","title":"K-means clustering method"},{"location":"MSBD5001/Lecture4/#k-means-by-python","text":"Original Graph import matplotlib.pyplot as plt import seaborn as sns ; sns . set () # for plot styling import numpy as np from sklearn.datasets.samples_generator import make_blobs X , y_true = make_blobs ( n_samples = 300 , centers = 4 , cluster_std = 0.60 , random_state = 0 ) plt . scatter ( X [:, 0 ], X [:, 1 ], s = 50 ); After grouping from sklearn.cluster import KMeans for i in range ( 1 , 10 ): kmeans = KMeans ( n_clusters = i ) kmeans . fit ( X ) y_kmeans = kmeans . predict ( X ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y_kmeans , s = 50 , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 ) plt . title ( f \"k-means cluster: {i}\" ) plt . show ()","title":"k means by python"},{"location":"MSBD5001/Lecture4/#example-1","text":"Suppose you have 5 points in 1-D: {1,2,4,7,10}. Use k-means to cluster these points with k=2. Start with initial partition {1} and {2,4,7,10}. The distance is difference of coordinate on the axis.","title":"Example 1"},{"location":"MSBD5001/Lecture4/#drawbacks","text":"Both large K and small K can lead to bad results: left K=4, right K=2. Didn't describe data well","title":"Drawbacks"},{"location":"MSBD5001/Lecture4/#hierarchical-clustering","text":"A method of cluster analysis which seeks to build a hierarchy of clusters No need to specify the number of clusters, we can generate partitions at different levels of the hierarchy. A dendrogram is a tree diagram that can be used to represent the hierarchical clustering structure between data points. The height of connections in the dendrogram represents the distance between partitions: The higher the connection, the larger distance between the two connected clusters. Cut the dendrogram: Clustering is obtained by cutting the dendrogram at the desired level, then each connected component forms a cluster.","title":"Hierarchical clustering"},{"location":"MSBD5001/Lecture4/#building-diagram","text":"Two types: bottom-up (agglomerative), and top-down (divisive) Bottom-up: two groups are merged if distance between them is less than a threshold Top-down: one group is split into two if inter- group distance is more than a threshold","title":"Building diagram"},{"location":"MSBD5001/Lecture4/#dimensionality-reduction","text":"Given data points in d dimensions, convert them to data points in r<d dimensions, with minimal loss of information. Used for statistical analysis, data compression, and data visualization","title":"Dimensionality Reduction"},{"location":"MSBD5001/Lecture4/#idea-of-principle-component-analysis","text":"Reduce from n-dimension to k-dimension: Find vectors \\(u^1,u^2,...,u^k\\) onto which to project the data, so as to minimize the projection error. These vectors should represent primary information of data, we call them principle components","title":"Idea of Principle Component Analysis"},{"location":"MSBD5001/Lecture4/#identity-matrix","text":"In linear algebra, the identity matrix (sometimes ambiguously called a unit matrix) of size n is the n \u00d7 n square matrix with ones on the main diagonal and zeros elsewhere. It is denoted by In, or simply by I if the size is immaterial or can be trivially determined by the context. In some fields, such as quantum mechanics, the identity matrix is denoted by a boldface one, 1; otherwise it is identical to I. Less frequently, some mathematics books use U or E to represent the identity matrix, meaning \"unit matrix\" and the German word Einheitsmatrix respectively.","title":"Identity matrix"},{"location":"MSBD5001/Lecture4/#eigenvalues-and-eigenvectors","text":"https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors https://www.khanacademy.org/math/linear-algebra/alternate-bases/eigen-everything/v/linear-algebra-introduction-to-eigenvalues-and-eigenvectors","title":"Eigenvalues and eigenvectors"},{"location":"MSBD5001/Lecture4/#intuitive-idea-of-pca","text":"https://dilloncamp.com/projects/pca.html What we DON\u2019T want for projection: Original data has large variance, but projected data has small variance. lt means original data is spread, but projected data is not: A lot of information loss during projection. PCA\u2018s goal: maximize the variance of projected data. In fact, the mathematical definition of principle. components (PC) is the eigenvectors of covariance matrix of data points The order of PCs follows the magnitude of. eigenvalues, e.g., the most Signi icant PC is the eigenvector corresponding to largest eigenvalue","title":"Intuitive Idea of PCA"},{"location":"MSBD5001/Lecture4/#page-rating","text":"A method for rating the importance of web pages using the link structure of the web","title":"Page rating"},{"location":"MSBD5001/Lecture4/#simple-recursive-formulation","text":"Each link\u2019s vote is proportional to the importance of its source page If page P with importance x has n out-links, each link gets x/n votes Page P\u2019s own importance is the sum of the votes on its in-links Final PageRank score: Importance=sum of votes from all in-links","title":"Simple Recursive Formulation"},{"location":"MSBD5001/Lecture4/#page-rank-in-python","text":"\\[ r_a = \\sum_{j=1}^n L_{a.j}r_j\\] import numpy as np import matplotlib.pyplot as plt # set plot size plt . rcParams [ 'figure.figsize' ] = [ 20 , 5 ] def page_rank ( matrix : np . array , iter = 3 ): shape = matrix . shape [ 1 ] r = np . full (( shape , 1 ), 1 / shape ) l = matrix total_results = None for i in range ( iter ): r = l . dot ( r ) if i == 0 : total_results = r else : total_results = np . concatenate (( total_results , r ), axis = 1 ) return r , total_results m = np . array ( [[ 0 , 0.5 , 0 , 0 ], [ 1 / 3 , 0 , 0 , 1 / 2 ], [ 1 / 3 , 0 , 0 , 1 / 2 ], [ 1 / 3 , 1 / 2 , 1 , 0 ] ] ) r , total_results = page_rank ( m , 20 ) for i , p in enumerate ( total_results ): plt . plot ( p , label = f \" { i } - line\" ) plt . legend () <matplotlib.legend.Legend at 0x7fc4907065c0>","title":"Page rank in python"},{"location":"MSBD5001/Lecture4/#random-walk-interpretation","text":"An equivalent view of PageRank. Imagine a random web surfer At any time t, surfer is on some page P At time t+1, the surfer follows an outlink from P uniformly at random. Ends up on some page Q linked from P, process repeats indefinitely Let p(t) be a vector whose \\(i^{th}\\) component is the probability that the surfer is at page i at time t * p(t) is a probability distribution on pages","title":"Random walk interpretation"},{"location":"MSBD5001/Lecture4/#stationary-distribution","text":"Where is the surfer at time t+1? - Follows a link uniformly at some probability - p(t+1) = Mp(t) where M is the transition probability Suppose the random walk reaches a state such that p(t+1) = Mp(t) = p(t) Then p(t) is called a stationary distribution for the random walk The PageRank score r is the stationary distribution, can be solved by r=Mr. Normalization by scaling sum of r to 1.","title":"Stationary Distribution"},{"location":"MSBD5001/Lecture4/#stationary-distributionpagerank-score","text":"Stationary distribution represents PageRank score. PageRank Score: A node\u2019s importance equals to the votes from adjacent nodes. Stationary Distribution: Probability of being at one node equals to sum of the probabilities coming from other nodes Both of them describe the stable state.","title":"Stationary Distribution=PageRank Score"},{"location":"MSBD5001/Lecture4/#spider-traps","text":"Agroup of pages is a spider trap if there are no links from pages within the group to pages outside the group Random surfer gets trapped, it continuously walk in the trap. Spider traps violate the conditions needed for the random walk theorem import numpy as np def pagerank ( M , num_iterations : int = 100 , d : float = 0.85 ): \"\"\"PageRank: The trillion dollar algorithm. Parameters ---------- M : numpy array adjacency matrix where M_i,j represents the link from 'j' to 'i', such that for all 'j' sum(i, M_i,j) = 1 num_iterations : int, optional number of iterations, by default 100 d : float, optional damping factor, by default 0.85 Returns ------- numpy array a vector of ranks such that v_i is the i-th rank from [0, 1], v sums to 1 \"\"\" N = M . shape [ 1 ] v = np . random . rand ( N , 1 ) v = v / np . linalg . norm ( v , 1 ) M_hat = ( d * M + ( 1 - d ) / N ) for i in range ( num_iterations ): v = M_hat @ v return v M = np . array ([[ 0.5 , 0.5 , 0 ], [ 0.5 , 0 , 0 ], [ 0 , 0.5 , 1 ], ]) v = pagerank ( M , 100 , 0.8 ) print ( v ) [[0.21212121] [0.15151515] [0.63636364]]","title":"Spider Traps"},{"location":"MSBD5001/Lecture6/","text":"Similarity and Dissimilarity \u00b6 Similarity is the numerical measure of how alike two data objects are. Similarity is important. It is the basic component of many data processing techniques, such as - data integration - data mining: classi cation, clustering, recommendation, anomaly detection Dissimilarity is the numerical measure of how two objects are different. The term distance is frequently used as a synonym for dissimilarity. Attibute Type \u00b6 Norminal (Categorical) Ordinal Interval or Ratio String Matching: \u00b6 Matching strings often appear quite di erently - Typing and OCR errors: David Smith vs. Davod Smith - Di erent formatting convertions: 10/8 vs Oct 8 - Custom abbreviation, shortening, or omission: Daniel Walker Herbert Smith vs. Dan W. Smith - Di erent names, nick names: William Smith vs. Bill Smith - Shu\u000fing parts of strings: Dept. of Computer Science, UST vs. Computer Science Dept., UST namea = \"Dave Smith\" nameb = 'David D. Smith' Edit Distance \u00b6 \\[min \\begin{cases}d(i-1, j) + 1\\\\ d(i, j-1)+1\\\\ d(i-1, j-1) +1\\ if x_1 \\neq y_j \\\\ d(i-1,j-1)\\ if x_1 =y_1 \\end{cases}\\] from pprint import pprint def minDistance ( word1 : str , word2 : str ) -> int : # padding one whitespace for empty string representation word_1 = ' ' + word1 word_2 = ' ' + word2 h , w = len ( word_1 ), len ( word_2 ) min_edit_dist = [ [ 0 for _ in range ( w ) ] for _ in range ( h ) ] # initialization for top row for x in range ( 1 , w ): min_edit_dist [ 0 ][ x ] = x # initialization for left-most column for y in range ( 1 , h ): min_edit_dist [ y ][ 0 ] = y # compute minimum edit distance with optimal substructure for y in range ( 1 , h ): for x in range ( 1 , w ): if word_1 [ y ] == word_2 [ x ]: # current character match, no need to edit min_edit_dist [ y ][ x ] = min_edit_dist [ y - 1 ][ x - 1 ] else : # current character mismatch, choose the method of lowest cost, among character replacement, character addition, or character deletion min_edit_dist [ y ][ x ] = min ( min_edit_dist [ y ][ x - 1 ], min_edit_dist [ y - 1 ][ x ], min_edit_dist [ y - 1 ][ x - 1 ]) + 1 pprint ( min_edit_dist ) return min_edit_dist [ - 1 ][ - 1 ] minDistance(namea, nameb) [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], [3, 2, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [4, 3, 2, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [5, 4, 3, 2, 2, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10], [6, 5, 4, 3, 3, 3, 3, 3, 4, 5, 5, 6, 7, 8, 9], [7, 6, 5, 4, 4, 4, 4, 4, 4, 5, 6, 5, 6, 7, 8], [8, 7, 6, 5, 4, 5, 5, 5, 5, 5, 6, 6, 5, 6, 7], [9, 8, 7, 6, 5, 5, 6, 6, 6, 6, 6, 7, 6, 5, 6], [10, 9, 8, 7, 6, 6, 6, 7, 7, 7, 7, 7, 7, 6, 5]] 5 Needleman wunch measure \u00b6 initialize matrix of size (n + 1)x(m + 1) where s(a; b) is the element at the a\udbc0\udc00th row and b\udbc0\udc00th column. fill matrix: \\(s(i,0) = -i*c_g, s(0,j)=-j*c_g\\) \\[s(i, j) = max \\begin{cases} s(u - 1, j) - c_g\\\\ s(i, j-1)-c_g\\\\ s(i-1,j-1)=c(x_i,y_j) \\end{cases}\\] from pprint import pprint def find_match ( word1 : str , word2 : str , cg , c , cm ): if word1 == word2 : return c elif word1 == ' ' or word2 == ' ' : return - cg else : return - cm def needle_man ( word1 : str , word2 : str , cg = 1 , c = 1 , cm = 1 ) -> int : \"\"\" cg: gap_penalty c: match award cm: mismatch penalty \"\"\" # padding one whitespace for empty string representation word_1 = ' ' + word1 word_2 = ' ' + word2 h , w = len ( word_1 ), len ( word_2 ) s = [ [ 0 for _ in range ( w ) ] for _ in range ( h ) ] # initialization for top row for j in range ( 1 , w ): s [ 0 ][ j ] = - j * cg # initialization for left-most column for i in range ( 1 , h ): s [ i ][ 0 ] = - i * cg for i in range ( 1 , h ): for j in range ( 1 , w ): s [ i ][ j ] = max ( s [ i - 1 ][ j ] - cg , s [ i ][ j - 1 ] - cg , s [ i - 1 ][ j - 1 ] + find_match ( word_1 [ i ], word_2 [ j ], cg , c , cm )) pprint ( s ) return s [ - 1 ][ - 1 ] needle_man(\"dva\", \"deeve\", c=2) [[0, -1, -2, -3, -4, -5], [-1, 2, 1, 0, -1, -2], [-2, 1, 1, 0, 2, 1], [-3, 0, 0, 0, 1, 1]] 1 Affine gap measure \u00b6 Define x = \\(x_1x_2...x_n\\) ; y = \\(y_1y_2..y_m\\) where xi and yj are the i-th and j-th prefixes of x and y Initialization: \\(M(0, 0) = 0, l_x(0,0)=-c_o, l_u(0,0)=-c_o\\) \\(l_x(i,0)=-c_o - c_r * (i - 1)\\) - \\(l_y(0, j) = -c_o - c_r * (j - 1)\\) Other cells in top row and leftmost column = \\(-\\infty\\) \\[M(i, j) = max \\begin{cases} M(i -1, j-1)+c(x_i,y_i)\\\\ l_x(i - 1, j -1) + c(x_i,y_i)\\\\ l_y(u - 1, j-1)+c(x_i, y_i) \\end{cases} \\] \\[ l_x(i, j) = max\\begin{cases} M(i - 1, j) - c_o \\\\ l_x(i - 1, j) - c_r \\end{cases} \\] \\[ l_y(i, j) = max\\begin{cases} M(i , j - 1) - c_o \\\\ l_y(i, j - 1) - c_r \\end{cases} \\] where \\(c_o\\) is the cost of opening a gap, \\(c_r\\) is the cost of continuing a gap, \\((x_i, y_j)\\) is the score for correspoding character \\(x_i\\) with \\(y_j\\) in the score matrix. Score: max(m, ix, iy) from math import inf def find_match ( word1 , word2 , reward , penalty ): if word1 == word2 : return reward else : return - penalty def affine_gap ( word1 : str , word2 : str , co = 1 , cr = 1 , cg = 1 , c = 1 , cm = 1 ) -> int : \"\"\" co: cost of opening a gap cm: cost of continuing the gap cg: gap_penalty c: match award cm: mismatch penalty \"\"\" # padding one whitespace for empty string representation word_1 = ' ' + word1 word_2 = ' ' + word2 h , w = len ( word_1 ), len ( word_2 ) m = [ [ - inf for _ in range ( w ) ] for _ in range ( h ) ] i_x = [ [ - inf for _ in range ( w ) ] for _ in range ( h ) ] i_y = [ [ - inf for _ in range ( w ) ] for _ in range ( h ) ] m [ 0 ][ 0 ] = 0 i_x [ 0 ][ 0 ] = - co i_y [ 0 ][ 0 ] = - co # initialization for top row for j in range ( 1 , w ): i_y [ 0 ][ j ] = - co - cr * ( j - 1 ) # initialization for left-most column for i in range ( 1 , h ): i_x [ i ][ 0 ] = - co - cr * ( i - 1 ) for i in range ( 1 , h ): for j in range ( 1 , w ): match_reward = find_match ( word_1 [ i ], word_2 [ j ], reward = c , penalty = cm ) m [ i ][ j ] = max ( m [ i - 1 ][ j - 1 ] + match_reward , i_x [ i - 1 ][ j - 1 ] + match_reward , i_y [ i - 1 ][ j - 1 ] + match_reward ) i_x [ i ][ j ] = max ( m [ i - 1 ][ j ] - co , i_x [ i - 1 ][ j ] - cr ) i_y [ i ][ j ] = max ( m [ i ][ j - 1 ] - co , i_y [ i ][ j - 1 ] - cr ) print ( \"m: \" ) pprint ( m ) print ( \"i_x: \" ) pprint ( i_x ) print ( \"i_y: \" ) pprint ( i_y ) return max ( m [ - 1 ][ - 1 ], i_x [ - 1 ][ - 1 ], i_y [ - 1 ][ - 1 ]) affine_gap(\"AAT\", \"ACACT\", c=1, co=4, cr=1) m: [[0, -inf, -inf, -inf, -inf, -inf], [-inf, 1, -5, -4, -7, -8], [-inf, -3, 0, -2, -5, -6], [-inf, -6, -4, -1, -3, -4]] i_x: [[-4, -inf, -inf, -inf, -inf, -inf], [-4, -inf, -inf, -inf, -inf, -inf], [-5, -3, -9, -8, -11, -12], [-6, -4, -4, -6, -9, -10]] i_y: [[-4, -4, -5, -6, -7, -8], [-inf, -inf, -3, -4, -5, -6], [-inf, -inf, -7, -4, -5, -6], [-inf, -inf, -10, -8, -5, -6]] -4 Smith-Waterman Measure \u00b6 Initialization: - initialize matrix of size (n + 1)x(m + 1) where s(a; b) is the element at the a-th row and b-th column. - fill matrix: s(i ; 0) = 0, s(0; j) = 0 \\[s(i, j) = max= \\begin {cases} 0 \\\\ s_i - 1, j)-c_g\\\\ s(i, j-1)-c_g\\\\ s(i-1,j-1)+c(x_i,y_i) \\end{cases}\\] def smith_waterman ( word1 : str , word2 : str , cm = 1 , c = 1 , cg = 1 ) -> int : \"\"\" cg: gap_penalty c: match award cm: mismatch penalty \"\"\" # padding one whitespace for empty string representation word_1 = ' ' + word1 word_2 = ' ' + word2 h , w = len ( word_1 ), len ( word_2 ) s = [ [ 0 for _ in range (w ) ] for _ in range ( h ) ] # initialization for top row for j in range ( 1 , w ) : s [ 0 ][ j ] = 0 # initialization for left - most column for i in range ( 1 , h ) : s [ i ][ 0 ] = 0 for i in range ( 1 , h ) : for j in range ( 1 , w ) : match_reward = find_match ( word_1 [ i ] , word_2 [ j ] , reward = c , penalty = cg ) s [ i ][ j ] = max ( 0 , s [ i - 1 ][ j ] - cg , s [ i ][ j - 1 ] - cg , s [ i - 1 ][ j - 1 ] + match_reward ) pprint ( s ) return s [ -1 ][ -1 ] smith_waterman(\" avd\", \"dave\") [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 2, 1], [0, 1, 0, 1, 1]] 1 Set base \u00b6 View strings as sets or multi-sets of tokens Common methods to generate tokens words delimited by space e.g. for the string \\david smith\", the tokens are \\david\" and \\smith\" stem the words if necessary remove stop words (e.g. the, and of) I q-grams, substrings of length q e.g. for the string \\david smith\", the set of 3-grams are ##d, #da, dav, avi, ..., h## special character # to handle the start and end of string fkccecia, Overlap Measure \u00b6 Let Bx = set of tokens generated for string x Let By = set of tokens generated for string y returns the number of common tokens \\( \\(O(x, y) = |B_x \\cap B_y |\\) \\) E.g., x = dave, y = dav, considering 2-grams $$ B_x = {#d, da, av, ve, e# }$$ \\[ B_y = \\{\\#d,da, av, v\\# \\}$$ $$ O(x, y) = 3 \\] def token ( word : str , max_token_len = 2 ): sets = [] new_word = f \"#{word}#\" length = len ( new_word ) for i in range ( length - 1 ): w = new_word [ i : i + 2 ] sets . append ( w ) return set ( sets ) def overlap ( word1 , word2 , max_token_len = 2 ): set1 = token ( word1 , max_token_len ) set2 = token ( word2 , max_token_len ) return set1 . intersection ( set2 ) overlap(\"dave\", \"dav\") {'#d', 'av', 'da'} Jaccard Measure \u00b6 Let Bx = set of tokens generated for string x Let By = set of tokens generated for string y returns the number of common tokens \\( \\(J(x, y) = |B_x \\cap B_y | / |B_x \\cup B_y |\\) \\) E.g., x = dave, y = dav, considering 2-grams Bx = {#d, da, av, ve, e#}, By = {#d, da, av, v#} J(x; y) = 3 / 6 def jaccard ( word1 , word2 , max_token_len = 2 ): set1 = token ( word1 , max_token_len ) set2 = token ( word2 , max_token_len ) print ( f \"Union: {set1.union(set2)}\" ) print ( f \"Intersection: {set1.intersection(set2)}\" ) return len ( set1 . intersection ( set2 )) / len ( set1 . union ( set2 )) jaccard(\"dave\", \"dav\") Union: {'#d', 'e#', 've', 'da', 'v#', 'av'} Intersection: {'#d', 'da', 'av'} 0.5","title":"Lecture6"},{"location":"MSBD5001/Lecture6/#similarity-and-dissimilarity","text":"Similarity is the numerical measure of how alike two data objects are. Similarity is important. It is the basic component of many data processing techniques, such as - data integration - data mining: classi cation, clustering, recommendation, anomaly detection Dissimilarity is the numerical measure of how two objects are different. The term distance is frequently used as a synonym for dissimilarity.","title":"Similarity and Dissimilarity"},{"location":"MSBD5001/Lecture6/#attibute-type","text":"Norminal (Categorical) Ordinal Interval or Ratio","title":"Attibute Type"},{"location":"MSBD5001/Lecture6/#string-matching","text":"Matching strings often appear quite di erently - Typing and OCR errors: David Smith vs. Davod Smith - Di erent formatting convertions: 10/8 vs Oct 8 - Custom abbreviation, shortening, or omission: Daniel Walker Herbert Smith vs. Dan W. Smith - Di erent names, nick names: William Smith vs. Bill Smith - Shu\u000fing parts of strings: Dept. of Computer Science, UST vs. Computer Science Dept., UST namea = \"Dave Smith\" nameb = 'David D. Smith'","title":"String Matching:"},{"location":"MSBD5001/Lecture6/#edit-distance","text":"\\[min \\begin{cases}d(i-1, j) + 1\\\\ d(i, j-1)+1\\\\ d(i-1, j-1) +1\\ if x_1 \\neq y_j \\\\ d(i-1,j-1)\\ if x_1 =y_1 \\end{cases}\\] from pprint import pprint def minDistance ( word1 : str , word2 : str ) -> int : # padding one whitespace for empty string representation word_1 = ' ' + word1 word_2 = ' ' + word2 h , w = len ( word_1 ), len ( word_2 ) min_edit_dist = [ [ 0 for _ in range ( w ) ] for _ in range ( h ) ] # initialization for top row for x in range ( 1 , w ): min_edit_dist [ 0 ][ x ] = x # initialization for left-most column for y in range ( 1 , h ): min_edit_dist [ y ][ 0 ] = y # compute minimum edit distance with optimal substructure for y in range ( 1 , h ): for x in range ( 1 , w ): if word_1 [ y ] == word_2 [ x ]: # current character match, no need to edit min_edit_dist [ y ][ x ] = min_edit_dist [ y - 1 ][ x - 1 ] else : # current character mismatch, choose the method of lowest cost, among character replacement, character addition, or character deletion min_edit_dist [ y ][ x ] = min ( min_edit_dist [ y ][ x - 1 ], min_edit_dist [ y - 1 ][ x ], min_edit_dist [ y - 1 ][ x - 1 ]) + 1 pprint ( min_edit_dist ) return min_edit_dist [ - 1 ][ - 1 ] minDistance(namea, nameb) [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], [3, 2, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [4, 3, 2, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [5, 4, 3, 2, 2, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10], [6, 5, 4, 3, 3, 3, 3, 3, 4, 5, 5, 6, 7, 8, 9], [7, 6, 5, 4, 4, 4, 4, 4, 4, 5, 6, 5, 6, 7, 8], [8, 7, 6, 5, 4, 5, 5, 5, 5, 5, 6, 6, 5, 6, 7], [9, 8, 7, 6, 5, 5, 6, 6, 6, 6, 6, 7, 6, 5, 6], [10, 9, 8, 7, 6, 6, 6, 7, 7, 7, 7, 7, 7, 6, 5]] 5","title":"Edit Distance"},{"location":"MSBD5001/Lecture6/#needleman-wunch-measure","text":"initialize matrix of size (n + 1)x(m + 1) where s(a; b) is the element at the a\udbc0\udc00th row and b\udbc0\udc00th column. fill matrix: \\(s(i,0) = -i*c_g, s(0,j)=-j*c_g\\) \\[s(i, j) = max \\begin{cases} s(u - 1, j) - c_g\\\\ s(i, j-1)-c_g\\\\ s(i-1,j-1)=c(x_i,y_j) \\end{cases}\\] from pprint import pprint def find_match ( word1 : str , word2 : str , cg , c , cm ): if word1 == word2 : return c elif word1 == ' ' or word2 == ' ' : return - cg else : return - cm def needle_man ( word1 : str , word2 : str , cg = 1 , c = 1 , cm = 1 ) -> int : \"\"\" cg: gap_penalty c: match award cm: mismatch penalty \"\"\" # padding one whitespace for empty string representation word_1 = ' ' + word1 word_2 = ' ' + word2 h , w = len ( word_1 ), len ( word_2 ) s = [ [ 0 for _ in range ( w ) ] for _ in range ( h ) ] # initialization for top row for j in range ( 1 , w ): s [ 0 ][ j ] = - j * cg # initialization for left-most column for i in range ( 1 , h ): s [ i ][ 0 ] = - i * cg for i in range ( 1 , h ): for j in range ( 1 , w ): s [ i ][ j ] = max ( s [ i - 1 ][ j ] - cg , s [ i ][ j - 1 ] - cg , s [ i - 1 ][ j - 1 ] + find_match ( word_1 [ i ], word_2 [ j ], cg , c , cm )) pprint ( s ) return s [ - 1 ][ - 1 ] needle_man(\"dva\", \"deeve\", c=2) [[0, -1, -2, -3, -4, -5], [-1, 2, 1, 0, -1, -2], [-2, 1, 1, 0, 2, 1], [-3, 0, 0, 0, 1, 1]] 1","title":"Needleman wunch measure"},{"location":"MSBD5001/Lecture6/#affine-gap-measure","text":"Define x = \\(x_1x_2...x_n\\) ; y = \\(y_1y_2..y_m\\) where xi and yj are the i-th and j-th prefixes of x and y Initialization: \\(M(0, 0) = 0, l_x(0,0)=-c_o, l_u(0,0)=-c_o\\) \\(l_x(i,0)=-c_o - c_r * (i - 1)\\) - \\(l_y(0, j) = -c_o - c_r * (j - 1)\\) Other cells in top row and leftmost column = \\(-\\infty\\) \\[M(i, j) = max \\begin{cases} M(i -1, j-1)+c(x_i,y_i)\\\\ l_x(i - 1, j -1) + c(x_i,y_i)\\\\ l_y(u - 1, j-1)+c(x_i, y_i) \\end{cases} \\] \\[ l_x(i, j) = max\\begin{cases} M(i - 1, j) - c_o \\\\ l_x(i - 1, j) - c_r \\end{cases} \\] \\[ l_y(i, j) = max\\begin{cases} M(i , j - 1) - c_o \\\\ l_y(i, j - 1) - c_r \\end{cases} \\] where \\(c_o\\) is the cost of opening a gap, \\(c_r\\) is the cost of continuing a gap, \\((x_i, y_j)\\) is the score for correspoding character \\(x_i\\) with \\(y_j\\) in the score matrix. Score: max(m, ix, iy) from math import inf def find_match ( word1 , word2 , reward , penalty ): if word1 == word2 : return reward else : return - penalty def affine_gap ( word1 : str , word2 : str , co = 1 , cr = 1 , cg = 1 , c = 1 , cm = 1 ) -> int : \"\"\" co: cost of opening a gap cm: cost of continuing the gap cg: gap_penalty c: match award cm: mismatch penalty \"\"\" # padding one whitespace for empty string representation word_1 = ' ' + word1 word_2 = ' ' + word2 h , w = len ( word_1 ), len ( word_2 ) m = [ [ - inf for _ in range ( w ) ] for _ in range ( h ) ] i_x = [ [ - inf for _ in range ( w ) ] for _ in range ( h ) ] i_y = [ [ - inf for _ in range ( w ) ] for _ in range ( h ) ] m [ 0 ][ 0 ] = 0 i_x [ 0 ][ 0 ] = - co i_y [ 0 ][ 0 ] = - co # initialization for top row for j in range ( 1 , w ): i_y [ 0 ][ j ] = - co - cr * ( j - 1 ) # initialization for left-most column for i in range ( 1 , h ): i_x [ i ][ 0 ] = - co - cr * ( i - 1 ) for i in range ( 1 , h ): for j in range ( 1 , w ): match_reward = find_match ( word_1 [ i ], word_2 [ j ], reward = c , penalty = cm ) m [ i ][ j ] = max ( m [ i - 1 ][ j - 1 ] + match_reward , i_x [ i - 1 ][ j - 1 ] + match_reward , i_y [ i - 1 ][ j - 1 ] + match_reward ) i_x [ i ][ j ] = max ( m [ i - 1 ][ j ] - co , i_x [ i - 1 ][ j ] - cr ) i_y [ i ][ j ] = max ( m [ i ][ j - 1 ] - co , i_y [ i ][ j - 1 ] - cr ) print ( \"m: \" ) pprint ( m ) print ( \"i_x: \" ) pprint ( i_x ) print ( \"i_y: \" ) pprint ( i_y ) return max ( m [ - 1 ][ - 1 ], i_x [ - 1 ][ - 1 ], i_y [ - 1 ][ - 1 ]) affine_gap(\"AAT\", \"ACACT\", c=1, co=4, cr=1) m: [[0, -inf, -inf, -inf, -inf, -inf], [-inf, 1, -5, -4, -7, -8], [-inf, -3, 0, -2, -5, -6], [-inf, -6, -4, -1, -3, -4]] i_x: [[-4, -inf, -inf, -inf, -inf, -inf], [-4, -inf, -inf, -inf, -inf, -inf], [-5, -3, -9, -8, -11, -12], [-6, -4, -4, -6, -9, -10]] i_y: [[-4, -4, -5, -6, -7, -8], [-inf, -inf, -3, -4, -5, -6], [-inf, -inf, -7, -4, -5, -6], [-inf, -inf, -10, -8, -5, -6]] -4","title":"Affine gap measure"},{"location":"MSBD5001/Lecture6/#smith-waterman-measure","text":"Initialization: - initialize matrix of size (n + 1)x(m + 1) where s(a; b) is the element at the a-th row and b-th column. - fill matrix: s(i ; 0) = 0, s(0; j) = 0 \\[s(i, j) = max= \\begin {cases} 0 \\\\ s_i - 1, j)-c_g\\\\ s(i, j-1)-c_g\\\\ s(i-1,j-1)+c(x_i,y_i) \\end{cases}\\] def smith_waterman ( word1 : str , word2 : str , cm = 1 , c = 1 , cg = 1 ) -> int : \"\"\" cg: gap_penalty c: match award cm: mismatch penalty \"\"\" # padding one whitespace for empty string representation word_1 = ' ' + word1 word_2 = ' ' + word2 h , w = len ( word_1 ), len ( word_2 ) s = [ [ 0 for _ in range (w ) ] for _ in range ( h ) ] # initialization for top row for j in range ( 1 , w ) : s [ 0 ][ j ] = 0 # initialization for left - most column for i in range ( 1 , h ) : s [ i ][ 0 ] = 0 for i in range ( 1 , h ) : for j in range ( 1 , w ) : match_reward = find_match ( word_1 [ i ] , word_2 [ j ] , reward = c , penalty = cg ) s [ i ][ j ] = max ( 0 , s [ i - 1 ][ j ] - cg , s [ i ][ j - 1 ] - cg , s [ i - 1 ][ j - 1 ] + match_reward ) pprint ( s ) return s [ -1 ][ -1 ] smith_waterman(\" avd\", \"dave\") [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 2, 1], [0, 1, 0, 1, 1]] 1","title":"Smith-Waterman Measure"},{"location":"MSBD5001/Lecture6/#set-base","text":"View strings as sets or multi-sets of tokens Common methods to generate tokens words delimited by space e.g. for the string \\david smith\", the tokens are \\david\" and \\smith\" stem the words if necessary remove stop words (e.g. the, and of) I q-grams, substrings of length q e.g. for the string \\david smith\", the set of 3-grams are ##d, #da, dav, avi, ..., h## special character # to handle the start and end of string fkccecia,","title":"Set base"},{"location":"MSBD5001/Lecture6/#overlap-measure","text":"Let Bx = set of tokens generated for string x Let By = set of tokens generated for string y returns the number of common tokens \\( \\(O(x, y) = |B_x \\cap B_y |\\) \\) E.g., x = dave, y = dav, considering 2-grams $$ B_x = {#d, da, av, ve, e# }$$ \\[ B_y = \\{\\#d,da, av, v\\# \\}$$ $$ O(x, y) = 3 \\] def token ( word : str , max_token_len = 2 ): sets = [] new_word = f \"#{word}#\" length = len ( new_word ) for i in range ( length - 1 ): w = new_word [ i : i + 2 ] sets . append ( w ) return set ( sets ) def overlap ( word1 , word2 , max_token_len = 2 ): set1 = token ( word1 , max_token_len ) set2 = token ( word2 , max_token_len ) return set1 . intersection ( set2 ) overlap(\"dave\", \"dav\") {'#d', 'av', 'da'}","title":"Overlap Measure"},{"location":"MSBD5001/Lecture6/#jaccard-measure","text":"Let Bx = set of tokens generated for string x Let By = set of tokens generated for string y returns the number of common tokens \\( \\(J(x, y) = |B_x \\cap B_y | / |B_x \\cup B_y |\\) \\) E.g., x = dave, y = dav, considering 2-grams Bx = {#d, da, av, ve, e#}, By = {#d, da, av, v#} J(x; y) = 3 / 6 def jaccard ( word1 , word2 , max_token_len = 2 ): set1 = token ( word1 , max_token_len ) set2 = token ( word2 , max_token_len ) print ( f \"Union: {set1.union(set2)}\" ) print ( f \"Intersection: {set1.intersection(set2)}\" ) return len ( set1 . intersection ( set2 )) / len ( set1 . union ( set2 )) jaccard(\"dave\", \"dav\") Union: {'#d', 'e#', 've', 'da', 'v#', 'av'} Intersection: {'#d', 'da', 'av'} 0.5","title":"Jaccard Measure"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/","text":"Speed Prediction \u00b6 In this project, we will use weather data to predict average speed. we will use weather data from openweathermap.org which provides temperature, wind, humidity, and weather conditions This notebook will be running on Google Colab Some of the data preprocessing techniques are based on the online notebooks from tensorflow org. https://www.tensorflow.org/tutorials/structured_data/time_series Install and import dependencies \u00b6 ! pip install pactools Collecting pactools \u001b[?25l Downloading https://files.pythonhosted.org/packages/17/14/4c4eba6e54408e536be27b9891cea68ea391d7d190936593aa71e5c6405e/pactools-0.3.1-py3-none-any.whl (82kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 3.8MB/s \u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pactools) (3.2.2) Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from pactools) (2.10.0) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pactools) (0.22.2.post1) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pactools) (1.18.5) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pactools) (1.4.1) Collecting mne \u001b[?25l Downloading https://files.pythonhosted.org/packages/4d/0e/6448521738d3357c205795fd5846d023bd7935bb83ba93a1ba0f7124205e/mne-0.21.2-py3-none-any.whl (6.8MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.8MB 8.5MB/s \u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pactools) (0.10.0) Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pactools) (2.8.1) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pactools) (2.4.7) Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pactools) (1.3.1) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->pactools) (1.15.0) Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pactools) (0.17.0) Installing collected packages: mne, pactools Successfully installed mne-0.21.2 pactools-0.3.1 import pandas as pd from sklearn.preprocessing import MinMaxScaler import requests from bs4 import BeautifulSoup from datetime import datetime import tensorflow as tf import seaborn as sns import matplotlib.pyplot as plt from sklearn.pipeline import Pipeline from sklearn.preprocessing import LabelBinarizer , OneHotEncoder , StandardScaler from sklearn.compose import ColumnTransformer from sklearn.model_selection import train_test_split from pactools.grid_search import GridSearchCVProgressBar from sklearn.model_selection import TimeSeriesSplit from sklearn.model_selection import GridSearchCV from sklearn import svm from xgboost import XGBRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error /usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject return f(*args, **kwds) /usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject return f(*args, **kwds) import tensorflow as tf print ( \"Num GPUs Available: \" , len ( tf . config . experimental . list_physical_devices ( 'GPU' ))) Num GPUs Available: 1 Preprocess \u00b6 Asign Holiday to the table \u00b6 def time_desc ( x : datetime ): Early_Morning = [ 4 , 5 , 6 , 7 ] Morning = [ 8 , 9 , 10 , 11 ] Afternoon = [ 12 , 13 , 14 , 15 ] Evening = [ 16 , 17 , 18 , 19 ] Night = [ 20 , 21 , 22 , 23 ] Late_Night = [ 0 , 1 , 2 , 3 ] if x . hour in Early_Morning : return 'early' elif x . hour in Morning : return 'morning' elif x . hour in Afternoon : return 'noon' elif x . hour in Evening : return 'afternoon' elif x . hour in Night : return 'night' else : return 'latenight' def add_holiday_and_weekend ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Add holiday and weekend to the dataset \"\"\" new_df = df . copy () new_df [ 'IsWeekend' ] = new_df [ 'date' ]. apply ( lambda x : 0 if x . weekday () in [ 0 , 1 , 2 , 3 , 4 ] else 1 ) new_df [ 'IsHoliday' ] = new_df [ 'date' ]. apply ( lambda x : 1 if ( x . date (). strftime ( '%Y-%m-%d' ) in [ '2017-01-02' , '2017-01-28' , '2017-01-30' , '2017-01-31' , '2017-04-04' , '2017-04-14' , '2017-04-15' , '2017-04-17' , '2017-05-01' , '2017-05-03' , '2017-05-30' , '2017-07-01' , '2017-10-02' , '2017-10-05' , '2017-10-28' , '2017-12-25' , '2017-12-26' , '2018-01-01' , '2018-02-16' , '2018-02-17' , '2018-02-19' , '2018-03-30' , '2018-03-31' , '2018-04-02' , '2018-04-05' , '2018-05-01' , '2018-05-22' , '2018-06-18' , '2018-07-02' , '2018-09-25' , '2018-10-01' , '2018-10-17' , '2018-12-25' , '2018-12-26' ]) or ( x . weekday () in [ 6 ]) else 0 ) return new_df I am using google drive to store all the data including the weather data. So please change this line to the your file paths. # change following two lines to your paths train = pd . read_csv ( '/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/train.csv' ) test = pd . read_csv ( '/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/test.csv' ) train = train . drop ( 'id' , axis = 1 ) train [ 'date' ] = pd . to_datetime ( train [ 'date' ]) train = add_holiday_and_weekend ( train ) train [ 'time desc' ] = train [ 'date' ]. apply ( time_desc ) train .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date speed IsWeekend IsHoliday time desc 0 2017-01-01 00:00:00 43.002930 1 1 latenight 1 2017-01-01 01:00:00 46.118696 1 1 latenight 2 2017-01-01 02:00:00 44.294158 1 1 latenight 3 2017-01-01 03:00:00 41.067468 1 1 latenight 4 2017-01-01 04:00:00 46.448653 1 1 early ... ... ... ... ... ... 14001 2018-12-31 12:00:00 19.865269 0 0 noon 14002 2018-12-31 15:00:00 17.820375 0 0 noon 14003 2018-12-31 16:00:00 12.501851 0 0 afternoon 14004 2018-12-31 18:00:00 15.979319 0 0 afternoon 14005 2018-12-31 20:00:00 40.594183 0 0 night 14006 rows \u00d7 5 columns train.plot(x='date', y='speed', figsize=(20, 10)) <matplotlib.axes._subplots.AxesSubplot at 0x7f26b0215400> Merge weather \u00b6 Pre-process weather data \u00b6 from datetime import datetime def k_to_c ( x ): return x - 273.15 we have two different weather sources. Weather from open weather (Provides hourly basis weather report) Weather from Hong Kong observatory (Provides daily basis weather report) Based on the experiments, I decide to use Hong Kong observatory's weather report which will bring a higher accuracy # Change this path to yours # weather = pd . read_csv ( \"/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/hongkong_weather_1970-2020.csv\" ) # weather [ 'dt_iso' ] = weather [ 'dt_iso' ] . apply ( lambda x : x . replace ( 'UTC' , '' )) # weather [ 'date' ] = pd . to_datetime ( weather [ 'dt_iso' ] ). dt . tz_convert ( \"Asia/Hong_Kong\" ). dt . tz_localize ( None ) # # Transform unit # weather [ 'temp' ] = weather [ 'temp' ] . apply ( k_to_c ) # weather [ 'feels_like' ] = weather [ 'feels_like' ] . apply ( k_to_c ) # weather [ 'temp_min' ] = weather [ 'temp_min' ] . apply ( k_to_c ) # weather [ 'temp_max' ] = weather [ 'temp_max' ] . apply ( k_to_c ) # weather = weather . drop ( [ \"dt_iso\", \"dt\", \"weather_icon\", \"rain_1h\", \"rain_3h\", \"snow_1h\", \"snow_3h\", \"sea_level\", \"grnd_level\", \"timezone\", \"lat\", \"lon\" ] , axis = 1 ) # mask = ( weather [ 'date' ] >= datetime ( 2017 , 1 , 1 )) & ( weather [ 'date' ] <= datetime ( 2019 , 1 , 1 )) # weather = weather . loc [ mask ] # weather weather = pd . read_csv ( \"/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/wr.csv\" ) weather [ 'date' ] = pd . to_datetime ( weather . year * 10000 + weather . month * 100 + weather . day , format = '%Y%m%d' ) weather . drop ( [ 'Unnamed: 0', 'year', 'month', 'day' ] , axis = 1 , inplace = True ) weather [ 'TRainfall(mm)' ]= weather [ 'TRainfall(mm)' ] . apply ( lambda x : 0 if x in [ '-','Trace' ] else x ) weather . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MPressure(hPa) MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm) #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) date 0 1021.7 20.8 18.4 72 0 0 60 34.2 2017-01-01 1 1020.2 23.3 18.4 28 0 0 70 17.7 2017-01-02 2 1019.8 21.3 18.9 56 0 5 70 26.1 2017-01-03 3 1018.7 21.7 18.7 51 0 0 70 27.7 2017-01-04 4 1016.9 23.4 18.9 61 0 0 40 14.3 2017-01-05 Merge \u00b6 from pandas import DatetimeIndex def merge_weather ( df : pd . DataFrame , weather : pd . DataFrame , how = 'left' ) -> pd . DataFrame : ''' Merge weather with data. ''' new_df = df . copy () new_weather = weather . copy () # new_df['tmp_date'] = new_df['date'] # new_weather['tmp_date'] = new_weather['date'] new_df [ 'tmp_date' ] = new_df [ 'date' ] . apply ( lambda date : date . date ()) new_weather [ 'tmp_date' ] = new_weather [ 'date' ] . apply ( lambda date : date . date ()) new_training_data = new_df . merge ( new_weather , on = 'tmp_date' , how = how ) new_training_data = new_training_data . drop ([ 'tmp_date' , 'date_y' ], axis = 1 ) new_training_data = new_training_data . rename ( columns = { 'date_x' : 'date' }) new_training_data [ 'hour' ] = DatetimeIndex ( new_training_data [ 'date' ]) . hour new_training_data [ 'day' ] = DatetimeIndex ( new_training_data [ 'date' ]) . day new_training_data [ 'month' ] = DatetimeIndex ( new_training_data [ 'date' ]) . month new_training_data [ 'year' ] = DatetimeIndex ( new_training_data [ 'date' ]) . year new_training_data [ 'weekday' ] = DatetimeIndex ( new_training_data [ 'date' ]) . weekday return new_training_data new_training_data = merge_weather ( train , weather ) new_training_data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date speed IsWeekend IsHoliday time desc MPressure(hPa) MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm) #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) hour day month year weekday 0 2017-01-01 00:00:00 43.002930 1 1 latenight 1021.7 20.8 18.4 72 0 0 60 34.2 0 1 1 2017 6 1 2017-01-01 01:00:00 46.118696 1 1 latenight 1021.7 20.8 18.4 72 0 0 60 34.2 1 1 1 2017 6 2 2017-01-01 02:00:00 44.294158 1 1 latenight 1021.7 20.8 18.4 72 0 0 60 34.2 2 1 1 2017 6 3 2017-01-01 03:00:00 41.067468 1 1 latenight 1021.7 20.8 18.4 72 0 0 60 34.2 3 1 1 2017 6 4 2017-01-01 04:00:00 46.448653 1 1 early 1021.7 20.8 18.4 72 0 0 60 34.2 4 1 1 2017 6 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 14001 2018-12-31 12:00:00 19.865269 0 0 noon 1027.0 15.6 11.8 77 0 0 360 26.8 12 31 12 2018 0 14002 2018-12-31 15:00:00 17.820375 0 0 noon 1027.0 15.6 11.8 77 0 0 360 26.8 15 31 12 2018 0 14003 2018-12-31 16:00:00 12.501851 0 0 afternoon 1027.0 15.6 11.8 77 0 0 360 26.8 16 31 12 2018 0 14004 2018-12-31 18:00:00 15.979319 0 0 afternoon 1027.0 15.6 11.8 77 0 0 360 26.8 18 31 12 2018 0 14005 2018-12-31 20:00:00 40.594183 0 0 night 1027.0 15.6 11.8 77 0 0 360 26.8 20 31 12 2018 0 14006 rows \u00d7 18 columns Plot data \u00b6 plt.figure(figsize=(6,4)) sns.boxplot('speed',data=new_training_data,orient='h',palette=\"Set3\",linewidth=2.5) plt.show() Traffic speed \u00b6 data_plot = new_training_data data_plot['month'] = data_plot['date'].dt.month data_plot tmp_data=new_training_data.groupby('month').aggregate({'speed':'mean'}) plt.figure(figsize=(8,6)) sns.lineplot(x=tmp_data.index,y=tmp_data.speed,data=tmp_data,palette=\"Set2\") plt.show() plt.figure(figsize=(8,6)) sns.countplot(y='time desc',data=new_training_data,palette=[\"#7fcdbb\",\"#edf8b1\",\"#fc9272\",\"#fee0d2\",\"#bcbddc\",\"#efedf5\"]) plt.show() new_training_data.hist(bins=50,figsize=(20,15)) plt.show() Train \u00b6 new_training_data.columns Index(['date', 'speed', 'IsWeekend', 'IsHoliday', 'time desc', 'MPressure(hPa)', 'MaxTemp(\u2103)', 'MinTemp(\u2103)', 'MCloud(%)', 'TRainfall(mm)', '#hRedVisi(h)', 'WindDirect(degrees)', 'MWindSpeed(km/h)', 'hour', 'day', 'month', 'year', 'weekday'], dtype='object') # using open weather dataset # cat_vars=['IsWeekend','IsHoliday','time desc'] # num_vars=['temp', 'pressure', 'wind_speed', 'humidity', \"clouds_all\", 'month', 'year', 'day', 'hour'] #using hong kong government dataset cat_vars = [ 'IsWeekend' , 'IsHoliday' , 'time desc' , \"weekday\" , \"day\" , 'month' , 'year' ] num_vars = [ 'MaxTemp(\u2103)' , 'MinTemp(\u2103)' , 'MCloud(%)' , 'TRainfall(mm)' , '#hRedVisi(h)' , 'WindDirect(degrees)' , 'MWindSpeed(km/h)' , 'hour' ] Transform Data \u00b6 numeric_transformer = Pipeline ( steps = [ ( 'scaler' , MinMaxScaler ())]) categorical_transformer = Pipeline ( steps = [ ( 'oneHot' , OneHotEncoder ( sparse = False ))]) preprocessor = ColumnTransformer ( transformers = [ ( 'num' , numeric_transformer , num_vars ), ( 'cat' , categorical_transformer , cat_vars )]) data_transformed = preprocessor . fit_transform ( new_training_data ) print ( data_transformed . shape ) (14006, 70) y = new_training_data['speed'] # y = y.to_numpy() # y = y.reshape(-1, 1) # print(y.shape) # print(y) We want to scale the speed import numpy as np Split data \u00b6 X_train,X_test,y_train,y_test=train_test_split(data_transformed,y,test_size=0.15,random_state=42) print(X_train) print(y_train) print(f\"Train x shape: {X_train.shape}\") print(f\"Train y shape: {y_train.shape}\") [[0.36923077 0.42608696 0.70833333 ... 0. 0. 1. ] [0.83076923 0.90434783 0.78125 ... 0. 1. 0. ] [0.84230769 0.95652174 0.5625 ... 0. 1. 0. ] ... [0.84230769 0.95652174 0.5625 ... 0. 1. 0. ] [0.76923077 0.94782609 0.875 ... 0. 1. 0. ] [0.55384615 0.53043478 0.48958333 ... 0. 1. 0. ]] 9443 46.813428 4822 15.018334 5391 48.414942 11099 19.663880 7854 10.389012 ... 5191 24.637279 13418 35.306286 5390 46.950711 860 46.623319 7270 16.718626 Name: speed, Length: 11905, dtype: float64 Train x shape: (11905, 70) Train y shape: (11905,) Training by using XGBRegression \u00b6 tscv = TimeSeriesSplit ( n_splits = 3 ) model = XGBRegressor () param_grid = {' nthread ' : [ 4 , 6 , 8 ], ' objective ' : [' reg : squarederror '], ' learning_rate ' : [ .03 , 0.05 , .07 ], ' max_depth ' : [ 5 , 6 , 7 ], ' min_child_weight ' : [ 4 ], ' subsample ' : [ 0.7 ], ' colsample_bytree ' : [ 0.7 ], ' n_estimators ' : [ 500 ]} xgb = GridSearchCV ( estimator = model , param_grid = param_grid , cv = tscv , n_jobs = 4 , verbose = 10 ) xgb . fit ( X_train , y_train ) Fitting 3 folds for each of 27 candidates, totalling 81 fits [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers. --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-54-e4f74d5b703b> in <module>() 12 13 xgb=GridSearchCV(estimator=model,param_grid=param_grid,cv=tscv,n_jobs=4, verbose=10) ---> 14 xgb.fit(X_train,y_train) /usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py in fit(self, X, y, groups, **fit_params) 708 return results 709 --> 710 self._run_search(evaluate_candidates) 711 712 # For multi-metric evaluation, store the best_index_, best_params_ and /usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py in _run_search(self, evaluate_candidates) 1149 def _run_search(self, evaluate_candidates): 1150 \"\"\"Search all candidates in param_grid\"\"\" -> 1151 evaluate_candidates(ParameterGrid(self.param_grid)) 1152 1153 /usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py in evaluate_candidates(candidate_params) 687 for parameters, (train, test) 688 in product(candidate_params, --> 689 cv.split(X, y, groups))) 690 691 if len(out) < 1: /usr/local/lib/python3.6/dist-packages/joblib/parallel.py in __call__(self, iterable) 1059 1060 with self._backend.retrieval_context(): -> 1061 self.retrieve() 1062 # Make sure that we get a last message telling us we are done 1063 elapsed_time = time.time() - self._start_time /usr/local/lib/python3.6/dist-packages/joblib/parallel.py in retrieve(self) 938 try: 939 if getattr(self._backend, 'supports_timeout', False): --> 940 self._output.extend(job.get(timeout=self.timeout)) 941 else: 942 self._output.extend(job.get()) /usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py in wrap_future_result(future, timeout) 540 AsyncResults.get from multiprocessing.\"\"\" 541 try: --> 542 return future.result(timeout=timeout) 543 except CfTimeoutError as e: 544 raise TimeoutError from e /usr/lib/python3.6/concurrent/futures/_base.py in result(self, timeout) 425 return self.__get_result() 426 --> 427 self._condition.wait(timeout) 428 429 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]: /usr/lib/python3.6/threading.py in wait(self, timeout) 293 try: # restore state no matter what (e.g., KeyboardInterrupt) 294 if timeout is None: --> 295 waiter.acquire() 296 gotit = True 297 else: KeyboardInterrupt: Training by using DNN \u00b6 import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.metrics import MeanSquaredError from tensorflow.keras import regularizers print ( X_train . shape ) (11905, 70) early_stop = keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 10 ) model = keras . Sequential ( [ layers . Dense ( 128 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dense ( 128 , activation = \"relu\" ), layers . Dropout ( 0 . 2 ), layers . Dense ( 64 , activation = \"relu\" ), layers . Dense ( 1 ), ] ) opt = keras . optimizers . Adam ( learning_rate = 0 . 001 ) model . compile ( loss = 'mean_squared_error' , optimizer = opt , metrics = [ 'mae' , 'mse' ]) history = model . fit ( X_train , y_train , epochs = 200 , validation_split = 0.2 , callbacks =[ early_stop ] ,) Epoch 1/200 298/298 [==============================] - 1s 4ms/step - loss: 169.2792 - mae: 9.2261 - mse: 169.2792 - val_loss: 49.0226 - val_mae: 5.3678 - val_mse: 49.0226 Epoch 2/200 298/298 [==============================] - 1s 3ms/step - loss: 53.3526 - mae: 5.5417 - mse: 53.3526 - val_loss: 45.0271 - val_mae: 5.0866 - val_mse: 45.0271 Epoch 3/200 298/298 [==============================] - 1s 3ms/step - loss: 49.2738 - mae: 5.3108 - mse: 49.2738 - val_loss: 43.7640 - val_mae: 5.1142 - val_mse: 43.7640 Epoch 4/200 298/298 [==============================] - 1s 4ms/step - loss: 46.0532 - mae: 5.1287 - mse: 46.0532 - val_loss: 40.5389 - val_mae: 4.7757 - val_mse: 40.5389 Epoch 5/200 298/298 [==============================] - 1s 3ms/step - loss: 42.8026 - mae: 4.9785 - mse: 42.8026 - val_loss: 40.7752 - val_mae: 4.9557 - val_mse: 40.7752 Epoch 6/200 298/298 [==============================] - 1s 4ms/step - loss: 40.8385 - mae: 4.9025 - mse: 40.8385 - val_loss: 38.6987 - val_mae: 4.7153 - val_mse: 38.6987 Epoch 7/200 298/298 [==============================] - 1s 3ms/step - loss: 38.1873 - mae: 4.7325 - mse: 38.1873 - val_loss: 38.2969 - val_mae: 4.6748 - val_mse: 38.2969 Epoch 8/200 298/298 [==============================] - 1s 4ms/step - loss: 37.1973 - mae: 4.6986 - mse: 37.1973 - val_loss: 37.6423 - val_mae: 4.6165 - val_mse: 37.6423 Epoch 9/200 298/298 [==============================] - 1s 4ms/step - loss: 35.9464 - mae: 4.6294 - mse: 35.9464 - val_loss: 39.2058 - val_mae: 4.9141 - val_mse: 39.2058 Epoch 10/200 298/298 [==============================] - 1s 3ms/step - loss: 34.3299 - mae: 4.5449 - mse: 34.3299 - val_loss: 37.6090 - val_mae: 4.7696 - val_mse: 37.6090 Epoch 11/200 298/298 [==============================] - 1s 3ms/step - loss: 32.5047 - mae: 4.4394 - mse: 32.5047 - val_loss: 37.4612 - val_mae: 4.8729 - val_mse: 37.4612 Epoch 12/200 298/298 [==============================] - 1s 4ms/step - loss: 32.2329 - mae: 4.4059 - mse: 32.2329 - val_loss: 41.9161 - val_mae: 5.2683 - val_mse: 41.9161 Epoch 13/200 298/298 [==============================] - 1s 4ms/step - loss: 31.2041 - mae: 4.3138 - mse: 31.2041 - val_loss: 35.0004 - val_mae: 4.6513 - val_mse: 35.0004 Epoch 14/200 298/298 [==============================] - 1s 3ms/step - loss: 30.4892 - mae: 4.2764 - mse: 30.4892 - val_loss: 33.7725 - val_mae: 4.4478 - val_mse: 33.7725 Epoch 15/200 298/298 [==============================] - 1s 4ms/step - loss: 29.2411 - mae: 4.1746 - mse: 29.2411 - val_loss: 32.9828 - val_mae: 4.2607 - val_mse: 32.9828 Epoch 16/200 298/298 [==============================] - 1s 4ms/step - loss: 28.5754 - mae: 4.1273 - mse: 28.5754 - val_loss: 32.2138 - val_mae: 4.2323 - val_mse: 32.2138 Epoch 17/200 298/298 [==============================] - 1s 4ms/step - loss: 27.3392 - mae: 4.0219 - mse: 27.3392 - val_loss: 32.4703 - val_mae: 4.4164 - val_mse: 32.4703 Epoch 18/200 298/298 [==============================] - 1s 3ms/step - loss: 27.5931 - mae: 4.0547 - mse: 27.5931 - val_loss: 31.8828 - val_mae: 4.2749 - val_mse: 31.8828 Epoch 19/200 298/298 [==============================] - 1s 3ms/step - loss: 26.7623 - mae: 3.9968 - mse: 26.7623 - val_loss: 32.7509 - val_mae: 4.3703 - val_mse: 32.7509 Epoch 20/200 298/298 [==============================] - 1s 4ms/step - loss: 26.7770 - mae: 3.9945 - mse: 26.7770 - val_loss: 31.6045 - val_mae: 4.2057 - val_mse: 31.6045 Epoch 21/200 298/298 [==============================] - 1s 3ms/step - loss: 25.9685 - mae: 3.9541 - mse: 25.9685 - val_loss: 31.0973 - val_mae: 4.1812 - val_mse: 31.0973 Epoch 22/200 298/298 [==============================] - 1s 4ms/step - loss: 26.3720 - mae: 3.9701 - mse: 26.3720 - val_loss: 31.9482 - val_mae: 4.2127 - val_mse: 31.9482 Epoch 23/200 298/298 [==============================] - 1s 3ms/step - loss: 25.3420 - mae: 3.9054 - mse: 25.3420 - val_loss: 34.5661 - val_mae: 4.6946 - val_mse: 34.5661 Epoch 24/200 298/298 [==============================] - 1s 4ms/step - loss: 25.2177 - mae: 3.8832 - mse: 25.2177 - val_loss: 31.2380 - val_mae: 4.1930 - val_mse: 31.2380 Epoch 25/200 298/298 [==============================] - 1s 4ms/step - loss: 24.5446 - mae: 3.8306 - mse: 24.5446 - val_loss: 31.1003 - val_mae: 4.2247 - val_mse: 31.1003 Epoch 26/200 298/298 [==============================] - 1s 4ms/step - loss: 24.4506 - mae: 3.8159 - mse: 24.4506 - val_loss: 31.3635 - val_mae: 4.3589 - val_mse: 31.3635 Epoch 27/200 298/298 [==============================] - 1s 4ms/step - loss: 24.0837 - mae: 3.7934 - mse: 24.0837 - val_loss: 30.2161 - val_mae: 4.2576 - val_mse: 30.2161 Epoch 28/200 298/298 [==============================] - 1s 3ms/step - loss: 23.7172 - mae: 3.7904 - mse: 23.7172 - val_loss: 30.6251 - val_mae: 4.1824 - val_mse: 30.6251 Epoch 29/200 298/298 [==============================] - 1s 3ms/step - loss: 23.0786 - mae: 3.7103 - mse: 23.0786 - val_loss: 30.5365 - val_mae: 4.2229 - val_mse: 30.5365 Epoch 30/200 298/298 [==============================] - 1s 3ms/step - loss: 23.7149 - mae: 3.7750 - mse: 23.7149 - val_loss: 30.7331 - val_mae: 4.1697 - val_mse: 30.7331 Epoch 31/200 298/298 [==============================] - 1s 3ms/step - loss: 22.5444 - mae: 3.6918 - mse: 22.5444 - val_loss: 29.5841 - val_mae: 4.1091 - val_mse: 29.5841 Epoch 32/200 298/298 [==============================] - 1s 4ms/step - loss: 22.6646 - mae: 3.6896 - mse: 22.6646 - val_loss: 31.0110 - val_mae: 4.2368 - val_mse: 31.0110 Epoch 33/200 298/298 [==============================] - 1s 3ms/step - loss: 22.2019 - mae: 3.6553 - mse: 22.2019 - val_loss: 30.2204 - val_mae: 4.1981 - val_mse: 30.2204 Epoch 34/200 298/298 [==============================] - 1s 3ms/step - loss: 22.1869 - mae: 3.6523 - mse: 22.1869 - val_loss: 29.4552 - val_mae: 4.0781 - val_mse: 29.4552 Epoch 35/200 298/298 [==============================] - 1s 3ms/step - loss: 21.6420 - mae: 3.6126 - mse: 21.6420 - val_loss: 28.8039 - val_mae: 4.0706 - val_mse: 28.8039 Epoch 36/200 298/298 [==============================] - 1s 4ms/step - loss: 21.1637 - mae: 3.5743 - mse: 21.1637 - val_loss: 29.7665 - val_mae: 4.1830 - val_mse: 29.7665 Epoch 37/200 298/298 [==============================] - 1s 4ms/step - loss: 21.5559 - mae: 3.6128 - mse: 21.5559 - val_loss: 29.8333 - val_mae: 4.0771 - val_mse: 29.8333 Epoch 38/200 298/298 [==============================] - 1s 4ms/step - loss: 20.9312 - mae: 3.5428 - mse: 20.9312 - val_loss: 33.7653 - val_mae: 4.3892 - val_mse: 33.7653 Epoch 39/200 298/298 [==============================] - 1s 4ms/step - loss: 20.9336 - mae: 3.5552 - mse: 20.9336 - val_loss: 29.2746 - val_mae: 4.1145 - val_mse: 29.2746 Epoch 40/200 298/298 [==============================] - 1s 4ms/step - loss: 20.7350 - mae: 3.5443 - mse: 20.7350 - val_loss: 30.5803 - val_mae: 4.2725 - val_mse: 30.5803 Epoch 41/200 298/298 [==============================] - 1s 3ms/step - loss: 20.3824 - mae: 3.5076 - mse: 20.3824 - val_loss: 29.8183 - val_mae: 4.1737 - val_mse: 29.8183 Epoch 42/200 298/298 [==============================] - 1s 4ms/step - loss: 20.4556 - mae: 3.5163 - mse: 20.4556 - val_loss: 29.3429 - val_mae: 4.0838 - val_mse: 29.3429 Epoch 43/200 298/298 [==============================] - 1s 3ms/step - loss: 20.0906 - mae: 3.4789 - mse: 20.0906 - val_loss: 30.3515 - val_mae: 4.1084 - val_mse: 30.3515 Epoch 44/200 298/298 [==============================] - 1s 3ms/step - loss: 19.9412 - mae: 3.4736 - mse: 19.9412 - val_loss: 30.2394 - val_mae: 4.2345 - val_mse: 30.2394 Epoch 45/200 298/298 [==============================] - 1s 3ms/step - loss: 19.6319 - mae: 3.4261 - mse: 19.6319 - val_loss: 30.2833 - val_mae: 4.1500 - val_mse: 30.2833 plot def plot_history ( history ) : hist = pd . DataFrame ( history . history ) hist [ 'epoch' ] = history . epoch plt . figure () plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Mean Abs Error [MPG]' ) plt . plot ( hist [ 'epoch' ] , hist [ 'mae' ] , label = 'Train Error' ) plt . plot ( hist [ 'epoch' ] , hist [ 'val_mae' ] , label = 'Val Error' ) plt . legend () plt . figure () plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Mean Square Error [$MPG^2$]' ) plt . plot ( hist [ 'epoch' ] , hist [ 'mse' ] , label = 'Train Error' ) plt . plot ( hist [ 'epoch' ] , hist [ 'val_mse' ] , label = 'Val Error' ) plt . legend () plt . show () plot_history ( history ) Training by using Decision Tree \u00b6 from sklearn import tree # tree_clf = tree.DecisionTreeRegressor() # tree_clf.fit(X_train, y_train) tscv = TimeSeriesSplit(n_splits=3) param_grid = { 'max_depth': range(1, 10), 'min_samples_split': range(1, 10), 'min_samples_leaf': range(1, 5) } tree_model = tree.DecisionTreeRegressor() tree_reg = GridSearchCV(estimator=tree_model, param_grid=param_grid, cv=tscv, n_jobs=4, verbose=10) tree_reg.fit(X_train, y_train) Fitting 3 folds for each of 324 candidates, totalling 972 fits [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=4)]: Done 5 tasks | elapsed: 1.2s [Parallel(n_jobs=4)]: Done 10 tasks | elapsed: 1.2s [Parallel(n_jobs=4)]: Done 17 tasks | elapsed: 1.3s [Parallel(n_jobs=4)]: Batch computation too fast (0.1742s.) Setting batch_size=2. [Parallel(n_jobs=4)]: Done 24 tasks | elapsed: 1.3s [Parallel(n_jobs=4)]: Batch computation too fast (0.0517s.) Setting batch_size=4. [Parallel(n_jobs=4)]: Done 44 tasks | elapsed: 1.4s [Parallel(n_jobs=4)]: Batch computation too fast (0.0866s.) Setting batch_size=8. [Parallel(n_jobs=4)]: Batch computation too fast (0.1193s.) Setting batch_size=16. [Parallel(n_jobs=4)]: Done 88 tasks | elapsed: 1.6s [Parallel(n_jobs=4)]: Done 216 tasks | elapsed: 2.3s [Parallel(n_jobs=4)]: Done 392 tasks | elapsed: 3.3s [Parallel(n_jobs=4)]: Done 600 tasks | elapsed: 5.4s [Parallel(n_jobs=4)]: Done 808 tasks | elapsed: 7.7s [Parallel(n_jobs=4)]: Done 972 out of 972 | elapsed: 9.7s finished GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=3), error_score=nan, estimator=DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort='deprecated', random_state=None, splitter='best'), iid='deprecated', n_jobs=4, param_grid={'max_depth': range(1, 10), 'min_samples_leaf': range(1, 5), 'min_samples_split': range(1, 10)}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=10) Training by using Ridge Regression \u00b6 from sklearn import linear_model param_grid = { 'alpha' : range ( 1 , 10 )} reg_model = linear_model . Ridge ( alpha =. 5 ) ridge = GridSearchCV ( estimator = reg_model , param_grid = param_grid , cv = tscv , n_jobs = 4 , verbose = 10 ) ridge . fit ( X_train , y_train ) [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers. Fitting 3 folds for each of 9 candidates, totalling 27 fits [Parallel(n_jobs=4)]: Batch computation too fast (0.0206s.) Setting batch_size=2. [Parallel(n_jobs=4)]: Done 5 tasks | elapsed: 0.0s [Parallel(n_jobs=4)]: Batch computation too fast (0.0476s.) Setting batch_size=4. [Parallel(n_jobs=4)]: Done 12 tasks | elapsed: 0.1s [Parallel(n_jobs=4)]: Done 20 out of 27 | elapsed: 0.1s remaining: 0.0s [Parallel(n_jobs=4)]: Done 23 out of 27 | elapsed: 0.1s remaining: 0.0s [Parallel(n_jobs=4)]: Done 27 out of 27 | elapsed: 0.1s finished GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=3), error_score=nan, estimator=Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver='auto', tol=0.001), iid='deprecated', n_jobs=4, param_grid={'alpha': range(1, 10)}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=10) Training by using Random forest \u00b6 model_forest = RandomForestRegressor () param_grid2 = { 'n_estimators' :[ 10 , 50 , 100 , 1000 ], 'max_features' : range ( 1 , 4 ) } random_forest = GridSearchCV ( estimator = model_forest , param_grid = param_grid2 , cv = tscv , n_jobs = 10 , verbose = 10 ) random_forest . fit ( X_train , y_train ) Fitting 3 folds for each of 12 candidates, totalling 36 fits [Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers. [Parallel(n_jobs=10)]: Done 5 tasks | elapsed: 1.5s [Parallel(n_jobs=10)]: Done 12 tasks | elapsed: 4.1s [Parallel(n_jobs=10)]: Done 21 out of 36 | elapsed: 7.9s remaining: 5.7s [Parallel(n_jobs=10)]: Done 25 out of 36 | elapsed: 10.6s remaining: 4.7s [Parallel(n_jobs=10)]: Done 29 out of 36 | elapsed: 23.0s remaining: 5.6s [Parallel(n_jobs=10)]: Done 33 out of 36 | elapsed: 35.6s remaining: 3.2s [Parallel(n_jobs=10)]: Done 36 out of 36 | elapsed: 42.2s finished GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=3), error_score=nan, estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), iid='deprecated', n_jobs=10, param_grid={'max_features': range(1, 4), 'n_estimators': [10, 50, 100, 1000]}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=10) Training by using SVM \u00b6 model_svm = svm . SVR () param_grid3 = { 'kernel' : [ 'rbf' ], 'gamma' : [ 1 e - 3 , 1 e - 4 ], 'C' : [ 1 , 10 , 100 , 1000 ] } svr = GridSearchCV ( estimator = model_svm , n_jobs = 10 , verbose = 10 , param_grid = param_grid3 ) svr . fit ( X_train , y_train , ) Fitting 5 folds for each of 8 candidates, totalling 40 fits [Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers. [Parallel(n_jobs=10)]: Done 5 tasks | elapsed: 48.8s [Parallel(n_jobs=10)]: Done 12 tasks | elapsed: 1.6min [Parallel(n_jobs=10)]: Done 21 tasks | elapsed: 2.4min [Parallel(n_jobs=10)]: Done 26 out of 40 | elapsed: 2.4min remaining: 1.3min [Parallel(n_jobs=10)]: Done 31 out of 40 | elapsed: 3.2min remaining: 56.1s [Parallel(n_jobs=10)]: Done 36 out of 40 | elapsed: 3.2min remaining: 21.6s [Parallel(n_jobs=10)]: Done 40 out of 40 | elapsed: 3.3min finished GridSearchCV(cv=None, error_score=nan, estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False), iid='deprecated', n_jobs=10, param_grid={'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=10) Training by using Neural network \u00b6 from sklearn.neural_network import MLPRegressor mlp_reg = MLPRegressor ( max_iter = 1000 ) mlp_reg . fit ( X_train , y_train ) MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=(100,), learning_rate='constant', learning_rate_init=0.001, max_fun=15000, max_iter=1000, momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5, random_state=None, shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False) Evaluation \u00b6 y_pred = model.predict(X_test) print(y_pred) print(y_test) [[35.947376] [19.731487] [46.31413 ] ... [40.15987 ] [14.448287] [32.13128 ]] 6405 17.479723 10497 19.555973 3669 48.410243 747 24.168746 902 44.950639 ... 11278 19.544291 2312 9.208905 6475 47.599270 6440 7.897116 10103 31.978004 Name: speed, Length: 2101, dtype: float64 # y_pred2 = xgb.predict(X_test) # y_pred3 = random_forest.predict(X_test) # y_pred4 = svr.predict(X_test) # y_pred5 = tree_reg.predict(X_test) # y_pred6 = ridge.predict(X_test) # y_pred7 = mlp_reg.predict(X_test) MSE_DNN = mean_squared_error ( y_pred , y_test ) # MSE_rg = mean_squared_error ( y_pred2 , y_test ) # MSE_rf = mean_squared_error ( y_pred3 , y_test ) # MSE_tree = mean_squared_error ( y_pred5 , y_test ) # MSE_svr = mean_squared_error ( y_pred4 , y_test ) # MSE_ridge = mean_squared_error ( y_pred6 , y_test ) # MSE_mlp = mean_squared_error ( y_pred7 , y_test ) print ( f \"MSE for dnn is: {MSE_DNN}\" ) # print ( 'MSE for XGBoost is ' + str ( MSE_rg )) # print ( 'MSE for RandomForest is ' + str ( MSE_rf )) # print ( f \"MSE for decision is: {MSE_tree}\" ) # print ( f \"MSE for svr is: {MSE_svr}\" ) # print ( f \"MSE for ridge is: {MSE_ridge}\" ) # print ( f \"MSE for neural network is: {MSE_mlp}\" ) MSE for dnn is: 28.724601352433957 Based on the observation, we found that XGV will provide the most accurate results. Make a prediction \u00b6 test = pd.read_csv('/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/test.csv') print(test.to_numpy().shape) (3504, 2) test['date'] = pd.to_datetime(test['date']) test = add_holiday_and_weekend(test) test['time desc']=test['date'].apply(time_desc) print(test.shape) (3504, 5) new_test = merge_weather(test, weather, how='inner') new_test = new_test.drop_duplicates(['date'], keep='last') new_test .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id date IsWeekend IsHoliday time desc MPressure(hPa) MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm) #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) hour day month year weekday 0 0 2018-01-01 02:00:00 0 1 latenight 1020.5 19.0 16.3 75 0 21 70 28.4 2 1 1 2018 0 1 1 2018-01-01 05:00:00 0 1 early 1020.5 19.0 16.3 75 0 21 70 28.4 5 1 1 2018 0 2 2 2018-01-01 07:00:00 0 1 early 1020.5 19.0 16.3 75 0 21 70 28.4 7 1 1 2018 0 3 3 2018-01-01 08:00:00 0 1 morning 1020.5 19.0 16.3 75 0 21 70 28.4 8 1 1 2018 0 4 4 2018-01-01 10:00:00 0 1 morning 1020.5 19.0 16.3 75 0 21 70 28.4 10 1 1 2018 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 3499 3499 2018-12-31 17:00:00 0 0 afternoon 1027.0 15.6 11.8 77 0 0 360 26.8 17 31 12 2018 0 3500 3500 2018-12-31 19:00:00 0 0 afternoon 1027.0 15.6 11.8 77 0 0 360 26.8 19 31 12 2018 0 3501 3501 2018-12-31 21:00:00 0 0 night 1027.0 15.6 11.8 77 0 0 360 26.8 21 31 12 2018 0 3502 3502 2018-12-31 22:00:00 0 0 night 1027.0 15.6 11.8 77 0 0 360 26.8 22 31 12 2018 0 3503 3503 2018-12-31 23:00:00 0 0 night 1027.0 15.6 11.8 77 0 0 360 26.8 23 31 12 2018 0 3504 rows \u00d7 18 columns transformed_data = preprocessor.fit_transform(new_test) print(transformed_data.shape) (1, 322368) pred = model.predict(transformed_data.reshape(-1, 1)) print(pred.shape) --------------------------------------------------------------------------- InvalidArgumentError Traceback (most recent call last) <ipython-input-47-872a2584594e> in <module>() ----> 1 pred = model.predict(transformed_data.reshape(-1, 1)) 2 print(pred.shape) /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs) 128 raise ValueError('{} is not supported in multi-worker mode.'.format( 129 method.__name__)) --> 130 return method(self, *args, **kwargs) 131 132 return tf_decorator.make_decorator( /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing) 1597 for step in data_handler.steps(): 1598 callbacks.on_predict_batch_begin(step) -> 1599 tmp_batch_outputs = predict_function(iterator) 1600 if data_handler.should_sync: 1601 context.async_wait() /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds) 778 else: 779 compiler = \"nonXla\" --> 780 result = self._call(*args, **kwds) 781 782 new_tracing_count = self._get_tracing_count() /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds) 812 # In this case we have not created variables on the first call. So we can 813 # run the first trace but we should fail if variables are created. --> 814 results = self._stateful_fn(*args, **kwds) 815 if self._created_variables: 816 raise ValueError(\"Creating variables on a non-first call to a function\" /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs) 2827 with self._lock: 2828 graph_function, args, kwargs = self._maybe_define_function(args, kwargs) -> 2829 return graph_function._filtered_call(args, kwargs) # pylint: disable=protected-access 2830 2831 @property /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager) 1846 resource_variable_ops.BaseResourceVariable))], 1847 captured_inputs=self.captured_inputs, -> 1848 cancellation_manager=cancellation_manager) 1849 1850 def _call_flat(self, args, captured_inputs, cancellation_manager=None): /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager) 1922 # No tape is watching; skip to running the function. 1923 return self._build_call_outputs(self._inference_function.call( -> 1924 ctx, args, cancellation_manager=cancellation_manager)) 1925 forward_backward = self._select_forward_and_backward_functions( 1926 args, /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager) 548 inputs=args, 549 attrs=attrs, --> 550 ctx=ctx) 551 else: 552 outputs = execute.execute_with_cancellation( /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name) 58 ctx.ensure_initialized() 59 tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, ---> 60 inputs, attrs, num_outputs) 61 except core._NotOkStatusException as e: 62 if name is not None: InvalidArgumentError: Matrix size-incompatible: In[0]: [32,1], In[1]: [93,128] [[node sequential_4/dense_21/MatMul (defined at <ipython-input-47-872a2584594e>:1) ]] [Op:__inference_predict_function_247180] Function call stack: predict_function submission = test.copy() submission['speed'] = pred submission = submission[['id', 'speed']] submission .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id speed 0 0 48.946854 1 1 47.451572 2 2 37.875401 3 3 30.919249 4 4 36.527435 ... ... ... 3499 3499 11.122622 3500 3500 22.704823 3501 3501 44.340752 3502 3502 36.847321 3503 3503 41.491169 3504 rows \u00d7 2 columns # Write submission to file submission . to_csv ( 'submission_20720230.csv' , index = False )","title":"Copy of individual project"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#speed-prediction","text":"In this project, we will use weather data to predict average speed. we will use weather data from openweathermap.org which provides temperature, wind, humidity, and weather conditions This notebook will be running on Google Colab Some of the data preprocessing techniques are based on the online notebooks from tensorflow org. https://www.tensorflow.org/tutorials/structured_data/time_series","title":"Speed Prediction"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#install-and-import-dependencies","text":"! pip install pactools Collecting pactools \u001b[?25l Downloading https://files.pythonhosted.org/packages/17/14/4c4eba6e54408e536be27b9891cea68ea391d7d190936593aa71e5c6405e/pactools-0.3.1-py3-none-any.whl (82kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 3.8MB/s \u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pactools) (3.2.2) Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from pactools) (2.10.0) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pactools) (0.22.2.post1) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pactools) (1.18.5) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pactools) (1.4.1) Collecting mne \u001b[?25l Downloading https://files.pythonhosted.org/packages/4d/0e/6448521738d3357c205795fd5846d023bd7935bb83ba93a1ba0f7124205e/mne-0.21.2-py3-none-any.whl (6.8MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.8MB 8.5MB/s \u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pactools) (0.10.0) Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pactools) (2.8.1) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pactools) (2.4.7) Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pactools) (1.3.1) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->pactools) (1.15.0) Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pactools) (0.17.0) Installing collected packages: mne, pactools Successfully installed mne-0.21.2 pactools-0.3.1 import pandas as pd from sklearn.preprocessing import MinMaxScaler import requests from bs4 import BeautifulSoup from datetime import datetime import tensorflow as tf import seaborn as sns import matplotlib.pyplot as plt from sklearn.pipeline import Pipeline from sklearn.preprocessing import LabelBinarizer , OneHotEncoder , StandardScaler from sklearn.compose import ColumnTransformer from sklearn.model_selection import train_test_split from pactools.grid_search import GridSearchCVProgressBar from sklearn.model_selection import TimeSeriesSplit from sklearn.model_selection import GridSearchCV from sklearn import svm from xgboost import XGBRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error /usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject return f(*args, **kwds) /usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject return f(*args, **kwds) import tensorflow as tf print ( \"Num GPUs Available: \" , len ( tf . config . experimental . list_physical_devices ( 'GPU' ))) Num GPUs Available: 1","title":"Install and import dependencies"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#preprocess","text":"","title":"Preprocess"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#asign-holiday-to-the-table","text":"def time_desc ( x : datetime ): Early_Morning = [ 4 , 5 , 6 , 7 ] Morning = [ 8 , 9 , 10 , 11 ] Afternoon = [ 12 , 13 , 14 , 15 ] Evening = [ 16 , 17 , 18 , 19 ] Night = [ 20 , 21 , 22 , 23 ] Late_Night = [ 0 , 1 , 2 , 3 ] if x . hour in Early_Morning : return 'early' elif x . hour in Morning : return 'morning' elif x . hour in Afternoon : return 'noon' elif x . hour in Evening : return 'afternoon' elif x . hour in Night : return 'night' else : return 'latenight' def add_holiday_and_weekend ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Add holiday and weekend to the dataset \"\"\" new_df = df . copy () new_df [ 'IsWeekend' ] = new_df [ 'date' ]. apply ( lambda x : 0 if x . weekday () in [ 0 , 1 , 2 , 3 , 4 ] else 1 ) new_df [ 'IsHoliday' ] = new_df [ 'date' ]. apply ( lambda x : 1 if ( x . date (). strftime ( '%Y-%m-%d' ) in [ '2017-01-02' , '2017-01-28' , '2017-01-30' , '2017-01-31' , '2017-04-04' , '2017-04-14' , '2017-04-15' , '2017-04-17' , '2017-05-01' , '2017-05-03' , '2017-05-30' , '2017-07-01' , '2017-10-02' , '2017-10-05' , '2017-10-28' , '2017-12-25' , '2017-12-26' , '2018-01-01' , '2018-02-16' , '2018-02-17' , '2018-02-19' , '2018-03-30' , '2018-03-31' , '2018-04-02' , '2018-04-05' , '2018-05-01' , '2018-05-22' , '2018-06-18' , '2018-07-02' , '2018-09-25' , '2018-10-01' , '2018-10-17' , '2018-12-25' , '2018-12-26' ]) or ( x . weekday () in [ 6 ]) else 0 ) return new_df I am using google drive to store all the data including the weather data. So please change this line to the your file paths. # change following two lines to your paths train = pd . read_csv ( '/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/train.csv' ) test = pd . read_csv ( '/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/test.csv' ) train = train . drop ( 'id' , axis = 1 ) train [ 'date' ] = pd . to_datetime ( train [ 'date' ]) train = add_holiday_and_weekend ( train ) train [ 'time desc' ] = train [ 'date' ]. apply ( time_desc ) train .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date speed IsWeekend IsHoliday time desc 0 2017-01-01 00:00:00 43.002930 1 1 latenight 1 2017-01-01 01:00:00 46.118696 1 1 latenight 2 2017-01-01 02:00:00 44.294158 1 1 latenight 3 2017-01-01 03:00:00 41.067468 1 1 latenight 4 2017-01-01 04:00:00 46.448653 1 1 early ... ... ... ... ... ... 14001 2018-12-31 12:00:00 19.865269 0 0 noon 14002 2018-12-31 15:00:00 17.820375 0 0 noon 14003 2018-12-31 16:00:00 12.501851 0 0 afternoon 14004 2018-12-31 18:00:00 15.979319 0 0 afternoon 14005 2018-12-31 20:00:00 40.594183 0 0 night 14006 rows \u00d7 5 columns train.plot(x='date', y='speed', figsize=(20, 10)) <matplotlib.axes._subplots.AxesSubplot at 0x7f26b0215400>","title":"Asign Holiday to the table"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#merge-weather","text":"","title":"Merge weather"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#pre-process-weather-data","text":"from datetime import datetime def k_to_c ( x ): return x - 273.15 we have two different weather sources. Weather from open weather (Provides hourly basis weather report) Weather from Hong Kong observatory (Provides daily basis weather report) Based on the experiments, I decide to use Hong Kong observatory's weather report which will bring a higher accuracy # Change this path to yours # weather = pd . read_csv ( \"/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/hongkong_weather_1970-2020.csv\" ) # weather [ 'dt_iso' ] = weather [ 'dt_iso' ] . apply ( lambda x : x . replace ( 'UTC' , '' )) # weather [ 'date' ] = pd . to_datetime ( weather [ 'dt_iso' ] ). dt . tz_convert ( \"Asia/Hong_Kong\" ). dt . tz_localize ( None ) # # Transform unit # weather [ 'temp' ] = weather [ 'temp' ] . apply ( k_to_c ) # weather [ 'feels_like' ] = weather [ 'feels_like' ] . apply ( k_to_c ) # weather [ 'temp_min' ] = weather [ 'temp_min' ] . apply ( k_to_c ) # weather [ 'temp_max' ] = weather [ 'temp_max' ] . apply ( k_to_c ) # weather = weather . drop ( [ \"dt_iso\", \"dt\", \"weather_icon\", \"rain_1h\", \"rain_3h\", \"snow_1h\", \"snow_3h\", \"sea_level\", \"grnd_level\", \"timezone\", \"lat\", \"lon\" ] , axis = 1 ) # mask = ( weather [ 'date' ] >= datetime ( 2017 , 1 , 1 )) & ( weather [ 'date' ] <= datetime ( 2019 , 1 , 1 )) # weather = weather . loc [ mask ] # weather weather = pd . read_csv ( \"/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/wr.csv\" ) weather [ 'date' ] = pd . to_datetime ( weather . year * 10000 + weather . month * 100 + weather . day , format = '%Y%m%d' ) weather . drop ( [ 'Unnamed: 0', 'year', 'month', 'day' ] , axis = 1 , inplace = True ) weather [ 'TRainfall(mm)' ]= weather [ 'TRainfall(mm)' ] . apply ( lambda x : 0 if x in [ '-','Trace' ] else x ) weather . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MPressure(hPa) MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm) #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) date 0 1021.7 20.8 18.4 72 0 0 60 34.2 2017-01-01 1 1020.2 23.3 18.4 28 0 0 70 17.7 2017-01-02 2 1019.8 21.3 18.9 56 0 5 70 26.1 2017-01-03 3 1018.7 21.7 18.7 51 0 0 70 27.7 2017-01-04 4 1016.9 23.4 18.9 61 0 0 40 14.3 2017-01-05","title":"Pre-process weather data"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#merge","text":"from pandas import DatetimeIndex def merge_weather ( df : pd . DataFrame , weather : pd . DataFrame , how = 'left' ) -> pd . DataFrame : ''' Merge weather with data. ''' new_df = df . copy () new_weather = weather . copy () # new_df['tmp_date'] = new_df['date'] # new_weather['tmp_date'] = new_weather['date'] new_df [ 'tmp_date' ] = new_df [ 'date' ] . apply ( lambda date : date . date ()) new_weather [ 'tmp_date' ] = new_weather [ 'date' ] . apply ( lambda date : date . date ()) new_training_data = new_df . merge ( new_weather , on = 'tmp_date' , how = how ) new_training_data = new_training_data . drop ([ 'tmp_date' , 'date_y' ], axis = 1 ) new_training_data = new_training_data . rename ( columns = { 'date_x' : 'date' }) new_training_data [ 'hour' ] = DatetimeIndex ( new_training_data [ 'date' ]) . hour new_training_data [ 'day' ] = DatetimeIndex ( new_training_data [ 'date' ]) . day new_training_data [ 'month' ] = DatetimeIndex ( new_training_data [ 'date' ]) . month new_training_data [ 'year' ] = DatetimeIndex ( new_training_data [ 'date' ]) . year new_training_data [ 'weekday' ] = DatetimeIndex ( new_training_data [ 'date' ]) . weekday return new_training_data new_training_data = merge_weather ( train , weather ) new_training_data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date speed IsWeekend IsHoliday time desc MPressure(hPa) MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm) #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) hour day month year weekday 0 2017-01-01 00:00:00 43.002930 1 1 latenight 1021.7 20.8 18.4 72 0 0 60 34.2 0 1 1 2017 6 1 2017-01-01 01:00:00 46.118696 1 1 latenight 1021.7 20.8 18.4 72 0 0 60 34.2 1 1 1 2017 6 2 2017-01-01 02:00:00 44.294158 1 1 latenight 1021.7 20.8 18.4 72 0 0 60 34.2 2 1 1 2017 6 3 2017-01-01 03:00:00 41.067468 1 1 latenight 1021.7 20.8 18.4 72 0 0 60 34.2 3 1 1 2017 6 4 2017-01-01 04:00:00 46.448653 1 1 early 1021.7 20.8 18.4 72 0 0 60 34.2 4 1 1 2017 6 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 14001 2018-12-31 12:00:00 19.865269 0 0 noon 1027.0 15.6 11.8 77 0 0 360 26.8 12 31 12 2018 0 14002 2018-12-31 15:00:00 17.820375 0 0 noon 1027.0 15.6 11.8 77 0 0 360 26.8 15 31 12 2018 0 14003 2018-12-31 16:00:00 12.501851 0 0 afternoon 1027.0 15.6 11.8 77 0 0 360 26.8 16 31 12 2018 0 14004 2018-12-31 18:00:00 15.979319 0 0 afternoon 1027.0 15.6 11.8 77 0 0 360 26.8 18 31 12 2018 0 14005 2018-12-31 20:00:00 40.594183 0 0 night 1027.0 15.6 11.8 77 0 0 360 26.8 20 31 12 2018 0 14006 rows \u00d7 18 columns","title":"Merge"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#plot-data","text":"plt.figure(figsize=(6,4)) sns.boxplot('speed',data=new_training_data,orient='h',palette=\"Set3\",linewidth=2.5) plt.show()","title":"Plot data"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#traffic-speed","text":"data_plot = new_training_data data_plot['month'] = data_plot['date'].dt.month data_plot tmp_data=new_training_data.groupby('month').aggregate({'speed':'mean'}) plt.figure(figsize=(8,6)) sns.lineplot(x=tmp_data.index,y=tmp_data.speed,data=tmp_data,palette=\"Set2\") plt.show() plt.figure(figsize=(8,6)) sns.countplot(y='time desc',data=new_training_data,palette=[\"#7fcdbb\",\"#edf8b1\",\"#fc9272\",\"#fee0d2\",\"#bcbddc\",\"#efedf5\"]) plt.show() new_training_data.hist(bins=50,figsize=(20,15)) plt.show()","title":"Traffic speed"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#train","text":"new_training_data.columns Index(['date', 'speed', 'IsWeekend', 'IsHoliday', 'time desc', 'MPressure(hPa)', 'MaxTemp(\u2103)', 'MinTemp(\u2103)', 'MCloud(%)', 'TRainfall(mm)', '#hRedVisi(h)', 'WindDirect(degrees)', 'MWindSpeed(km/h)', 'hour', 'day', 'month', 'year', 'weekday'], dtype='object') # using open weather dataset # cat_vars=['IsWeekend','IsHoliday','time desc'] # num_vars=['temp', 'pressure', 'wind_speed', 'humidity', \"clouds_all\", 'month', 'year', 'day', 'hour'] #using hong kong government dataset cat_vars = [ 'IsWeekend' , 'IsHoliday' , 'time desc' , \"weekday\" , \"day\" , 'month' , 'year' ] num_vars = [ 'MaxTemp(\u2103)' , 'MinTemp(\u2103)' , 'MCloud(%)' , 'TRainfall(mm)' , '#hRedVisi(h)' , 'WindDirect(degrees)' , 'MWindSpeed(km/h)' , 'hour' ]","title":"Train"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#transform-data","text":"numeric_transformer = Pipeline ( steps = [ ( 'scaler' , MinMaxScaler ())]) categorical_transformer = Pipeline ( steps = [ ( 'oneHot' , OneHotEncoder ( sparse = False ))]) preprocessor = ColumnTransformer ( transformers = [ ( 'num' , numeric_transformer , num_vars ), ( 'cat' , categorical_transformer , cat_vars )]) data_transformed = preprocessor . fit_transform ( new_training_data ) print ( data_transformed . shape ) (14006, 70) y = new_training_data['speed'] # y = y.to_numpy() # y = y.reshape(-1, 1) # print(y.shape) # print(y) We want to scale the speed import numpy as np","title":"Transform Data"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#split-data","text":"X_train,X_test,y_train,y_test=train_test_split(data_transformed,y,test_size=0.15,random_state=42) print(X_train) print(y_train) print(f\"Train x shape: {X_train.shape}\") print(f\"Train y shape: {y_train.shape}\") [[0.36923077 0.42608696 0.70833333 ... 0. 0. 1. ] [0.83076923 0.90434783 0.78125 ... 0. 1. 0. ] [0.84230769 0.95652174 0.5625 ... 0. 1. 0. ] ... [0.84230769 0.95652174 0.5625 ... 0. 1. 0. ] [0.76923077 0.94782609 0.875 ... 0. 1. 0. ] [0.55384615 0.53043478 0.48958333 ... 0. 1. 0. ]] 9443 46.813428 4822 15.018334 5391 48.414942 11099 19.663880 7854 10.389012 ... 5191 24.637279 13418 35.306286 5390 46.950711 860 46.623319 7270 16.718626 Name: speed, Length: 11905, dtype: float64 Train x shape: (11905, 70) Train y shape: (11905,)","title":"Split data"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#training-by-using-xgbregression","text":"tscv = TimeSeriesSplit ( n_splits = 3 ) model = XGBRegressor () param_grid = {' nthread ' : [ 4 , 6 , 8 ], ' objective ' : [' reg : squarederror '], ' learning_rate ' : [ .03 , 0.05 , .07 ], ' max_depth ' : [ 5 , 6 , 7 ], ' min_child_weight ' : [ 4 ], ' subsample ' : [ 0.7 ], ' colsample_bytree ' : [ 0.7 ], ' n_estimators ' : [ 500 ]} xgb = GridSearchCV ( estimator = model , param_grid = param_grid , cv = tscv , n_jobs = 4 , verbose = 10 ) xgb . fit ( X_train , y_train ) Fitting 3 folds for each of 27 candidates, totalling 81 fits [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers. --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-54-e4f74d5b703b> in <module>() 12 13 xgb=GridSearchCV(estimator=model,param_grid=param_grid,cv=tscv,n_jobs=4, verbose=10) ---> 14 xgb.fit(X_train,y_train) /usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py in fit(self, X, y, groups, **fit_params) 708 return results 709 --> 710 self._run_search(evaluate_candidates) 711 712 # For multi-metric evaluation, store the best_index_, best_params_ and /usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py in _run_search(self, evaluate_candidates) 1149 def _run_search(self, evaluate_candidates): 1150 \"\"\"Search all candidates in param_grid\"\"\" -> 1151 evaluate_candidates(ParameterGrid(self.param_grid)) 1152 1153 /usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py in evaluate_candidates(candidate_params) 687 for parameters, (train, test) 688 in product(candidate_params, --> 689 cv.split(X, y, groups))) 690 691 if len(out) < 1: /usr/local/lib/python3.6/dist-packages/joblib/parallel.py in __call__(self, iterable) 1059 1060 with self._backend.retrieval_context(): -> 1061 self.retrieve() 1062 # Make sure that we get a last message telling us we are done 1063 elapsed_time = time.time() - self._start_time /usr/local/lib/python3.6/dist-packages/joblib/parallel.py in retrieve(self) 938 try: 939 if getattr(self._backend, 'supports_timeout', False): --> 940 self._output.extend(job.get(timeout=self.timeout)) 941 else: 942 self._output.extend(job.get()) /usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py in wrap_future_result(future, timeout) 540 AsyncResults.get from multiprocessing.\"\"\" 541 try: --> 542 return future.result(timeout=timeout) 543 except CfTimeoutError as e: 544 raise TimeoutError from e /usr/lib/python3.6/concurrent/futures/_base.py in result(self, timeout) 425 return self.__get_result() 426 --> 427 self._condition.wait(timeout) 428 429 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]: /usr/lib/python3.6/threading.py in wait(self, timeout) 293 try: # restore state no matter what (e.g., KeyboardInterrupt) 294 if timeout is None: --> 295 waiter.acquire() 296 gotit = True 297 else: KeyboardInterrupt:","title":"Training by using XGBRegression"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#training-by-using-dnn","text":"import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.metrics import MeanSquaredError from tensorflow.keras import regularizers print ( X_train . shape ) (11905, 70) early_stop = keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 10 ) model = keras . Sequential ( [ layers . Dense ( 128 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dense ( 128 , activation = \"relu\" ), layers . Dropout ( 0 . 2 ), layers . Dense ( 64 , activation = \"relu\" ), layers . Dense ( 1 ), ] ) opt = keras . optimizers . Adam ( learning_rate = 0 . 001 ) model . compile ( loss = 'mean_squared_error' , optimizer = opt , metrics = [ 'mae' , 'mse' ]) history = model . fit ( X_train , y_train , epochs = 200 , validation_split = 0.2 , callbacks =[ early_stop ] ,) Epoch 1/200 298/298 [==============================] - 1s 4ms/step - loss: 169.2792 - mae: 9.2261 - mse: 169.2792 - val_loss: 49.0226 - val_mae: 5.3678 - val_mse: 49.0226 Epoch 2/200 298/298 [==============================] - 1s 3ms/step - loss: 53.3526 - mae: 5.5417 - mse: 53.3526 - val_loss: 45.0271 - val_mae: 5.0866 - val_mse: 45.0271 Epoch 3/200 298/298 [==============================] - 1s 3ms/step - loss: 49.2738 - mae: 5.3108 - mse: 49.2738 - val_loss: 43.7640 - val_mae: 5.1142 - val_mse: 43.7640 Epoch 4/200 298/298 [==============================] - 1s 4ms/step - loss: 46.0532 - mae: 5.1287 - mse: 46.0532 - val_loss: 40.5389 - val_mae: 4.7757 - val_mse: 40.5389 Epoch 5/200 298/298 [==============================] - 1s 3ms/step - loss: 42.8026 - mae: 4.9785 - mse: 42.8026 - val_loss: 40.7752 - val_mae: 4.9557 - val_mse: 40.7752 Epoch 6/200 298/298 [==============================] - 1s 4ms/step - loss: 40.8385 - mae: 4.9025 - mse: 40.8385 - val_loss: 38.6987 - val_mae: 4.7153 - val_mse: 38.6987 Epoch 7/200 298/298 [==============================] - 1s 3ms/step - loss: 38.1873 - mae: 4.7325 - mse: 38.1873 - val_loss: 38.2969 - val_mae: 4.6748 - val_mse: 38.2969 Epoch 8/200 298/298 [==============================] - 1s 4ms/step - loss: 37.1973 - mae: 4.6986 - mse: 37.1973 - val_loss: 37.6423 - val_mae: 4.6165 - val_mse: 37.6423 Epoch 9/200 298/298 [==============================] - 1s 4ms/step - loss: 35.9464 - mae: 4.6294 - mse: 35.9464 - val_loss: 39.2058 - val_mae: 4.9141 - val_mse: 39.2058 Epoch 10/200 298/298 [==============================] - 1s 3ms/step - loss: 34.3299 - mae: 4.5449 - mse: 34.3299 - val_loss: 37.6090 - val_mae: 4.7696 - val_mse: 37.6090 Epoch 11/200 298/298 [==============================] - 1s 3ms/step - loss: 32.5047 - mae: 4.4394 - mse: 32.5047 - val_loss: 37.4612 - val_mae: 4.8729 - val_mse: 37.4612 Epoch 12/200 298/298 [==============================] - 1s 4ms/step - loss: 32.2329 - mae: 4.4059 - mse: 32.2329 - val_loss: 41.9161 - val_mae: 5.2683 - val_mse: 41.9161 Epoch 13/200 298/298 [==============================] - 1s 4ms/step - loss: 31.2041 - mae: 4.3138 - mse: 31.2041 - val_loss: 35.0004 - val_mae: 4.6513 - val_mse: 35.0004 Epoch 14/200 298/298 [==============================] - 1s 3ms/step - loss: 30.4892 - mae: 4.2764 - mse: 30.4892 - val_loss: 33.7725 - val_mae: 4.4478 - val_mse: 33.7725 Epoch 15/200 298/298 [==============================] - 1s 4ms/step - loss: 29.2411 - mae: 4.1746 - mse: 29.2411 - val_loss: 32.9828 - val_mae: 4.2607 - val_mse: 32.9828 Epoch 16/200 298/298 [==============================] - 1s 4ms/step - loss: 28.5754 - mae: 4.1273 - mse: 28.5754 - val_loss: 32.2138 - val_mae: 4.2323 - val_mse: 32.2138 Epoch 17/200 298/298 [==============================] - 1s 4ms/step - loss: 27.3392 - mae: 4.0219 - mse: 27.3392 - val_loss: 32.4703 - val_mae: 4.4164 - val_mse: 32.4703 Epoch 18/200 298/298 [==============================] - 1s 3ms/step - loss: 27.5931 - mae: 4.0547 - mse: 27.5931 - val_loss: 31.8828 - val_mae: 4.2749 - val_mse: 31.8828 Epoch 19/200 298/298 [==============================] - 1s 3ms/step - loss: 26.7623 - mae: 3.9968 - mse: 26.7623 - val_loss: 32.7509 - val_mae: 4.3703 - val_mse: 32.7509 Epoch 20/200 298/298 [==============================] - 1s 4ms/step - loss: 26.7770 - mae: 3.9945 - mse: 26.7770 - val_loss: 31.6045 - val_mae: 4.2057 - val_mse: 31.6045 Epoch 21/200 298/298 [==============================] - 1s 3ms/step - loss: 25.9685 - mae: 3.9541 - mse: 25.9685 - val_loss: 31.0973 - val_mae: 4.1812 - val_mse: 31.0973 Epoch 22/200 298/298 [==============================] - 1s 4ms/step - loss: 26.3720 - mae: 3.9701 - mse: 26.3720 - val_loss: 31.9482 - val_mae: 4.2127 - val_mse: 31.9482 Epoch 23/200 298/298 [==============================] - 1s 3ms/step - loss: 25.3420 - mae: 3.9054 - mse: 25.3420 - val_loss: 34.5661 - val_mae: 4.6946 - val_mse: 34.5661 Epoch 24/200 298/298 [==============================] - 1s 4ms/step - loss: 25.2177 - mae: 3.8832 - mse: 25.2177 - val_loss: 31.2380 - val_mae: 4.1930 - val_mse: 31.2380 Epoch 25/200 298/298 [==============================] - 1s 4ms/step - loss: 24.5446 - mae: 3.8306 - mse: 24.5446 - val_loss: 31.1003 - val_mae: 4.2247 - val_mse: 31.1003 Epoch 26/200 298/298 [==============================] - 1s 4ms/step - loss: 24.4506 - mae: 3.8159 - mse: 24.4506 - val_loss: 31.3635 - val_mae: 4.3589 - val_mse: 31.3635 Epoch 27/200 298/298 [==============================] - 1s 4ms/step - loss: 24.0837 - mae: 3.7934 - mse: 24.0837 - val_loss: 30.2161 - val_mae: 4.2576 - val_mse: 30.2161 Epoch 28/200 298/298 [==============================] - 1s 3ms/step - loss: 23.7172 - mae: 3.7904 - mse: 23.7172 - val_loss: 30.6251 - val_mae: 4.1824 - val_mse: 30.6251 Epoch 29/200 298/298 [==============================] - 1s 3ms/step - loss: 23.0786 - mae: 3.7103 - mse: 23.0786 - val_loss: 30.5365 - val_mae: 4.2229 - val_mse: 30.5365 Epoch 30/200 298/298 [==============================] - 1s 3ms/step - loss: 23.7149 - mae: 3.7750 - mse: 23.7149 - val_loss: 30.7331 - val_mae: 4.1697 - val_mse: 30.7331 Epoch 31/200 298/298 [==============================] - 1s 3ms/step - loss: 22.5444 - mae: 3.6918 - mse: 22.5444 - val_loss: 29.5841 - val_mae: 4.1091 - val_mse: 29.5841 Epoch 32/200 298/298 [==============================] - 1s 4ms/step - loss: 22.6646 - mae: 3.6896 - mse: 22.6646 - val_loss: 31.0110 - val_mae: 4.2368 - val_mse: 31.0110 Epoch 33/200 298/298 [==============================] - 1s 3ms/step - loss: 22.2019 - mae: 3.6553 - mse: 22.2019 - val_loss: 30.2204 - val_mae: 4.1981 - val_mse: 30.2204 Epoch 34/200 298/298 [==============================] - 1s 3ms/step - loss: 22.1869 - mae: 3.6523 - mse: 22.1869 - val_loss: 29.4552 - val_mae: 4.0781 - val_mse: 29.4552 Epoch 35/200 298/298 [==============================] - 1s 3ms/step - loss: 21.6420 - mae: 3.6126 - mse: 21.6420 - val_loss: 28.8039 - val_mae: 4.0706 - val_mse: 28.8039 Epoch 36/200 298/298 [==============================] - 1s 4ms/step - loss: 21.1637 - mae: 3.5743 - mse: 21.1637 - val_loss: 29.7665 - val_mae: 4.1830 - val_mse: 29.7665 Epoch 37/200 298/298 [==============================] - 1s 4ms/step - loss: 21.5559 - mae: 3.6128 - mse: 21.5559 - val_loss: 29.8333 - val_mae: 4.0771 - val_mse: 29.8333 Epoch 38/200 298/298 [==============================] - 1s 4ms/step - loss: 20.9312 - mae: 3.5428 - mse: 20.9312 - val_loss: 33.7653 - val_mae: 4.3892 - val_mse: 33.7653 Epoch 39/200 298/298 [==============================] - 1s 4ms/step - loss: 20.9336 - mae: 3.5552 - mse: 20.9336 - val_loss: 29.2746 - val_mae: 4.1145 - val_mse: 29.2746 Epoch 40/200 298/298 [==============================] - 1s 4ms/step - loss: 20.7350 - mae: 3.5443 - mse: 20.7350 - val_loss: 30.5803 - val_mae: 4.2725 - val_mse: 30.5803 Epoch 41/200 298/298 [==============================] - 1s 3ms/step - loss: 20.3824 - mae: 3.5076 - mse: 20.3824 - val_loss: 29.8183 - val_mae: 4.1737 - val_mse: 29.8183 Epoch 42/200 298/298 [==============================] - 1s 4ms/step - loss: 20.4556 - mae: 3.5163 - mse: 20.4556 - val_loss: 29.3429 - val_mae: 4.0838 - val_mse: 29.3429 Epoch 43/200 298/298 [==============================] - 1s 3ms/step - loss: 20.0906 - mae: 3.4789 - mse: 20.0906 - val_loss: 30.3515 - val_mae: 4.1084 - val_mse: 30.3515 Epoch 44/200 298/298 [==============================] - 1s 3ms/step - loss: 19.9412 - mae: 3.4736 - mse: 19.9412 - val_loss: 30.2394 - val_mae: 4.2345 - val_mse: 30.2394 Epoch 45/200 298/298 [==============================] - 1s 3ms/step - loss: 19.6319 - mae: 3.4261 - mse: 19.6319 - val_loss: 30.2833 - val_mae: 4.1500 - val_mse: 30.2833 plot def plot_history ( history ) : hist = pd . DataFrame ( history . history ) hist [ 'epoch' ] = history . epoch plt . figure () plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Mean Abs Error [MPG]' ) plt . plot ( hist [ 'epoch' ] , hist [ 'mae' ] , label = 'Train Error' ) plt . plot ( hist [ 'epoch' ] , hist [ 'val_mae' ] , label = 'Val Error' ) plt . legend () plt . figure () plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Mean Square Error [$MPG^2$]' ) plt . plot ( hist [ 'epoch' ] , hist [ 'mse' ] , label = 'Train Error' ) plt . plot ( hist [ 'epoch' ] , hist [ 'val_mse' ] , label = 'Val Error' ) plt . legend () plt . show () plot_history ( history )","title":"Training by using DNN"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#training-by-using-decision-tree","text":"from sklearn import tree # tree_clf = tree.DecisionTreeRegressor() # tree_clf.fit(X_train, y_train) tscv = TimeSeriesSplit(n_splits=3) param_grid = { 'max_depth': range(1, 10), 'min_samples_split': range(1, 10), 'min_samples_leaf': range(1, 5) } tree_model = tree.DecisionTreeRegressor() tree_reg = GridSearchCV(estimator=tree_model, param_grid=param_grid, cv=tscv, n_jobs=4, verbose=10) tree_reg.fit(X_train, y_train) Fitting 3 folds for each of 324 candidates, totalling 972 fits [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=4)]: Done 5 tasks | elapsed: 1.2s [Parallel(n_jobs=4)]: Done 10 tasks | elapsed: 1.2s [Parallel(n_jobs=4)]: Done 17 tasks | elapsed: 1.3s [Parallel(n_jobs=4)]: Batch computation too fast (0.1742s.) Setting batch_size=2. [Parallel(n_jobs=4)]: Done 24 tasks | elapsed: 1.3s [Parallel(n_jobs=4)]: Batch computation too fast (0.0517s.) Setting batch_size=4. [Parallel(n_jobs=4)]: Done 44 tasks | elapsed: 1.4s [Parallel(n_jobs=4)]: Batch computation too fast (0.0866s.) Setting batch_size=8. [Parallel(n_jobs=4)]: Batch computation too fast (0.1193s.) Setting batch_size=16. [Parallel(n_jobs=4)]: Done 88 tasks | elapsed: 1.6s [Parallel(n_jobs=4)]: Done 216 tasks | elapsed: 2.3s [Parallel(n_jobs=4)]: Done 392 tasks | elapsed: 3.3s [Parallel(n_jobs=4)]: Done 600 tasks | elapsed: 5.4s [Parallel(n_jobs=4)]: Done 808 tasks | elapsed: 7.7s [Parallel(n_jobs=4)]: Done 972 out of 972 | elapsed: 9.7s finished GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=3), error_score=nan, estimator=DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort='deprecated', random_state=None, splitter='best'), iid='deprecated', n_jobs=4, param_grid={'max_depth': range(1, 10), 'min_samples_leaf': range(1, 5), 'min_samples_split': range(1, 10)}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=10)","title":"Training by using Decision Tree"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#training-by-using-ridge-regression","text":"from sklearn import linear_model param_grid = { 'alpha' : range ( 1 , 10 )} reg_model = linear_model . Ridge ( alpha =. 5 ) ridge = GridSearchCV ( estimator = reg_model , param_grid = param_grid , cv = tscv , n_jobs = 4 , verbose = 10 ) ridge . fit ( X_train , y_train ) [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers. Fitting 3 folds for each of 9 candidates, totalling 27 fits [Parallel(n_jobs=4)]: Batch computation too fast (0.0206s.) Setting batch_size=2. [Parallel(n_jobs=4)]: Done 5 tasks | elapsed: 0.0s [Parallel(n_jobs=4)]: Batch computation too fast (0.0476s.) Setting batch_size=4. [Parallel(n_jobs=4)]: Done 12 tasks | elapsed: 0.1s [Parallel(n_jobs=4)]: Done 20 out of 27 | elapsed: 0.1s remaining: 0.0s [Parallel(n_jobs=4)]: Done 23 out of 27 | elapsed: 0.1s remaining: 0.0s [Parallel(n_jobs=4)]: Done 27 out of 27 | elapsed: 0.1s finished GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=3), error_score=nan, estimator=Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver='auto', tol=0.001), iid='deprecated', n_jobs=4, param_grid={'alpha': range(1, 10)}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=10)","title":"Training by using Ridge Regression"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#training-by-using-random-forest","text":"model_forest = RandomForestRegressor () param_grid2 = { 'n_estimators' :[ 10 , 50 , 100 , 1000 ], 'max_features' : range ( 1 , 4 ) } random_forest = GridSearchCV ( estimator = model_forest , param_grid = param_grid2 , cv = tscv , n_jobs = 10 , verbose = 10 ) random_forest . fit ( X_train , y_train ) Fitting 3 folds for each of 12 candidates, totalling 36 fits [Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers. [Parallel(n_jobs=10)]: Done 5 tasks | elapsed: 1.5s [Parallel(n_jobs=10)]: Done 12 tasks | elapsed: 4.1s [Parallel(n_jobs=10)]: Done 21 out of 36 | elapsed: 7.9s remaining: 5.7s [Parallel(n_jobs=10)]: Done 25 out of 36 | elapsed: 10.6s remaining: 4.7s [Parallel(n_jobs=10)]: Done 29 out of 36 | elapsed: 23.0s remaining: 5.6s [Parallel(n_jobs=10)]: Done 33 out of 36 | elapsed: 35.6s remaining: 3.2s [Parallel(n_jobs=10)]: Done 36 out of 36 | elapsed: 42.2s finished GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=3), error_score=nan, estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), iid='deprecated', n_jobs=10, param_grid={'max_features': range(1, 4), 'n_estimators': [10, 50, 100, 1000]}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=10)","title":"Training by using Random forest"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#training-by-using-svm","text":"model_svm = svm . SVR () param_grid3 = { 'kernel' : [ 'rbf' ], 'gamma' : [ 1 e - 3 , 1 e - 4 ], 'C' : [ 1 , 10 , 100 , 1000 ] } svr = GridSearchCV ( estimator = model_svm , n_jobs = 10 , verbose = 10 , param_grid = param_grid3 ) svr . fit ( X_train , y_train , ) Fitting 5 folds for each of 8 candidates, totalling 40 fits [Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers. [Parallel(n_jobs=10)]: Done 5 tasks | elapsed: 48.8s [Parallel(n_jobs=10)]: Done 12 tasks | elapsed: 1.6min [Parallel(n_jobs=10)]: Done 21 tasks | elapsed: 2.4min [Parallel(n_jobs=10)]: Done 26 out of 40 | elapsed: 2.4min remaining: 1.3min [Parallel(n_jobs=10)]: Done 31 out of 40 | elapsed: 3.2min remaining: 56.1s [Parallel(n_jobs=10)]: Done 36 out of 40 | elapsed: 3.2min remaining: 21.6s [Parallel(n_jobs=10)]: Done 40 out of 40 | elapsed: 3.3min finished GridSearchCV(cv=None, error_score=nan, estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False), iid='deprecated', n_jobs=10, param_grid={'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=10)","title":"Training by using SVM"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#training-by-using-neural-network","text":"from sklearn.neural_network import MLPRegressor mlp_reg = MLPRegressor ( max_iter = 1000 ) mlp_reg . fit ( X_train , y_train ) MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=(100,), learning_rate='constant', learning_rate_init=0.001, max_fun=15000, max_iter=1000, momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5, random_state=None, shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False)","title":"Training by using Neural network"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#evaluation","text":"y_pred = model.predict(X_test) print(y_pred) print(y_test) [[35.947376] [19.731487] [46.31413 ] ... [40.15987 ] [14.448287] [32.13128 ]] 6405 17.479723 10497 19.555973 3669 48.410243 747 24.168746 902 44.950639 ... 11278 19.544291 2312 9.208905 6475 47.599270 6440 7.897116 10103 31.978004 Name: speed, Length: 2101, dtype: float64 # y_pred2 = xgb.predict(X_test) # y_pred3 = random_forest.predict(X_test) # y_pred4 = svr.predict(X_test) # y_pred5 = tree_reg.predict(X_test) # y_pred6 = ridge.predict(X_test) # y_pred7 = mlp_reg.predict(X_test) MSE_DNN = mean_squared_error ( y_pred , y_test ) # MSE_rg = mean_squared_error ( y_pred2 , y_test ) # MSE_rf = mean_squared_error ( y_pred3 , y_test ) # MSE_tree = mean_squared_error ( y_pred5 , y_test ) # MSE_svr = mean_squared_error ( y_pred4 , y_test ) # MSE_ridge = mean_squared_error ( y_pred6 , y_test ) # MSE_mlp = mean_squared_error ( y_pred7 , y_test ) print ( f \"MSE for dnn is: {MSE_DNN}\" ) # print ( 'MSE for XGBoost is ' + str ( MSE_rg )) # print ( 'MSE for RandomForest is ' + str ( MSE_rf )) # print ( f \"MSE for decision is: {MSE_tree}\" ) # print ( f \"MSE for svr is: {MSE_svr}\" ) # print ( f \"MSE for ridge is: {MSE_ridge}\" ) # print ( f \"MSE for neural network is: {MSE_mlp}\" ) MSE for dnn is: 28.724601352433957 Based on the observation, we found that XGV will provide the most accurate results.","title":"Evaluation"},{"location":"MSBD5001/project/Copy%20of%20Individual%20Project/#make-a-prediction","text":"test = pd.read_csv('/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/test.csv') print(test.to_numpy().shape) (3504, 2) test['date'] = pd.to_datetime(test['date']) test = add_holiday_and_weekend(test) test['time desc']=test['date'].apply(time_desc) print(test.shape) (3504, 5) new_test = merge_weather(test, weather, how='inner') new_test = new_test.drop_duplicates(['date'], keep='last') new_test .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id date IsWeekend IsHoliday time desc MPressure(hPa) MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm) #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) hour day month year weekday 0 0 2018-01-01 02:00:00 0 1 latenight 1020.5 19.0 16.3 75 0 21 70 28.4 2 1 1 2018 0 1 1 2018-01-01 05:00:00 0 1 early 1020.5 19.0 16.3 75 0 21 70 28.4 5 1 1 2018 0 2 2 2018-01-01 07:00:00 0 1 early 1020.5 19.0 16.3 75 0 21 70 28.4 7 1 1 2018 0 3 3 2018-01-01 08:00:00 0 1 morning 1020.5 19.0 16.3 75 0 21 70 28.4 8 1 1 2018 0 4 4 2018-01-01 10:00:00 0 1 morning 1020.5 19.0 16.3 75 0 21 70 28.4 10 1 1 2018 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 3499 3499 2018-12-31 17:00:00 0 0 afternoon 1027.0 15.6 11.8 77 0 0 360 26.8 17 31 12 2018 0 3500 3500 2018-12-31 19:00:00 0 0 afternoon 1027.0 15.6 11.8 77 0 0 360 26.8 19 31 12 2018 0 3501 3501 2018-12-31 21:00:00 0 0 night 1027.0 15.6 11.8 77 0 0 360 26.8 21 31 12 2018 0 3502 3502 2018-12-31 22:00:00 0 0 night 1027.0 15.6 11.8 77 0 0 360 26.8 22 31 12 2018 0 3503 3503 2018-12-31 23:00:00 0 0 night 1027.0 15.6 11.8 77 0 0 360 26.8 23 31 12 2018 0 3504 rows \u00d7 18 columns transformed_data = preprocessor.fit_transform(new_test) print(transformed_data.shape) (1, 322368) pred = model.predict(transformed_data.reshape(-1, 1)) print(pred.shape) --------------------------------------------------------------------------- InvalidArgumentError Traceback (most recent call last) <ipython-input-47-872a2584594e> in <module>() ----> 1 pred = model.predict(transformed_data.reshape(-1, 1)) 2 print(pred.shape) /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs) 128 raise ValueError('{} is not supported in multi-worker mode.'.format( 129 method.__name__)) --> 130 return method(self, *args, **kwargs) 131 132 return tf_decorator.make_decorator( /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing) 1597 for step in data_handler.steps(): 1598 callbacks.on_predict_batch_begin(step) -> 1599 tmp_batch_outputs = predict_function(iterator) 1600 if data_handler.should_sync: 1601 context.async_wait() /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds) 778 else: 779 compiler = \"nonXla\" --> 780 result = self._call(*args, **kwds) 781 782 new_tracing_count = self._get_tracing_count() /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds) 812 # In this case we have not created variables on the first call. So we can 813 # run the first trace but we should fail if variables are created. --> 814 results = self._stateful_fn(*args, **kwds) 815 if self._created_variables: 816 raise ValueError(\"Creating variables on a non-first call to a function\" /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs) 2827 with self._lock: 2828 graph_function, args, kwargs = self._maybe_define_function(args, kwargs) -> 2829 return graph_function._filtered_call(args, kwargs) # pylint: disable=protected-access 2830 2831 @property /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager) 1846 resource_variable_ops.BaseResourceVariable))], 1847 captured_inputs=self.captured_inputs, -> 1848 cancellation_manager=cancellation_manager) 1849 1850 def _call_flat(self, args, captured_inputs, cancellation_manager=None): /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager) 1922 # No tape is watching; skip to running the function. 1923 return self._build_call_outputs(self._inference_function.call( -> 1924 ctx, args, cancellation_manager=cancellation_manager)) 1925 forward_backward = self._select_forward_and_backward_functions( 1926 args, /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager) 548 inputs=args, 549 attrs=attrs, --> 550 ctx=ctx) 551 else: 552 outputs = execute.execute_with_cancellation( /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name) 58 ctx.ensure_initialized() 59 tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, ---> 60 inputs, attrs, num_outputs) 61 except core._NotOkStatusException as e: 62 if name is not None: InvalidArgumentError: Matrix size-incompatible: In[0]: [32,1], In[1]: [93,128] [[node sequential_4/dense_21/MatMul (defined at <ipython-input-47-872a2584594e>:1) ]] [Op:__inference_predict_function_247180] Function call stack: predict_function submission = test.copy() submission['speed'] = pred submission = submission[['id', 'speed']] submission .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id speed 0 0 48.946854 1 1 47.451572 2 2 37.875401 3 3 30.919249 4 4 36.527435 ... ... ... 3499 3499 11.122622 3500 3500 22.704823 3501 3501 44.340752 3502 3502 36.847321 3503 3503 41.491169 3504 rows \u00d7 2 columns # Write submission to file submission . to_csv ( 'submission_20720230.csv' , index = False )","title":"Make a prediction"},{"location":"MSBD5001/project/Group%20Project/","text":"import pandas as pd from sklearn.preprocessing import MinMaxScaler , RobustScaler import requests from datetime import datetime import tensorflow as tf import seaborn as sns import matplotlib.pyplot as plt from sklearn.pipeline import Pipeline from sklearn.preprocessing import LabelBinarizer , OneHotEncoder , StandardScaler from sklearn.compose import ColumnTransformer from sklearn.model_selection import train_test_split from sklearn.model_selection import TimeSeriesSplit from sklearn.model_selection import GridSearchCV from sklearn import svm from xgboost import XGBRegressor from sklearn.ensemble import RandomForestRegressor import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.metrics import MeanSquaredError from tensorflow.keras import regularizers import numpy as np from sklearn.metrics import mean_squared_error Preprocess Data \u00b6 train = pd . read_csv ( '/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/group project/disney_shanghai.csv' , parse_dates = [ 'Time' ]) test = pd . read_csv ( '/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/group project/disney_shanghai_test.csv' , names = [ 'Time' , 'Facility ID' , 'Name' , 'Wait time' , 'Ride yype' , 'Temperature' , 'Max temperature' , 'Min temperature' , 'Humidity' , 'Pressure' , 'Wind degree' , 'Wind speed' , 'Cloud' , 'Weather' , 'Weather description' ], parse_dates = [ 'Time' ]) print ( train . info ()) print ( test . info ()) <class 'pandas.core.frame.DataFrame'> RangeIndex: 32024 entries, 0 to 32023 Data columns (total 17 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Name 32024 non-null object 1 Ride type 32024 non-null object 2 Time 32024 non-null datetime64[ns, pytz.FixedOffset(480)] 3 Fastpass-avaliable 32024 non-null bool 4 Status 32024 non-null object 5 Wait time 14621 non-null float64 6 Weather 32024 non-null object 7 Weather description 32024 non-null object 8 Temperature 32024 non-null float64 9 Max temperature 32024 non-null float64 10 Min temperature 32024 non-null float64 11 Pressure 32024 non-null int64 12 Humidity 32024 non-null int64 13 Wind degree 32024 non-null float64 14 Wind speed 32024 non-null float64 15 Cloud 32024 non-null int64 16 Visibility 32024 non-null int64 dtypes: bool(1), datetime64[ns, pytz.FixedOffset(480)](1), float64(6), int64(4), object(5) memory usage: 3.9+ MB None <class 'pandas.core.frame.DataFrame'> RangeIndex: 10449 entries, 0 to 10448 Data columns (total 15 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Time 10449 non-null datetime64[ns, UTC] 1 Facility ID 10449 non-null object 2 Name 10449 non-null object 3 Wait time 4016 non-null float64 4 Ride yype 10449 non-null object 5 Temperature 10449 non-null int64 6 Max temperature 10449 non-null float64 7 Min temperature 10449 non-null float64 8 Humidity 10449 non-null int64 9 Pressure 10449 non-null int64 10 Wind degree 10449 non-null int64 11 Wind speed 10449 non-null float64 12 Cloud 10449 non-null int64 13 Weather 10449 non-null object 14 Weather description 10449 non-null object dtypes: datetime64[ns, UTC](1), float64(4), int64(5), object(5) memory usage: 1.2+ MB None show data train = train[['Name', 'Time', 'Wait time', 'Weather', 'Temperature', 'Max temperature', 'Min temperature', 'Pressure', 'Humidity', 'Wind degree', 'Wind speed', 'Cloud']] train.head(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Name Time Wait time Weather Temperature Max temperature Min temperature Pressure Humidity Wind degree Wind speed Cloud 0 Camp Discovery 2020-08-31 13:38:47.638019+08:00 0.0 Clouds 33.28 34.0 34.0 1009 66 110.0 5.0 75 1 Challenge Trails at Camp Discovery 2020-08-31 13:38:47.638035+08:00 10.0 Clouds 33.28 34.0 34.0 1009 66 110.0 5.0 75 show test test = test[['Name', 'Time', 'Wait time', 'Weather', 'Temperature', 'Max temperature', 'Min temperature', 'Pressure', 'Humidity', 'Wind degree', 'Wind speed', 'Cloud']] test.head(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Name Time Wait time Weather Temperature Max temperature Min temperature Pressure Humidity Wind degree Wind speed Cloud 0 Challenge Trails at Camp Discovery 2020-11-14 10:07:34+00:00 30.0 Clouds 18 18.0 18.0 1026 68 70 3.0 20 1 Vista Trail at Camp Discovery 2020-11-14 10:07:34+00:00 0.0 Clouds 18 18.0 18.0 1026 68 70 3.0 20 Change name into values \u00b6 unique_names = train[['Name']].drop_duplicates() unique_names['id'] = range(len(unique_names)) train = train.merge(unique_names, on='Name') unique_names .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Name id 0 Camp Discovery 0 1 Challenge Trails at Camp Discovery 1 2 Vista Trail at Camp Discovery 2 3 Soaring Over the Horizon 3 4 \u201cOnce Upon a Time\u201d Adventure 4 5 Alice in Wonderland Maze 5 6 Frozen: A Sing-Along Celebration 6 7 Hunny Pot Spin 7 8 Peter Pan\u2019s Flight 8 9 Seven Dwarfs Mine Train 9 10 The Many Adventures of Winnie the Pooh 10 11 Voyage to the Crystal Grotto 11 12 Dumbo the Flying Elephant 12 13 Fantasia Carousel 13 14 TRON Lightcycle Power Run \u2013 Presented by Chevr... 14 15 Stitch Encounter 15 16 Jet Packs 16 17 Buzz Lightyear Planet Rescue 17 18 Siren's Revenge 18 19 Shipwreck Shore 19 20 Pirates of the Caribbean Battle for the Sunken... 20 21 Explorer Canoes 21 22 Eye of the Storm: Captain Jack\u2019s Stunt Spectac... 22 23 Roaring Rapids 23 24 Ignite the Dream - A Nighttime Spectacular of ... 24 25 Mickey\u2019s Storybook Express 25 26 Golden Fairytale Fanfare 26 27 TRON Realm, Chevrolet Digital Challenge 27 28 Rex\u2019s Racer 28 29 Slinky Dog Spin 29 30 Woody\u2019s Roundup 30 31 Marvel Universe 31 32 Club Destin-E 32 33 Color Wall 33 34 Avengers Training Initiative 34 35 Wave Hello to Your Favorite Mickey Avenue Char... 35 36 Mickey Avenue Kiss Goodnight 36 37 Adventurous Friends Exploration 37 38 Catch a Glimpse of Jack Sparrow 38 39 Hundred Acre Wood Character Sighting 39 40 Princess Balcony Greetings 40 41 Avengers Assemble at the E-Stage 41 42 Woody\u2019s Rescue Patrol 42 Dropna \u00b6 train = train.dropna() test = test.dropna() Add busy label \u00b6 def is_busy(time: int): if time < 30: return [1, 0, 0] elif time >= 30 and time <= 70: return [0, 1, 0] else: return [0, 0, 1] Add weekend and public holiday \u00b6 def hour_modify ( x : datetime ) : Early_Morning = [ 4,5,6,7 ] Morning = [ 8,9,10,11 ] Afternoon = [ 12,13,14,15 ] Evening = [ 16,17,18,19 ] Night = [ 20,21,22,23 ] Late_Night = [ 0,1,2,3 ] if x . hour in Early_Morning : return 'Early_Morning' elif x . hour in Morning : return 'Morning' elif x . hour in Afternoon : return 'Afternoon' elif x . hour in Evening : return 'Evening' elif x . hour in Night : return 'Night' else : return 'Late_Night' def add_holiday_and_weekend ( df : pd . DataFrame , date_field_str = 'date' ) -> pd . DataFrame : \"\"\" Add holiday and weekend to the dataset \"\"\" new_df = df . copy () new_df [ 'IsWeekend' ] = new_df [ date_field_str ] . apply ( lambda x : 0 if x . weekday () in [ 0,1,2,3,4 ] else 1 ) new_df [ 'IsHoliday' ]= new_df [ date_field_str ] . apply ( lambda x : 1 if ( x . date (). strftime ( '%Y-%m-%d' ) in [ '2020-01-01', '2020-01-24', '2020-01-24', '2020-01-25', '2020-01-26', '2020-01-27', '2020-01-28', '2020-01-29', '2020-01-30', '2020-01-31', '2020-02-01', '2020-02-02', '2020-04-04', '2020-05-01', '2020-05-02', '2020-05-03', '2020-05-04', '2020-05-05', '2020-06-25', '2020-06-26', '2020-06-27', '2020-10-01', '2020-10-02', '2020-10-03', '2020-10-04', '2020-10-05', '2020-10-06', '2020-10-07', '2020-10-08', '2020-10-31' ] ) or ( x . weekday () in [ 6 ] ) else 0 ) return new_df convert time zone train [ 'Time' ] = train [ 'Time' ]. dt . tz_localize ( None ) print ( train [ 'Time' ]) 0 2020-08-31 13:38:47.638019 1 2020-08-31 14:09:47.630487 2 2020-08-31 14:26:10.487907 3 2020-08-31 14:29:48.273505 4 2020-08-31 15:09:11.331162 ... 23808 2020-10-24 15:23:47.123270 23809 2020-10-24 16:25:44.947592 23810 2020-10-24 17:23:16.431926 23816 2020-10-25 10:26:28.625480 23817 2020-10-25 11:24:10.125045 Name: Time, Length: 14621, dtype: datetime64[ns] test['Time'] = test['Time'].dt.tz_convert(\"Asia/Shanghai\").dt.tz_localize(None) print(test['Time']) 0 2020-11-14 18:07:34 1 2020-11-14 18:07:34 2 2020-11-14 18:07:34 3 2020-11-14 18:07:34 4 2020-11-14 18:07:34 ... 10340 2020-12-01 19:07:47 10348 2020-12-01 19:07:47 10349 2020-12-01 19:07:47 10350 2020-12-01 19:07:47 10351 2020-12-01 19:07:47 Name: Time, Length: 4016, dtype: datetime64[ns] Add public holidays train = add_holiday_and_weekend ( train , 'Time' ) test = add_holiday_and_weekend ( test , 'Time' ) train [ 'Hour modify' ] = train [ 'Time' ]. apply ( hour_modify ) test [ 'Hour modify' ] = test [ 'Time' ]. apply ( hour_modify ) train [ 'Is busy' ] = train [ 'Wait time' ]. apply ( is_busy ) train . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Name Time Wait time Weather Temperature Max temperature Min temperature Pressure Humidity Wind degree Wind speed Cloud id IsWeekend IsHoliday Hour modify Is busy 0 Camp Discovery 2020-08-31 13:38:47.638019 0.0 Clouds 33.28 34.00 34.00 1009 66 110.0 5.0 75 0 0 0 Afternoon [1, 0, 0] 1 Camp Discovery 2020-08-31 14:09:47.630487 0.0 Clouds 33.80 35.56 35.56 1008 70 110.0 5.0 75 0 0 0 Afternoon [1, 0, 0] 2 Camp Discovery 2020-08-31 14:26:10.487907 0.0 Clouds 34.11 35.56 35.56 1008 66 110.0 6.0 75 0 0 0 Afternoon [1, 0, 0] Plot Data \u00b6 plt.figure(figsize=(6,4)) sns.boxplot('Wait time',data=train,orient='h',palette=\"Set3\",linewidth=2.5) plt.show() /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning train[['Wait time', 'Temperature', 'Humidity', 'Wind degree', 'Wind speed', 'Cloud']].describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Wait time Temperature Humidity Wind degree Wind speed Cloud count 14621.000000 14621.000000 14621.000000 14621.000000 14621.000000 14621.000000 mean 23.616374 24.423189 64.933862 127.520347 5.352968 45.041174 std 26.388862 4.075267 14.270916 119.544200 1.962247 31.870902 min 0.000000 14.800000 33.000000 0.000000 0.450000 0.000000 25% 5.000000 21.570000 56.000000 30.000000 4.000000 20.000000 50% 15.000000 23.930000 61.000000 70.000000 5.000000 40.000000 75% 30.000000 26.900000 74.000000 180.000000 7.000000 75.000000 max 195.000000 35.050000 100.000000 360.000000 11.000000 100.000000 Train \u00b6 train.columns Index(['Name', 'Time', 'Wait time', 'Weather', 'Temperature', 'Max temperature', 'Min temperature', 'Pressure', 'Humidity', 'Wind degree', 'Wind speed', 'Cloud', 'id', 'IsWeekend', 'IsHoliday', 'Hour modify', 'Is busy'], dtype='object') def plot_history ( history ) : hist = pd . DataFrame ( history . history ) hist [ 'epoch' ] = history . epoch plt . figure () plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Mean Abs Error [MPG]' ) plt . plot ( hist [ 'epoch' ] , hist [ 'mae' ] , label = 'Train Error' ) plt . plot ( hist [ 'epoch' ] , hist [ 'val_mae' ] , label = 'Val Error' ) plt . legend () plt . figure () plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Mean Square Error [$MPG^2$]' ) plt . plot ( hist [ 'epoch' ] , hist [ 'mse' ] , label = 'Train Error' ) plt . plot ( hist [ 'epoch' ] , hist [ 'val_mse' ] , label = 'Val Error' ) plt . legend () plt . show () plt . savefig ( 'MSE.png' ) def generate_training_data ( data : pd . DataFrame , prediction_label , cat_vars =[ 'id', 'IsWeekend','IsHoliday','Hour modify', 'Weather' ] , num_vars =[ 'Temperature', 'Pressure', 'Humidity', 'Cloud', 'Wind degree' ] , should_reshape = True , should_split = True ) : x = train . copy () y = x [ prediction_label ] . to_list () y = np . array ( y ) numeric_transformer = Pipeline ( steps =[ ('scaler', RobustScaler()) ] ) categorical_transformer = Pipeline ( steps =[ ('oneHot',OneHotEncoder(sparse=False)) ] ) preprocessor = ColumnTransformer ( transformers =[ ('num',numeric_transformer,num_vars), ('cat',categorical_transformer,cat_vars) ] ) data_transformed = preprocessor . fit_transform ( x ) if should_split : if should_reshape : y = y . reshape ( - 1 , 1 ) scaler = MinMaxScaler () scaled_y = scaler . fit_transform ( y ) return train_test_split ( data_transformed , scaled_y , test_size = 0.02 , random_state = 42 ), scaler else : return train_test_split ( data_transformed , y , test_size = 0.02 , random_state = 42 ) else : return data_transformed , y data, scaler=generate_training_data(train, 'Wait time') X_train,X_test,y_train,y_test = data print(y_train.shape) print(X_train.shape) (14328, 1) (14328, 44) Training by using DNN \u00b6 early_stop = keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 20 ) model = keras . Sequential ( [ layers . Dense ( 64 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dense ( 512 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dropout ( 0 . 5 ), layers . Dense ( 256 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dropout ( 0 . 5 ), layers . Dense ( 128 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dense ( y_train . shape [ 1 ] ), ] ) opt = keras . optimizers . Adam ( learning_rate = 0 . 001 ) model . compile ( loss = 'mean_squared_error' , optimizer = opt , metrics = [ 'mae' , 'mse' ]) history = model . fit ( X_train , y_train , epochs = 1000 , callbacks =[ early_stop ] , validation_split = 0.2 ) plot_history ( history ) Epoch 1/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0778 - mse: 0.0127 - val_loss: 0.0115 - val_mae: 0.0670 - val_mse: 0.0115 Epoch 2/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0069 - mae: 0.0563 - mse: 0.0069 - val_loss: 0.0065 - val_mae: 0.0515 - val_mse: 0.0065 Epoch 3/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0058 - mae: 0.0510 - mse: 0.0058 - val_loss: 0.0060 - val_mae: 0.0485 - val_mse: 0.0060 Epoch 4/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0052 - mae: 0.0480 - mse: 0.0052 - val_loss: 0.0061 - val_mae: 0.0472 - val_mse: 0.0061 Epoch 5/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0052 - mae: 0.0470 - mse: 0.0052 - val_loss: 0.0055 - val_mae: 0.0464 - val_mse: 0.0055 Epoch 6/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0046 - mae: 0.0449 - mse: 0.0046 - val_loss: 0.0051 - val_mae: 0.0482 - val_mse: 0.0051 Epoch 7/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0046 - mae: 0.0439 - mse: 0.0046 - val_loss: 0.0045 - val_mae: 0.0434 - val_mse: 0.0045 Epoch 8/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0044 - mae: 0.0434 - mse: 0.0044 - val_loss: 0.0053 - val_mae: 0.0485 - val_mse: 0.0053 Epoch 9/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0042 - mae: 0.0423 - mse: 0.0042 - val_loss: 0.0054 - val_mae: 0.0442 - val_mse: 0.0054 Epoch 10/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0039 - mae: 0.0410 - mse: 0.0039 - val_loss: 0.0049 - val_mae: 0.0448 - val_mse: 0.0049 Epoch 11/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0040 - mae: 0.0410 - mse: 0.0040 - val_loss: 0.0041 - val_mae: 0.0422 - val_mse: 0.0041 Epoch 12/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0039 - mae: 0.0409 - mse: 0.0039 - val_loss: 0.0043 - val_mae: 0.0416 - val_mse: 0.0043 Epoch 13/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0037 - mae: 0.0399 - mse: 0.0037 - val_loss: 0.0041 - val_mae: 0.0415 - val_mse: 0.0041 Epoch 14/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0038 - mae: 0.0403 - mse: 0.0038 - val_loss: 0.0040 - val_mae: 0.0432 - val_mse: 0.0040 Epoch 15/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0034 - mae: 0.0390 - mse: 0.0034 - val_loss: 0.0039 - val_mae: 0.0398 - val_mse: 0.0039 Epoch 16/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0034 - mae: 0.0383 - mse: 0.0034 - val_loss: 0.0038 - val_mae: 0.0405 - val_mse: 0.0038 Epoch 17/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0034 - mae: 0.0382 - mse: 0.0034 - val_loss: 0.0038 - val_mae: 0.0391 - val_mse: 0.0038 Epoch 18/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0034 - mae: 0.0381 - mse: 0.0034 - val_loss: 0.0038 - val_mae: 0.0386 - val_mse: 0.0038 Epoch 19/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0032 - mae: 0.0372 - mse: 0.0032 - val_loss: 0.0039 - val_mae: 0.0427 - val_mse: 0.0039 Epoch 20/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0031 - mae: 0.0372 - mse: 0.0031 - val_loss: 0.0039 - val_mae: 0.0397 - val_mse: 0.0039 Epoch 21/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0032 - mae: 0.0368 - mse: 0.0032 - val_loss: 0.0039 - val_mae: 0.0407 - val_mse: 0.0039 Epoch 22/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0029 - mae: 0.0358 - mse: 0.0029 - val_loss: 0.0038 - val_mae: 0.0393 - val_mse: 0.0038 Epoch 23/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0029 - mae: 0.0359 - mse: 0.0029 - val_loss: 0.0035 - val_mae: 0.0384 - val_mse: 0.0035 Epoch 24/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0029 - mae: 0.0352 - mse: 0.0029 - val_loss: 0.0037 - val_mae: 0.0403 - val_mse: 0.0037 Epoch 25/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0028 - mae: 0.0349 - mse: 0.0028 - val_loss: 0.0035 - val_mae: 0.0379 - val_mse: 0.0035 Epoch 26/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0029 - mae: 0.0351 - mse: 0.0029 - val_loss: 0.0039 - val_mae: 0.0395 - val_mse: 0.0039 Epoch 27/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0028 - mae: 0.0349 - mse: 0.0028 - val_loss: 0.0035 - val_mae: 0.0362 - val_mse: 0.0035 Epoch 28/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0027 - mae: 0.0342 - mse: 0.0027 - val_loss: 0.0042 - val_mae: 0.0387 - val_mse: 0.0042 Epoch 29/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0029 - mae: 0.0354 - mse: 0.0029 - val_loss: 0.0037 - val_mae: 0.0376 - val_mse: 0.0037 Epoch 30/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0027 - mae: 0.0344 - mse: 0.0027 - val_loss: 0.0037 - val_mae: 0.0397 - val_mse: 0.0037 Epoch 31/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0027 - mae: 0.0342 - mse: 0.0027 - val_loss: 0.0036 - val_mae: 0.0371 - val_mse: 0.0036 Epoch 32/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0026 - mae: 0.0339 - mse: 0.0026 - val_loss: 0.0037 - val_mae: 0.0365 - val_mse: 0.0037 Epoch 33/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0026 - mae: 0.0337 - mse: 0.0026 - val_loss: 0.0036 - val_mae: 0.0364 - val_mse: 0.0036 Epoch 34/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0326 - mse: 0.0024 - val_loss: 0.0036 - val_mae: 0.0373 - val_mse: 0.0036 Epoch 35/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0025 - mae: 0.0329 - mse: 0.0025 - val_loss: 0.0036 - val_mae: 0.0367 - val_mse: 0.0036 Epoch 36/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0321 - mse: 0.0024 - val_loss: 0.0036 - val_mae: 0.0363 - val_mse: 0.0036 Epoch 37/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0322 - mse: 0.0024 - val_loss: 0.0034 - val_mae: 0.0363 - val_mse: 0.0034 Epoch 38/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0323 - mse: 0.0024 - val_loss: 0.0036 - val_mae: 0.0354 - val_mse: 0.0036 Epoch 39/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0025 - mae: 0.0327 - mse: 0.0025 - val_loss: 0.0036 - val_mae: 0.0360 - val_mse: 0.0036 Epoch 40/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0023 - mae: 0.0319 - mse: 0.0023 - val_loss: 0.0034 - val_mae: 0.0360 - val_mse: 0.0034 Epoch 41/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0318 - mse: 0.0024 - val_loss: 0.0034 - val_mae: 0.0358 - val_mse: 0.0034 Epoch 42/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0023 - mae: 0.0314 - mse: 0.0023 - val_loss: 0.0036 - val_mae: 0.0366 - val_mse: 0.0036 Epoch 43/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0022 - mae: 0.0312 - mse: 0.0022 - val_loss: 0.0034 - val_mae: 0.0361 - val_mse: 0.0034 Epoch 44/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0022 - mae: 0.0312 - mse: 0.0022 - val_loss: 0.0035 - val_mae: 0.0354 - val_mse: 0.0035 Epoch 45/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0023 - mae: 0.0314 - mse: 0.0023 - val_loss: 0.0035 - val_mae: 0.0386 - val_mse: 0.0035 Epoch 46/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0022 - mae: 0.0310 - mse: 0.0022 - val_loss: 0.0035 - val_mae: 0.0359 - val_mse: 0.0035 Epoch 47/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0022 - mae: 0.0313 - mse: 0.0022 - val_loss: 0.0038 - val_mae: 0.0374 - val_mse: 0.0038 Epoch 48/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0022 - mae: 0.0308 - mse: 0.0022 - val_loss: 0.0034 - val_mae: 0.0350 - val_mse: 0.0034 Epoch 49/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0022 - mae: 0.0307 - mse: 0.0022 - val_loss: 0.0035 - val_mae: 0.0357 - val_mse: 0.0035 Epoch 50/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0021 - mae: 0.0305 - mse: 0.0021 - val_loss: 0.0035 - val_mae: 0.0381 - val_mse: 0.0035 Epoch 51/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0022 - mae: 0.0309 - mse: 0.0022 - val_loss: 0.0034 - val_mae: 0.0357 - val_mse: 0.0034 Epoch 52/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0021 - mae: 0.0304 - mse: 0.0021 - val_loss: 0.0034 - val_mae: 0.0347 - val_mse: 0.0034 Epoch 53/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0022 - mae: 0.0304 - mse: 0.0022 - val_loss: 0.0035 - val_mae: 0.0354 - val_mse: 0.0035 Epoch 54/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0021 - mae: 0.0301 - mse: 0.0021 - val_loss: 0.0033 - val_mae: 0.0366 - val_mse: 0.0033 Epoch 55/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0021 - mae: 0.0301 - mse: 0.0021 - val_loss: 0.0034 - val_mae: 0.0348 - val_mse: 0.0034 Epoch 56/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0019 - mae: 0.0288 - mse: 0.0019 - val_loss: 0.0033 - val_mae: 0.0347 - val_mse: 0.0033 Epoch 57/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0298 - mse: 0.0020 - val_loss: 0.0035 - val_mae: 0.0361 - val_mse: 0.0035 Epoch 58/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0021 - mae: 0.0299 - mse: 0.0021 - val_loss: 0.0034 - val_mae: 0.0353 - val_mse: 0.0034 Epoch 59/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0021 - mae: 0.0299 - mse: 0.0021 - val_loss: 0.0032 - val_mae: 0.0361 - val_mse: 0.0032 Epoch 60/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0021 - mae: 0.0296 - mse: 0.0021 - val_loss: 0.0033 - val_mae: 0.0344 - val_mse: 0.0033 Epoch 61/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0290 - mse: 0.0020 - val_loss: 0.0033 - val_mae: 0.0346 - val_mse: 0.0033 Epoch 62/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0297 - mse: 0.0020 - val_loss: 0.0033 - val_mae: 0.0351 - val_mse: 0.0033 Epoch 63/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0294 - mse: 0.0020 - val_loss: 0.0034 - val_mae: 0.0352 - val_mse: 0.0034 Epoch 64/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0294 - mse: 0.0020 - val_loss: 0.0034 - val_mae: 0.0360 - val_mse: 0.0034 Epoch 65/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0292 - mse: 0.0020 - val_loss: 0.0035 - val_mae: 0.0383 - val_mse: 0.0035 Epoch 66/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0295 - mse: 0.0020 - val_loss: 0.0034 - val_mae: 0.0348 - val_mse: 0.0034 Epoch 67/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0292 - mse: 0.0020 - val_loss: 0.0034 - val_mae: 0.0346 - val_mse: 0.0034 Epoch 68/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0019 - mae: 0.0287 - mse: 0.0019 - val_loss: 0.0033 - val_mae: 0.0347 - val_mse: 0.0033 Epoch 69/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0284 - mse: 0.0018 - val_loss: 0.0032 - val_mae: 0.0348 - val_mse: 0.0032 Epoch 70/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0019 - mae: 0.0289 - mse: 0.0019 - val_loss: 0.0034 - val_mae: 0.0355 - val_mse: 0.0034 Epoch 71/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0019 - mae: 0.0290 - mse: 0.0019 - val_loss: 0.0035 - val_mae: 0.0372 - val_mse: 0.0035 Epoch 72/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0019 - mae: 0.0284 - mse: 0.0019 - val_loss: 0.0034 - val_mae: 0.0343 - val_mse: 0.0034 Epoch 73/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0283 - mse: 0.0018 - val_loss: 0.0033 - val_mae: 0.0364 - val_mse: 0.0033 Epoch 74/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0019 - mae: 0.0289 - mse: 0.0019 - val_loss: 0.0033 - val_mae: 0.0364 - val_mse: 0.0033 Epoch 75/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0282 - mse: 0.0018 - val_loss: 0.0034 - val_mae: 0.0345 - val_mse: 0.0034 Epoch 76/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0019 - mae: 0.0285 - mse: 0.0019 - val_loss: 0.0033 - val_mae: 0.0344 - val_mse: 0.0033 Epoch 77/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0019 - mae: 0.0283 - mse: 0.0019 - val_loss: 0.0034 - val_mae: 0.0352 - val_mse: 0.0034 Epoch 78/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0278 - mse: 0.0018 - val_loss: 0.0034 - val_mae: 0.0355 - val_mse: 0.0034 Epoch 79/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0277 - mse: 0.0018 - val_loss: 0.0034 - val_mae: 0.0362 - val_mse: 0.0034 Epoch 80/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0017 - mae: 0.0277 - mse: 0.0017 - val_loss: 0.0033 - val_mae: 0.0361 - val_mse: 0.0033 Epoch 81/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0277 - mse: 0.0018 - val_loss: 0.0034 - val_mae: 0.0369 - val_mse: 0.0034 Epoch 82/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0282 - mse: 0.0018 - val_loss: 0.0034 - val_mae: 0.0352 - val_mse: 0.0034 Epoch 83/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0276 - mse: 0.0018 - val_loss: 0.0035 - val_mae: 0.0354 - val_mse: 0.0035 Epoch 84/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0282 - mse: 0.0018 - val_loss: 0.0034 - val_mae: 0.0343 - val_mse: 0.0034 Epoch 85/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0019 - mae: 0.0280 - mse: 0.0019 - val_loss: 0.0035 - val_mae: 0.0375 - val_mse: 0.0035 Epoch 86/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0017 - mae: 0.0273 - mse: 0.0017 - val_loss: 0.0035 - val_mae: 0.0352 - val_mse: 0.0035 Epoch 87/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0277 - mse: 0.0018 - val_loss: 0.0034 - val_mae: 0.0348 - val_mse: 0.0034 Epoch 88/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0017 - mae: 0.0273 - mse: 0.0017 - val_loss: 0.0033 - val_mae: 0.0346 - val_mse: 0.0033 Epoch 89/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0276 - mse: 0.0018 - val_loss: 0.0034 - val_mae: 0.0354 - val_mse: 0.0034 y_pred = model.predict(X_test) err=mean_squared_error(y_pred,y_test) y_pred_ori = scaler.inverse_transform(y_pred) y_test_ori = scaler.inverse_transform(y_test) print(err) print(y_pred_ori[:10]) print(y_test_ori[:10]) 0.0023450270504444958 [[ 7.5200114] [36.162247 ] [-1.5508894] [11.4153385] [30.12201 ] [ 4.0796814] [ 2.721789 ] [43.332848 ] [23.49451 ] [47.286026 ]] [[10.] [30.] [ 0.] [15.] [15.] [ 5.] [ 5.] [60.] [10.] [50.]] diff = y_test_ori - y_pred_ori plt.figure(figsize=(20, 10)) plt.plot(y_pred_ori, label=\"Prediction\") plt.plot(y_test_ori, label=\"True\") plt.plot(diff, label=\"Difference\") plt.ylabel('Waiting time') plt.xlabel('Day') plt.legend() plt.savefig('DNN_regresion.png') Training on category \u00b6 in this case, we will pre-process the wait time into 3 different categories. [1, 0, 0]: Wait time < 30 mins [0, 1, 0]: Wait time >= 30 and <= 70 cat_vars = [ 'Name' , 'IsWeekend' , 'IsHoliday' , 'Hour modify' , 'Weather' ] num_vars = [ 'Temperature' , 'Pressure' , 'Humidity' , 'Cloud' , 'Wind degree' , ] X_train , X_test , y_train , y_test = generate_training_data ( train , 'Is busy' , cat_vars = cat_vars , num_vars = num_vars , should_reshape = False ) print ( y_train . shape ) print ( X_train . shape ) (14328, 3) (14328, 44) early_stop = keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 20 ) model = keras . Sequential ( [ layers . Dense ( 64 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dense ( 512 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dropout ( 0 . 5 ), layers . Dense ( 256 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dropout ( 0 . 5 ), layers . Dense ( 128 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dense ( y_train . shape [ 1 ], activation = 'softmax' ), ] ) opt = keras . optimizers . Adam ( learning_rate = 0 . 001 ) model . compile ( loss = 'categorical_crossentropy' , optimizer = opt , metrics = [ 'acc' , 'mae' , 'mse' ]) history = model . fit ( X_train , y_train , epochs = 1000 , callbacks =[ early_stop ] , validation_split = 0.2 ) plot_history ( history ) Epoch 1/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2155 - acc: 0.9014 - mae: 0.0904 - mse: 0.0453 - val_loss: 0.4462 - val_acc: 0.8472 - val_mae: 0.1254 - val_mse: 0.0757 Epoch 2/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2120 - acc: 0.9025 - mae: 0.0880 - mse: 0.0441 - val_loss: 0.4410 - val_acc: 0.8381 - val_mae: 0.1258 - val_mse: 0.0754 Epoch 3/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2112 - acc: 0.9035 - mae: 0.0883 - mse: 0.0443 - val_loss: 0.4480 - val_acc: 0.8416 - val_mae: 0.1229 - val_mse: 0.0752 Epoch 4/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2063 - acc: 0.9074 - mae: 0.0856 - mse: 0.0429 - val_loss: 0.4622 - val_acc: 0.8458 - val_mae: 0.1219 - val_mse: 0.0739 Epoch 5/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2055 - acc: 0.9060 - mae: 0.0859 - mse: 0.0430 - val_loss: 0.4529 - val_acc: 0.8423 - val_mae: 0.1225 - val_mse: 0.0752 Epoch 6/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1968 - acc: 0.9093 - mae: 0.0822 - mse: 0.0413 - val_loss: 0.4663 - val_acc: 0.8458 - val_mae: 0.1188 - val_mse: 0.0763 Epoch 7/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2024 - acc: 0.9108 - mae: 0.0838 - mse: 0.0419 - val_loss: 0.5039 - val_acc: 0.8503 - val_mae: 0.1188 - val_mse: 0.0762 Epoch 8/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2025 - acc: 0.9101 - mae: 0.0841 - mse: 0.0419 - val_loss: 0.5043 - val_acc: 0.8486 - val_mae: 0.1139 - val_mse: 0.0758 Epoch 9/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2047 - acc: 0.9053 - mae: 0.0851 - mse: 0.0432 - val_loss: 0.4379 - val_acc: 0.8514 - val_mae: 0.1199 - val_mse: 0.0717 Epoch 10/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1961 - acc: 0.9114 - mae: 0.0826 - mse: 0.0412 - val_loss: 0.4864 - val_acc: 0.8479 - val_mae: 0.1186 - val_mse: 0.0759 Epoch 11/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1993 - acc: 0.9097 - mae: 0.0819 - mse: 0.0412 - val_loss: 0.4532 - val_acc: 0.8465 - val_mae: 0.1209 - val_mse: 0.0736 Epoch 12/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1934 - acc: 0.9108 - mae: 0.0816 - mse: 0.0409 - val_loss: 0.4550 - val_acc: 0.8548 - val_mae: 0.1178 - val_mse: 0.0731 Epoch 13/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1870 - acc: 0.9135 - mae: 0.0788 - mse: 0.0394 - val_loss: 0.4832 - val_acc: 0.8416 - val_mae: 0.1207 - val_mse: 0.0762 Epoch 14/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1905 - acc: 0.9126 - mae: 0.0793 - mse: 0.0398 - val_loss: 0.5012 - val_acc: 0.8486 - val_mae: 0.1182 - val_mse: 0.0759 Epoch 15/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1882 - acc: 0.9163 - mae: 0.0781 - mse: 0.0393 - val_loss: 0.5033 - val_acc: 0.8503 - val_mae: 0.1180 - val_mse: 0.0750 Epoch 16/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1890 - acc: 0.9164 - mae: 0.0788 - mse: 0.0395 - val_loss: 0.4769 - val_acc: 0.8500 - val_mae: 0.1179 - val_mse: 0.0747 Epoch 17/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1904 - acc: 0.9094 - mae: 0.0799 - mse: 0.0402 - val_loss: 0.5425 - val_acc: 0.8524 - val_mae: 0.1129 - val_mse: 0.0751 Epoch 18/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1865 - acc: 0.9167 - mae: 0.0763 - mse: 0.0386 - val_loss: 0.4673 - val_acc: 0.8510 - val_mae: 0.1175 - val_mse: 0.0738 Epoch 19/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1857 - acc: 0.9156 - mae: 0.0769 - mse: 0.0386 - val_loss: 0.4718 - val_acc: 0.8458 - val_mae: 0.1176 - val_mse: 0.0735 Epoch 20/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1864 - acc: 0.9157 - mae: 0.0776 - mse: 0.0390 - val_loss: 0.5050 - val_acc: 0.8423 - val_mae: 0.1175 - val_mse: 0.0760 Epoch 21/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1795 - acc: 0.9187 - mae: 0.0753 - mse: 0.0378 - val_loss: 0.4781 - val_acc: 0.8514 - val_mae: 0.1176 - val_mse: 0.0739 Epoch 22/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1805 - acc: 0.9169 - mae: 0.0750 - mse: 0.0379 - val_loss: 0.4897 - val_acc: 0.8489 - val_mae: 0.1166 - val_mse: 0.0754 Epoch 23/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1818 - acc: 0.9181 - mae: 0.0757 - mse: 0.0378 - val_loss: 0.5106 - val_acc: 0.8510 - val_mae: 0.1143 - val_mse: 0.0748 Epoch 24/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1819 - acc: 0.9193 - mae: 0.0748 - mse: 0.0378 - val_loss: 0.5458 - val_acc: 0.8440 - val_mae: 0.1169 - val_mse: 0.0771 Epoch 25/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1768 - acc: 0.9211 - mae: 0.0736 - mse: 0.0367 - val_loss: 0.5249 - val_acc: 0.8419 - val_mae: 0.1169 - val_mse: 0.0773 Epoch 26/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1752 - acc: 0.9232 - mae: 0.0721 - mse: 0.0362 - val_loss: 0.5045 - val_acc: 0.8475 - val_mae: 0.1176 - val_mse: 0.0743 Epoch 27/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1733 - acc: 0.9235 - mae: 0.0722 - mse: 0.0361 - val_loss: 0.5165 - val_acc: 0.8444 - val_mae: 0.1163 - val_mse: 0.0761 Epoch 28/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1762 - acc: 0.9205 - mae: 0.0729 - mse: 0.0366 - val_loss: 0.4968 - val_acc: 0.8479 - val_mae: 0.1153 - val_mse: 0.0741 Epoch 29/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1708 - acc: 0.9238 - mae: 0.0709 - mse: 0.0353 - val_loss: 0.5015 - val_acc: 0.8454 - val_mae: 0.1162 - val_mse: 0.0761 <Figure size 432x288 with 0 Axes> y_pred = model . predict ( X_test ) def print_acc ( pred , true ) : right = 0 for i in range ( len ( pred )) : p = np . argmax ( pred [ i ] ) t = np . argmax ( true [ i ] ) if p == t : right += 1 print ( f \"acc: {right}/{len(pred)}, {right/len(pred)}\" ) print_acc ( y_pred , y_test ) acc: 250/293, 0.8532423208191127 Training by using category \u00b6 In this example, we will use average-wait-time to compute their waiting time's category If wait-time is less than the average-wait time of that facility, then it is not busy. If wait-time is around average wait-time, then it is in a normal situation If wait-time is above average-wait-time, then it is busy. In this model, we are basically assume facilities are different. Some will have more popularity than others. And we will use the model to predict whether a model is above its average or not. def is_busy_2 ( row ): if row [ 3 ] < row [ 1 ] - 5 : return [ 1 , 0 , 0 ] elif row [ 3 ] >= row [ 1 ] - 5 and row [ 3 ] <= row [ 1 ] + 5 : return [ 0 , 1 , 0 ] else : return [ 0 , 0 , 1 ] def calculate_average_wait_time ( df : pd . DataFrame ) -> pd . DataFrame : new_df = df . copy () new_df2 = df . copy () new_df [ 'tmp_date' ] = new_df [ 'Time' ]. apply ( lambda x : x . date ()) new_df = new_df [[ 'tmp_date' , 'Wait time' , 'Name' ]] new_df = new_df . groupby ([ 'Name' ]). mean () new_df = new_df . rename ( columns = { 'Wait time' : 'Average wait time' } ) new_df = new_df . merge ( new_df2 , on = 'Name' , how = 'right' ) return new_df average_sampled_train = calculate_average_wait_time ( train ) average_sampled_train [ 'Is busy' ] = average_sampled_train . apply ( is_busy_2 , axis = 1 ) average_sampled_train . sample ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Name Average wait time Time Wait time Weather Temperature Max temperature Min temperature Pressure Humidity Wind degree Wind speed Cloud id IsWeekend IsHoliday Hour modify Is busy 6302 Dumbo the Flying Elephant 24.476950 2020-10-03 17:19:47.471631 40.0 Clouds 23.73 24.00 24.00 1012 78 90.0 6.0 75 12 1 1 Evening [0, 0, 1] 10102 Siren's Revenge 5.000000 2020-10-21 13:22:34.657099 5.0 Clouds 18.36 19.44 19.44 1018 88 340.0 5.0 90 18 0 0 Afternoon [0, 1, 0] 5149 The Many Adventures of Winnie the Pooh 21.727129 2020-10-08 13:17:12.792022 20.0 Clouds 23.83 25.00 25.00 1022 53 20.0 9.0 75 10 0 1 Afternoon [0, 1, 0] 8802 Jet Packs 34.943639 2020-10-17 13:21:49.642192 40.0 Clouds 21.54 22.00 22.00 1023 56 10.0 5.0 20 16 1 0 Afternoon [0, 0, 1] 12292 Rex\u2019s Racer 56.937984 2020-09-04 13:17:48.730671 90.0 Clouds 33.22 36.67 36.67 1012 45 170.0 6.0 91 28 0 0 Afternoon [0, 0, 1] 4527 Seven Dwarfs Mine Train 52.788310 2020-10-09 11:21:36.066646 45.0 Clouds 23.20 24.00 24.00 1021 53 10.0 6.0 23 9 0 0 Morning [1, 0, 0] 13143 Slinky Dog Spin 24.613601 2020-09-22 16:21:22.856577 20.0 Clouds 24.80 25.00 25.00 1014 61 60.0 6.0 40 29 0 0 Evening [0, 1, 0] 5309 The Many Adventures of Winnie the Pooh 21.727129 2020-10-22 17:23:25.739682 40.0 Clouds 19.46 20.00 20.00 1016 68 10.0 8.0 40 10 0 0 Evening [0, 0, 1] 5307 The Many Adventures of Winnie the Pooh 21.727129 2020-10-22 15:23:48.253335 10.0 Clouds 22.51 23.33 23.33 1016 56 340.0 5.0 13 10 0 0 Afternoon [1, 0, 0] 12042 Roaring Rapids 31.629555 2020-09-25 21:18:18.834521 5.0 Rain 22.66 23.00 23.00 1014 69 10.0 5.0 100 23 0 0 Night [1, 0, 0] 5539 Voyage to the Crystal Grotto 17.301459 2020-09-18 17:18:57.544340 40.0 Clouds 19.43 20.00 20.00 1018 88 270.0 3.0 40 11 0 0 Evening [0, 0, 1] 11842 Roaring Rapids 31.629555 2020-09-08 14:21:00.395020 20.0 Clouds 33.61 35.00 35.00 1010 49 180.0 8.0 20 23 0 0 Afternoon [1, 0, 0] 6323 Dumbo the Flying Elephant 24.476950 2020-10-05 17:20:52.785435 30.0 Clouds 20.67 21.11 21.11 1020 52 10.0 8.0 36 12 0 1 Evening [0, 0, 1] 11943 Roaring Rapids 31.629555 2020-09-17 16:20:41.428588 25.0 Rain 21.05 21.67 21.67 1010 100 350.0 7.0 75 23 0 0 Evening [1, 0, 0] 5281 The Many Adventures of Winnie the Pooh 21.727129 2020-10-20 10:24:19.155382 20.0 Clouds 21.59 22.00 22.00 1025 53 50.0 6.0 40 10 0 0 Morning [0, 1, 0] 9435 Buzz Lightyear Planet Rescue 7.097039 2020-10-19 15:24:32.051678 10.0 Clouds 21.24 23.33 23.33 1023 56 30.0 5.0 40 17 0 0 Afternoon [0, 1, 0] 1890 \u201cOnce Upon a Time\u201d Adventure 7.246127 2020-09-17 10:21:16.322591 5.0 Rain 22.64 23.00 23.00 1010 88 40.0 7.0 75 4 0 0 Morning [0, 1, 0] 9942 Siren's Revenge 5.000000 2020-10-07 17:21:19.897009 5.0 Clouds 21.37 22.00 22.00 1021 56 20.0 8.0 20 18 0 1 Evening [0, 1, 0] 2997 Hunny Pot Spin 9.258114 2020-09-12 09:22:28.558742 5.0 Clouds 26.08 27.78 27.78 1013 78 330.0 3.0 20 7 1 0 Morning [0, 1, 0] 13470 Slinky Dog Spin 24.613601 2020-10-19 12:25:41.694352 40.0 Clouds 20.87 22.78 22.78 1024 56 20.0 6.0 40 29 0 0 Afternoon [0, 0, 1] cat_vars = [ 'Name' , 'IsWeekend' , 'IsHoliday' , 'Hour modify' , 'Weather' ] num_vars = [ 'Temperature' , 'Pressure' , 'Humidity' , 'Cloud' , 'Wind degree' , ] X_train , X_test , y_train , y_test = generate_training_data ( average_sampled_train , 'Is busy' , cat_vars = cat_vars , num_vars = num_vars , should_reshape = False ) early_stop = keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 20 ) model = keras . Sequential ( [ layers . Dense ( 64 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dense ( 512 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dropout ( 0 . 5 ), layers . Dense ( 256 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dropout ( 0 . 5 ), # layers . Dense ( 512 , activation = \"relu\" , # input_shape = ( X_train . shape [ 1 ],),), # layers . Dropout ( 0 . 5 ), # layers . Dense ( 512 , activation = \"relu\" , # input_shape = ( X_train . shape [ 1 ],),), # layers . Dropout ( 0 . 5 ), # layers . Dense ( 256 , activation = \"relu\" , # input_shape = ( X_train . shape [ 1 ],),), # layers . Dropout ( 0 . 5 ), layers . Dense ( 128 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dense ( y_train . shape [ 1 ], activation = 'softmax' ), ] ) opt = keras . optimizers . Adam ( learning_rate = 0 . 001 ) model . compile ( loss = 'categorical_crossentropy' , optimizer = opt , metrics = [ 'acc' , 'mae' , 'mse' ]) history = model . fit ( X_train , y_train , epochs = 1000 , callbacks =[ early_stop ] , validation_split = 0.2 ) plot_history ( history ) Epoch 1/1000 359/359 [==============================] - 1s 4ms/step - loss: 0.5742 - acc: 0.7383 - mae: 0.2304 - mse: 0.1161 - val_loss: 0.4259 - val_acc: 0.8102 - val_mae: 0.1846 - val_mse: 0.0883 Epoch 2/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.4323 - acc: 0.7988 - mae: 0.1772 - mse: 0.0895 - val_loss: 0.4156 - val_acc: 0.8123 - val_mae: 0.1713 - val_mse: 0.0858 Epoch 3/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.4067 - acc: 0.8137 - mae: 0.1683 - mse: 0.0848 - val_loss: 0.4012 - val_acc: 0.8158 - val_mae: 0.1741 - val_mse: 0.0836 Epoch 4/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3872 - acc: 0.8274 - mae: 0.1599 - mse: 0.0805 - val_loss: 0.3918 - val_acc: 0.8266 - val_mae: 0.1623 - val_mse: 0.0805 Epoch 5/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3733 - acc: 0.8318 - mae: 0.1553 - mse: 0.0777 - val_loss: 0.3877 - val_acc: 0.8200 - val_mae: 0.1607 - val_mse: 0.0801 Epoch 6/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3590 - acc: 0.8396 - mae: 0.1489 - mse: 0.0750 - val_loss: 0.3819 - val_acc: 0.8248 - val_mae: 0.1600 - val_mse: 0.0793 Epoch 7/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3567 - acc: 0.8396 - mae: 0.1478 - mse: 0.0742 - val_loss: 0.3821 - val_acc: 0.8287 - val_mae: 0.1570 - val_mse: 0.0788 Epoch 8/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3472 - acc: 0.8437 - mae: 0.1445 - mse: 0.0725 - val_loss: 0.3714 - val_acc: 0.8339 - val_mae: 0.1493 - val_mse: 0.0763 Epoch 9/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3415 - acc: 0.8475 - mae: 0.1421 - mse: 0.0711 - val_loss: 0.3853 - val_acc: 0.8308 - val_mae: 0.1499 - val_mse: 0.0786 Epoch 10/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3323 - acc: 0.8504 - mae: 0.1386 - mse: 0.0694 - val_loss: 0.3726 - val_acc: 0.8374 - val_mae: 0.1404 - val_mse: 0.0752 Epoch 11/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3264 - acc: 0.8540 - mae: 0.1360 - mse: 0.0681 - val_loss: 0.3828 - val_acc: 0.8381 - val_mae: 0.1456 - val_mse: 0.0760 Epoch 12/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3233 - acc: 0.8567 - mae: 0.1334 - mse: 0.0671 - val_loss: 0.3683 - val_acc: 0.8353 - val_mae: 0.1471 - val_mse: 0.0748 Epoch 13/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3184 - acc: 0.8574 - mae: 0.1322 - mse: 0.0663 - val_loss: 0.3639 - val_acc: 0.8454 - val_mae: 0.1422 - val_mse: 0.0734 Epoch 14/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3033 - acc: 0.8622 - mae: 0.1269 - mse: 0.0631 - val_loss: 0.3802 - val_acc: 0.8398 - val_mae: 0.1372 - val_mse: 0.0756 Epoch 15/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3044 - acc: 0.8618 - mae: 0.1260 - mse: 0.0634 - val_loss: 0.3642 - val_acc: 0.8391 - val_mae: 0.1368 - val_mse: 0.0728 Epoch 16/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2985 - acc: 0.8649 - mae: 0.1247 - mse: 0.0624 - val_loss: 0.3836 - val_acc: 0.8364 - val_mae: 0.1338 - val_mse: 0.0762 Epoch 17/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2926 - acc: 0.8694 - mae: 0.1215 - mse: 0.0611 - val_loss: 0.3739 - val_acc: 0.8423 - val_mae: 0.1384 - val_mse: 0.0740 Epoch 18/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2936 - acc: 0.8684 - mae: 0.1221 - mse: 0.0611 - val_loss: 0.3752 - val_acc: 0.8385 - val_mae: 0.1329 - val_mse: 0.0732 Epoch 19/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2824 - acc: 0.8709 - mae: 0.1180 - mse: 0.0591 - val_loss: 0.3726 - val_acc: 0.8437 - val_mae: 0.1309 - val_mse: 0.0733 Epoch 20/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2793 - acc: 0.8775 - mae: 0.1155 - mse: 0.0581 - val_loss: 0.3680 - val_acc: 0.8378 - val_mae: 0.1411 - val_mse: 0.0732 Epoch 21/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2773 - acc: 0.8749 - mae: 0.1157 - mse: 0.0577 - val_loss: 0.3865 - val_acc: 0.8451 - val_mae: 0.1316 - val_mse: 0.0735 Epoch 22/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2718 - acc: 0.8777 - mae: 0.1130 - mse: 0.0566 - val_loss: 0.3870 - val_acc: 0.8398 - val_mae: 0.1293 - val_mse: 0.0748 Epoch 23/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2668 - acc: 0.8810 - mae: 0.1110 - mse: 0.0558 - val_loss: 0.3765 - val_acc: 0.8465 - val_mae: 0.1308 - val_mse: 0.0730 Epoch 24/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2646 - acc: 0.8838 - mae: 0.1099 - mse: 0.0550 - val_loss: 0.3926 - val_acc: 0.8364 - val_mae: 0.1301 - val_mse: 0.0740 Epoch 25/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2581 - acc: 0.8841 - mae: 0.1073 - mse: 0.0538 - val_loss: 0.3855 - val_acc: 0.8493 - val_mae: 0.1284 - val_mse: 0.0728 Epoch 26/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2576 - acc: 0.8854 - mae: 0.1070 - mse: 0.0537 - val_loss: 0.3911 - val_acc: 0.8440 - val_mae: 0.1263 - val_mse: 0.0743 Epoch 27/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2541 - acc: 0.8856 - mae: 0.1053 - mse: 0.0531 - val_loss: 0.3986 - val_acc: 0.8447 - val_mae: 0.1278 - val_mse: 0.0740 Epoch 28/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2489 - acc: 0.8878 - mae: 0.1038 - mse: 0.0521 - val_loss: 0.4028 - val_acc: 0.8447 - val_mae: 0.1259 - val_mse: 0.0744 Epoch 29/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2455 - acc: 0.8897 - mae: 0.1019 - mse: 0.0513 - val_loss: 0.4089 - val_acc: 0.8468 - val_mae: 0.1248 - val_mse: 0.0732 Epoch 30/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2400 - acc: 0.8934 - mae: 0.1000 - mse: 0.0500 - val_loss: 0.4043 - val_acc: 0.8409 - val_mae: 0.1237 - val_mse: 0.0741 Epoch 31/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2405 - acc: 0.8936 - mae: 0.0993 - mse: 0.0499 - val_loss: 0.3976 - val_acc: 0.8430 - val_mae: 0.1284 - val_mse: 0.0748 Epoch 32/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2382 - acc: 0.8943 - mae: 0.0989 - mse: 0.0496 - val_loss: 0.4170 - val_acc: 0.8402 - val_mae: 0.1280 - val_mse: 0.0755 Epoch 33/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2382 - acc: 0.8934 - mae: 0.0989 - mse: 0.0498 - val_loss: 0.3866 - val_acc: 0.8461 - val_mae: 0.1285 - val_mse: 0.0718 <Figure size 432x288 with 0 Axes> y_pred = model . predict ( X_test ) print_acc ( y_pred , y_test ) acc: 243/293, 0.8293515358361775 LSTM Training \u00b6 Window generator \u00b6 To make a single prediction 24h into the future, given 24h of history you might define a window like this: class WindowGenerator () : def __init__ ( self , input_width , offset , data , train_split ) : self . data = data self . input_width = input_width self . offset = offset self . train_split = train_split def to_sequences ( self ) : \"\"\" Return both data and label \"\"\" data_len = len ( self . data ) ret = [] ret_label = [] for i in range ( data_len - self . offset - self . input_width + 1 ) : tmp = self . data [ i : i + self . input_width ] tmp_label = self . data [ i + self . input_width + self . offset - 1 ] ret . append ( tmp ) ret_label . append ( tmp_label ) return np . array ( ret ), np . array ( ret_label ) def split ( self ) : x , y = self . to_sequences () num_train = int (( 1 - self . train_split ) * x . shape [ 0 ]) X_train = x [ : num_train ] y_train = y [ : num_train ] X_test = x [ num_train :] y_test = y [ num_train :] return X_train , y_train , X_test , y_test SEQ_LEN = 10 cat_vars = [ ' IsWeekend ',' IsHoliday ' ] num_vars = [ ' Temperature ' , ' Pressure ' , ' Humidity ' , ' Cloud ' , ' Wind degree ' ] from tensorflow.keras.layers import Bidirectional , Dropout , LSTM , Dense , Activation Preprocess data \u00b6 average_sampled_train['temp_date'] = average_sampled_train['Time'].apply(lambda x: x.date()) grouped_average = average_sampled_train.groupby('temp_date').mean() grouped_average = grouped_average[['Temperature', 'Max temperature', 'Min temperature', 'Wind degree', 'Humidity' , 'Wind speed', 'Cloud', 'IsWeekend', 'IsHoliday', \"Wait time\", \"Pressure\"]] grouped_average.columns Index(['Temperature', 'Max temperature', 'Min temperature', 'Wind degree', 'Humidity', 'Wind speed', 'Cloud', 'IsWeekend', 'IsHoliday', 'Wait time', 'Pressure'], dtype='object') numeric_transformer = Pipeline ( steps = [ ( 'scaler' , RobustScaler ())]) categorical_transformer = Pipeline ( steps = [ ( 'oneHot' , OneHotEncoder ( sparse = False ))]) preprocessor = ColumnTransformer ( transformers = [ ( 'num' , numeric_transformer , num_vars ), ( 'cat' , categorical_transformer , cat_vars )]) data_transformed = preprocessor . fit_transform ( grouped_average ) print ( data_transformed . shape ) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) /usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py in _get_column_indices(X, key) 462 try: --> 463 column_indices = [all_columns.index(col) for col in columns] 464 except ValueError as e: /usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py in <listcomp>(.0) 462 try: --> 463 column_indices = [all_columns.index(col) for col in columns] 464 except ValueError as e: ValueError: 'Name' is not in list The above exception was the direct cause of the following exception: ValueError Traceback (most recent call last) <ipython-input-38-c2d6048db054> in <module>() 8 ('cat',categorical_transformer,cat_vars)]) 9 ---> 10 data_transformed=preprocessor.fit_transform(grouped_average) 11 print(data_transformed.shape) /usr/local/lib/python3.6/dist-packages/sklearn/compose/_column_transformer.py in fit_transform(self, X, y) 514 self._validate_transformers() 515 self._validate_column_callables(X) --> 516 self._validate_remainder(X) 517 518 result = self._fit_transform(X, y, _fit_transform_one) /usr/local/lib/python3.6/dist-packages/sklearn/compose/_column_transformer.py in _validate_remainder(self, X) 322 cols = [] 323 for columns in self._columns: --> 324 cols.extend(_get_column_indices(X, columns)) 325 remaining_idx = list(set(range(self._n_features)) - set(cols)) 326 remaining_idx = sorted(remaining_idx) or None /usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py in _get_column_indices(X, key) 466 raise ValueError( 467 \"A given column is not a column of the dataframe\" --> 468 ) from e 469 raise 470 ValueError: A given column is not a column of the dataframe wg = WindowGenerator(data=data_transformed, input_width=SEQ_LEN, offset=0, train_split=0.1) wg_2 = WindowGenerator(data=grouped_average['Wait time'].to_numpy(), input_width=SEQ_LEN, offset=0, train_split=0.1) X_train, _, X_test, _ = wg.split() _, y_train, _, y_test = wg_2.split() print(X_train.shape) print(y_train.shape) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-39-3e9d8c3f67d6> in <module>() ----> 1 wg = WindowGenerator(data=data_transformed, input_width=SEQ_LEN, offset=0, train_split=0.1) 2 wg_2 = WindowGenerator(data=grouped_average['Wait time'].to_numpy(), input_width=SEQ_LEN, offset=0, train_split=0.1) 3 X_train, _, X_test, _ = wg.split() 4 _, y_train, _, y_test = wg_2.split() 5 print(X_train.shape) NameError: name 'WindowGenerator' is not defined WINDOW_SIZE = SEQ_LEN model = keras . Sequential () # Input layer model . add ( Bidirectional ( LSTM ( WINDOW_SIZE , return_sequences = True ), input_shape = ( WINDOW_SIZE , X_train . shape [ - 1 ]))) \"\"\"Bidirectional RNNs allows to train on the sequence data in forward and backward direction.\"\"\" model . add ( Dropout ( rate = 0 . 2 )) # 1 st Hidden layer model . add ( Bidirectional ( LSTM (( WINDOW_SIZE * 2 ), return_sequences = True ))) model . add ( Dropout ( rate = 0 . 2 )) # 2 nd Hidden layer model . add ( Bidirectional ( LSTM ( WINDOW_SIZE , return_sequences = False ))) # output layer model . add ( Dense ( units = 1 )) model . add ( Activation ( 'linear' )) model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' , metrics = [ 'mae' , 'mse' ]) model . summary () history = model . fit ( X_train , y_train , epochs = 1000 , shuffle = False , validation_split = 0.1 , callbacks =[ early_stop ] ) Epoch 1/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2527 - acc: 0.8907 - mae: 0.1016 - mse: 0.0513 - val_loss: 0.3764 - val_acc: 0.8332 - val_mae: 0.1546 - val_mse: 0.0773 Epoch 2/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2506 - acc: 0.8893 - mae: 0.1032 - mse: 0.0517 - val_loss: 0.3732 - val_acc: 0.8353 - val_mae: 0.1477 - val_mse: 0.0752 Epoch 3/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2428 - acc: 0.8924 - mae: 0.1001 - mse: 0.0501 - val_loss: 0.3711 - val_acc: 0.8276 - val_mae: 0.1433 - val_mse: 0.0750 Epoch 4/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2407 - acc: 0.8952 - mae: 0.0993 - mse: 0.0495 - val_loss: 0.3701 - val_acc: 0.8332 - val_mae: 0.1468 - val_mse: 0.0752 Epoch 5/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2404 - acc: 0.8929 - mae: 0.0995 - mse: 0.0496 - val_loss: 0.3691 - val_acc: 0.8409 - val_mae: 0.1387 - val_mse: 0.0740 Epoch 6/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2375 - acc: 0.8962 - mae: 0.0976 - mse: 0.0488 - val_loss: 0.3705 - val_acc: 0.8367 - val_mae: 0.1407 - val_mse: 0.0743 Epoch 7/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2360 - acc: 0.8923 - mae: 0.0981 - mse: 0.0491 - val_loss: 0.3755 - val_acc: 0.8360 - val_mae: 0.1370 - val_mse: 0.0748 Epoch 8/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2347 - acc: 0.8962 - mae: 0.0964 - mse: 0.0482 - val_loss: 0.3740 - val_acc: 0.8451 - val_mae: 0.1411 - val_mse: 0.0746 Epoch 9/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2283 - acc: 0.8962 - mae: 0.0954 - mse: 0.0478 - val_loss: 0.3884 - val_acc: 0.8353 - val_mae: 0.1369 - val_mse: 0.0751 Epoch 10/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2257 - acc: 0.9017 - mae: 0.0935 - mse: 0.0469 - val_loss: 0.3705 - val_acc: 0.8395 - val_mae: 0.1339 - val_mse: 0.0737 Epoch 11/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2231 - acc: 0.8997 - mae: 0.0924 - mse: 0.0463 - val_loss: 0.3835 - val_acc: 0.8416 - val_mae: 0.1296 - val_mse: 0.0732 Epoch 12/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2272 - acc: 0.8972 - mae: 0.0946 - mse: 0.0475 - val_loss: 0.3905 - val_acc: 0.8430 - val_mae: 0.1307 - val_mse: 0.0745 Epoch 13/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2156 - acc: 0.9003 - mae: 0.0904 - mse: 0.0453 - val_loss: 0.3855 - val_acc: 0.8465 - val_mae: 0.1280 - val_mse: 0.0729 Epoch 14/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2194 - acc: 0.9041 - mae: 0.0901 - mse: 0.0452 - val_loss: 0.3853 - val_acc: 0.8381 - val_mae: 0.1290 - val_mse: 0.0740 Epoch 15/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2144 - acc: 0.9045 - mae: 0.0893 - mse: 0.0446 - val_loss: 0.3937 - val_acc: 0.8472 - val_mae: 0.1296 - val_mse: 0.0745 Epoch 16/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2119 - acc: 0.9065 - mae: 0.0874 - mse: 0.0438 - val_loss: 0.3989 - val_acc: 0.8458 - val_mae: 0.1282 - val_mse: 0.0749 Epoch 17/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2132 - acc: 0.9031 - mae: 0.0889 - mse: 0.0446 - val_loss: 0.4087 - val_acc: 0.8458 - val_mae: 0.1235 - val_mse: 0.0749 Epoch 18/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2098 - acc: 0.9048 - mae: 0.0872 - mse: 0.0438 - val_loss: 0.3939 - val_acc: 0.8395 - val_mae: 0.1307 - val_mse: 0.0753 Epoch 19/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2060 - acc: 0.9079 - mae: 0.0858 - mse: 0.0429 - val_loss: 0.3988 - val_acc: 0.8493 - val_mae: 0.1234 - val_mse: 0.0736 Epoch 20/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2082 - acc: 0.9059 - mae: 0.0865 - mse: 0.0435 - val_loss: 0.4140 - val_acc: 0.8507 - val_mae: 0.1226 - val_mse: 0.0741 Epoch 21/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2008 - acc: 0.9093 - mae: 0.0833 - mse: 0.0419 - val_loss: 0.4228 - val_acc: 0.8388 - val_mae: 0.1248 - val_mse: 0.0767 Epoch 22/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2061 - acc: 0.9080 - mae: 0.0849 - mse: 0.0426 - val_loss: 0.4233 - val_acc: 0.8500 - val_mae: 0.1232 - val_mse: 0.0751 Epoch 23/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2051 - acc: 0.9044 - mae: 0.0854 - mse: 0.0431 - val_loss: 0.4062 - val_acc: 0.8493 - val_mae: 0.1255 - val_mse: 0.0747 Epoch 24/1000 21/403 [>.............................] - ETA: 0s - loss: 0.2305 - acc: 0.9033 - mae: 0.0996 - mse: 0.0482 --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-40-52ca1206cf0b> in <module>() ----> 1 history = model.fit(X_train, y_train, epochs=1000, shuffle=False, validation_split=0.1, callbacks=[early_stop]) /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs) 106 def _method_wrapper(self, *args, **kwargs): 107 if not self._in_multi_worker_mode(): # pylint: disable=protected-access --> 108 return method(self, *args, **kwargs) 109 110 # Running inside `run_distribute_coordinator` already. /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing) 1096 batch_size=batch_size): 1097 callbacks.on_train_batch_begin(step) -> 1098 tmp_logs = train_function(iterator) 1099 if data_handler.should_sync: 1100 context.async_wait() /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds) 778 else: 779 compiler = \"nonXla\" --> 780 result = self._call(*args, **kwds) 781 782 new_tracing_count = self._get_tracing_count() /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds) 805 # In this case we have created variables on the first call, so we run the 806 # defunned version which is guaranteed to never create variables. --> 807 return self._stateless_fn(*args, **kwds) # pylint: disable=not-callable 808 elif self._stateful_fn is not None: 809 # Release the lock early so that multiple threads can perform the call /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs) 2827 with self._lock: 2828 graph_function, args, kwargs = self._maybe_define_function(args, kwargs) -> 2829 return graph_function._filtered_call(args, kwargs) # pylint: disable=protected-access 2830 2831 @property /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager) 1846 resource_variable_ops.BaseResourceVariable))], 1847 captured_inputs=self.captured_inputs, -> 1848 cancellation_manager=cancellation_manager) 1849 1850 def _call_flat(self, args, captured_inputs, cancellation_manager=None): /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager) 1922 # No tape is watching; skip to running the function. 1923 return self._build_call_outputs(self._inference_function.call( -> 1924 ctx, args, cancellation_manager=cancellation_manager)) 1925 forward_backward = self._select_forward_and_backward_functions( 1926 args, /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager) 548 inputs=args, 549 attrs=attrs, --> 550 ctx=ctx) 551 else: 552 outputs = execute.execute_with_cancellation( /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name) 58 ctx.ensure_initialized() 59 tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, ---> 60 inputs, attrs, num_outputs) 61 except core._NotOkStatusException as e: 62 if name is not None: KeyboardInterrupt: plot_history(history) y_pred = model.predict(X_test) print() print(y_test[:10]) plt.plot(y_test) plt.plot(y_pred.flatten()) [15.10416667 25.24390244 20.91836735 37.47126437 19.53846154] [<matplotlib.lines.Line2D at 0x7f4ce57342e8>] Conclusion \u00b6 Because we are using averge daily data for LSTM model, we found that it may not be able to predict accurate results based on the current volume of dataset. However, DNN model will provide good prediction for both category results as well as regression results","title":"Group project"},{"location":"MSBD5001/project/Group%20Project/#preprocess-data","text":"train = pd . read_csv ( '/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/group project/disney_shanghai.csv' , parse_dates = [ 'Time' ]) test = pd . read_csv ( '/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/group project/disney_shanghai_test.csv' , names = [ 'Time' , 'Facility ID' , 'Name' , 'Wait time' , 'Ride yype' , 'Temperature' , 'Max temperature' , 'Min temperature' , 'Humidity' , 'Pressure' , 'Wind degree' , 'Wind speed' , 'Cloud' , 'Weather' , 'Weather description' ], parse_dates = [ 'Time' ]) print ( train . info ()) print ( test . info ()) <class 'pandas.core.frame.DataFrame'> RangeIndex: 32024 entries, 0 to 32023 Data columns (total 17 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Name 32024 non-null object 1 Ride type 32024 non-null object 2 Time 32024 non-null datetime64[ns, pytz.FixedOffset(480)] 3 Fastpass-avaliable 32024 non-null bool 4 Status 32024 non-null object 5 Wait time 14621 non-null float64 6 Weather 32024 non-null object 7 Weather description 32024 non-null object 8 Temperature 32024 non-null float64 9 Max temperature 32024 non-null float64 10 Min temperature 32024 non-null float64 11 Pressure 32024 non-null int64 12 Humidity 32024 non-null int64 13 Wind degree 32024 non-null float64 14 Wind speed 32024 non-null float64 15 Cloud 32024 non-null int64 16 Visibility 32024 non-null int64 dtypes: bool(1), datetime64[ns, pytz.FixedOffset(480)](1), float64(6), int64(4), object(5) memory usage: 3.9+ MB None <class 'pandas.core.frame.DataFrame'> RangeIndex: 10449 entries, 0 to 10448 Data columns (total 15 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Time 10449 non-null datetime64[ns, UTC] 1 Facility ID 10449 non-null object 2 Name 10449 non-null object 3 Wait time 4016 non-null float64 4 Ride yype 10449 non-null object 5 Temperature 10449 non-null int64 6 Max temperature 10449 non-null float64 7 Min temperature 10449 non-null float64 8 Humidity 10449 non-null int64 9 Pressure 10449 non-null int64 10 Wind degree 10449 non-null int64 11 Wind speed 10449 non-null float64 12 Cloud 10449 non-null int64 13 Weather 10449 non-null object 14 Weather description 10449 non-null object dtypes: datetime64[ns, UTC](1), float64(4), int64(5), object(5) memory usage: 1.2+ MB None show data train = train[['Name', 'Time', 'Wait time', 'Weather', 'Temperature', 'Max temperature', 'Min temperature', 'Pressure', 'Humidity', 'Wind degree', 'Wind speed', 'Cloud']] train.head(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Name Time Wait time Weather Temperature Max temperature Min temperature Pressure Humidity Wind degree Wind speed Cloud 0 Camp Discovery 2020-08-31 13:38:47.638019+08:00 0.0 Clouds 33.28 34.0 34.0 1009 66 110.0 5.0 75 1 Challenge Trails at Camp Discovery 2020-08-31 13:38:47.638035+08:00 10.0 Clouds 33.28 34.0 34.0 1009 66 110.0 5.0 75 show test test = test[['Name', 'Time', 'Wait time', 'Weather', 'Temperature', 'Max temperature', 'Min temperature', 'Pressure', 'Humidity', 'Wind degree', 'Wind speed', 'Cloud']] test.head(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Name Time Wait time Weather Temperature Max temperature Min temperature Pressure Humidity Wind degree Wind speed Cloud 0 Challenge Trails at Camp Discovery 2020-11-14 10:07:34+00:00 30.0 Clouds 18 18.0 18.0 1026 68 70 3.0 20 1 Vista Trail at Camp Discovery 2020-11-14 10:07:34+00:00 0.0 Clouds 18 18.0 18.0 1026 68 70 3.0 20","title":"Preprocess Data"},{"location":"MSBD5001/project/Group%20Project/#change-name-into-values","text":"unique_names = train[['Name']].drop_duplicates() unique_names['id'] = range(len(unique_names)) train = train.merge(unique_names, on='Name') unique_names .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Name id 0 Camp Discovery 0 1 Challenge Trails at Camp Discovery 1 2 Vista Trail at Camp Discovery 2 3 Soaring Over the Horizon 3 4 \u201cOnce Upon a Time\u201d Adventure 4 5 Alice in Wonderland Maze 5 6 Frozen: A Sing-Along Celebration 6 7 Hunny Pot Spin 7 8 Peter Pan\u2019s Flight 8 9 Seven Dwarfs Mine Train 9 10 The Many Adventures of Winnie the Pooh 10 11 Voyage to the Crystal Grotto 11 12 Dumbo the Flying Elephant 12 13 Fantasia Carousel 13 14 TRON Lightcycle Power Run \u2013 Presented by Chevr... 14 15 Stitch Encounter 15 16 Jet Packs 16 17 Buzz Lightyear Planet Rescue 17 18 Siren's Revenge 18 19 Shipwreck Shore 19 20 Pirates of the Caribbean Battle for the Sunken... 20 21 Explorer Canoes 21 22 Eye of the Storm: Captain Jack\u2019s Stunt Spectac... 22 23 Roaring Rapids 23 24 Ignite the Dream - A Nighttime Spectacular of ... 24 25 Mickey\u2019s Storybook Express 25 26 Golden Fairytale Fanfare 26 27 TRON Realm, Chevrolet Digital Challenge 27 28 Rex\u2019s Racer 28 29 Slinky Dog Spin 29 30 Woody\u2019s Roundup 30 31 Marvel Universe 31 32 Club Destin-E 32 33 Color Wall 33 34 Avengers Training Initiative 34 35 Wave Hello to Your Favorite Mickey Avenue Char... 35 36 Mickey Avenue Kiss Goodnight 36 37 Adventurous Friends Exploration 37 38 Catch a Glimpse of Jack Sparrow 38 39 Hundred Acre Wood Character Sighting 39 40 Princess Balcony Greetings 40 41 Avengers Assemble at the E-Stage 41 42 Woody\u2019s Rescue Patrol 42","title":"Change name into values"},{"location":"MSBD5001/project/Group%20Project/#dropna","text":"train = train.dropna() test = test.dropna()","title":"Dropna"},{"location":"MSBD5001/project/Group%20Project/#add-busy-label","text":"def is_busy(time: int): if time < 30: return [1, 0, 0] elif time >= 30 and time <= 70: return [0, 1, 0] else: return [0, 0, 1]","title":"Add busy label"},{"location":"MSBD5001/project/Group%20Project/#add-weekend-and-public-holiday","text":"def hour_modify ( x : datetime ) : Early_Morning = [ 4,5,6,7 ] Morning = [ 8,9,10,11 ] Afternoon = [ 12,13,14,15 ] Evening = [ 16,17,18,19 ] Night = [ 20,21,22,23 ] Late_Night = [ 0,1,2,3 ] if x . hour in Early_Morning : return 'Early_Morning' elif x . hour in Morning : return 'Morning' elif x . hour in Afternoon : return 'Afternoon' elif x . hour in Evening : return 'Evening' elif x . hour in Night : return 'Night' else : return 'Late_Night' def add_holiday_and_weekend ( df : pd . DataFrame , date_field_str = 'date' ) -> pd . DataFrame : \"\"\" Add holiday and weekend to the dataset \"\"\" new_df = df . copy () new_df [ 'IsWeekend' ] = new_df [ date_field_str ] . apply ( lambda x : 0 if x . weekday () in [ 0,1,2,3,4 ] else 1 ) new_df [ 'IsHoliday' ]= new_df [ date_field_str ] . apply ( lambda x : 1 if ( x . date (). strftime ( '%Y-%m-%d' ) in [ '2020-01-01', '2020-01-24', '2020-01-24', '2020-01-25', '2020-01-26', '2020-01-27', '2020-01-28', '2020-01-29', '2020-01-30', '2020-01-31', '2020-02-01', '2020-02-02', '2020-04-04', '2020-05-01', '2020-05-02', '2020-05-03', '2020-05-04', '2020-05-05', '2020-06-25', '2020-06-26', '2020-06-27', '2020-10-01', '2020-10-02', '2020-10-03', '2020-10-04', '2020-10-05', '2020-10-06', '2020-10-07', '2020-10-08', '2020-10-31' ] ) or ( x . weekday () in [ 6 ] ) else 0 ) return new_df convert time zone train [ 'Time' ] = train [ 'Time' ]. dt . tz_localize ( None ) print ( train [ 'Time' ]) 0 2020-08-31 13:38:47.638019 1 2020-08-31 14:09:47.630487 2 2020-08-31 14:26:10.487907 3 2020-08-31 14:29:48.273505 4 2020-08-31 15:09:11.331162 ... 23808 2020-10-24 15:23:47.123270 23809 2020-10-24 16:25:44.947592 23810 2020-10-24 17:23:16.431926 23816 2020-10-25 10:26:28.625480 23817 2020-10-25 11:24:10.125045 Name: Time, Length: 14621, dtype: datetime64[ns] test['Time'] = test['Time'].dt.tz_convert(\"Asia/Shanghai\").dt.tz_localize(None) print(test['Time']) 0 2020-11-14 18:07:34 1 2020-11-14 18:07:34 2 2020-11-14 18:07:34 3 2020-11-14 18:07:34 4 2020-11-14 18:07:34 ... 10340 2020-12-01 19:07:47 10348 2020-12-01 19:07:47 10349 2020-12-01 19:07:47 10350 2020-12-01 19:07:47 10351 2020-12-01 19:07:47 Name: Time, Length: 4016, dtype: datetime64[ns] Add public holidays train = add_holiday_and_weekend ( train , 'Time' ) test = add_holiday_and_weekend ( test , 'Time' ) train [ 'Hour modify' ] = train [ 'Time' ]. apply ( hour_modify ) test [ 'Hour modify' ] = test [ 'Time' ]. apply ( hour_modify ) train [ 'Is busy' ] = train [ 'Wait time' ]. apply ( is_busy ) train . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Name Time Wait time Weather Temperature Max temperature Min temperature Pressure Humidity Wind degree Wind speed Cloud id IsWeekend IsHoliday Hour modify Is busy 0 Camp Discovery 2020-08-31 13:38:47.638019 0.0 Clouds 33.28 34.00 34.00 1009 66 110.0 5.0 75 0 0 0 Afternoon [1, 0, 0] 1 Camp Discovery 2020-08-31 14:09:47.630487 0.0 Clouds 33.80 35.56 35.56 1008 70 110.0 5.0 75 0 0 0 Afternoon [1, 0, 0] 2 Camp Discovery 2020-08-31 14:26:10.487907 0.0 Clouds 34.11 35.56 35.56 1008 66 110.0 6.0 75 0 0 0 Afternoon [1, 0, 0]","title":"Add weekend and public holiday"},{"location":"MSBD5001/project/Group%20Project/#plot-data","text":"plt.figure(figsize=(6,4)) sns.boxplot('Wait time',data=train,orient='h',palette=\"Set3\",linewidth=2.5) plt.show() /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning train[['Wait time', 'Temperature', 'Humidity', 'Wind degree', 'Wind speed', 'Cloud']].describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Wait time Temperature Humidity Wind degree Wind speed Cloud count 14621.000000 14621.000000 14621.000000 14621.000000 14621.000000 14621.000000 mean 23.616374 24.423189 64.933862 127.520347 5.352968 45.041174 std 26.388862 4.075267 14.270916 119.544200 1.962247 31.870902 min 0.000000 14.800000 33.000000 0.000000 0.450000 0.000000 25% 5.000000 21.570000 56.000000 30.000000 4.000000 20.000000 50% 15.000000 23.930000 61.000000 70.000000 5.000000 40.000000 75% 30.000000 26.900000 74.000000 180.000000 7.000000 75.000000 max 195.000000 35.050000 100.000000 360.000000 11.000000 100.000000","title":"Plot Data"},{"location":"MSBD5001/project/Group%20Project/#train","text":"train.columns Index(['Name', 'Time', 'Wait time', 'Weather', 'Temperature', 'Max temperature', 'Min temperature', 'Pressure', 'Humidity', 'Wind degree', 'Wind speed', 'Cloud', 'id', 'IsWeekend', 'IsHoliday', 'Hour modify', 'Is busy'], dtype='object') def plot_history ( history ) : hist = pd . DataFrame ( history . history ) hist [ 'epoch' ] = history . epoch plt . figure () plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Mean Abs Error [MPG]' ) plt . plot ( hist [ 'epoch' ] , hist [ 'mae' ] , label = 'Train Error' ) plt . plot ( hist [ 'epoch' ] , hist [ 'val_mae' ] , label = 'Val Error' ) plt . legend () plt . figure () plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Mean Square Error [$MPG^2$]' ) plt . plot ( hist [ 'epoch' ] , hist [ 'mse' ] , label = 'Train Error' ) plt . plot ( hist [ 'epoch' ] , hist [ 'val_mse' ] , label = 'Val Error' ) plt . legend () plt . show () plt . savefig ( 'MSE.png' ) def generate_training_data ( data : pd . DataFrame , prediction_label , cat_vars =[ 'id', 'IsWeekend','IsHoliday','Hour modify', 'Weather' ] , num_vars =[ 'Temperature', 'Pressure', 'Humidity', 'Cloud', 'Wind degree' ] , should_reshape = True , should_split = True ) : x = train . copy () y = x [ prediction_label ] . to_list () y = np . array ( y ) numeric_transformer = Pipeline ( steps =[ ('scaler', RobustScaler()) ] ) categorical_transformer = Pipeline ( steps =[ ('oneHot',OneHotEncoder(sparse=False)) ] ) preprocessor = ColumnTransformer ( transformers =[ ('num',numeric_transformer,num_vars), ('cat',categorical_transformer,cat_vars) ] ) data_transformed = preprocessor . fit_transform ( x ) if should_split : if should_reshape : y = y . reshape ( - 1 , 1 ) scaler = MinMaxScaler () scaled_y = scaler . fit_transform ( y ) return train_test_split ( data_transformed , scaled_y , test_size = 0.02 , random_state = 42 ), scaler else : return train_test_split ( data_transformed , y , test_size = 0.02 , random_state = 42 ) else : return data_transformed , y data, scaler=generate_training_data(train, 'Wait time') X_train,X_test,y_train,y_test = data print(y_train.shape) print(X_train.shape) (14328, 1) (14328, 44)","title":"Train"},{"location":"MSBD5001/project/Group%20Project/#training-by-using-dnn","text":"early_stop = keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 20 ) model = keras . Sequential ( [ layers . Dense ( 64 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dense ( 512 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dropout ( 0 . 5 ), layers . Dense ( 256 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dropout ( 0 . 5 ), layers . Dense ( 128 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dense ( y_train . shape [ 1 ] ), ] ) opt = keras . optimizers . Adam ( learning_rate = 0 . 001 ) model . compile ( loss = 'mean_squared_error' , optimizer = opt , metrics = [ 'mae' , 'mse' ]) history = model . fit ( X_train , y_train , epochs = 1000 , callbacks =[ early_stop ] , validation_split = 0.2 ) plot_history ( history ) Epoch 1/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0778 - mse: 0.0127 - val_loss: 0.0115 - val_mae: 0.0670 - val_mse: 0.0115 Epoch 2/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0069 - mae: 0.0563 - mse: 0.0069 - val_loss: 0.0065 - val_mae: 0.0515 - val_mse: 0.0065 Epoch 3/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0058 - mae: 0.0510 - mse: 0.0058 - val_loss: 0.0060 - val_mae: 0.0485 - val_mse: 0.0060 Epoch 4/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0052 - mae: 0.0480 - mse: 0.0052 - val_loss: 0.0061 - val_mae: 0.0472 - val_mse: 0.0061 Epoch 5/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0052 - mae: 0.0470 - mse: 0.0052 - val_loss: 0.0055 - val_mae: 0.0464 - val_mse: 0.0055 Epoch 6/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0046 - mae: 0.0449 - mse: 0.0046 - val_loss: 0.0051 - val_mae: 0.0482 - val_mse: 0.0051 Epoch 7/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0046 - mae: 0.0439 - mse: 0.0046 - val_loss: 0.0045 - val_mae: 0.0434 - val_mse: 0.0045 Epoch 8/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0044 - mae: 0.0434 - mse: 0.0044 - val_loss: 0.0053 - val_mae: 0.0485 - val_mse: 0.0053 Epoch 9/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0042 - mae: 0.0423 - mse: 0.0042 - val_loss: 0.0054 - val_mae: 0.0442 - val_mse: 0.0054 Epoch 10/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0039 - mae: 0.0410 - mse: 0.0039 - val_loss: 0.0049 - val_mae: 0.0448 - val_mse: 0.0049 Epoch 11/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0040 - mae: 0.0410 - mse: 0.0040 - val_loss: 0.0041 - val_mae: 0.0422 - val_mse: 0.0041 Epoch 12/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0039 - mae: 0.0409 - mse: 0.0039 - val_loss: 0.0043 - val_mae: 0.0416 - val_mse: 0.0043 Epoch 13/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0037 - mae: 0.0399 - mse: 0.0037 - val_loss: 0.0041 - val_mae: 0.0415 - val_mse: 0.0041 Epoch 14/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0038 - mae: 0.0403 - mse: 0.0038 - val_loss: 0.0040 - val_mae: 0.0432 - val_mse: 0.0040 Epoch 15/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0034 - mae: 0.0390 - mse: 0.0034 - val_loss: 0.0039 - val_mae: 0.0398 - val_mse: 0.0039 Epoch 16/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0034 - mae: 0.0383 - mse: 0.0034 - val_loss: 0.0038 - val_mae: 0.0405 - val_mse: 0.0038 Epoch 17/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0034 - mae: 0.0382 - mse: 0.0034 - val_loss: 0.0038 - val_mae: 0.0391 - val_mse: 0.0038 Epoch 18/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0034 - mae: 0.0381 - mse: 0.0034 - val_loss: 0.0038 - val_mae: 0.0386 - val_mse: 0.0038 Epoch 19/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0032 - mae: 0.0372 - mse: 0.0032 - val_loss: 0.0039 - val_mae: 0.0427 - val_mse: 0.0039 Epoch 20/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0031 - mae: 0.0372 - mse: 0.0031 - val_loss: 0.0039 - val_mae: 0.0397 - val_mse: 0.0039 Epoch 21/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0032 - mae: 0.0368 - mse: 0.0032 - val_loss: 0.0039 - val_mae: 0.0407 - val_mse: 0.0039 Epoch 22/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0029 - mae: 0.0358 - mse: 0.0029 - val_loss: 0.0038 - val_mae: 0.0393 - val_mse: 0.0038 Epoch 23/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0029 - mae: 0.0359 - mse: 0.0029 - val_loss: 0.0035 - val_mae: 0.0384 - val_mse: 0.0035 Epoch 24/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0029 - mae: 0.0352 - mse: 0.0029 - val_loss: 0.0037 - val_mae: 0.0403 - val_mse: 0.0037 Epoch 25/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0028 - mae: 0.0349 - mse: 0.0028 - val_loss: 0.0035 - val_mae: 0.0379 - val_mse: 0.0035 Epoch 26/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0029 - mae: 0.0351 - mse: 0.0029 - val_loss: 0.0039 - val_mae: 0.0395 - val_mse: 0.0039 Epoch 27/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0028 - mae: 0.0349 - mse: 0.0028 - val_loss: 0.0035 - val_mae: 0.0362 - val_mse: 0.0035 Epoch 28/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0027 - mae: 0.0342 - mse: 0.0027 - val_loss: 0.0042 - val_mae: 0.0387 - val_mse: 0.0042 Epoch 29/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0029 - mae: 0.0354 - mse: 0.0029 - val_loss: 0.0037 - val_mae: 0.0376 - val_mse: 0.0037 Epoch 30/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0027 - mae: 0.0344 - mse: 0.0027 - val_loss: 0.0037 - val_mae: 0.0397 - val_mse: 0.0037 Epoch 31/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0027 - mae: 0.0342 - mse: 0.0027 - val_loss: 0.0036 - val_mae: 0.0371 - val_mse: 0.0036 Epoch 32/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0026 - mae: 0.0339 - mse: 0.0026 - val_loss: 0.0037 - val_mae: 0.0365 - val_mse: 0.0037 Epoch 33/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0026 - mae: 0.0337 - mse: 0.0026 - val_loss: 0.0036 - val_mae: 0.0364 - val_mse: 0.0036 Epoch 34/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0326 - mse: 0.0024 - val_loss: 0.0036 - val_mae: 0.0373 - val_mse: 0.0036 Epoch 35/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0025 - mae: 0.0329 - mse: 0.0025 - val_loss: 0.0036 - val_mae: 0.0367 - val_mse: 0.0036 Epoch 36/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0321 - mse: 0.0024 - val_loss: 0.0036 - val_mae: 0.0363 - val_mse: 0.0036 Epoch 37/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0322 - mse: 0.0024 - val_loss: 0.0034 - val_mae: 0.0363 - val_mse: 0.0034 Epoch 38/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0323 - mse: 0.0024 - val_loss: 0.0036 - val_mae: 0.0354 - val_mse: 0.0036 Epoch 39/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0025 - mae: 0.0327 - mse: 0.0025 - val_loss: 0.0036 - val_mae: 0.0360 - val_mse: 0.0036 Epoch 40/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0023 - mae: 0.0319 - mse: 0.0023 - val_loss: 0.0034 - val_mae: 0.0360 - val_mse: 0.0034 Epoch 41/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0318 - mse: 0.0024 - val_loss: 0.0034 - val_mae: 0.0358 - val_mse: 0.0034 Epoch 42/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0023 - mae: 0.0314 - mse: 0.0023 - val_loss: 0.0036 - val_mae: 0.0366 - val_mse: 0.0036 Epoch 43/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0022 - mae: 0.0312 - mse: 0.0022 - val_loss: 0.0034 - val_mae: 0.0361 - val_mse: 0.0034 Epoch 44/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0022 - mae: 0.0312 - mse: 0.0022 - val_loss: 0.0035 - val_mae: 0.0354 - val_mse: 0.0035 Epoch 45/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0023 - mae: 0.0314 - mse: 0.0023 - val_loss: 0.0035 - val_mae: 0.0386 - val_mse: 0.0035 Epoch 46/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0022 - mae: 0.0310 - mse: 0.0022 - val_loss: 0.0035 - val_mae: 0.0359 - val_mse: 0.0035 Epoch 47/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0022 - mae: 0.0313 - mse: 0.0022 - val_loss: 0.0038 - val_mae: 0.0374 - val_mse: 0.0038 Epoch 48/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0022 - mae: 0.0308 - mse: 0.0022 - val_loss: 0.0034 - val_mae: 0.0350 - val_mse: 0.0034 Epoch 49/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0022 - mae: 0.0307 - mse: 0.0022 - val_loss: 0.0035 - val_mae: 0.0357 - val_mse: 0.0035 Epoch 50/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0021 - mae: 0.0305 - mse: 0.0021 - val_loss: 0.0035 - val_mae: 0.0381 - val_mse: 0.0035 Epoch 51/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0022 - mae: 0.0309 - mse: 0.0022 - val_loss: 0.0034 - val_mae: 0.0357 - val_mse: 0.0034 Epoch 52/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0021 - mae: 0.0304 - mse: 0.0021 - val_loss: 0.0034 - val_mae: 0.0347 - val_mse: 0.0034 Epoch 53/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0022 - mae: 0.0304 - mse: 0.0022 - val_loss: 0.0035 - val_mae: 0.0354 - val_mse: 0.0035 Epoch 54/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0021 - mae: 0.0301 - mse: 0.0021 - val_loss: 0.0033 - val_mae: 0.0366 - val_mse: 0.0033 Epoch 55/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0021 - mae: 0.0301 - mse: 0.0021 - val_loss: 0.0034 - val_mae: 0.0348 - val_mse: 0.0034 Epoch 56/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0019 - mae: 0.0288 - mse: 0.0019 - val_loss: 0.0033 - val_mae: 0.0347 - val_mse: 0.0033 Epoch 57/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0298 - mse: 0.0020 - val_loss: 0.0035 - val_mae: 0.0361 - val_mse: 0.0035 Epoch 58/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0021 - mae: 0.0299 - mse: 0.0021 - val_loss: 0.0034 - val_mae: 0.0353 - val_mse: 0.0034 Epoch 59/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0021 - mae: 0.0299 - mse: 0.0021 - val_loss: 0.0032 - val_mae: 0.0361 - val_mse: 0.0032 Epoch 60/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0021 - mae: 0.0296 - mse: 0.0021 - val_loss: 0.0033 - val_mae: 0.0344 - val_mse: 0.0033 Epoch 61/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0290 - mse: 0.0020 - val_loss: 0.0033 - val_mae: 0.0346 - val_mse: 0.0033 Epoch 62/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0297 - mse: 0.0020 - val_loss: 0.0033 - val_mae: 0.0351 - val_mse: 0.0033 Epoch 63/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0294 - mse: 0.0020 - val_loss: 0.0034 - val_mae: 0.0352 - val_mse: 0.0034 Epoch 64/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0294 - mse: 0.0020 - val_loss: 0.0034 - val_mae: 0.0360 - val_mse: 0.0034 Epoch 65/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0292 - mse: 0.0020 - val_loss: 0.0035 - val_mae: 0.0383 - val_mse: 0.0035 Epoch 66/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0295 - mse: 0.0020 - val_loss: 0.0034 - val_mae: 0.0348 - val_mse: 0.0034 Epoch 67/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0292 - mse: 0.0020 - val_loss: 0.0034 - val_mae: 0.0346 - val_mse: 0.0034 Epoch 68/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0019 - mae: 0.0287 - mse: 0.0019 - val_loss: 0.0033 - val_mae: 0.0347 - val_mse: 0.0033 Epoch 69/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0284 - mse: 0.0018 - val_loss: 0.0032 - val_mae: 0.0348 - val_mse: 0.0032 Epoch 70/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0019 - mae: 0.0289 - mse: 0.0019 - val_loss: 0.0034 - val_mae: 0.0355 - val_mse: 0.0034 Epoch 71/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0019 - mae: 0.0290 - mse: 0.0019 - val_loss: 0.0035 - val_mae: 0.0372 - val_mse: 0.0035 Epoch 72/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0019 - mae: 0.0284 - mse: 0.0019 - val_loss: 0.0034 - val_mae: 0.0343 - val_mse: 0.0034 Epoch 73/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0283 - mse: 0.0018 - val_loss: 0.0033 - val_mae: 0.0364 - val_mse: 0.0033 Epoch 74/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0019 - mae: 0.0289 - mse: 0.0019 - val_loss: 0.0033 - val_mae: 0.0364 - val_mse: 0.0033 Epoch 75/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0282 - mse: 0.0018 - val_loss: 0.0034 - val_mae: 0.0345 - val_mse: 0.0034 Epoch 76/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0019 - mae: 0.0285 - mse: 0.0019 - val_loss: 0.0033 - val_mae: 0.0344 - val_mse: 0.0033 Epoch 77/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0019 - mae: 0.0283 - mse: 0.0019 - val_loss: 0.0034 - val_mae: 0.0352 - val_mse: 0.0034 Epoch 78/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0278 - mse: 0.0018 - val_loss: 0.0034 - val_mae: 0.0355 - val_mse: 0.0034 Epoch 79/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0277 - mse: 0.0018 - val_loss: 0.0034 - val_mae: 0.0362 - val_mse: 0.0034 Epoch 80/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0017 - mae: 0.0277 - mse: 0.0017 - val_loss: 0.0033 - val_mae: 0.0361 - val_mse: 0.0033 Epoch 81/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0277 - mse: 0.0018 - val_loss: 0.0034 - val_mae: 0.0369 - val_mse: 0.0034 Epoch 82/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0282 - mse: 0.0018 - val_loss: 0.0034 - val_mae: 0.0352 - val_mse: 0.0034 Epoch 83/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0276 - mse: 0.0018 - val_loss: 0.0035 - val_mae: 0.0354 - val_mse: 0.0035 Epoch 84/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0282 - mse: 0.0018 - val_loss: 0.0034 - val_mae: 0.0343 - val_mse: 0.0034 Epoch 85/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0019 - mae: 0.0280 - mse: 0.0019 - val_loss: 0.0035 - val_mae: 0.0375 - val_mse: 0.0035 Epoch 86/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0017 - mae: 0.0273 - mse: 0.0017 - val_loss: 0.0035 - val_mae: 0.0352 - val_mse: 0.0035 Epoch 87/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0277 - mse: 0.0018 - val_loss: 0.0034 - val_mae: 0.0348 - val_mse: 0.0034 Epoch 88/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0017 - mae: 0.0273 - mse: 0.0017 - val_loss: 0.0033 - val_mae: 0.0346 - val_mse: 0.0033 Epoch 89/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0276 - mse: 0.0018 - val_loss: 0.0034 - val_mae: 0.0354 - val_mse: 0.0034 y_pred = model.predict(X_test) err=mean_squared_error(y_pred,y_test) y_pred_ori = scaler.inverse_transform(y_pred) y_test_ori = scaler.inverse_transform(y_test) print(err) print(y_pred_ori[:10]) print(y_test_ori[:10]) 0.0023450270504444958 [[ 7.5200114] [36.162247 ] [-1.5508894] [11.4153385] [30.12201 ] [ 4.0796814] [ 2.721789 ] [43.332848 ] [23.49451 ] [47.286026 ]] [[10.] [30.] [ 0.] [15.] [15.] [ 5.] [ 5.] [60.] [10.] [50.]] diff = y_test_ori - y_pred_ori plt.figure(figsize=(20, 10)) plt.plot(y_pred_ori, label=\"Prediction\") plt.plot(y_test_ori, label=\"True\") plt.plot(diff, label=\"Difference\") plt.ylabel('Waiting time') plt.xlabel('Day') plt.legend() plt.savefig('DNN_regresion.png')","title":"Training by using DNN"},{"location":"MSBD5001/project/Group%20Project/#training-on-category","text":"in this case, we will pre-process the wait time into 3 different categories. [1, 0, 0]: Wait time < 30 mins [0, 1, 0]: Wait time >= 30 and <= 70 cat_vars = [ 'Name' , 'IsWeekend' , 'IsHoliday' , 'Hour modify' , 'Weather' ] num_vars = [ 'Temperature' , 'Pressure' , 'Humidity' , 'Cloud' , 'Wind degree' , ] X_train , X_test , y_train , y_test = generate_training_data ( train , 'Is busy' , cat_vars = cat_vars , num_vars = num_vars , should_reshape = False ) print ( y_train . shape ) print ( X_train . shape ) (14328, 3) (14328, 44) early_stop = keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 20 ) model = keras . Sequential ( [ layers . Dense ( 64 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dense ( 512 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dropout ( 0 . 5 ), layers . Dense ( 256 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dropout ( 0 . 5 ), layers . Dense ( 128 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dense ( y_train . shape [ 1 ], activation = 'softmax' ), ] ) opt = keras . optimizers . Adam ( learning_rate = 0 . 001 ) model . compile ( loss = 'categorical_crossentropy' , optimizer = opt , metrics = [ 'acc' , 'mae' , 'mse' ]) history = model . fit ( X_train , y_train , epochs = 1000 , callbacks =[ early_stop ] , validation_split = 0.2 ) plot_history ( history ) Epoch 1/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2155 - acc: 0.9014 - mae: 0.0904 - mse: 0.0453 - val_loss: 0.4462 - val_acc: 0.8472 - val_mae: 0.1254 - val_mse: 0.0757 Epoch 2/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2120 - acc: 0.9025 - mae: 0.0880 - mse: 0.0441 - val_loss: 0.4410 - val_acc: 0.8381 - val_mae: 0.1258 - val_mse: 0.0754 Epoch 3/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2112 - acc: 0.9035 - mae: 0.0883 - mse: 0.0443 - val_loss: 0.4480 - val_acc: 0.8416 - val_mae: 0.1229 - val_mse: 0.0752 Epoch 4/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2063 - acc: 0.9074 - mae: 0.0856 - mse: 0.0429 - val_loss: 0.4622 - val_acc: 0.8458 - val_mae: 0.1219 - val_mse: 0.0739 Epoch 5/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2055 - acc: 0.9060 - mae: 0.0859 - mse: 0.0430 - val_loss: 0.4529 - val_acc: 0.8423 - val_mae: 0.1225 - val_mse: 0.0752 Epoch 6/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1968 - acc: 0.9093 - mae: 0.0822 - mse: 0.0413 - val_loss: 0.4663 - val_acc: 0.8458 - val_mae: 0.1188 - val_mse: 0.0763 Epoch 7/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2024 - acc: 0.9108 - mae: 0.0838 - mse: 0.0419 - val_loss: 0.5039 - val_acc: 0.8503 - val_mae: 0.1188 - val_mse: 0.0762 Epoch 8/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2025 - acc: 0.9101 - mae: 0.0841 - mse: 0.0419 - val_loss: 0.5043 - val_acc: 0.8486 - val_mae: 0.1139 - val_mse: 0.0758 Epoch 9/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2047 - acc: 0.9053 - mae: 0.0851 - mse: 0.0432 - val_loss: 0.4379 - val_acc: 0.8514 - val_mae: 0.1199 - val_mse: 0.0717 Epoch 10/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1961 - acc: 0.9114 - mae: 0.0826 - mse: 0.0412 - val_loss: 0.4864 - val_acc: 0.8479 - val_mae: 0.1186 - val_mse: 0.0759 Epoch 11/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1993 - acc: 0.9097 - mae: 0.0819 - mse: 0.0412 - val_loss: 0.4532 - val_acc: 0.8465 - val_mae: 0.1209 - val_mse: 0.0736 Epoch 12/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1934 - acc: 0.9108 - mae: 0.0816 - mse: 0.0409 - val_loss: 0.4550 - val_acc: 0.8548 - val_mae: 0.1178 - val_mse: 0.0731 Epoch 13/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1870 - acc: 0.9135 - mae: 0.0788 - mse: 0.0394 - val_loss: 0.4832 - val_acc: 0.8416 - val_mae: 0.1207 - val_mse: 0.0762 Epoch 14/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1905 - acc: 0.9126 - mae: 0.0793 - mse: 0.0398 - val_loss: 0.5012 - val_acc: 0.8486 - val_mae: 0.1182 - val_mse: 0.0759 Epoch 15/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1882 - acc: 0.9163 - mae: 0.0781 - mse: 0.0393 - val_loss: 0.5033 - val_acc: 0.8503 - val_mae: 0.1180 - val_mse: 0.0750 Epoch 16/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1890 - acc: 0.9164 - mae: 0.0788 - mse: 0.0395 - val_loss: 0.4769 - val_acc: 0.8500 - val_mae: 0.1179 - val_mse: 0.0747 Epoch 17/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1904 - acc: 0.9094 - mae: 0.0799 - mse: 0.0402 - val_loss: 0.5425 - val_acc: 0.8524 - val_mae: 0.1129 - val_mse: 0.0751 Epoch 18/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1865 - acc: 0.9167 - mae: 0.0763 - mse: 0.0386 - val_loss: 0.4673 - val_acc: 0.8510 - val_mae: 0.1175 - val_mse: 0.0738 Epoch 19/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1857 - acc: 0.9156 - mae: 0.0769 - mse: 0.0386 - val_loss: 0.4718 - val_acc: 0.8458 - val_mae: 0.1176 - val_mse: 0.0735 Epoch 20/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1864 - acc: 0.9157 - mae: 0.0776 - mse: 0.0390 - val_loss: 0.5050 - val_acc: 0.8423 - val_mae: 0.1175 - val_mse: 0.0760 Epoch 21/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1795 - acc: 0.9187 - mae: 0.0753 - mse: 0.0378 - val_loss: 0.4781 - val_acc: 0.8514 - val_mae: 0.1176 - val_mse: 0.0739 Epoch 22/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1805 - acc: 0.9169 - mae: 0.0750 - mse: 0.0379 - val_loss: 0.4897 - val_acc: 0.8489 - val_mae: 0.1166 - val_mse: 0.0754 Epoch 23/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1818 - acc: 0.9181 - mae: 0.0757 - mse: 0.0378 - val_loss: 0.5106 - val_acc: 0.8510 - val_mae: 0.1143 - val_mse: 0.0748 Epoch 24/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1819 - acc: 0.9193 - mae: 0.0748 - mse: 0.0378 - val_loss: 0.5458 - val_acc: 0.8440 - val_mae: 0.1169 - val_mse: 0.0771 Epoch 25/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1768 - acc: 0.9211 - mae: 0.0736 - mse: 0.0367 - val_loss: 0.5249 - val_acc: 0.8419 - val_mae: 0.1169 - val_mse: 0.0773 Epoch 26/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1752 - acc: 0.9232 - mae: 0.0721 - mse: 0.0362 - val_loss: 0.5045 - val_acc: 0.8475 - val_mae: 0.1176 - val_mse: 0.0743 Epoch 27/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1733 - acc: 0.9235 - mae: 0.0722 - mse: 0.0361 - val_loss: 0.5165 - val_acc: 0.8444 - val_mae: 0.1163 - val_mse: 0.0761 Epoch 28/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1762 - acc: 0.9205 - mae: 0.0729 - mse: 0.0366 - val_loss: 0.4968 - val_acc: 0.8479 - val_mae: 0.1153 - val_mse: 0.0741 Epoch 29/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.1708 - acc: 0.9238 - mae: 0.0709 - mse: 0.0353 - val_loss: 0.5015 - val_acc: 0.8454 - val_mae: 0.1162 - val_mse: 0.0761 <Figure size 432x288 with 0 Axes> y_pred = model . predict ( X_test ) def print_acc ( pred , true ) : right = 0 for i in range ( len ( pred )) : p = np . argmax ( pred [ i ] ) t = np . argmax ( true [ i ] ) if p == t : right += 1 print ( f \"acc: {right}/{len(pred)}, {right/len(pred)}\" ) print_acc ( y_pred , y_test ) acc: 250/293, 0.8532423208191127","title":"Training on category"},{"location":"MSBD5001/project/Group%20Project/#training-by-using-category","text":"In this example, we will use average-wait-time to compute their waiting time's category If wait-time is less than the average-wait time of that facility, then it is not busy. If wait-time is around average wait-time, then it is in a normal situation If wait-time is above average-wait-time, then it is busy. In this model, we are basically assume facilities are different. Some will have more popularity than others. And we will use the model to predict whether a model is above its average or not. def is_busy_2 ( row ): if row [ 3 ] < row [ 1 ] - 5 : return [ 1 , 0 , 0 ] elif row [ 3 ] >= row [ 1 ] - 5 and row [ 3 ] <= row [ 1 ] + 5 : return [ 0 , 1 , 0 ] else : return [ 0 , 0 , 1 ] def calculate_average_wait_time ( df : pd . DataFrame ) -> pd . DataFrame : new_df = df . copy () new_df2 = df . copy () new_df [ 'tmp_date' ] = new_df [ 'Time' ]. apply ( lambda x : x . date ()) new_df = new_df [[ 'tmp_date' , 'Wait time' , 'Name' ]] new_df = new_df . groupby ([ 'Name' ]). mean () new_df = new_df . rename ( columns = { 'Wait time' : 'Average wait time' } ) new_df = new_df . merge ( new_df2 , on = 'Name' , how = 'right' ) return new_df average_sampled_train = calculate_average_wait_time ( train ) average_sampled_train [ 'Is busy' ] = average_sampled_train . apply ( is_busy_2 , axis = 1 ) average_sampled_train . sample ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Name Average wait time Time Wait time Weather Temperature Max temperature Min temperature Pressure Humidity Wind degree Wind speed Cloud id IsWeekend IsHoliday Hour modify Is busy 6302 Dumbo the Flying Elephant 24.476950 2020-10-03 17:19:47.471631 40.0 Clouds 23.73 24.00 24.00 1012 78 90.0 6.0 75 12 1 1 Evening [0, 0, 1] 10102 Siren's Revenge 5.000000 2020-10-21 13:22:34.657099 5.0 Clouds 18.36 19.44 19.44 1018 88 340.0 5.0 90 18 0 0 Afternoon [0, 1, 0] 5149 The Many Adventures of Winnie the Pooh 21.727129 2020-10-08 13:17:12.792022 20.0 Clouds 23.83 25.00 25.00 1022 53 20.0 9.0 75 10 0 1 Afternoon [0, 1, 0] 8802 Jet Packs 34.943639 2020-10-17 13:21:49.642192 40.0 Clouds 21.54 22.00 22.00 1023 56 10.0 5.0 20 16 1 0 Afternoon [0, 0, 1] 12292 Rex\u2019s Racer 56.937984 2020-09-04 13:17:48.730671 90.0 Clouds 33.22 36.67 36.67 1012 45 170.0 6.0 91 28 0 0 Afternoon [0, 0, 1] 4527 Seven Dwarfs Mine Train 52.788310 2020-10-09 11:21:36.066646 45.0 Clouds 23.20 24.00 24.00 1021 53 10.0 6.0 23 9 0 0 Morning [1, 0, 0] 13143 Slinky Dog Spin 24.613601 2020-09-22 16:21:22.856577 20.0 Clouds 24.80 25.00 25.00 1014 61 60.0 6.0 40 29 0 0 Evening [0, 1, 0] 5309 The Many Adventures of Winnie the Pooh 21.727129 2020-10-22 17:23:25.739682 40.0 Clouds 19.46 20.00 20.00 1016 68 10.0 8.0 40 10 0 0 Evening [0, 0, 1] 5307 The Many Adventures of Winnie the Pooh 21.727129 2020-10-22 15:23:48.253335 10.0 Clouds 22.51 23.33 23.33 1016 56 340.0 5.0 13 10 0 0 Afternoon [1, 0, 0] 12042 Roaring Rapids 31.629555 2020-09-25 21:18:18.834521 5.0 Rain 22.66 23.00 23.00 1014 69 10.0 5.0 100 23 0 0 Night [1, 0, 0] 5539 Voyage to the Crystal Grotto 17.301459 2020-09-18 17:18:57.544340 40.0 Clouds 19.43 20.00 20.00 1018 88 270.0 3.0 40 11 0 0 Evening [0, 0, 1] 11842 Roaring Rapids 31.629555 2020-09-08 14:21:00.395020 20.0 Clouds 33.61 35.00 35.00 1010 49 180.0 8.0 20 23 0 0 Afternoon [1, 0, 0] 6323 Dumbo the Flying Elephant 24.476950 2020-10-05 17:20:52.785435 30.0 Clouds 20.67 21.11 21.11 1020 52 10.0 8.0 36 12 0 1 Evening [0, 0, 1] 11943 Roaring Rapids 31.629555 2020-09-17 16:20:41.428588 25.0 Rain 21.05 21.67 21.67 1010 100 350.0 7.0 75 23 0 0 Evening [1, 0, 0] 5281 The Many Adventures of Winnie the Pooh 21.727129 2020-10-20 10:24:19.155382 20.0 Clouds 21.59 22.00 22.00 1025 53 50.0 6.0 40 10 0 0 Morning [0, 1, 0] 9435 Buzz Lightyear Planet Rescue 7.097039 2020-10-19 15:24:32.051678 10.0 Clouds 21.24 23.33 23.33 1023 56 30.0 5.0 40 17 0 0 Afternoon [0, 1, 0] 1890 \u201cOnce Upon a Time\u201d Adventure 7.246127 2020-09-17 10:21:16.322591 5.0 Rain 22.64 23.00 23.00 1010 88 40.0 7.0 75 4 0 0 Morning [0, 1, 0] 9942 Siren's Revenge 5.000000 2020-10-07 17:21:19.897009 5.0 Clouds 21.37 22.00 22.00 1021 56 20.0 8.0 20 18 0 1 Evening [0, 1, 0] 2997 Hunny Pot Spin 9.258114 2020-09-12 09:22:28.558742 5.0 Clouds 26.08 27.78 27.78 1013 78 330.0 3.0 20 7 1 0 Morning [0, 1, 0] 13470 Slinky Dog Spin 24.613601 2020-10-19 12:25:41.694352 40.0 Clouds 20.87 22.78 22.78 1024 56 20.0 6.0 40 29 0 0 Afternoon [0, 0, 1] cat_vars = [ 'Name' , 'IsWeekend' , 'IsHoliday' , 'Hour modify' , 'Weather' ] num_vars = [ 'Temperature' , 'Pressure' , 'Humidity' , 'Cloud' , 'Wind degree' , ] X_train , X_test , y_train , y_test = generate_training_data ( average_sampled_train , 'Is busy' , cat_vars = cat_vars , num_vars = num_vars , should_reshape = False ) early_stop = keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 20 ) model = keras . Sequential ( [ layers . Dense ( 64 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dense ( 512 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dropout ( 0 . 5 ), layers . Dense ( 256 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dropout ( 0 . 5 ), # layers . Dense ( 512 , activation = \"relu\" , # input_shape = ( X_train . shape [ 1 ],),), # layers . Dropout ( 0 . 5 ), # layers . Dense ( 512 , activation = \"relu\" , # input_shape = ( X_train . shape [ 1 ],),), # layers . Dropout ( 0 . 5 ), # layers . Dense ( 256 , activation = \"relu\" , # input_shape = ( X_train . shape [ 1 ],),), # layers . Dropout ( 0 . 5 ), layers . Dense ( 128 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dense ( y_train . shape [ 1 ], activation = 'softmax' ), ] ) opt = keras . optimizers . Adam ( learning_rate = 0 . 001 ) model . compile ( loss = 'categorical_crossentropy' , optimizer = opt , metrics = [ 'acc' , 'mae' , 'mse' ]) history = model . fit ( X_train , y_train , epochs = 1000 , callbacks =[ early_stop ] , validation_split = 0.2 ) plot_history ( history ) Epoch 1/1000 359/359 [==============================] - 1s 4ms/step - loss: 0.5742 - acc: 0.7383 - mae: 0.2304 - mse: 0.1161 - val_loss: 0.4259 - val_acc: 0.8102 - val_mae: 0.1846 - val_mse: 0.0883 Epoch 2/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.4323 - acc: 0.7988 - mae: 0.1772 - mse: 0.0895 - val_loss: 0.4156 - val_acc: 0.8123 - val_mae: 0.1713 - val_mse: 0.0858 Epoch 3/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.4067 - acc: 0.8137 - mae: 0.1683 - mse: 0.0848 - val_loss: 0.4012 - val_acc: 0.8158 - val_mae: 0.1741 - val_mse: 0.0836 Epoch 4/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3872 - acc: 0.8274 - mae: 0.1599 - mse: 0.0805 - val_loss: 0.3918 - val_acc: 0.8266 - val_mae: 0.1623 - val_mse: 0.0805 Epoch 5/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3733 - acc: 0.8318 - mae: 0.1553 - mse: 0.0777 - val_loss: 0.3877 - val_acc: 0.8200 - val_mae: 0.1607 - val_mse: 0.0801 Epoch 6/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3590 - acc: 0.8396 - mae: 0.1489 - mse: 0.0750 - val_loss: 0.3819 - val_acc: 0.8248 - val_mae: 0.1600 - val_mse: 0.0793 Epoch 7/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3567 - acc: 0.8396 - mae: 0.1478 - mse: 0.0742 - val_loss: 0.3821 - val_acc: 0.8287 - val_mae: 0.1570 - val_mse: 0.0788 Epoch 8/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3472 - acc: 0.8437 - mae: 0.1445 - mse: 0.0725 - val_loss: 0.3714 - val_acc: 0.8339 - val_mae: 0.1493 - val_mse: 0.0763 Epoch 9/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3415 - acc: 0.8475 - mae: 0.1421 - mse: 0.0711 - val_loss: 0.3853 - val_acc: 0.8308 - val_mae: 0.1499 - val_mse: 0.0786 Epoch 10/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3323 - acc: 0.8504 - mae: 0.1386 - mse: 0.0694 - val_loss: 0.3726 - val_acc: 0.8374 - val_mae: 0.1404 - val_mse: 0.0752 Epoch 11/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3264 - acc: 0.8540 - mae: 0.1360 - mse: 0.0681 - val_loss: 0.3828 - val_acc: 0.8381 - val_mae: 0.1456 - val_mse: 0.0760 Epoch 12/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3233 - acc: 0.8567 - mae: 0.1334 - mse: 0.0671 - val_loss: 0.3683 - val_acc: 0.8353 - val_mae: 0.1471 - val_mse: 0.0748 Epoch 13/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3184 - acc: 0.8574 - mae: 0.1322 - mse: 0.0663 - val_loss: 0.3639 - val_acc: 0.8454 - val_mae: 0.1422 - val_mse: 0.0734 Epoch 14/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3033 - acc: 0.8622 - mae: 0.1269 - mse: 0.0631 - val_loss: 0.3802 - val_acc: 0.8398 - val_mae: 0.1372 - val_mse: 0.0756 Epoch 15/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.3044 - acc: 0.8618 - mae: 0.1260 - mse: 0.0634 - val_loss: 0.3642 - val_acc: 0.8391 - val_mae: 0.1368 - val_mse: 0.0728 Epoch 16/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2985 - acc: 0.8649 - mae: 0.1247 - mse: 0.0624 - val_loss: 0.3836 - val_acc: 0.8364 - val_mae: 0.1338 - val_mse: 0.0762 Epoch 17/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2926 - acc: 0.8694 - mae: 0.1215 - mse: 0.0611 - val_loss: 0.3739 - val_acc: 0.8423 - val_mae: 0.1384 - val_mse: 0.0740 Epoch 18/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2936 - acc: 0.8684 - mae: 0.1221 - mse: 0.0611 - val_loss: 0.3752 - val_acc: 0.8385 - val_mae: 0.1329 - val_mse: 0.0732 Epoch 19/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2824 - acc: 0.8709 - mae: 0.1180 - mse: 0.0591 - val_loss: 0.3726 - val_acc: 0.8437 - val_mae: 0.1309 - val_mse: 0.0733 Epoch 20/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2793 - acc: 0.8775 - mae: 0.1155 - mse: 0.0581 - val_loss: 0.3680 - val_acc: 0.8378 - val_mae: 0.1411 - val_mse: 0.0732 Epoch 21/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2773 - acc: 0.8749 - mae: 0.1157 - mse: 0.0577 - val_loss: 0.3865 - val_acc: 0.8451 - val_mae: 0.1316 - val_mse: 0.0735 Epoch 22/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2718 - acc: 0.8777 - mae: 0.1130 - mse: 0.0566 - val_loss: 0.3870 - val_acc: 0.8398 - val_mae: 0.1293 - val_mse: 0.0748 Epoch 23/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2668 - acc: 0.8810 - mae: 0.1110 - mse: 0.0558 - val_loss: 0.3765 - val_acc: 0.8465 - val_mae: 0.1308 - val_mse: 0.0730 Epoch 24/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2646 - acc: 0.8838 - mae: 0.1099 - mse: 0.0550 - val_loss: 0.3926 - val_acc: 0.8364 - val_mae: 0.1301 - val_mse: 0.0740 Epoch 25/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2581 - acc: 0.8841 - mae: 0.1073 - mse: 0.0538 - val_loss: 0.3855 - val_acc: 0.8493 - val_mae: 0.1284 - val_mse: 0.0728 Epoch 26/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2576 - acc: 0.8854 - mae: 0.1070 - mse: 0.0537 - val_loss: 0.3911 - val_acc: 0.8440 - val_mae: 0.1263 - val_mse: 0.0743 Epoch 27/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2541 - acc: 0.8856 - mae: 0.1053 - mse: 0.0531 - val_loss: 0.3986 - val_acc: 0.8447 - val_mae: 0.1278 - val_mse: 0.0740 Epoch 28/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2489 - acc: 0.8878 - mae: 0.1038 - mse: 0.0521 - val_loss: 0.4028 - val_acc: 0.8447 - val_mae: 0.1259 - val_mse: 0.0744 Epoch 29/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2455 - acc: 0.8897 - mae: 0.1019 - mse: 0.0513 - val_loss: 0.4089 - val_acc: 0.8468 - val_mae: 0.1248 - val_mse: 0.0732 Epoch 30/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2400 - acc: 0.8934 - mae: 0.1000 - mse: 0.0500 - val_loss: 0.4043 - val_acc: 0.8409 - val_mae: 0.1237 - val_mse: 0.0741 Epoch 31/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2405 - acc: 0.8936 - mae: 0.0993 - mse: 0.0499 - val_loss: 0.3976 - val_acc: 0.8430 - val_mae: 0.1284 - val_mse: 0.0748 Epoch 32/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2382 - acc: 0.8943 - mae: 0.0989 - mse: 0.0496 - val_loss: 0.4170 - val_acc: 0.8402 - val_mae: 0.1280 - val_mse: 0.0755 Epoch 33/1000 359/359 [==============================] - 1s 3ms/step - loss: 0.2382 - acc: 0.8934 - mae: 0.0989 - mse: 0.0498 - val_loss: 0.3866 - val_acc: 0.8461 - val_mae: 0.1285 - val_mse: 0.0718 <Figure size 432x288 with 0 Axes> y_pred = model . predict ( X_test ) print_acc ( y_pred , y_test ) acc: 243/293, 0.8293515358361775","title":"Training by using category"},{"location":"MSBD5001/project/Group%20Project/#lstm-training","text":"","title":"LSTM Training"},{"location":"MSBD5001/project/Group%20Project/#window-generator","text":"To make a single prediction 24h into the future, given 24h of history you might define a window like this: class WindowGenerator () : def __init__ ( self , input_width , offset , data , train_split ) : self . data = data self . input_width = input_width self . offset = offset self . train_split = train_split def to_sequences ( self ) : \"\"\" Return both data and label \"\"\" data_len = len ( self . data ) ret = [] ret_label = [] for i in range ( data_len - self . offset - self . input_width + 1 ) : tmp = self . data [ i : i + self . input_width ] tmp_label = self . data [ i + self . input_width + self . offset - 1 ] ret . append ( tmp ) ret_label . append ( tmp_label ) return np . array ( ret ), np . array ( ret_label ) def split ( self ) : x , y = self . to_sequences () num_train = int (( 1 - self . train_split ) * x . shape [ 0 ]) X_train = x [ : num_train ] y_train = y [ : num_train ] X_test = x [ num_train :] y_test = y [ num_train :] return X_train , y_train , X_test , y_test SEQ_LEN = 10 cat_vars = [ ' IsWeekend ',' IsHoliday ' ] num_vars = [ ' Temperature ' , ' Pressure ' , ' Humidity ' , ' Cloud ' , ' Wind degree ' ] from tensorflow.keras.layers import Bidirectional , Dropout , LSTM , Dense , Activation","title":"Window generator"},{"location":"MSBD5001/project/Group%20Project/#preprocess-data_1","text":"average_sampled_train['temp_date'] = average_sampled_train['Time'].apply(lambda x: x.date()) grouped_average = average_sampled_train.groupby('temp_date').mean() grouped_average = grouped_average[['Temperature', 'Max temperature', 'Min temperature', 'Wind degree', 'Humidity' , 'Wind speed', 'Cloud', 'IsWeekend', 'IsHoliday', \"Wait time\", \"Pressure\"]] grouped_average.columns Index(['Temperature', 'Max temperature', 'Min temperature', 'Wind degree', 'Humidity', 'Wind speed', 'Cloud', 'IsWeekend', 'IsHoliday', 'Wait time', 'Pressure'], dtype='object') numeric_transformer = Pipeline ( steps = [ ( 'scaler' , RobustScaler ())]) categorical_transformer = Pipeline ( steps = [ ( 'oneHot' , OneHotEncoder ( sparse = False ))]) preprocessor = ColumnTransformer ( transformers = [ ( 'num' , numeric_transformer , num_vars ), ( 'cat' , categorical_transformer , cat_vars )]) data_transformed = preprocessor . fit_transform ( grouped_average ) print ( data_transformed . shape ) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) /usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py in _get_column_indices(X, key) 462 try: --> 463 column_indices = [all_columns.index(col) for col in columns] 464 except ValueError as e: /usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py in <listcomp>(.0) 462 try: --> 463 column_indices = [all_columns.index(col) for col in columns] 464 except ValueError as e: ValueError: 'Name' is not in list The above exception was the direct cause of the following exception: ValueError Traceback (most recent call last) <ipython-input-38-c2d6048db054> in <module>() 8 ('cat',categorical_transformer,cat_vars)]) 9 ---> 10 data_transformed=preprocessor.fit_transform(grouped_average) 11 print(data_transformed.shape) /usr/local/lib/python3.6/dist-packages/sklearn/compose/_column_transformer.py in fit_transform(self, X, y) 514 self._validate_transformers() 515 self._validate_column_callables(X) --> 516 self._validate_remainder(X) 517 518 result = self._fit_transform(X, y, _fit_transform_one) /usr/local/lib/python3.6/dist-packages/sklearn/compose/_column_transformer.py in _validate_remainder(self, X) 322 cols = [] 323 for columns in self._columns: --> 324 cols.extend(_get_column_indices(X, columns)) 325 remaining_idx = list(set(range(self._n_features)) - set(cols)) 326 remaining_idx = sorted(remaining_idx) or None /usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py in _get_column_indices(X, key) 466 raise ValueError( 467 \"A given column is not a column of the dataframe\" --> 468 ) from e 469 raise 470 ValueError: A given column is not a column of the dataframe wg = WindowGenerator(data=data_transformed, input_width=SEQ_LEN, offset=0, train_split=0.1) wg_2 = WindowGenerator(data=grouped_average['Wait time'].to_numpy(), input_width=SEQ_LEN, offset=0, train_split=0.1) X_train, _, X_test, _ = wg.split() _, y_train, _, y_test = wg_2.split() print(X_train.shape) print(y_train.shape) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-39-3e9d8c3f67d6> in <module>() ----> 1 wg = WindowGenerator(data=data_transformed, input_width=SEQ_LEN, offset=0, train_split=0.1) 2 wg_2 = WindowGenerator(data=grouped_average['Wait time'].to_numpy(), input_width=SEQ_LEN, offset=0, train_split=0.1) 3 X_train, _, X_test, _ = wg.split() 4 _, y_train, _, y_test = wg_2.split() 5 print(X_train.shape) NameError: name 'WindowGenerator' is not defined WINDOW_SIZE = SEQ_LEN model = keras . Sequential () # Input layer model . add ( Bidirectional ( LSTM ( WINDOW_SIZE , return_sequences = True ), input_shape = ( WINDOW_SIZE , X_train . shape [ - 1 ]))) \"\"\"Bidirectional RNNs allows to train on the sequence data in forward and backward direction.\"\"\" model . add ( Dropout ( rate = 0 . 2 )) # 1 st Hidden layer model . add ( Bidirectional ( LSTM (( WINDOW_SIZE * 2 ), return_sequences = True ))) model . add ( Dropout ( rate = 0 . 2 )) # 2 nd Hidden layer model . add ( Bidirectional ( LSTM ( WINDOW_SIZE , return_sequences = False ))) # output layer model . add ( Dense ( units = 1 )) model . add ( Activation ( 'linear' )) model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' , metrics = [ 'mae' , 'mse' ]) model . summary () history = model . fit ( X_train , y_train , epochs = 1000 , shuffle = False , validation_split = 0.1 , callbacks =[ early_stop ] ) Epoch 1/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2527 - acc: 0.8907 - mae: 0.1016 - mse: 0.0513 - val_loss: 0.3764 - val_acc: 0.8332 - val_mae: 0.1546 - val_mse: 0.0773 Epoch 2/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2506 - acc: 0.8893 - mae: 0.1032 - mse: 0.0517 - val_loss: 0.3732 - val_acc: 0.8353 - val_mae: 0.1477 - val_mse: 0.0752 Epoch 3/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2428 - acc: 0.8924 - mae: 0.1001 - mse: 0.0501 - val_loss: 0.3711 - val_acc: 0.8276 - val_mae: 0.1433 - val_mse: 0.0750 Epoch 4/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2407 - acc: 0.8952 - mae: 0.0993 - mse: 0.0495 - val_loss: 0.3701 - val_acc: 0.8332 - val_mae: 0.1468 - val_mse: 0.0752 Epoch 5/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2404 - acc: 0.8929 - mae: 0.0995 - mse: 0.0496 - val_loss: 0.3691 - val_acc: 0.8409 - val_mae: 0.1387 - val_mse: 0.0740 Epoch 6/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2375 - acc: 0.8962 - mae: 0.0976 - mse: 0.0488 - val_loss: 0.3705 - val_acc: 0.8367 - val_mae: 0.1407 - val_mse: 0.0743 Epoch 7/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2360 - acc: 0.8923 - mae: 0.0981 - mse: 0.0491 - val_loss: 0.3755 - val_acc: 0.8360 - val_mae: 0.1370 - val_mse: 0.0748 Epoch 8/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2347 - acc: 0.8962 - mae: 0.0964 - mse: 0.0482 - val_loss: 0.3740 - val_acc: 0.8451 - val_mae: 0.1411 - val_mse: 0.0746 Epoch 9/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2283 - acc: 0.8962 - mae: 0.0954 - mse: 0.0478 - val_loss: 0.3884 - val_acc: 0.8353 - val_mae: 0.1369 - val_mse: 0.0751 Epoch 10/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2257 - acc: 0.9017 - mae: 0.0935 - mse: 0.0469 - val_loss: 0.3705 - val_acc: 0.8395 - val_mae: 0.1339 - val_mse: 0.0737 Epoch 11/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2231 - acc: 0.8997 - mae: 0.0924 - mse: 0.0463 - val_loss: 0.3835 - val_acc: 0.8416 - val_mae: 0.1296 - val_mse: 0.0732 Epoch 12/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2272 - acc: 0.8972 - mae: 0.0946 - mse: 0.0475 - val_loss: 0.3905 - val_acc: 0.8430 - val_mae: 0.1307 - val_mse: 0.0745 Epoch 13/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2156 - acc: 0.9003 - mae: 0.0904 - mse: 0.0453 - val_loss: 0.3855 - val_acc: 0.8465 - val_mae: 0.1280 - val_mse: 0.0729 Epoch 14/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2194 - acc: 0.9041 - mae: 0.0901 - mse: 0.0452 - val_loss: 0.3853 - val_acc: 0.8381 - val_mae: 0.1290 - val_mse: 0.0740 Epoch 15/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2144 - acc: 0.9045 - mae: 0.0893 - mse: 0.0446 - val_loss: 0.3937 - val_acc: 0.8472 - val_mae: 0.1296 - val_mse: 0.0745 Epoch 16/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2119 - acc: 0.9065 - mae: 0.0874 - mse: 0.0438 - val_loss: 0.3989 - val_acc: 0.8458 - val_mae: 0.1282 - val_mse: 0.0749 Epoch 17/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2132 - acc: 0.9031 - mae: 0.0889 - mse: 0.0446 - val_loss: 0.4087 - val_acc: 0.8458 - val_mae: 0.1235 - val_mse: 0.0749 Epoch 18/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2098 - acc: 0.9048 - mae: 0.0872 - mse: 0.0438 - val_loss: 0.3939 - val_acc: 0.8395 - val_mae: 0.1307 - val_mse: 0.0753 Epoch 19/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2060 - acc: 0.9079 - mae: 0.0858 - mse: 0.0429 - val_loss: 0.3988 - val_acc: 0.8493 - val_mae: 0.1234 - val_mse: 0.0736 Epoch 20/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2082 - acc: 0.9059 - mae: 0.0865 - mse: 0.0435 - val_loss: 0.4140 - val_acc: 0.8507 - val_mae: 0.1226 - val_mse: 0.0741 Epoch 21/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2008 - acc: 0.9093 - mae: 0.0833 - mse: 0.0419 - val_loss: 0.4228 - val_acc: 0.8388 - val_mae: 0.1248 - val_mse: 0.0767 Epoch 22/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2061 - acc: 0.9080 - mae: 0.0849 - mse: 0.0426 - val_loss: 0.4233 - val_acc: 0.8500 - val_mae: 0.1232 - val_mse: 0.0751 Epoch 23/1000 403/403 [==============================] - 1s 3ms/step - loss: 0.2051 - acc: 0.9044 - mae: 0.0854 - mse: 0.0431 - val_loss: 0.4062 - val_acc: 0.8493 - val_mae: 0.1255 - val_mse: 0.0747 Epoch 24/1000 21/403 [>.............................] - ETA: 0s - loss: 0.2305 - acc: 0.9033 - mae: 0.0996 - mse: 0.0482 --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-40-52ca1206cf0b> in <module>() ----> 1 history = model.fit(X_train, y_train, epochs=1000, shuffle=False, validation_split=0.1, callbacks=[early_stop]) /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs) 106 def _method_wrapper(self, *args, **kwargs): 107 if not self._in_multi_worker_mode(): # pylint: disable=protected-access --> 108 return method(self, *args, **kwargs) 109 110 # Running inside `run_distribute_coordinator` already. /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing) 1096 batch_size=batch_size): 1097 callbacks.on_train_batch_begin(step) -> 1098 tmp_logs = train_function(iterator) 1099 if data_handler.should_sync: 1100 context.async_wait() /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds) 778 else: 779 compiler = \"nonXla\" --> 780 result = self._call(*args, **kwds) 781 782 new_tracing_count = self._get_tracing_count() /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds) 805 # In this case we have created variables on the first call, so we run the 806 # defunned version which is guaranteed to never create variables. --> 807 return self._stateless_fn(*args, **kwds) # pylint: disable=not-callable 808 elif self._stateful_fn is not None: 809 # Release the lock early so that multiple threads can perform the call /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs) 2827 with self._lock: 2828 graph_function, args, kwargs = self._maybe_define_function(args, kwargs) -> 2829 return graph_function._filtered_call(args, kwargs) # pylint: disable=protected-access 2830 2831 @property /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager) 1846 resource_variable_ops.BaseResourceVariable))], 1847 captured_inputs=self.captured_inputs, -> 1848 cancellation_manager=cancellation_manager) 1849 1850 def _call_flat(self, args, captured_inputs, cancellation_manager=None): /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager) 1922 # No tape is watching; skip to running the function. 1923 return self._build_call_outputs(self._inference_function.call( -> 1924 ctx, args, cancellation_manager=cancellation_manager)) 1925 forward_backward = self._select_forward_and_backward_functions( 1926 args, /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager) 548 inputs=args, 549 attrs=attrs, --> 550 ctx=ctx) 551 else: 552 outputs = execute.execute_with_cancellation( /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name) 58 ctx.ensure_initialized() 59 tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, ---> 60 inputs, attrs, num_outputs) 61 except core._NotOkStatusException as e: 62 if name is not None: KeyboardInterrupt: plot_history(history) y_pred = model.predict(X_test) print() print(y_test[:10]) plt.plot(y_test) plt.plot(y_pred.flatten()) [15.10416667 25.24390244 20.91836735 37.47126437 19.53846154] [<matplotlib.lines.Line2D at 0x7f4ce57342e8>]","title":"Preprocess data"},{"location":"MSBD5001/project/Group%20Project/#conclusion","text":"Because we are using averge daily data for LSTM model, we found that it may not be able to predict accurate results based on the current volume of dataset. However, DNN model will provide good prediction for both category results as well as regression results","title":"Conclusion"},{"location":"MSBD5001/project/Individual%20Project/","text":"Speed Prediction \u00b6 In this project, we will use weather data to predict average speed. we will use weather data from openweathermap.org which provides temperature, wind, humidity, and weather conditions This notebook will be running on Google Colab Some of the data preprocessing techniques are based on the online notebooks from tensorflow org. https://www.tensorflow.org/tutorials/structured_data/time_series Other references are: https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ Install and import dependencies \u00b6 ! pip install pactools Requirement already satisfied: pactools in /usr/local/lib/python3.6/dist-packages (0.3.1) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pactools) (1.4.1) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pactools) (0.22.2.post1) Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from pactools) (2.10.0) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pactools) (1.18.5) Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pactools) (3.2.2) Requirement already satisfied: mne in /usr/local/lib/python3.6/dist-packages (from pactools) (0.21.2) Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pactools) (0.17.0) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->pactools) (1.15.0) Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pactools) (1.3.1) Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pactools) (0.10.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pactools) (2.4.7) Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pactools) (2.8.1) import pandas as pd from sklearn.preprocessing import MinMaxScaler import requests from bs4 import BeautifulSoup from datetime import datetime import tensorflow as tf import seaborn as sns import matplotlib.pyplot as plt from sklearn.pipeline import Pipeline from sklearn.preprocessing import LabelBinarizer , OneHotEncoder , StandardScaler from sklearn.compose import ColumnTransformer from sklearn.model_selection import train_test_split from pactools.grid_search import GridSearchCVProgressBar from sklearn.model_selection import TimeSeriesSplit from sklearn.model_selection import GridSearchCV from sklearn import svm from xgboost import XGBRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error /usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject return f(*args, **kwds) /usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject return f(*args, **kwds) import tensorflow as tf print ( \"Num GPUs Available: \" , len ( tf . config . experimental . list_physical_devices ( 'GPU' ))) Num GPUs Available: 1 Preprocess \u00b6 Asign Holiday to the table \u00b6 def time_desc ( x : datetime ): Early_Morning = [ 4 , 5 , 6 , 7 ] Morning = [ 8 , 9 , 10 , 11 ] Afternoon = [ 12 , 13 , 14 , 15 ] Evening = [ 16 , 17 , 18 , 19 ] Night = [ 20 , 21 , 22 , 23 ] Late_Night = [ 0 , 1 , 2 , 3 ] if x . hour in Early_Morning : return 'early' elif x . hour in Morning : return 'morning' elif x . hour in Afternoon : return 'noon' elif x . hour in Evening : return 'afternoon' elif x . hour in Night : return 'night' else : return 'latenight' def add_holiday_and_weekend ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Add holiday and weekend to the dataset \"\"\" new_df = df . copy () new_df [ 'IsWeekend' ] = new_df [ 'date' ]. apply ( lambda x : 0 if x . weekday () in [ 0 , 1 , 2 , 3 , 4 ] else 1 ) new_df [ 'IsHoliday' ] = new_df [ 'date' ]. apply ( lambda x : 1 if ( x . date (). strftime ( '%Y-%m-%d' ) in [ '2017-01-02' , '2017-01-28' , '2017-01-30' , '2017-01-31' , '2017-04-04' , '2017-04-14' , '2017-04-15' , '2017-04-17' , '2017-05-01' , '2017-05-03' , '2017-05-30' , '2017-07-01' , '2017-10-02' , '2017-10-05' , '2017-10-28' , '2017-12-25' , '2017-12-26' , '2018-01-01' , '2018-02-16' , '2018-02-17' , '2018-02-19' , '2018-03-30' , '2018-03-31' , '2018-04-02' , '2018-04-05' , '2018-05-01' , '2018-05-22' , '2018-06-18' , '2018-07-02' , '2018-09-25' , '2018-10-01' , '2018-10-17' , '2018-12-25' , '2018-12-26' ]) or ( x . weekday () in [ 6 ]) else 0 ) return new_df I am using google drive to store all the data including the weather data. So please change this line to the your file paths. # change following two lines to your paths train = pd . read_csv ( '/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/train.csv' ) test = pd . read_csv ( '/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/test.csv' ) train = train . drop ( 'id' , axis = 1 ) train [ 'date' ] = pd . to_datetime ( train [ 'date' ]) train = add_holiday_and_weekend ( train ) train [ 'time desc' ] = train [ 'date' ]. apply ( time_desc ) train .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date speed IsWeekend IsHoliday time desc 0 2017-01-01 00:00:00 43.002930 1 1 latenight 1 2017-01-01 01:00:00 46.118696 1 1 latenight 2 2017-01-01 02:00:00 44.294158 1 1 latenight 3 2017-01-01 03:00:00 41.067468 1 1 latenight 4 2017-01-01 04:00:00 46.448653 1 1 early ... ... ... ... ... ... 14001 2018-12-31 12:00:00 19.865269 0 0 noon 14002 2018-12-31 15:00:00 17.820375 0 0 noon 14003 2018-12-31 16:00:00 12.501851 0 0 afternoon 14004 2018-12-31 18:00:00 15.979319 0 0 afternoon 14005 2018-12-31 20:00:00 40.594183 0 0 night 14006 rows \u00d7 5 columns train.plot(x='date', y='speed', figsize=(20, 10)) <matplotlib.axes._subplots.AxesSubplot at 0x7f5321240400> Merge weather \u00b6 Pre-process weather data \u00b6 from datetime import datetime def k_to_c ( x ): return x - 273.15 we have two different weather sources. Weather from open weather (Provides hourly basis weather report) Weather from Hong Kong observatory (Provides daily basis weather report) Based on the experiments, I decide to use Hong Kong observatory's weather report which will bring a higher accuracy # Change this path to yours # weather = pd . read_csv ( \"/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/hongkong_weather_1970-2020.csv\" ) # weather [ 'dt_iso' ] = weather [ 'dt_iso' ] . apply ( lambda x : x . replace ( 'UTC' , '' )) # weather [ 'date' ] = pd . to_datetime ( weather [ 'dt_iso' ] ). dt . tz_convert ( \"Asia/Hong_Kong\" ). dt . tz_localize ( None ) # # Transform unit # weather [ 'temp' ] = weather [ 'temp' ] . apply ( k_to_c ) # weather [ 'feels_like' ] = weather [ 'feels_like' ] . apply ( k_to_c ) # weather [ 'temp_min' ] = weather [ 'temp_min' ] . apply ( k_to_c ) # weather [ 'temp_max' ] = weather [ 'temp_max' ] . apply ( k_to_c ) # weather = weather . drop ( [ \"dt_iso\", \"dt\", \"weather_icon\", \"rain_1h\", \"rain_3h\", \"snow_1h\", \"snow_3h\", \"sea_level\", \"grnd_level\", \"timezone\", \"lat\", \"lon\" ] , axis = 1 ) # mask = ( weather [ 'date' ] >= datetime ( 2017 , 1 , 1 )) & ( weather [ 'date' ] <= datetime ( 2019 , 1 , 1 )) # weather = weather . loc [ mask ] # weather weather = pd . read_csv ( \"/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/hong-kong-weather-histories.csv\" ) weather [ 'date' ] = pd . to_datetime ( weather . year * 10000 + weather . month * 100 + weather . day , format = '%Y%m%d' ) weather . drop ( [ 'Unnamed: 0', 'year', 'month', 'day' ] , axis = 1 , inplace = True ) weather [ 'TRainfall(mm)' ]= weather [ 'TRainfall(mm)' ] . apply ( lambda x : 0 if x in [ '-','Trace' ] else x ) weather . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MPressure(hPa) MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm) #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) date 0 1021.7 20.8 18.4 72 0 0 60 34.2 2017-01-01 1 1020.2 23.3 18.4 28 0 0 70 17.7 2017-01-02 2 1019.8 21.3 18.9 56 0 5 70 26.1 2017-01-03 3 1018.7 21.7 18.7 51 0 0 70 27.7 2017-01-04 4 1016.9 23.4 18.9 61 0 0 40 14.3 2017-01-05 Merge \u00b6 from pandas import DatetimeIndex def merge_weather ( df : pd . DataFrame , weather : pd . DataFrame , how = 'left' ) -> pd . DataFrame : ''' Merge weather with data. ''' new_df = df . copy () new_weather = weather . copy () # new_df['tmp_date'] = new_df['date'] # new_weather['tmp_date'] = new_weather['date'] new_df [ 'tmp_date' ] = new_df [ 'date' ] . apply ( lambda date : date . date ()) new_weather [ 'tmp_date' ] = new_weather [ 'date' ] . apply ( lambda date : date . date ()) new_training_data = new_df . merge ( new_weather , on = 'tmp_date' , how = how ) new_training_data = new_training_data . drop ([ 'tmp_date' , 'date_y' ], axis = 1 ) new_training_data = new_training_data . rename ( columns = { 'date_x' : 'date' }) new_training_data [ 'hour' ] = DatetimeIndex ( new_training_data [ 'date' ]) . hour new_training_data [ 'day' ] = DatetimeIndex ( new_training_data [ 'date' ]) . day new_training_data [ 'month' ] = DatetimeIndex ( new_training_data [ 'date' ]) . month new_training_data [ 'year' ] = DatetimeIndex ( new_training_data [ 'date' ]) . year new_training_data [ 'weekday' ] = DatetimeIndex ( new_training_data [ 'date' ]) . weekday return new_training_data new_training_data = merge_weather ( train , weather ) new_training_data . sample ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date speed IsWeekend IsHoliday time desc MPressure(hPa) MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm) #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) hour day month year weekday 10903 2018-05-27 00:00:00 45.356636 1 1 latenight 1008.9 33.4 26.9 63 3.4 0 230 23.7 0 27 5 2018 6 1201 2017-02-20 10:00:00 32.278186 0 0 morning 1013.9 25.5 18.3 72 0 3 30 12.8 10 20 2 2017 0 5038 2017-07-30 08:00:00 45.045055 1 1 morning 996.0 34.8 29.6 69 0 4 300 24.3 8 30 7 2017 6 1107 2017-02-16 12:00:00 21.056502 0 0 noon 1021.6 24.0 15.4 11 0 0 40 16.3 12 16 2 2017 3 5044 2017-07-30 14:00:00 37.517872 1 1 noon 996.0 34.8 29.6 69 0 4 300 24.3 14 30 7 2017 6 10121 2018-03-04 20:00:00 39.774170 1 1 night 1011.0 27.3 21.9 86 0 0 140 8.0 20 4 3 2018 6 8027 2017-01-12 21:00:00 43.835431 0 0 night 1015.5 20.3 16.9 93 0 15 70 28.0 21 12 1 2017 3 1336 2017-02-26 01:00:00 48.388743 1 1 latenight 1021.2 17.0 10.6 88 1.4 0 360 21.7 1 26 2 2017 6 436 2017-01-19 13:00:00 19.198023 0 0 noon 1020.1 24.1 18.7 76 0 8 60 12.8 13 19 1 2017 3 6499 2017-09-29 05:00:00 48.814754 0 0 early 1012.2 33.1 28.8 63 0 0 90 21.1 5 29 9 2017 4 317 2017-01-14 05:00:00 45.863401 1 0 early 1017.9 16.5 14.5 92 1.0 0 50 29.8 5 14 1 2017 5 788 2017-03-02 05:00:00 46.502654 0 0 early 1019.2 23.9 17.2 12 0 0 10 25.5 5 2 3 2017 3 4910 2017-07-25 00:00:00 48.773277 0 0 latenight 1005.1 33.1 27.7 55 0 0 100 15.0 0 25 7 2017 1 6698 2017-07-10 12:00:00 31.192037 0 0 noon 1008.5 32.1 27.5 83 0.6 0 190 14.6 12 10 7 2017 0 7041 2017-10-21 19:00:00 29.392348 1 0 afternoon 1012.1 27.2 21.6 33 0 0 360 33.3 19 21 10 2017 5 6451 2017-09-27 05:00:00 48.713259 0 0 early 1009.6 33.0 27.7 34 0 1 200 6.2 5 27 9 2017 2 779 2017-02-02 20:00:00 36.991496 0 0 night 1022.7 17.7 16.2 86 0 0 80 43.0 20 2 2 2017 3 8127 2017-06-12 01:00:00 50.491144 0 0 latenight 1001.9 30.0 25.3 80 37.7 0 80 53.5 1 12 6 2017 0 9211 2018-01-02 20:00:00 39.546576 0 0 night 1019.3 19.2 16.0 77 0 10 60 31.8 20 2 1 2018 1 5850 2017-02-09 04:00:00 47.510284 0 0 early 1020.2 16.8 11.1 72 0 0 10 39.9 4 9 2 2017 3 Plot data \u00b6 plt.figure(figsize=(6,4)) sns.boxplot('speed',data=new_training_data,orient='h',palette=\"Set3\",linewidth=2.5) plt.show() /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning Traffic speed \u00b6 data_plot = new_training_data data_plot['month'] = data_plot['date'].dt.month data_plot .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date speed IsWeekend IsHoliday time desc MPressure(hPa) MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm) #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) hour day month year weekday 0 2017-01-01 00:00:00 43.002930 1 1 latenight 1021.7 20.8 18.4 72 0 0 60 34.2 0 1 1 2017 6 1 2017-01-01 01:00:00 46.118696 1 1 latenight 1021.7 20.8 18.4 72 0 0 60 34.2 1 1 1 2017 6 2 2017-01-01 02:00:00 44.294158 1 1 latenight 1021.7 20.8 18.4 72 0 0 60 34.2 2 1 1 2017 6 3 2017-01-01 03:00:00 41.067468 1 1 latenight 1021.7 20.8 18.4 72 0 0 60 34.2 3 1 1 2017 6 4 2017-01-01 04:00:00 46.448653 1 1 early 1021.7 20.8 18.4 72 0 0 60 34.2 4 1 1 2017 6 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 14001 2018-12-31 12:00:00 19.865269 0 0 noon 1027.0 15.6 11.8 77 0 0 360 26.8 12 31 12 2018 0 14002 2018-12-31 15:00:00 17.820375 0 0 noon 1027.0 15.6 11.8 77 0 0 360 26.8 15 31 12 2018 0 14003 2018-12-31 16:00:00 12.501851 0 0 afternoon 1027.0 15.6 11.8 77 0 0 360 26.8 16 31 12 2018 0 14004 2018-12-31 18:00:00 15.979319 0 0 afternoon 1027.0 15.6 11.8 77 0 0 360 26.8 18 31 12 2018 0 14005 2018-12-31 20:00:00 40.594183 0 0 night 1027.0 15.6 11.8 77 0 0 360 26.8 20 31 12 2018 0 14006 rows \u00d7 18 columns tmp_data=new_training_data.groupby('month').aggregate({'speed':'mean'}) plt.figure(figsize=(8,6)) sns.lineplot(x=tmp_data.index,y=tmp_data.speed,data=tmp_data,palette=\"Set2\") plt.show() plt.figure(figsize=(8,6)) sns.countplot(y='time desc',data=new_training_data,palette=[\"#7fcdbb\",\"#edf8b1\",\"#fc9272\",\"#fee0d2\",\"#bcbddc\",\"#efedf5\"]) plt.show() new_training_data.hist(bins=50,figsize=(20,15)) plt.show() Train \u00b6 new_training_data.columns Index(['date', 'speed', 'IsWeekend', 'IsHoliday', 'time desc', 'MPressure(hPa)', 'MaxTemp(\u2103)', 'MinTemp(\u2103)', 'MCloud(%)', 'TRainfall(mm)', '#hRedVisi(h)', 'WindDirect(degrees)', 'MWindSpeed(km/h)', 'hour', 'day', 'month', 'year', 'weekday'], dtype='object') # using open weather dataset # cat_vars=['IsWeekend','IsHoliday','time desc'] # num_vars=['temp', 'pressure', 'wind_speed', 'humidity', \"clouds_all\", 'month', 'year', 'day', 'hour'] #using hong kong government dataset cat_vars = [ 'IsWeekend' , 'IsHoliday' , 'time desc' ] num_vars = [ 'year' , 'month' , 'day' , 'hour' , 'MaxTemp(\u2103)' , 'MinTemp(\u2103)' , 'MCloud(%)' , 'TRainfall(mm)' , '#hRedVisi(h)' , 'WindDirect(degrees)' , 'MWindSpeed(km/h)' ] Transform Data \u00b6 numeric_transformer = Pipeline ( steps = [ ( 'scaler' , MinMaxScaler ())]) categorical_transformer = Pipeline ( steps = [ ( 'oneHot' , OneHotEncoder ())]) preprocessor = ColumnTransformer ( transformers = [ ( 'num' , numeric_transformer , num_vars ), ( 'cat' , categorical_transformer , cat_vars )]) scaled_data = preprocessor . fit_transform ( new_training_data ) print ( scaled_data . shape ) (14006, 21) y = new_training_data['speed'] # y = y.to_numpy() # y = y.reshape(-1, 1) # print(y.shape) # print(y) We want to scale the speed import numpy as np Split data \u00b6 X_train,X_test,y_train,y_test=train_test_split(scaled_data,y,test_size=0.15,random_state=42) print(X_train) print(y_train) print(f\"Train x shape: {X_train.shape}\") print(f\"Train y shape: {y_train.shape}\") [[1. 0.09090909 0.53333333 ... 0. 0. 0. ] [0. 0.54545455 0.66666667 ... 1. 0. 0. ] [0. 0.63636364 0.43333333 ... 0. 0. 0. ] ... [0. 0.63636364 0.43333333 ... 0. 0. 0. ] [0. 0.45454545 0.03333333 ... 0. 0. 0. ] [0. 0.81818182 1. ... 1. 0. 0. ]] 9443 46.813428 4822 15.018334 5391 48.414942 11099 19.663880 7854 10.389012 ... 5191 24.637279 13418 35.306286 5390 46.950711 860 46.623319 7270 16.718626 Name: speed, Length: 11905, dtype: float64 Train x shape: (11905, 21) Train y shape: (11905,) Training by using XGBRegression \u00b6 tscv = TimeSeriesSplit ( n_splits = 3 ) model = XGBRegressor () param_grid = {' nthread ' : [ 4 , 6 , 8 ], ' objective ' : [' reg : squarederror '], ' learning_rate ' : [ .03 , 0.05 , .07 ], ' max_depth ' : [ 5 , 6 , 7 , 8 ], ' min_child_weight ' : [ 4 ], ' subsample ' : [ 0.7 ], ' colsample_bytree ' : [ 0.7 ], ' n_estimators ' : [ 500 ]} xgb = GridSearchCV ( estimator = model , param_grid = param_grid , cv = tscv , n_jobs = 2 , verbose = 10 ) xgb . fit ( X_train , y_train ) Fitting 3 folds for each of 36 candidates, totalling 108 fits [Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers. [Parallel(n_jobs=2)]: Done 1 tasks | elapsed: 37.4s [Parallel(n_jobs=2)]: Done 4 tasks | elapsed: 51.0s [Parallel(n_jobs=2)]: Done 9 tasks | elapsed: 1.0min [Parallel(n_jobs=2)]: Done 14 tasks | elapsed: 2.6min [Parallel(n_jobs=2)]: Done 21 tasks | elapsed: 5.8min [Parallel(n_jobs=2)]: Done 28 tasks | elapsed: 8.1min [Parallel(n_jobs=2)]: Done 37 tasks | elapsed: 10.0min [Parallel(n_jobs=2)]: Done 46 tasks | elapsed: 11.5min [Parallel(n_jobs=2)]: Done 57 tasks | elapsed: 16.0min [Parallel(n_jobs=2)]: Done 68 tasks | elapsed: 18.1min [Parallel(n_jobs=2)]: Done 81 tasks | elapsed: 20.3min [Parallel(n_jobs=2)]: Done 94 tasks | elapsed: 25.2min [Parallel(n_jobs=2)]: Done 108 out of 108 | elapsed: 29.2min finished /usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py:242: DeprecationWarning: The nthread parameter is deprecated as of version .6.Please use n_jobs instead.nthread is deprecated. 'nthread is deprecated.', DeprecationWarning) GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=3), error_score=nan, estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, importance_type='gain', learning_rate=0.1, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, n_estimators=100, n_jobs=1, nthread=None, objectiv... scale_pos_weight=1, seed=None, silent=None, subsample=1, verbosity=1), iid='deprecated', n_jobs=2, param_grid={'colsample_bytree': [0.7], 'learning_rate': [0.03, 0.05, 0.07], 'max_depth': [5, 6, 7, 8], 'min_child_weight': [4], 'n_estimators': [500], 'nthread': [4, 6, 8], 'objective': ['reg:squarederror'], 'subsample': [0.7]}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=10) Training by using DNN \u00b6 import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.metrics import MeanSquaredError from tensorflow.keras import regularizers print ( X_train . shape ) (11905, 58) early_stop = keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 20 ) model = keras . Sequential ( [ layers . Dense ( 128 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dense ( 128 , activation = \"relu\" ), layers . Dropout ( 0 . 2 ), layers . Dense ( 256 , activation = \"relu\" ), layers . Dense ( 512 , activation = \"relu\" ), layers . Dropout ( 0 . 2 ), layers . Dense ( 64 , activation = \"relu\" ), layers . Dense ( 1 ), ] ) opt = keras . optimizers . Adam ( learning_rate = 0 . 01 ) model . compile ( loss = 'mean_squared_error' , optimizer = opt , metrics = [ 'mae' , 'mse' ]) history = model . fit ( X_train , y_train , epochs = 200 , validation_split = 0.1 , callbacks =[ early_stop ] ,) Epoch 1/200 335/335 [==============================] - 1s 3ms/step - loss: 88.5822 - mae: 6.5761 - mse: 88.5822 - val_loss: 36.1129 - val_mae: 4.6599 - val_mse: 36.1129 Epoch 2/200 335/335 [==============================] - 1s 3ms/step - loss: 38.0175 - mae: 4.6946 - mse: 38.0175 - val_loss: 30.1431 - val_mae: 4.1988 - val_mse: 30.1431 Epoch 3/200 335/335 [==============================] - 1s 3ms/step - loss: 33.4089 - mae: 4.3429 - mse: 33.4089 - val_loss: 34.9154 - val_mae: 4.8873 - val_mse: 34.9154 Epoch 4/200 335/335 [==============================] - 1s 3ms/step - loss: 28.9762 - mae: 4.0474 - mse: 28.9762 - val_loss: 36.5322 - val_mae: 4.8710 - val_mse: 36.5322 Epoch 5/200 335/335 [==============================] - 1s 3ms/step - loss: 26.6944 - mae: 3.9239 - mse: 26.6944 - val_loss: 36.9679 - val_mae: 5.0986 - val_mse: 36.9679 Epoch 6/200 335/335 [==============================] - 1s 3ms/step - loss: 22.6390 - mae: 3.5917 - mse: 22.6390 - val_loss: 25.5548 - val_mae: 3.9797 - val_mse: 25.5548 Epoch 7/200 335/335 [==============================] - 1s 3ms/step - loss: 21.3222 - mae: 3.4796 - mse: 21.3222 - val_loss: 24.1274 - val_mae: 3.8104 - val_mse: 24.1274 Epoch 8/200 335/335 [==============================] - 1s 3ms/step - loss: 21.2355 - mae: 3.4528 - mse: 21.2355 - val_loss: 23.0463 - val_mae: 3.6462 - val_mse: 23.0463 Epoch 9/200 335/335 [==============================] - 1s 3ms/step - loss: 19.8987 - mae: 3.3513 - mse: 19.8987 - val_loss: 23.3246 - val_mae: 3.5491 - val_mse: 23.3246 Epoch 10/200 335/335 [==============================] - 1s 3ms/step - loss: 20.1989 - mae: 3.3736 - mse: 20.1989 - val_loss: 21.5967 - val_mae: 3.5246 - val_mse: 21.5967 Epoch 11/200 335/335 [==============================] - 1s 3ms/step - loss: 17.1561 - mae: 3.1038 - mse: 17.1561 - val_loss: 21.4864 - val_mae: 3.3486 - val_mse: 21.4864 Epoch 12/200 335/335 [==============================] - 1s 3ms/step - loss: 18.4897 - mae: 3.2406 - mse: 18.4897 - val_loss: 23.1777 - val_mae: 3.6755 - val_mse: 23.1777 Epoch 13/200 335/335 [==============================] - 1s 3ms/step - loss: 16.9098 - mae: 3.1090 - mse: 16.9098 - val_loss: 19.4548 - val_mae: 3.1942 - val_mse: 19.4548 Epoch 14/200 335/335 [==============================] - 1s 3ms/step - loss: 17.2672 - mae: 3.1229 - mse: 17.2672 - val_loss: 20.6426 - val_mae: 3.4684 - val_mse: 20.6426 Epoch 15/200 335/335 [==============================] - 1s 3ms/step - loss: 17.6549 - mae: 3.1584 - mse: 17.6549 - val_loss: 22.3610 - val_mae: 3.4008 - val_mse: 22.3610 Epoch 16/200 335/335 [==============================] - 1s 3ms/step - loss: 16.9222 - mae: 3.1086 - mse: 16.9222 - val_loss: 20.2673 - val_mae: 3.2451 - val_mse: 20.2673 Epoch 17/200 335/335 [==============================] - 1s 3ms/step - loss: 15.9358 - mae: 3.0255 - mse: 15.9358 - val_loss: 20.4191 - val_mae: 3.2302 - val_mse: 20.4191 Epoch 18/200 335/335 [==============================] - 1s 3ms/step - loss: 16.7693 - mae: 3.0742 - mse: 16.7693 - val_loss: 21.7748 - val_mae: 3.6764 - val_mse: 21.7748 Epoch 19/200 335/335 [==============================] - 1s 3ms/step - loss: 16.3996 - mae: 3.0702 - mse: 16.3996 - val_loss: 19.8259 - val_mae: 3.1752 - val_mse: 19.8259 Epoch 20/200 335/335 [==============================] - 1s 3ms/step - loss: 16.5213 - mae: 3.0592 - mse: 16.5213 - val_loss: 21.4091 - val_mae: 3.6836 - val_mse: 21.4091 Epoch 21/200 335/335 [==============================] - 1s 3ms/step - loss: 15.9956 - mae: 3.0162 - mse: 15.9956 - val_loss: 19.6310 - val_mae: 3.2997 - val_mse: 19.6310 Epoch 22/200 335/335 [==============================] - 1s 3ms/step - loss: 16.0111 - mae: 3.0024 - mse: 16.0111 - val_loss: 20.6067 - val_mae: 3.3334 - val_mse: 20.6067 Epoch 23/200 335/335 [==============================] - 1s 3ms/step - loss: 15.1770 - mae: 2.9399 - mse: 15.1770 - val_loss: 22.1456 - val_mae: 3.6386 - val_mse: 22.1456 Epoch 24/200 335/335 [==============================] - 1s 3ms/step - loss: 14.6462 - mae: 2.8749 - mse: 14.6462 - val_loss: 18.6738 - val_mae: 3.2064 - val_mse: 18.6738 Epoch 25/200 335/335 [==============================] - 1s 3ms/step - loss: 13.9560 - mae: 2.8153 - mse: 13.9560 - val_loss: 21.1440 - val_mae: 3.5123 - val_mse: 21.1440 Epoch 26/200 335/335 [==============================] - 1s 3ms/step - loss: 14.0086 - mae: 2.8230 - mse: 14.0086 - val_loss: 19.5613 - val_mae: 3.2137 - val_mse: 19.5613 Epoch 27/200 335/335 [==============================] - 1s 3ms/step - loss: 14.3125 - mae: 2.8489 - mse: 14.3125 - val_loss: 19.7472 - val_mae: 3.3587 - val_mse: 19.7472 Epoch 28/200 335/335 [==============================] - 1s 3ms/step - loss: 14.1316 - mae: 2.8411 - mse: 14.1316 - val_loss: 19.7847 - val_mae: 3.3107 - val_mse: 19.7847 Epoch 29/200 335/335 [==============================] - 1s 3ms/step - loss: 13.7052 - mae: 2.7982 - mse: 13.7052 - val_loss: 20.4032 - val_mae: 3.4941 - val_mse: 20.4032 Epoch 30/200 335/335 [==============================] - 1s 3ms/step - loss: 13.0548 - mae: 2.7319 - mse: 13.0548 - val_loss: 18.7457 - val_mae: 3.1877 - val_mse: 18.7457 Epoch 31/200 335/335 [==============================] - 1s 3ms/step - loss: 13.0223 - mae: 2.7370 - mse: 13.0223 - val_loss: 19.7470 - val_mae: 3.3078 - val_mse: 19.7470 Epoch 32/200 335/335 [==============================] - 1s 3ms/step - loss: 13.7680 - mae: 2.7799 - mse: 13.7680 - val_loss: 31.8045 - val_mae: 4.6455 - val_mse: 31.8045 Epoch 33/200 335/335 [==============================] - 1s 3ms/step - loss: 16.9060 - mae: 3.0815 - mse: 16.9060 - val_loss: 20.7011 - val_mae: 3.5447 - val_mse: 20.7011 Epoch 34/200 335/335 [==============================] - 1s 3ms/step - loss: 13.4084 - mae: 2.7451 - mse: 13.4084 - val_loss: 19.2511 - val_mae: 3.1178 - val_mse: 19.2511 Epoch 35/200 335/335 [==============================] - 1s 3ms/step - loss: 12.4933 - mae: 2.6592 - mse: 12.4933 - val_loss: 20.6078 - val_mae: 3.3862 - val_mse: 20.6078 Epoch 36/200 335/335 [==============================] - 1s 3ms/step - loss: 12.3008 - mae: 2.6765 - mse: 12.3008 - val_loss: 19.3481 - val_mae: 3.1230 - val_mse: 19.3481 Epoch 37/200 335/335 [==============================] - 1s 3ms/step - loss: 11.9626 - mae: 2.6093 - mse: 11.9626 - val_loss: 21.1786 - val_mae: 3.2431 - val_mse: 21.1786 Epoch 38/200 335/335 [==============================] - 1s 3ms/step - loss: 12.3632 - mae: 2.6454 - mse: 12.3632 - val_loss: 22.4643 - val_mae: 3.6553 - val_mse: 22.4643 Epoch 39/200 335/335 [==============================] - 1s 3ms/step - loss: 12.7137 - mae: 2.6874 - mse: 12.7137 - val_loss: 22.2701 - val_mae: 3.5854 - val_mse: 22.2701 Epoch 40/200 335/335 [==============================] - 1s 3ms/step - loss: 11.7284 - mae: 2.5673 - mse: 11.7284 - val_loss: 21.5855 - val_mae: 3.4258 - val_mse: 21.5855 Epoch 41/200 335/335 [==============================] - 1s 3ms/step - loss: 12.7227 - mae: 2.6810 - mse: 12.7227 - val_loss: 23.4930 - val_mae: 3.9059 - val_mse: 23.4930 Epoch 42/200 335/335 [==============================] - 1s 3ms/step - loss: 12.4669 - mae: 2.6337 - mse: 12.4669 - val_loss: 22.7228 - val_mae: 3.7075 - val_mse: 22.7228 Epoch 43/200 335/335 [==============================] - 1s 3ms/step - loss: 12.0860 - mae: 2.6287 - mse: 12.0860 - val_loss: 20.9738 - val_mae: 3.2877 - val_mse: 20.9738 Epoch 44/200 335/335 [==============================] - 1s 3ms/step - loss: 11.8580 - mae: 2.5910 - mse: 11.8580 - val_loss: 21.4522 - val_mae: 3.3515 - val_mse: 21.4522 plot def plot_history ( history ) : hist = pd . DataFrame ( history . history ) hist [ 'epoch' ] = history . epoch plt . figure () plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Mean Abs Error [MPG]' ) plt . plot ( hist [ 'epoch' ] , hist [ 'mae' ] , label = 'Train Error' ) plt . plot ( hist [ 'epoch' ] , hist [ 'val_mae' ] , label = 'Val Error' ) plt . legend () plt . figure () plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Mean Square Error [$MPG^2$]' ) plt . plot ( hist [ 'epoch' ] , hist [ 'mse' ] , label = 'Train Error' ) plt . plot ( hist [ 'epoch' ] , hist [ 'val_mse' ] , label = 'Val Error' ) plt . legend () plt . show () plot_history ( history ) Training by using Decision Tree \u00b6 from sklearn import tree # tree_clf = tree.DecisionTreeRegressor() # tree_clf.fit(X_train, y_train) tscv = TimeSeriesSplit(n_splits=3) param_grid = { 'max_depth': range(1, 10), 'min_samples_split': range(1, 10), 'min_samples_leaf': range(1, 5) } tree_model = tree.DecisionTreeRegressor() tree_reg = GridSearchCV(estimator=tree_model, param_grid=param_grid, cv=tscv, n_jobs=4, verbose=10) tree_reg.fit(X_train, y_train) Fitting 3 folds for each of 324 candidates, totalling 972 fits [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=4)]: Done 5 tasks | elapsed: 1.2s [Parallel(n_jobs=4)]: Done 10 tasks | elapsed: 1.2s [Parallel(n_jobs=4)]: Done 17 tasks | elapsed: 1.3s [Parallel(n_jobs=4)]: Batch computation too fast (0.1742s.) Setting batch_size=2. [Parallel(n_jobs=4)]: Done 24 tasks | elapsed: 1.3s [Parallel(n_jobs=4)]: Batch computation too fast (0.0517s.) Setting batch_size=4. [Parallel(n_jobs=4)]: Done 44 tasks | elapsed: 1.4s [Parallel(n_jobs=4)]: Batch computation too fast (0.0866s.) Setting batch_size=8. [Parallel(n_jobs=4)]: Batch computation too fast (0.1193s.) Setting batch_size=16. [Parallel(n_jobs=4)]: Done 88 tasks | elapsed: 1.6s [Parallel(n_jobs=4)]: Done 216 tasks | elapsed: 2.3s [Parallel(n_jobs=4)]: Done 392 tasks | elapsed: 3.3s [Parallel(n_jobs=4)]: Done 600 tasks | elapsed: 5.4s [Parallel(n_jobs=4)]: Done 808 tasks | elapsed: 7.7s [Parallel(n_jobs=4)]: Done 972 out of 972 | elapsed: 9.7s finished GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=3), error_score=nan, estimator=DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort='deprecated', random_state=None, splitter='best'), iid='deprecated', n_jobs=4, param_grid={'max_depth': range(1, 10), 'min_samples_leaf': range(1, 5), 'min_samples_split': range(1, 10)}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=10) Training by using Ridge Regression \u00b6 from sklearn import linear_model param_grid = { 'alpha' : range ( 1 , 10 )} reg_model = linear_model . Ridge ( alpha =. 5 ) ridge = GridSearchCV ( estimator = reg_model , param_grid = param_grid , cv = tscv , n_jobs = 4 , verbose = 10 ) ridge . fit ( X_train , y_train ) [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers. Fitting 3 folds for each of 9 candidates, totalling 27 fits [Parallel(n_jobs=4)]: Batch computation too fast (0.0206s.) Setting batch_size=2. [Parallel(n_jobs=4)]: Done 5 tasks | elapsed: 0.0s [Parallel(n_jobs=4)]: Batch computation too fast (0.0476s.) Setting batch_size=4. [Parallel(n_jobs=4)]: Done 12 tasks | elapsed: 0.1s [Parallel(n_jobs=4)]: Done 20 out of 27 | elapsed: 0.1s remaining: 0.0s [Parallel(n_jobs=4)]: Done 23 out of 27 | elapsed: 0.1s remaining: 0.0s [Parallel(n_jobs=4)]: Done 27 out of 27 | elapsed: 0.1s finished GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=3), error_score=nan, estimator=Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver='auto', tol=0.001), iid='deprecated', n_jobs=4, param_grid={'alpha': range(1, 10)}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=10) Training by using Random forest \u00b6 model_forest = RandomForestRegressor () param_grid2 = { 'n_estimators' :[ 10 , 50 , 100 , 1000 ], 'max_features' : range ( 1 , 4 ) } random_forest = GridSearchCV ( estimator = model_forest , param_grid = param_grid2 , cv = tscv , n_jobs = 10 , verbose = 10 ) random_forest . fit ( X_train , y_train ) Fitting 3 folds for each of 12 candidates, totalling 36 fits [Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers. [Parallel(n_jobs=10)]: Done 5 tasks | elapsed: 1.5s [Parallel(n_jobs=10)]: Done 12 tasks | elapsed: 4.1s [Parallel(n_jobs=10)]: Done 21 out of 36 | elapsed: 7.9s remaining: 5.7s [Parallel(n_jobs=10)]: Done 25 out of 36 | elapsed: 10.6s remaining: 4.7s [Parallel(n_jobs=10)]: Done 29 out of 36 | elapsed: 23.0s remaining: 5.6s [Parallel(n_jobs=10)]: Done 33 out of 36 | elapsed: 35.6s remaining: 3.2s [Parallel(n_jobs=10)]: Done 36 out of 36 | elapsed: 42.2s finished GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=3), error_score=nan, estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), iid='deprecated', n_jobs=10, param_grid={'max_features': range(1, 4), 'n_estimators': [10, 50, 100, 1000]}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=10) Training by using SVM \u00b6 model_svm = svm . SVR () param_grid3 = { 'kernel' : [ 'rbf' ], 'gamma' : [ 1 e - 3 , 1 e - 4 ], 'C' : [ 1 , 10 , 100 , 1000 ] } svr = GridSearchCV ( estimator = model_svm , n_jobs = 10 , verbose = 10 , param_grid = param_grid3 ) svr . fit ( X_train , y_train , ) Fitting 5 folds for each of 8 candidates, totalling 40 fits [Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers. [Parallel(n_jobs=10)]: Done 5 tasks | elapsed: 48.8s [Parallel(n_jobs=10)]: Done 12 tasks | elapsed: 1.6min [Parallel(n_jobs=10)]: Done 21 tasks | elapsed: 2.4min [Parallel(n_jobs=10)]: Done 26 out of 40 | elapsed: 2.4min remaining: 1.3min [Parallel(n_jobs=10)]: Done 31 out of 40 | elapsed: 3.2min remaining: 56.1s [Parallel(n_jobs=10)]: Done 36 out of 40 | elapsed: 3.2min remaining: 21.6s [Parallel(n_jobs=10)]: Done 40 out of 40 | elapsed: 3.3min finished GridSearchCV(cv=None, error_score=nan, estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False), iid='deprecated', n_jobs=10, param_grid={'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=10) Training by using Neural network \u00b6 from sklearn.neural_network import MLPRegressor mlp_reg = MLPRegressor ( max_iter = 1000 ) mlp_reg . fit ( X_train , y_train ) MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=(100,), learning_rate='constant', learning_rate_init=0.001, max_fun=15000, max_iter=1000, momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5, random_state=None, shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False) Evaluation \u00b6 y_pred = model.predict(X_test) print(y_pred) print(y_test) [[18.308086] [16.099003] [43.96459 ] ... [44.576817] [10.564673] [30.076843]] 6405 17.479723 10497 19.555973 3669 48.410243 747 24.168746 902 44.950639 ... 11278 19.544291 2312 9.208905 6475 47.599270 6440 7.897116 10103 31.978004 Name: speed, Length: 2101, dtype: float64 y_pred2 = xgb.predict(X_test) # y_pred3 = random_forest.predict(X_test) # y_pred4 = svr.predict(X_test) # y_pred5 = tree_reg.predict(X_test) # y_pred6 = ridge.predict(X_test) # y_pred7 = mlp_reg.predict(X_test) # MSE_DNN = mean_squared_error ( y_pred , y_test ) MSE_rg = mean_squared_error ( y_pred2 , y_test ) # MSE_rf = mean_squared_error ( y_pred3 , y_test ) # MSE_tree = mean_squared_error ( y_pred5 , y_test ) # MSE_svr = mean_squared_error ( y_pred4 , y_test ) # MSE_ridge = mean_squared_error ( y_pred6 , y_test ) # MSE_mlp = mean_squared_error ( y_pred7 , y_test ) print ( f \"MSE for dnn is: {MSE_DNN}\" ) print ( 'MSE for XGBoost is ' + str ( MSE_rg )) # print ( 'MSE for RandomForest is ' + str ( MSE_rf )) # print ( f \"MSE for decision is: {MSE_tree}\" ) # print ( f \"MSE for svr is: {MSE_svr}\" ) # print ( f \"MSE for ridge is: {MSE_ridge}\" ) # print ( f \"MSE for neural network is: {MSE_mlp}\" ) MSE for dnn is: 17.01656963733734 MSE for XGBoost is 14.680223822593183 Based on the observation, we found that XGV will provide the most accurate results. Make a prediction \u00b6 test = pd.read_csv('/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/test.csv') print(test.to_numpy().shape) (3504, 2) test['date'] = pd.to_datetime(test['date']) test = add_holiday_and_weekend(test) test['time desc']=test['date'].apply(time_desc) print(test.shape) (3504, 5) new_test = merge_weather(test, weather, how='inner') new_test = new_test.drop_duplicates(['date'], keep='last') new_test .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id date IsWeekend IsHoliday time desc MPressure(hPa) MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm) #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) hour day month year 0 0 2018-01-01 02:00:00 0 1 Late_Night 1020.5 19.0 16.3 75 0 21 70 28.4 2 1 1 2018 1 1 2018-01-01 05:00:00 0 1 Early_Morning 1020.5 19.0 16.3 75 0 21 70 28.4 5 1 1 2018 2 2 2018-01-01 07:00:00 0 1 Early_Morning 1020.5 19.0 16.3 75 0 21 70 28.4 7 1 1 2018 3 3 2018-01-01 08:00:00 0 1 Morning 1020.5 19.0 16.3 75 0 21 70 28.4 8 1 1 2018 4 4 2018-01-01 10:00:00 0 1 Morning 1020.5 19.0 16.3 75 0 21 70 28.4 10 1 1 2018 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 3499 3499 2018-12-31 17:00:00 0 0 Evening 1027.0 15.6 11.8 77 0 0 360 26.8 17 31 12 2018 3500 3500 2018-12-31 19:00:00 0 0 Evening 1027.0 15.6 11.8 77 0 0 360 26.8 19 31 12 2018 3501 3501 2018-12-31 21:00:00 0 0 Night 1027.0 15.6 11.8 77 0 0 360 26.8 21 31 12 2018 3502 3502 2018-12-31 22:00:00 0 0 Night 1027.0 15.6 11.8 77 0 0 360 26.8 22 31 12 2018 3503 3503 2018-12-31 23:00:00 0 0 Night 1027.0 15.6 11.8 77 0 0 360 26.8 23 31 12 2018 3504 rows \u00d7 17 columns transformed_data = preprocessor.fit_transform(new_test) print(transformed_data.shape) (3504, 21) pred = xgb.predict(transformed_data) print(pred.shape) (3504,) submission = test.copy() submission['speed'] = pred submission = submission[['id', 'speed']] # Write submission to file submission . to_csv ( 'submission_20720230.csv' , index = False )","title":"Individual project"},{"location":"MSBD5001/project/Individual%20Project/#speed-prediction","text":"In this project, we will use weather data to predict average speed. we will use weather data from openweathermap.org which provides temperature, wind, humidity, and weather conditions This notebook will be running on Google Colab Some of the data preprocessing techniques are based on the online notebooks from tensorflow org. https://www.tensorflow.org/tutorials/structured_data/time_series Other references are: https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/","title":"Speed Prediction"},{"location":"MSBD5001/project/Individual%20Project/#install-and-import-dependencies","text":"! pip install pactools Requirement already satisfied: pactools in /usr/local/lib/python3.6/dist-packages (0.3.1) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pactools) (1.4.1) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pactools) (0.22.2.post1) Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from pactools) (2.10.0) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pactools) (1.18.5) Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pactools) (3.2.2) Requirement already satisfied: mne in /usr/local/lib/python3.6/dist-packages (from pactools) (0.21.2) Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pactools) (0.17.0) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->pactools) (1.15.0) Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pactools) (1.3.1) Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pactools) (0.10.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pactools) (2.4.7) Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pactools) (2.8.1) import pandas as pd from sklearn.preprocessing import MinMaxScaler import requests from bs4 import BeautifulSoup from datetime import datetime import tensorflow as tf import seaborn as sns import matplotlib.pyplot as plt from sklearn.pipeline import Pipeline from sklearn.preprocessing import LabelBinarizer , OneHotEncoder , StandardScaler from sklearn.compose import ColumnTransformer from sklearn.model_selection import train_test_split from pactools.grid_search import GridSearchCVProgressBar from sklearn.model_selection import TimeSeriesSplit from sklearn.model_selection import GridSearchCV from sklearn import svm from xgboost import XGBRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error /usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject return f(*args, **kwds) /usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject return f(*args, **kwds) import tensorflow as tf print ( \"Num GPUs Available: \" , len ( tf . config . experimental . list_physical_devices ( 'GPU' ))) Num GPUs Available: 1","title":"Install and import dependencies"},{"location":"MSBD5001/project/Individual%20Project/#preprocess","text":"","title":"Preprocess"},{"location":"MSBD5001/project/Individual%20Project/#asign-holiday-to-the-table","text":"def time_desc ( x : datetime ): Early_Morning = [ 4 , 5 , 6 , 7 ] Morning = [ 8 , 9 , 10 , 11 ] Afternoon = [ 12 , 13 , 14 , 15 ] Evening = [ 16 , 17 , 18 , 19 ] Night = [ 20 , 21 , 22 , 23 ] Late_Night = [ 0 , 1 , 2 , 3 ] if x . hour in Early_Morning : return 'early' elif x . hour in Morning : return 'morning' elif x . hour in Afternoon : return 'noon' elif x . hour in Evening : return 'afternoon' elif x . hour in Night : return 'night' else : return 'latenight' def add_holiday_and_weekend ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Add holiday and weekend to the dataset \"\"\" new_df = df . copy () new_df [ 'IsWeekend' ] = new_df [ 'date' ]. apply ( lambda x : 0 if x . weekday () in [ 0 , 1 , 2 , 3 , 4 ] else 1 ) new_df [ 'IsHoliday' ] = new_df [ 'date' ]. apply ( lambda x : 1 if ( x . date (). strftime ( '%Y-%m-%d' ) in [ '2017-01-02' , '2017-01-28' , '2017-01-30' , '2017-01-31' , '2017-04-04' , '2017-04-14' , '2017-04-15' , '2017-04-17' , '2017-05-01' , '2017-05-03' , '2017-05-30' , '2017-07-01' , '2017-10-02' , '2017-10-05' , '2017-10-28' , '2017-12-25' , '2017-12-26' , '2018-01-01' , '2018-02-16' , '2018-02-17' , '2018-02-19' , '2018-03-30' , '2018-03-31' , '2018-04-02' , '2018-04-05' , '2018-05-01' , '2018-05-22' , '2018-06-18' , '2018-07-02' , '2018-09-25' , '2018-10-01' , '2018-10-17' , '2018-12-25' , '2018-12-26' ]) or ( x . weekday () in [ 6 ]) else 0 ) return new_df I am using google drive to store all the data including the weather data. So please change this line to the your file paths. # change following two lines to your paths train = pd . read_csv ( '/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/train.csv' ) test = pd . read_csv ( '/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/test.csv' ) train = train . drop ( 'id' , axis = 1 ) train [ 'date' ] = pd . to_datetime ( train [ 'date' ]) train = add_holiday_and_weekend ( train ) train [ 'time desc' ] = train [ 'date' ]. apply ( time_desc ) train .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date speed IsWeekend IsHoliday time desc 0 2017-01-01 00:00:00 43.002930 1 1 latenight 1 2017-01-01 01:00:00 46.118696 1 1 latenight 2 2017-01-01 02:00:00 44.294158 1 1 latenight 3 2017-01-01 03:00:00 41.067468 1 1 latenight 4 2017-01-01 04:00:00 46.448653 1 1 early ... ... ... ... ... ... 14001 2018-12-31 12:00:00 19.865269 0 0 noon 14002 2018-12-31 15:00:00 17.820375 0 0 noon 14003 2018-12-31 16:00:00 12.501851 0 0 afternoon 14004 2018-12-31 18:00:00 15.979319 0 0 afternoon 14005 2018-12-31 20:00:00 40.594183 0 0 night 14006 rows \u00d7 5 columns train.plot(x='date', y='speed', figsize=(20, 10)) <matplotlib.axes._subplots.AxesSubplot at 0x7f5321240400>","title":"Asign Holiday to the table"},{"location":"MSBD5001/project/Individual%20Project/#merge-weather","text":"","title":"Merge weather"},{"location":"MSBD5001/project/Individual%20Project/#pre-process-weather-data","text":"from datetime import datetime def k_to_c ( x ): return x - 273.15 we have two different weather sources. Weather from open weather (Provides hourly basis weather report) Weather from Hong Kong observatory (Provides daily basis weather report) Based on the experiments, I decide to use Hong Kong observatory's weather report which will bring a higher accuracy # Change this path to yours # weather = pd . read_csv ( \"/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/hongkong_weather_1970-2020.csv\" ) # weather [ 'dt_iso' ] = weather [ 'dt_iso' ] . apply ( lambda x : x . replace ( 'UTC' , '' )) # weather [ 'date' ] = pd . to_datetime ( weather [ 'dt_iso' ] ). dt . tz_convert ( \"Asia/Hong_Kong\" ). dt . tz_localize ( None ) # # Transform unit # weather [ 'temp' ] = weather [ 'temp' ] . apply ( k_to_c ) # weather [ 'feels_like' ] = weather [ 'feels_like' ] . apply ( k_to_c ) # weather [ 'temp_min' ] = weather [ 'temp_min' ] . apply ( k_to_c ) # weather [ 'temp_max' ] = weather [ 'temp_max' ] . apply ( k_to_c ) # weather = weather . drop ( [ \"dt_iso\", \"dt\", \"weather_icon\", \"rain_1h\", \"rain_3h\", \"snow_1h\", \"snow_3h\", \"sea_level\", \"grnd_level\", \"timezone\", \"lat\", \"lon\" ] , axis = 1 ) # mask = ( weather [ 'date' ] >= datetime ( 2017 , 1 , 1 )) & ( weather [ 'date' ] <= datetime ( 2019 , 1 , 1 )) # weather = weather . loc [ mask ] # weather weather = pd . read_csv ( \"/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/hong-kong-weather-histories.csv\" ) weather [ 'date' ] = pd . to_datetime ( weather . year * 10000 + weather . month * 100 + weather . day , format = '%Y%m%d' ) weather . drop ( [ 'Unnamed: 0', 'year', 'month', 'day' ] , axis = 1 , inplace = True ) weather [ 'TRainfall(mm)' ]= weather [ 'TRainfall(mm)' ] . apply ( lambda x : 0 if x in [ '-','Trace' ] else x ) weather . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MPressure(hPa) MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm) #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) date 0 1021.7 20.8 18.4 72 0 0 60 34.2 2017-01-01 1 1020.2 23.3 18.4 28 0 0 70 17.7 2017-01-02 2 1019.8 21.3 18.9 56 0 5 70 26.1 2017-01-03 3 1018.7 21.7 18.7 51 0 0 70 27.7 2017-01-04 4 1016.9 23.4 18.9 61 0 0 40 14.3 2017-01-05","title":"Pre-process weather data"},{"location":"MSBD5001/project/Individual%20Project/#merge","text":"from pandas import DatetimeIndex def merge_weather ( df : pd . DataFrame , weather : pd . DataFrame , how = 'left' ) -> pd . DataFrame : ''' Merge weather with data. ''' new_df = df . copy () new_weather = weather . copy () # new_df['tmp_date'] = new_df['date'] # new_weather['tmp_date'] = new_weather['date'] new_df [ 'tmp_date' ] = new_df [ 'date' ] . apply ( lambda date : date . date ()) new_weather [ 'tmp_date' ] = new_weather [ 'date' ] . apply ( lambda date : date . date ()) new_training_data = new_df . merge ( new_weather , on = 'tmp_date' , how = how ) new_training_data = new_training_data . drop ([ 'tmp_date' , 'date_y' ], axis = 1 ) new_training_data = new_training_data . rename ( columns = { 'date_x' : 'date' }) new_training_data [ 'hour' ] = DatetimeIndex ( new_training_data [ 'date' ]) . hour new_training_data [ 'day' ] = DatetimeIndex ( new_training_data [ 'date' ]) . day new_training_data [ 'month' ] = DatetimeIndex ( new_training_data [ 'date' ]) . month new_training_data [ 'year' ] = DatetimeIndex ( new_training_data [ 'date' ]) . year new_training_data [ 'weekday' ] = DatetimeIndex ( new_training_data [ 'date' ]) . weekday return new_training_data new_training_data = merge_weather ( train , weather ) new_training_data . sample ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date speed IsWeekend IsHoliday time desc MPressure(hPa) MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm) #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) hour day month year weekday 10903 2018-05-27 00:00:00 45.356636 1 1 latenight 1008.9 33.4 26.9 63 3.4 0 230 23.7 0 27 5 2018 6 1201 2017-02-20 10:00:00 32.278186 0 0 morning 1013.9 25.5 18.3 72 0 3 30 12.8 10 20 2 2017 0 5038 2017-07-30 08:00:00 45.045055 1 1 morning 996.0 34.8 29.6 69 0 4 300 24.3 8 30 7 2017 6 1107 2017-02-16 12:00:00 21.056502 0 0 noon 1021.6 24.0 15.4 11 0 0 40 16.3 12 16 2 2017 3 5044 2017-07-30 14:00:00 37.517872 1 1 noon 996.0 34.8 29.6 69 0 4 300 24.3 14 30 7 2017 6 10121 2018-03-04 20:00:00 39.774170 1 1 night 1011.0 27.3 21.9 86 0 0 140 8.0 20 4 3 2018 6 8027 2017-01-12 21:00:00 43.835431 0 0 night 1015.5 20.3 16.9 93 0 15 70 28.0 21 12 1 2017 3 1336 2017-02-26 01:00:00 48.388743 1 1 latenight 1021.2 17.0 10.6 88 1.4 0 360 21.7 1 26 2 2017 6 436 2017-01-19 13:00:00 19.198023 0 0 noon 1020.1 24.1 18.7 76 0 8 60 12.8 13 19 1 2017 3 6499 2017-09-29 05:00:00 48.814754 0 0 early 1012.2 33.1 28.8 63 0 0 90 21.1 5 29 9 2017 4 317 2017-01-14 05:00:00 45.863401 1 0 early 1017.9 16.5 14.5 92 1.0 0 50 29.8 5 14 1 2017 5 788 2017-03-02 05:00:00 46.502654 0 0 early 1019.2 23.9 17.2 12 0 0 10 25.5 5 2 3 2017 3 4910 2017-07-25 00:00:00 48.773277 0 0 latenight 1005.1 33.1 27.7 55 0 0 100 15.0 0 25 7 2017 1 6698 2017-07-10 12:00:00 31.192037 0 0 noon 1008.5 32.1 27.5 83 0.6 0 190 14.6 12 10 7 2017 0 7041 2017-10-21 19:00:00 29.392348 1 0 afternoon 1012.1 27.2 21.6 33 0 0 360 33.3 19 21 10 2017 5 6451 2017-09-27 05:00:00 48.713259 0 0 early 1009.6 33.0 27.7 34 0 1 200 6.2 5 27 9 2017 2 779 2017-02-02 20:00:00 36.991496 0 0 night 1022.7 17.7 16.2 86 0 0 80 43.0 20 2 2 2017 3 8127 2017-06-12 01:00:00 50.491144 0 0 latenight 1001.9 30.0 25.3 80 37.7 0 80 53.5 1 12 6 2017 0 9211 2018-01-02 20:00:00 39.546576 0 0 night 1019.3 19.2 16.0 77 0 10 60 31.8 20 2 1 2018 1 5850 2017-02-09 04:00:00 47.510284 0 0 early 1020.2 16.8 11.1 72 0 0 10 39.9 4 9 2 2017 3","title":"Merge"},{"location":"MSBD5001/project/Individual%20Project/#plot-data","text":"plt.figure(figsize=(6,4)) sns.boxplot('speed',data=new_training_data,orient='h',palette=\"Set3\",linewidth=2.5) plt.show() /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning","title":"Plot data"},{"location":"MSBD5001/project/Individual%20Project/#traffic-speed","text":"data_plot = new_training_data data_plot['month'] = data_plot['date'].dt.month data_plot .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date speed IsWeekend IsHoliday time desc MPressure(hPa) MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm) #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) hour day month year weekday 0 2017-01-01 00:00:00 43.002930 1 1 latenight 1021.7 20.8 18.4 72 0 0 60 34.2 0 1 1 2017 6 1 2017-01-01 01:00:00 46.118696 1 1 latenight 1021.7 20.8 18.4 72 0 0 60 34.2 1 1 1 2017 6 2 2017-01-01 02:00:00 44.294158 1 1 latenight 1021.7 20.8 18.4 72 0 0 60 34.2 2 1 1 2017 6 3 2017-01-01 03:00:00 41.067468 1 1 latenight 1021.7 20.8 18.4 72 0 0 60 34.2 3 1 1 2017 6 4 2017-01-01 04:00:00 46.448653 1 1 early 1021.7 20.8 18.4 72 0 0 60 34.2 4 1 1 2017 6 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 14001 2018-12-31 12:00:00 19.865269 0 0 noon 1027.0 15.6 11.8 77 0 0 360 26.8 12 31 12 2018 0 14002 2018-12-31 15:00:00 17.820375 0 0 noon 1027.0 15.6 11.8 77 0 0 360 26.8 15 31 12 2018 0 14003 2018-12-31 16:00:00 12.501851 0 0 afternoon 1027.0 15.6 11.8 77 0 0 360 26.8 16 31 12 2018 0 14004 2018-12-31 18:00:00 15.979319 0 0 afternoon 1027.0 15.6 11.8 77 0 0 360 26.8 18 31 12 2018 0 14005 2018-12-31 20:00:00 40.594183 0 0 night 1027.0 15.6 11.8 77 0 0 360 26.8 20 31 12 2018 0 14006 rows \u00d7 18 columns tmp_data=new_training_data.groupby('month').aggregate({'speed':'mean'}) plt.figure(figsize=(8,6)) sns.lineplot(x=tmp_data.index,y=tmp_data.speed,data=tmp_data,palette=\"Set2\") plt.show() plt.figure(figsize=(8,6)) sns.countplot(y='time desc',data=new_training_data,palette=[\"#7fcdbb\",\"#edf8b1\",\"#fc9272\",\"#fee0d2\",\"#bcbddc\",\"#efedf5\"]) plt.show() new_training_data.hist(bins=50,figsize=(20,15)) plt.show()","title":"Traffic speed"},{"location":"MSBD5001/project/Individual%20Project/#train","text":"new_training_data.columns Index(['date', 'speed', 'IsWeekend', 'IsHoliday', 'time desc', 'MPressure(hPa)', 'MaxTemp(\u2103)', 'MinTemp(\u2103)', 'MCloud(%)', 'TRainfall(mm)', '#hRedVisi(h)', 'WindDirect(degrees)', 'MWindSpeed(km/h)', 'hour', 'day', 'month', 'year', 'weekday'], dtype='object') # using open weather dataset # cat_vars=['IsWeekend','IsHoliday','time desc'] # num_vars=['temp', 'pressure', 'wind_speed', 'humidity', \"clouds_all\", 'month', 'year', 'day', 'hour'] #using hong kong government dataset cat_vars = [ 'IsWeekend' , 'IsHoliday' , 'time desc' ] num_vars = [ 'year' , 'month' , 'day' , 'hour' , 'MaxTemp(\u2103)' , 'MinTemp(\u2103)' , 'MCloud(%)' , 'TRainfall(mm)' , '#hRedVisi(h)' , 'WindDirect(degrees)' , 'MWindSpeed(km/h)' ]","title":"Train"},{"location":"MSBD5001/project/Individual%20Project/#transform-data","text":"numeric_transformer = Pipeline ( steps = [ ( 'scaler' , MinMaxScaler ())]) categorical_transformer = Pipeline ( steps = [ ( 'oneHot' , OneHotEncoder ())]) preprocessor = ColumnTransformer ( transformers = [ ( 'num' , numeric_transformer , num_vars ), ( 'cat' , categorical_transformer , cat_vars )]) scaled_data = preprocessor . fit_transform ( new_training_data ) print ( scaled_data . shape ) (14006, 21) y = new_training_data['speed'] # y = y.to_numpy() # y = y.reshape(-1, 1) # print(y.shape) # print(y) We want to scale the speed import numpy as np","title":"Transform Data"},{"location":"MSBD5001/project/Individual%20Project/#split-data","text":"X_train,X_test,y_train,y_test=train_test_split(scaled_data,y,test_size=0.15,random_state=42) print(X_train) print(y_train) print(f\"Train x shape: {X_train.shape}\") print(f\"Train y shape: {y_train.shape}\") [[1. 0.09090909 0.53333333 ... 0. 0. 0. ] [0. 0.54545455 0.66666667 ... 1. 0. 0. ] [0. 0.63636364 0.43333333 ... 0. 0. 0. ] ... [0. 0.63636364 0.43333333 ... 0. 0. 0. ] [0. 0.45454545 0.03333333 ... 0. 0. 0. ] [0. 0.81818182 1. ... 1. 0. 0. ]] 9443 46.813428 4822 15.018334 5391 48.414942 11099 19.663880 7854 10.389012 ... 5191 24.637279 13418 35.306286 5390 46.950711 860 46.623319 7270 16.718626 Name: speed, Length: 11905, dtype: float64 Train x shape: (11905, 21) Train y shape: (11905,)","title":"Split data"},{"location":"MSBD5001/project/Individual%20Project/#training-by-using-xgbregression","text":"tscv = TimeSeriesSplit ( n_splits = 3 ) model = XGBRegressor () param_grid = {' nthread ' : [ 4 , 6 , 8 ], ' objective ' : [' reg : squarederror '], ' learning_rate ' : [ .03 , 0.05 , .07 ], ' max_depth ' : [ 5 , 6 , 7 , 8 ], ' min_child_weight ' : [ 4 ], ' subsample ' : [ 0.7 ], ' colsample_bytree ' : [ 0.7 ], ' n_estimators ' : [ 500 ]} xgb = GridSearchCV ( estimator = model , param_grid = param_grid , cv = tscv , n_jobs = 2 , verbose = 10 ) xgb . fit ( X_train , y_train ) Fitting 3 folds for each of 36 candidates, totalling 108 fits [Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers. [Parallel(n_jobs=2)]: Done 1 tasks | elapsed: 37.4s [Parallel(n_jobs=2)]: Done 4 tasks | elapsed: 51.0s [Parallel(n_jobs=2)]: Done 9 tasks | elapsed: 1.0min [Parallel(n_jobs=2)]: Done 14 tasks | elapsed: 2.6min [Parallel(n_jobs=2)]: Done 21 tasks | elapsed: 5.8min [Parallel(n_jobs=2)]: Done 28 tasks | elapsed: 8.1min [Parallel(n_jobs=2)]: Done 37 tasks | elapsed: 10.0min [Parallel(n_jobs=2)]: Done 46 tasks | elapsed: 11.5min [Parallel(n_jobs=2)]: Done 57 tasks | elapsed: 16.0min [Parallel(n_jobs=2)]: Done 68 tasks | elapsed: 18.1min [Parallel(n_jobs=2)]: Done 81 tasks | elapsed: 20.3min [Parallel(n_jobs=2)]: Done 94 tasks | elapsed: 25.2min [Parallel(n_jobs=2)]: Done 108 out of 108 | elapsed: 29.2min finished /usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py:242: DeprecationWarning: The nthread parameter is deprecated as of version .6.Please use n_jobs instead.nthread is deprecated. 'nthread is deprecated.', DeprecationWarning) GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=3), error_score=nan, estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, importance_type='gain', learning_rate=0.1, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, n_estimators=100, n_jobs=1, nthread=None, objectiv... scale_pos_weight=1, seed=None, silent=None, subsample=1, verbosity=1), iid='deprecated', n_jobs=2, param_grid={'colsample_bytree': [0.7], 'learning_rate': [0.03, 0.05, 0.07], 'max_depth': [5, 6, 7, 8], 'min_child_weight': [4], 'n_estimators': [500], 'nthread': [4, 6, 8], 'objective': ['reg:squarederror'], 'subsample': [0.7]}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=10)","title":"Training by using XGBRegression"},{"location":"MSBD5001/project/Individual%20Project/#training-by-using-dnn","text":"import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.metrics import MeanSquaredError from tensorflow.keras import regularizers print ( X_train . shape ) (11905, 58) early_stop = keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 20 ) model = keras . Sequential ( [ layers . Dense ( 128 , activation = \"relu\" , input_shape = ( X_train . shape [ 1 ],),), layers . Dense ( 128 , activation = \"relu\" ), layers . Dropout ( 0 . 2 ), layers . Dense ( 256 , activation = \"relu\" ), layers . Dense ( 512 , activation = \"relu\" ), layers . Dropout ( 0 . 2 ), layers . Dense ( 64 , activation = \"relu\" ), layers . Dense ( 1 ), ] ) opt = keras . optimizers . Adam ( learning_rate = 0 . 01 ) model . compile ( loss = 'mean_squared_error' , optimizer = opt , metrics = [ 'mae' , 'mse' ]) history = model . fit ( X_train , y_train , epochs = 200 , validation_split = 0.1 , callbacks =[ early_stop ] ,) Epoch 1/200 335/335 [==============================] - 1s 3ms/step - loss: 88.5822 - mae: 6.5761 - mse: 88.5822 - val_loss: 36.1129 - val_mae: 4.6599 - val_mse: 36.1129 Epoch 2/200 335/335 [==============================] - 1s 3ms/step - loss: 38.0175 - mae: 4.6946 - mse: 38.0175 - val_loss: 30.1431 - val_mae: 4.1988 - val_mse: 30.1431 Epoch 3/200 335/335 [==============================] - 1s 3ms/step - loss: 33.4089 - mae: 4.3429 - mse: 33.4089 - val_loss: 34.9154 - val_mae: 4.8873 - val_mse: 34.9154 Epoch 4/200 335/335 [==============================] - 1s 3ms/step - loss: 28.9762 - mae: 4.0474 - mse: 28.9762 - val_loss: 36.5322 - val_mae: 4.8710 - val_mse: 36.5322 Epoch 5/200 335/335 [==============================] - 1s 3ms/step - loss: 26.6944 - mae: 3.9239 - mse: 26.6944 - val_loss: 36.9679 - val_mae: 5.0986 - val_mse: 36.9679 Epoch 6/200 335/335 [==============================] - 1s 3ms/step - loss: 22.6390 - mae: 3.5917 - mse: 22.6390 - val_loss: 25.5548 - val_mae: 3.9797 - val_mse: 25.5548 Epoch 7/200 335/335 [==============================] - 1s 3ms/step - loss: 21.3222 - mae: 3.4796 - mse: 21.3222 - val_loss: 24.1274 - val_mae: 3.8104 - val_mse: 24.1274 Epoch 8/200 335/335 [==============================] - 1s 3ms/step - loss: 21.2355 - mae: 3.4528 - mse: 21.2355 - val_loss: 23.0463 - val_mae: 3.6462 - val_mse: 23.0463 Epoch 9/200 335/335 [==============================] - 1s 3ms/step - loss: 19.8987 - mae: 3.3513 - mse: 19.8987 - val_loss: 23.3246 - val_mae: 3.5491 - val_mse: 23.3246 Epoch 10/200 335/335 [==============================] - 1s 3ms/step - loss: 20.1989 - mae: 3.3736 - mse: 20.1989 - val_loss: 21.5967 - val_mae: 3.5246 - val_mse: 21.5967 Epoch 11/200 335/335 [==============================] - 1s 3ms/step - loss: 17.1561 - mae: 3.1038 - mse: 17.1561 - val_loss: 21.4864 - val_mae: 3.3486 - val_mse: 21.4864 Epoch 12/200 335/335 [==============================] - 1s 3ms/step - loss: 18.4897 - mae: 3.2406 - mse: 18.4897 - val_loss: 23.1777 - val_mae: 3.6755 - val_mse: 23.1777 Epoch 13/200 335/335 [==============================] - 1s 3ms/step - loss: 16.9098 - mae: 3.1090 - mse: 16.9098 - val_loss: 19.4548 - val_mae: 3.1942 - val_mse: 19.4548 Epoch 14/200 335/335 [==============================] - 1s 3ms/step - loss: 17.2672 - mae: 3.1229 - mse: 17.2672 - val_loss: 20.6426 - val_mae: 3.4684 - val_mse: 20.6426 Epoch 15/200 335/335 [==============================] - 1s 3ms/step - loss: 17.6549 - mae: 3.1584 - mse: 17.6549 - val_loss: 22.3610 - val_mae: 3.4008 - val_mse: 22.3610 Epoch 16/200 335/335 [==============================] - 1s 3ms/step - loss: 16.9222 - mae: 3.1086 - mse: 16.9222 - val_loss: 20.2673 - val_mae: 3.2451 - val_mse: 20.2673 Epoch 17/200 335/335 [==============================] - 1s 3ms/step - loss: 15.9358 - mae: 3.0255 - mse: 15.9358 - val_loss: 20.4191 - val_mae: 3.2302 - val_mse: 20.4191 Epoch 18/200 335/335 [==============================] - 1s 3ms/step - loss: 16.7693 - mae: 3.0742 - mse: 16.7693 - val_loss: 21.7748 - val_mae: 3.6764 - val_mse: 21.7748 Epoch 19/200 335/335 [==============================] - 1s 3ms/step - loss: 16.3996 - mae: 3.0702 - mse: 16.3996 - val_loss: 19.8259 - val_mae: 3.1752 - val_mse: 19.8259 Epoch 20/200 335/335 [==============================] - 1s 3ms/step - loss: 16.5213 - mae: 3.0592 - mse: 16.5213 - val_loss: 21.4091 - val_mae: 3.6836 - val_mse: 21.4091 Epoch 21/200 335/335 [==============================] - 1s 3ms/step - loss: 15.9956 - mae: 3.0162 - mse: 15.9956 - val_loss: 19.6310 - val_mae: 3.2997 - val_mse: 19.6310 Epoch 22/200 335/335 [==============================] - 1s 3ms/step - loss: 16.0111 - mae: 3.0024 - mse: 16.0111 - val_loss: 20.6067 - val_mae: 3.3334 - val_mse: 20.6067 Epoch 23/200 335/335 [==============================] - 1s 3ms/step - loss: 15.1770 - mae: 2.9399 - mse: 15.1770 - val_loss: 22.1456 - val_mae: 3.6386 - val_mse: 22.1456 Epoch 24/200 335/335 [==============================] - 1s 3ms/step - loss: 14.6462 - mae: 2.8749 - mse: 14.6462 - val_loss: 18.6738 - val_mae: 3.2064 - val_mse: 18.6738 Epoch 25/200 335/335 [==============================] - 1s 3ms/step - loss: 13.9560 - mae: 2.8153 - mse: 13.9560 - val_loss: 21.1440 - val_mae: 3.5123 - val_mse: 21.1440 Epoch 26/200 335/335 [==============================] - 1s 3ms/step - loss: 14.0086 - mae: 2.8230 - mse: 14.0086 - val_loss: 19.5613 - val_mae: 3.2137 - val_mse: 19.5613 Epoch 27/200 335/335 [==============================] - 1s 3ms/step - loss: 14.3125 - mae: 2.8489 - mse: 14.3125 - val_loss: 19.7472 - val_mae: 3.3587 - val_mse: 19.7472 Epoch 28/200 335/335 [==============================] - 1s 3ms/step - loss: 14.1316 - mae: 2.8411 - mse: 14.1316 - val_loss: 19.7847 - val_mae: 3.3107 - val_mse: 19.7847 Epoch 29/200 335/335 [==============================] - 1s 3ms/step - loss: 13.7052 - mae: 2.7982 - mse: 13.7052 - val_loss: 20.4032 - val_mae: 3.4941 - val_mse: 20.4032 Epoch 30/200 335/335 [==============================] - 1s 3ms/step - loss: 13.0548 - mae: 2.7319 - mse: 13.0548 - val_loss: 18.7457 - val_mae: 3.1877 - val_mse: 18.7457 Epoch 31/200 335/335 [==============================] - 1s 3ms/step - loss: 13.0223 - mae: 2.7370 - mse: 13.0223 - val_loss: 19.7470 - val_mae: 3.3078 - val_mse: 19.7470 Epoch 32/200 335/335 [==============================] - 1s 3ms/step - loss: 13.7680 - mae: 2.7799 - mse: 13.7680 - val_loss: 31.8045 - val_mae: 4.6455 - val_mse: 31.8045 Epoch 33/200 335/335 [==============================] - 1s 3ms/step - loss: 16.9060 - mae: 3.0815 - mse: 16.9060 - val_loss: 20.7011 - val_mae: 3.5447 - val_mse: 20.7011 Epoch 34/200 335/335 [==============================] - 1s 3ms/step - loss: 13.4084 - mae: 2.7451 - mse: 13.4084 - val_loss: 19.2511 - val_mae: 3.1178 - val_mse: 19.2511 Epoch 35/200 335/335 [==============================] - 1s 3ms/step - loss: 12.4933 - mae: 2.6592 - mse: 12.4933 - val_loss: 20.6078 - val_mae: 3.3862 - val_mse: 20.6078 Epoch 36/200 335/335 [==============================] - 1s 3ms/step - loss: 12.3008 - mae: 2.6765 - mse: 12.3008 - val_loss: 19.3481 - val_mae: 3.1230 - val_mse: 19.3481 Epoch 37/200 335/335 [==============================] - 1s 3ms/step - loss: 11.9626 - mae: 2.6093 - mse: 11.9626 - val_loss: 21.1786 - val_mae: 3.2431 - val_mse: 21.1786 Epoch 38/200 335/335 [==============================] - 1s 3ms/step - loss: 12.3632 - mae: 2.6454 - mse: 12.3632 - val_loss: 22.4643 - val_mae: 3.6553 - val_mse: 22.4643 Epoch 39/200 335/335 [==============================] - 1s 3ms/step - loss: 12.7137 - mae: 2.6874 - mse: 12.7137 - val_loss: 22.2701 - val_mae: 3.5854 - val_mse: 22.2701 Epoch 40/200 335/335 [==============================] - 1s 3ms/step - loss: 11.7284 - mae: 2.5673 - mse: 11.7284 - val_loss: 21.5855 - val_mae: 3.4258 - val_mse: 21.5855 Epoch 41/200 335/335 [==============================] - 1s 3ms/step - loss: 12.7227 - mae: 2.6810 - mse: 12.7227 - val_loss: 23.4930 - val_mae: 3.9059 - val_mse: 23.4930 Epoch 42/200 335/335 [==============================] - 1s 3ms/step - loss: 12.4669 - mae: 2.6337 - mse: 12.4669 - val_loss: 22.7228 - val_mae: 3.7075 - val_mse: 22.7228 Epoch 43/200 335/335 [==============================] - 1s 3ms/step - loss: 12.0860 - mae: 2.6287 - mse: 12.0860 - val_loss: 20.9738 - val_mae: 3.2877 - val_mse: 20.9738 Epoch 44/200 335/335 [==============================] - 1s 3ms/step - loss: 11.8580 - mae: 2.5910 - mse: 11.8580 - val_loss: 21.4522 - val_mae: 3.3515 - val_mse: 21.4522 plot def plot_history ( history ) : hist = pd . DataFrame ( history . history ) hist [ 'epoch' ] = history . epoch plt . figure () plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Mean Abs Error [MPG]' ) plt . plot ( hist [ 'epoch' ] , hist [ 'mae' ] , label = 'Train Error' ) plt . plot ( hist [ 'epoch' ] , hist [ 'val_mae' ] , label = 'Val Error' ) plt . legend () plt . figure () plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Mean Square Error [$MPG^2$]' ) plt . plot ( hist [ 'epoch' ] , hist [ 'mse' ] , label = 'Train Error' ) plt . plot ( hist [ 'epoch' ] , hist [ 'val_mse' ] , label = 'Val Error' ) plt . legend () plt . show () plot_history ( history )","title":"Training by using DNN"},{"location":"MSBD5001/project/Individual%20Project/#training-by-using-decision-tree","text":"from sklearn import tree # tree_clf = tree.DecisionTreeRegressor() # tree_clf.fit(X_train, y_train) tscv = TimeSeriesSplit(n_splits=3) param_grid = { 'max_depth': range(1, 10), 'min_samples_split': range(1, 10), 'min_samples_leaf': range(1, 5) } tree_model = tree.DecisionTreeRegressor() tree_reg = GridSearchCV(estimator=tree_model, param_grid=param_grid, cv=tscv, n_jobs=4, verbose=10) tree_reg.fit(X_train, y_train) Fitting 3 folds for each of 324 candidates, totalling 972 fits [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=4)]: Done 5 tasks | elapsed: 1.2s [Parallel(n_jobs=4)]: Done 10 tasks | elapsed: 1.2s [Parallel(n_jobs=4)]: Done 17 tasks | elapsed: 1.3s [Parallel(n_jobs=4)]: Batch computation too fast (0.1742s.) Setting batch_size=2. [Parallel(n_jobs=4)]: Done 24 tasks | elapsed: 1.3s [Parallel(n_jobs=4)]: Batch computation too fast (0.0517s.) Setting batch_size=4. [Parallel(n_jobs=4)]: Done 44 tasks | elapsed: 1.4s [Parallel(n_jobs=4)]: Batch computation too fast (0.0866s.) Setting batch_size=8. [Parallel(n_jobs=4)]: Batch computation too fast (0.1193s.) Setting batch_size=16. [Parallel(n_jobs=4)]: Done 88 tasks | elapsed: 1.6s [Parallel(n_jobs=4)]: Done 216 tasks | elapsed: 2.3s [Parallel(n_jobs=4)]: Done 392 tasks | elapsed: 3.3s [Parallel(n_jobs=4)]: Done 600 tasks | elapsed: 5.4s [Parallel(n_jobs=4)]: Done 808 tasks | elapsed: 7.7s [Parallel(n_jobs=4)]: Done 972 out of 972 | elapsed: 9.7s finished GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=3), error_score=nan, estimator=DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort='deprecated', random_state=None, splitter='best'), iid='deprecated', n_jobs=4, param_grid={'max_depth': range(1, 10), 'min_samples_leaf': range(1, 5), 'min_samples_split': range(1, 10)}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=10)","title":"Training by using Decision Tree"},{"location":"MSBD5001/project/Individual%20Project/#training-by-using-ridge-regression","text":"from sklearn import linear_model param_grid = { 'alpha' : range ( 1 , 10 )} reg_model = linear_model . Ridge ( alpha =. 5 ) ridge = GridSearchCV ( estimator = reg_model , param_grid = param_grid , cv = tscv , n_jobs = 4 , verbose = 10 ) ridge . fit ( X_train , y_train ) [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers. Fitting 3 folds for each of 9 candidates, totalling 27 fits [Parallel(n_jobs=4)]: Batch computation too fast (0.0206s.) Setting batch_size=2. [Parallel(n_jobs=4)]: Done 5 tasks | elapsed: 0.0s [Parallel(n_jobs=4)]: Batch computation too fast (0.0476s.) Setting batch_size=4. [Parallel(n_jobs=4)]: Done 12 tasks | elapsed: 0.1s [Parallel(n_jobs=4)]: Done 20 out of 27 | elapsed: 0.1s remaining: 0.0s [Parallel(n_jobs=4)]: Done 23 out of 27 | elapsed: 0.1s remaining: 0.0s [Parallel(n_jobs=4)]: Done 27 out of 27 | elapsed: 0.1s finished GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=3), error_score=nan, estimator=Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver='auto', tol=0.001), iid='deprecated', n_jobs=4, param_grid={'alpha': range(1, 10)}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=10)","title":"Training by using Ridge Regression"},{"location":"MSBD5001/project/Individual%20Project/#training-by-using-random-forest","text":"model_forest = RandomForestRegressor () param_grid2 = { 'n_estimators' :[ 10 , 50 , 100 , 1000 ], 'max_features' : range ( 1 , 4 ) } random_forest = GridSearchCV ( estimator = model_forest , param_grid = param_grid2 , cv = tscv , n_jobs = 10 , verbose = 10 ) random_forest . fit ( X_train , y_train ) Fitting 3 folds for each of 12 candidates, totalling 36 fits [Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers. [Parallel(n_jobs=10)]: Done 5 tasks | elapsed: 1.5s [Parallel(n_jobs=10)]: Done 12 tasks | elapsed: 4.1s [Parallel(n_jobs=10)]: Done 21 out of 36 | elapsed: 7.9s remaining: 5.7s [Parallel(n_jobs=10)]: Done 25 out of 36 | elapsed: 10.6s remaining: 4.7s [Parallel(n_jobs=10)]: Done 29 out of 36 | elapsed: 23.0s remaining: 5.6s [Parallel(n_jobs=10)]: Done 33 out of 36 | elapsed: 35.6s remaining: 3.2s [Parallel(n_jobs=10)]: Done 36 out of 36 | elapsed: 42.2s finished GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=3), error_score=nan, estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), iid='deprecated', n_jobs=10, param_grid={'max_features': range(1, 4), 'n_estimators': [10, 50, 100, 1000]}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=10)","title":"Training by using Random forest"},{"location":"MSBD5001/project/Individual%20Project/#training-by-using-svm","text":"model_svm = svm . SVR () param_grid3 = { 'kernel' : [ 'rbf' ], 'gamma' : [ 1 e - 3 , 1 e - 4 ], 'C' : [ 1 , 10 , 100 , 1000 ] } svr = GridSearchCV ( estimator = model_svm , n_jobs = 10 , verbose = 10 , param_grid = param_grid3 ) svr . fit ( X_train , y_train , ) Fitting 5 folds for each of 8 candidates, totalling 40 fits [Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers. [Parallel(n_jobs=10)]: Done 5 tasks | elapsed: 48.8s [Parallel(n_jobs=10)]: Done 12 tasks | elapsed: 1.6min [Parallel(n_jobs=10)]: Done 21 tasks | elapsed: 2.4min [Parallel(n_jobs=10)]: Done 26 out of 40 | elapsed: 2.4min remaining: 1.3min [Parallel(n_jobs=10)]: Done 31 out of 40 | elapsed: 3.2min remaining: 56.1s [Parallel(n_jobs=10)]: Done 36 out of 40 | elapsed: 3.2min remaining: 21.6s [Parallel(n_jobs=10)]: Done 40 out of 40 | elapsed: 3.3min finished GridSearchCV(cv=None, error_score=nan, estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False), iid='deprecated', n_jobs=10, param_grid={'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=10)","title":"Training by using SVM"},{"location":"MSBD5001/project/Individual%20Project/#training-by-using-neural-network","text":"from sklearn.neural_network import MLPRegressor mlp_reg = MLPRegressor ( max_iter = 1000 ) mlp_reg . fit ( X_train , y_train ) MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=(100,), learning_rate='constant', learning_rate_init=0.001, max_fun=15000, max_iter=1000, momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5, random_state=None, shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False)","title":"Training by using Neural network"},{"location":"MSBD5001/project/Individual%20Project/#evaluation","text":"y_pred = model.predict(X_test) print(y_pred) print(y_test) [[18.308086] [16.099003] [43.96459 ] ... [44.576817] [10.564673] [30.076843]] 6405 17.479723 10497 19.555973 3669 48.410243 747 24.168746 902 44.950639 ... 11278 19.544291 2312 9.208905 6475 47.599270 6440 7.897116 10103 31.978004 Name: speed, Length: 2101, dtype: float64 y_pred2 = xgb.predict(X_test) # y_pred3 = random_forest.predict(X_test) # y_pred4 = svr.predict(X_test) # y_pred5 = tree_reg.predict(X_test) # y_pred6 = ridge.predict(X_test) # y_pred7 = mlp_reg.predict(X_test) # MSE_DNN = mean_squared_error ( y_pred , y_test ) MSE_rg = mean_squared_error ( y_pred2 , y_test ) # MSE_rf = mean_squared_error ( y_pred3 , y_test ) # MSE_tree = mean_squared_error ( y_pred5 , y_test ) # MSE_svr = mean_squared_error ( y_pred4 , y_test ) # MSE_ridge = mean_squared_error ( y_pred6 , y_test ) # MSE_mlp = mean_squared_error ( y_pred7 , y_test ) print ( f \"MSE for dnn is: {MSE_DNN}\" ) print ( 'MSE for XGBoost is ' + str ( MSE_rg )) # print ( 'MSE for RandomForest is ' + str ( MSE_rf )) # print ( f \"MSE for decision is: {MSE_tree}\" ) # print ( f \"MSE for svr is: {MSE_svr}\" ) # print ( f \"MSE for ridge is: {MSE_ridge}\" ) # print ( f \"MSE for neural network is: {MSE_mlp}\" ) MSE for dnn is: 17.01656963733734 MSE for XGBoost is 14.680223822593183 Based on the observation, we found that XGV will provide the most accurate results.","title":"Evaluation"},{"location":"MSBD5001/project/Individual%20Project/#make-a-prediction","text":"test = pd.read_csv('/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/test.csv') print(test.to_numpy().shape) (3504, 2) test['date'] = pd.to_datetime(test['date']) test = add_holiday_and_weekend(test) test['time desc']=test['date'].apply(time_desc) print(test.shape) (3504, 5) new_test = merge_weather(test, weather, how='inner') new_test = new_test.drop_duplicates(['date'], keep='last') new_test .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id date IsWeekend IsHoliday time desc MPressure(hPa) MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm) #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) hour day month year 0 0 2018-01-01 02:00:00 0 1 Late_Night 1020.5 19.0 16.3 75 0 21 70 28.4 2 1 1 2018 1 1 2018-01-01 05:00:00 0 1 Early_Morning 1020.5 19.0 16.3 75 0 21 70 28.4 5 1 1 2018 2 2 2018-01-01 07:00:00 0 1 Early_Morning 1020.5 19.0 16.3 75 0 21 70 28.4 7 1 1 2018 3 3 2018-01-01 08:00:00 0 1 Morning 1020.5 19.0 16.3 75 0 21 70 28.4 8 1 1 2018 4 4 2018-01-01 10:00:00 0 1 Morning 1020.5 19.0 16.3 75 0 21 70 28.4 10 1 1 2018 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 3499 3499 2018-12-31 17:00:00 0 0 Evening 1027.0 15.6 11.8 77 0 0 360 26.8 17 31 12 2018 3500 3500 2018-12-31 19:00:00 0 0 Evening 1027.0 15.6 11.8 77 0 0 360 26.8 19 31 12 2018 3501 3501 2018-12-31 21:00:00 0 0 Night 1027.0 15.6 11.8 77 0 0 360 26.8 21 31 12 2018 3502 3502 2018-12-31 22:00:00 0 0 Night 1027.0 15.6 11.8 77 0 0 360 26.8 22 31 12 2018 3503 3503 2018-12-31 23:00:00 0 0 Night 1027.0 15.6 11.8 77 0 0 360 26.8 23 31 12 2018 3504 rows \u00d7 17 columns transformed_data = preprocessor.fit_transform(new_test) print(transformed_data.shape) (3504, 21) pred = xgb.predict(transformed_data) print(pred.shape) (3504,) submission = test.copy() submission['speed'] = pred submission = submission[['id', 'speed']] # Write submission to file submission . to_csv ( 'submission_20720230.csv' , index = False )","title":"Make a prediction"},{"location":"MSBD5001/project/msbd5001-fall2020/","text":"Task description \u00b6 Use the train data which contains 2017 and part of 2018 average traffic speed of a major road in Hong Kong and their corresponding timestamp to predict speed in 2018. The total work includes data integration, feature engineering, model training and final prediction. Task is roughly categorized as regression, so I tried a collection of regressors like RandomForestRegressor or XGBoostRegressor and selected the best one. For model construction, I mainly tried two ways. One is built on randomly spliting trainset and testset from train.csv and use the model to predict test.csv. Another is built on data in 2017, evaluate the model on 2018 in train.csv, and use the final model trained to predict test.csv. Performance evaluation mainly includes mse. All of the decision is based on entensive comparison. My exploration of the data is reported as follows. Outline * Take a brief look at data * Extract useful features from train data * Merge weather data from other sources * Plot analysis * Prepare the data for machine learning * Train models * Fine tune models * Model evaluation * Prediction on test data # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ): for filename in filenames : print ( os . path . join ( dirname , filename )) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session # Import necessary libraries import requests from bs4 import BeautifulSoup import datetime import matplotlib.pyplot as plt import seaborn as sns from sklearn.pipeline import Pipeline from sklearn.preprocessing import LabelBinarizer , OneHotEncoder , StandardScaler from sklearn.compose import ColumnTransformer from sklearn.model_selection import train_test_split from sklearn.model_selection import TimeSeriesSplit from sklearn.model_selection import GridSearchCV from xgboost import XGBRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error import warnings warnings . filterwarnings ( \"ignore\" ) from google.colab import drive drive . mount ( '/content/drive' ) Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True). Take a brief look at data \u00b6 # Importing dataset from csv to data frame train = pd . read_csv ( '/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/train.csv' ) test = pd . read_csv ( '/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/test.csv' ) train . head ( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id date speed 0 0 1/1/2017 0:00 43.002930 1 1 1/1/2017 1:00 46.118696 2 2 1/1/2017 2:00 44.294158 3 3 1/1/2017 3:00 41.067468 4 4 1/1/2017 4:00 46.448653 5 5 1/1/2017 5:00 46.797766 6 6 1/1/2017 6:00 44.404925 7 7 1/1/2017 7:00 45.255897 8 8 1/1/2017 8:00 45.680859 9 9 1/1/2017 9:00 48.435676 print ( train . shape ) print ( test . shape ) (14006, 3) (3504, 2) train . dtypes id int64 date object speed float64 dtype: object train . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 14006 entries, 0 to 14005 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 14006 non-null int64 1 date 14006 non-null object 2 speed 14006 non-null float64 dtypes: float64(1), int64(1), object(1) memory usage: 328.4+ KB train . isna () . value_counts () id date speed False False False 14006 dtype: int64 No Na in data. train . drop ( 'id' , axis = 1 ) . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } speed count 14006.000000 mean 32.779118 std 13.573813 min 2.573417 25% 19.301089 50% 36.580595 75% 45.877665 max 53.161286 Speed is around (32.78 \u00b1 13.57). print ( \"max date : \" + train . date . max ()) print ( \"min date : \" + train . date . min ()) max date : 9/9/2018 8:00 min date : 1/1/2017 0:00 trainset = train . copy () trainset . drop ( 'id' , axis = 1 , inplace = True ) Extract useful features from train data \u00b6 # Split datetime trainset [ 'year' ] = trainset [ 'date' ] . apply ( lambda y : int ( y . split ()[ 0 ] . split ( '/' )[ 2 ])) trainset [ 'month' ] = trainset [ 'date' ] . apply ( lambda m : int ( m . split ()[ 0 ] . split ( '/' )[ 1 ])) trainset [ 'day' ] = trainset [ 'date' ] . apply ( lambda d : int ( d . split ()[ 0 ] . split ( '/' )[ 0 ])) trainset [ 'time' ] = trainset [ 'date' ] . apply ( lambda t : int ( t . split ()[ 1 ] . split ( ':' )[ 0 ])) trainset [ 'datetime' ] = trainset [ 'date' ] . apply ( lambda x : datetime . datetime ( int ( x . split ()[ 0 ] . split ( '/' )[ 2 ]), int ( x . split ()[ 0 ] . split ( '/' )[ 1 ]), int ( x . split ()[ 0 ] . split ( '/' )[ 0 ]), int ( x . split ()[ 1 ] . split ( ':' )[ 0 ]) ) ) # Whether a day is weekend or not trainset [ 'IsWeekend' ] = trainset [ 'datetime' ] . apply ( lambda x : 0 if x . weekday () in [ 0 , 1 , 2 , 3 , 4 ] else 1 ) # Whether a day is Hong Kong General Holidays trainset [ 'IsHoliday' ] = trainset [ 'datetime' ] . apply ( lambda x : 1 if ( x . date () . strftime ( '%Y-%m- %d ' ) in [ '2017-01-02' , '2017-01-28' , '2017-01-30' , '2017-01-31' , '2017-04-04' , '2017-04-14' , '2017-04-15' , '2017-04-17' , '2017-05-01' , '2017-05-03' , '2017-05-30' , '2017-07-01' , '2017-10-02' , '2017-10-05' , '2017-10-28' , '2017-12-25' , '2017-12-26' , '2018-01-01' , '2018-02-16' , '2018-02-17' , '2018-02-19' , '2018-03-30' , '2018-03-31' , '2018-04-02' , '2018-04-05' , '2018-05-01' , '2018-05-22' , '2018-06-18' , '2018-07-02' , '2018-09-25' , '2018-10-01' , '2018-10-17' , '2018-12-25' , '2018-12-26' ]) or ( x . weekday () in [ 6 ]) else 0 ) Hong Kong General Holiday information is taken from https://www.gov.hk/en/about/abouthk/holiday/2018.htm # Categorizing hours to different time periods like morning, afternoon, etc. def hour_modify ( x ): Early_Morning = [ 4 , 5 , 6 , 7 ] Morning = [ 8 , 9 , 10 , 11 ] Afternoon = [ 12 , 13 , 14 , 15 ] Evening = [ 16 , 17 , 18 , 19 ] Night = [ 20 , 21 , 22 , 23 ] Late_Night = [ 0 , 1 , 2 , 3 ] if x in Early_Morning : return 'Early_Morning' elif x in Morning : return 'Morning' elif x in Afternoon : return 'Afternoon' elif x in Evening : return 'Evening' elif x in Night : return 'Night' else : return 'Late_Night' trainset [ 'hour_modify' ] = trainset [ 'time' ] . apply ( hour_modify ) trainset . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date speed year month day time datetime IsWeekend IsHoliday hour_modify 0 1/1/2017 0:00 43.002930 2017 1 1 0 2017-01-01 00:00:00 1 1 Late_Night 1 1/1/2017 1:00 46.118696 2017 1 1 1 2017-01-01 01:00:00 1 1 Late_Night 2 1/1/2017 2:00 44.294158 2017 1 1 2 2017-01-01 02:00:00 1 1 Late_Night 3 1/1/2017 3:00 41.067468 2017 1 1 3 2017-01-01 03:00:00 1 1 Late_Night 4 1/1/2017 4:00 46.448653 2017 1 1 4 2017-01-01 04:00:00 1 1 Early_Morning Merge weather data from other sources \u00b6 As is known to all, the traffic speed is largely related to weather like visibility, amount of rainfall. And quite often an accident is likely to occur in a foggy or snowy weather, which would also affect speed in traffic. Broadly, I collected daily weather data from HONG KONG OBSERVATORY. For collection, I crawlled the data from their website as follows. def crawl_weather (): YEAR = [ '2017' , '2018' ] MONTH = [ '01' , '02' , '03' , '04' , '05' , '06' , '07' , '08' , '09' , '10' , '11' , '12' ] URL = 'https://www.hko.gov.hk/en/wxinfo/pastwx/metob' wr = pd . DataFrame ({}, columns = [ 'year' , 'month' , 'day' , 'MPressure(hPa)' , 'MaxTemp(\u2103)' , 'MinTemp(\u2103)' , 'MCloud(%)' , 'TRainfall(mm)' , '#hRedVisi(h)' , 'WindDirect(degrees)' , 'MWindSpeed(km/h)' ], index = range ( 365 * 2 )) cnt = 0 for yr in YEAR : for mon in MONTH : url = URL + yr + mon + '.htm' ymhtml = requests . get ( url ) soup = BeautifulSoup ( ymhtml . text , 'lxml' ) tbls = soup . find_all ( 'table' ) tbl0 = tbls [ 0 ] . find_all ( 'tr' ) tbl1 = tbls [ 1 ] . find_all ( 'tr' ) for tr1 , tr2 in zip ( tbl0 [ 2 : - 3 ], tbl1 [ 1 : - 3 ]): wr . iloc [ cnt , 0 ] = yr wr . iloc [ cnt , 1 ] = mon tds1 = tr1 . find_all ( 'td' ) tds2 = tr2 . find_all ( 'td' ) wr . iloc [ cnt , 2 ] = tds1 [ 0 ] . contents [ 0 ] wr . iloc [ cnt , 3 ] = tds1 [ 1 ] . contents [ 0 ] wr . iloc [ cnt , 4 ] = tds1 [ 2 ] . contents [ 0 ] wr . iloc [ cnt , 5 ] = tds1 [ 4 ] . contents [ 0 ] wr . iloc [ cnt , 6 ] = tds1 [ 7 ] . contents [ 0 ] wr . iloc [ cnt , 7 ] = tds1 [ 8 ] . contents [ 0 ] wr . iloc [ cnt , 8 ] = tds2 [ 1 ] . contents [ 0 ] wr . iloc [ cnt , 9 ] = tds2 [ 5 ] . contents [ 0 ] wr . iloc [ cnt , 10 ] = tds2 [ 6 ] . contents [ 0 ] cnt += 1 wr . to_csv ( '../input/weather/wr.csv' ) For connection reason on kaggle, I can only run the crawl_weather() above locally and upload csv file. weather = pd . read_csv ( \"/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/wr.csv\" ) weather . drop ( \"Unnamed: 0\" , axis = 1 , inplace = True ) weather [ 'year' ] . apply ( lambda t : int ( t )) weather [ 'month' ] . apply ( lambda t : int ( t )) weather [ 'day' ] . apply ( lambda t : int ( t )) weather . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day MPressure(hPa) MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm) #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) 0 2017 1 1 1021.7 20.8 18.4 72 - 0 60 34.2 1 2017 1 2 1020.2 23.3 18.4 28 - 0 70 17.7 2 2017 1 3 1019.8 21.3 18.9 56 - 5 70 26.1 3 2017 1 4 1018.7 21.7 18.7 51 - 0 70 27.7 4 2017 1 5 1016.9 23.4 18.9 61 - 0 40 14.3 I extracted these necessary information in weather table. Column information is listed as follows: * year: Year of weather * month: Month of weather * day: Day of weather * MPressure(hPa): Mean Pressure in hPa * MaxTemp(\u2103): Maximum air temperature in deg. C of a day * MinTemp(\u2103): Minimum air temperature in deg. C of a day * MCloud(%): Mean amount of cloud (%) of a day * TRainfall(mm): Total rainfall (mm) of a day * #hRedVisi(h): Number of hours of reduced visibility# (hours) * WindDirect(degrees): Prevailing wind direction (degrees) * MWindSpeed(km/h): Mean wind speed (km/h) weather . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 730 entries, 0 to 729 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 year 730 non-null int64 1 month 730 non-null int64 2 day 730 non-null int64 3 MPressure(hPa) 730 non-null float64 4 MaxTemp(\u2103) 730 non-null float64 5 MinTemp(\u2103) 730 non-null float64 6 MCloud(%) 730 non-null int64 7 TRainfall(mm) 730 non-null object 8 #hRedVisi(h) 730 non-null int64 9 WindDirect(degrees) 730 non-null int64 10 MWindSpeed(km/h) 730 non-null float64 dtypes: float64(4), int64(6), object(1) memory usage: 62.9+ KB weather . drop ([ 'year' , 'month' , 'day' ], axis = 1 ) . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MPressure(hPa) MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) count 730.000000 730.000000 730.000000 730.000000 730.000000 730.000000 730.000000 mean 1012.883562 26.564110 22.012877 70.315068 1.032877 140.671233 23.796849 std 6.504514 5.354513 5.088792 22.101572 2.924569 110.679229 10.542179 min 990.900000 10.600000 6.800000 4.000000 0.000000 10.000000 4.000000 25% 1008.400000 22.700000 17.900000 59.000000 0.000000 60.000000 15.900000 50% 1013.400000 27.400000 23.000000 79.000000 0.000000 80.000000 23.600000 75% 1017.500000 31.100000 26.400000 88.000000 0.000000 220.000000 30.650000 max 1028.200000 36.600000 29.800000 100.000000 21.000000 360.000000 101.300000 weather [ 'TRainfall(mm)' ] . value_counts () - 297 Trace 153 0.2 17 0.1 15 0.3 13 ... 2.1 1 8.5 1 14.4 1 13.5 1 9.7 1 Name: TRainfall(mm), Length: 174, dtype: int64 # Clean the Na weather [ 'TRainfall(mm)_num' ] = weather [ 'TRainfall(mm)' ] . apply ( lambda x : 0 if x in [ '-' , 'Trace' ] else x ) Although cloud coverage, rainfall, and visibility vary from time to time in a day, consequently having an influence on speed, we assume they do not change too much within a day's range. Then we can merge weather table onto train table. trainset [ 'common' ] = trainset [ 'datetime' ] . apply ( lambda x : x . date ()) def addup ( x ): return datetime . datetime ( x [ 0 ], x [ 1 ], x [ 2 ]) . date () weather [ 'common' ] = weather . apply ( addup , axis = 1 ) data = pd . merge ( trainset [[ 'datetime' , 'year' , 'month' , 'day' , 'time' , 'speed' , 'IsWeekend' , 'IsHoliday' , 'hour_modify' , 'common' ]], weather [[ 'MaxTemp(\u2103)' , 'MinTemp(\u2103)' , 'MCloud(%)' , 'TRainfall(mm)_num' , '#hRedVisi(h)' , 'WindDirect(degrees)' , 'MWindSpeed(km/h)' , 'common' ]], on = 'common' , how = 'left' ) data . sample ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } datetime year month day time speed IsWeekend IsHoliday hour_modify common MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm)_num #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) 13441 2018-11-19 19:00:00 2018 11 19 19 24.992212 0 0 Evening 2018-11-19 25.8 22.0 78 0 2 10 21.5 6787 2017-10-11 05:00:00 2017 10 11 5 47.357381 0 0 Early_Morning 2017-10-11 32.5 28.3 43 0.2 0 70 31.9 10969 2018-05-31 00:00:00 2018 5 31 0 45.111513 0 0 Late_Night 2018-05-31 34.8 28.9 57 0 0 230 20.2 2458 2017-04-13 19:00:00 2017 4 13 19 13.613956 0 0 Evening 2017-04-13 21.5 18.8 86 0 0 40 22.8 2926 2017-05-03 07:00:00 2017 5 3 7 40.589572 0 1 Early_Morning 2017-05-03 31.3 25.6 79 0 0 130 12.2 6448 2017-09-27 02:00:00 2017 9 27 2 49.145292 0 0 Late_Night 2017-09-27 33.0 27.7 34 0 1 200 6.2 11853 2018-07-29 10:00:00 2018 7 29 10 38.890240 1 1 Morning 2018-07-29 34.3 27.9 41 0 0 200 9.0 7206 2017-10-28 16:00:00 2017 10 28 16 35.552712 1 1 Evening 2017-10-28 28.0 22.5 21 0 0 360 18.5 874 2017-02-06 19:00:00 2017 2 6 19 33.605201 0 0 Evening 2017-02-06 19.7 16.9 80 0 3 70 38.5 12781 2018-10-02 23:00:00 2018 10 2 23 43.320265 0 0 Night 2018-10-02 30.4 25.3 33 0 0 80 24.8 3414 2017-05-23 15:00:00 2017 5 23 15 22.593669 0 0 Afternoon 2017-05-23 28.5 24.6 86 4.1 1 100 15.9 5404 2017-08-14 14:00:00 2017 8 14 14 17.242391 0 0 Afternoon 2017-08-14 32.5 28.8 58 0 0 240 23.6 6675 2017-10-06 13:00:00 2017 10 6 13 22.288706 0 0 Afternoon 2017-10-06 31.1 27.4 83 0.2 0 70 36.4 4080 2017-06-20 09:00:00 2017 6 20 9 16.305993 0 0 Morning 2017-06-20 28.2 25.2 88 24.8 0 240 13.9 1939 2017-03-23 04:00:00 2017 3 23 4 48.648829 0 0 Early_Morning 2017-03-23 24.6 19.0 83 0 6 50 16.3 5128 2017-08-03 02:00:00 2017 8 3 2 45.672921 0 0 Late_Night 2017-08-03 29.8 25.3 90 66.7 0 70 6.2 10234 2018-04-11 07:00:00 2018 4 11 7 29.919863 0 0 Early_Morning 2018-04-11 27.6 22.5 75 0 0 130 9.9 10740 2018-05-16 03:00:00 2018 5 16 3 48.558058 0 0 Late_Night 2018-05-16 32.2 26.1 46 0 0 150 12.6 1991 2017-03-25 08:00:00 2017 3 25 8 27.639233 1 0 Morning 2017-03-25 23.4 16.5 82 0 2 20 23.1 13536 2018-11-27 07:00:00 2018 11 27 7 24.001901 0 0 Early_Morning 2018-11-27 22.5 19.0 89 16.3 0 40 25.9 Plot analysis \u00b6 Univariate analysis data_taken_for_train = data . drop ([ 'datetime' , 'common' ], axis = 1 ) data_taken_for_train . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day time speed IsWeekend IsHoliday hour_modify MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm)_num #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) 0 2017 1 1 0 43.002930 1 1 Late_Night 20.8 18.4 72 0 0 60 34.2 1 2017 1 1 1 46.118696 1 1 Late_Night 20.8 18.4 72 0 0 60 34.2 2 2017 1 1 2 44.294158 1 1 Late_Night 20.8 18.4 72 0 0 60 34.2 3 2017 1 1 3 41.067468 1 1 Late_Night 20.8 18.4 72 0 0 60 34.2 4 2017 1 1 4 46.448653 1 1 Early_Morning 20.8 18.4 72 0 0 60 34.2 plt . figure ( figsize = ( 6 , 4 )) sns . boxplot ( 'speed' , data = data_taken_for_train , orient = 'h' , palette = \"Set3\" , linewidth = 2.5 ) plt . show () No significant outlier. No need to eliminate data. Traffic speed across months. tmp_data = data_taken_for_train . groupby ( 'month' ) . aggregate ({ 'speed' : 'mean' }) plt . figure ( figsize = ( 8 , 6 )) sns . lineplot ( x = tmp_data . index , y = tmp_data . speed , data = tmp_data , palette = \"Set2\" ) plt . show () Count on different hour stage. plt . figure ( figsize = ( 8 , 6 )) sns . countplot ( y = 'hour_modify' , data = data_taken_for_train , palette = [ \"#7fcdbb\" , \"#edf8b1\" , \"#fc9272\" , \"#fee0d2\" , \"#bcbddc\" , \"#efedf5\" ]) plt . show () data_taken_for_train . hist ( bins = 50 , figsize = ( 20 , 15 )) plt . show () Speed is high in rush hour, and speed in the afternoon is higher than morning. There seems some positive correlation between max/ min temperature, mean cloud coverage and speed. And negative correlation between wind mean wind speed and speed. Some left skewed in number of hours of reduced visibility# (hours) and some right skewed in mean cloud coverage (%). Categorical variables are almost balanced. Bivariate analysis Plotting relationship between speed, MaxTemp, MinTemp, MCloud, #hRedVisi(h), WindDirect, MWindSpeed. num_vars = [ 'speed' , 'MaxTemp(\u2103)' , 'MinTemp(\u2103)' , 'MCloud(%)' , '#hRedVisi(h)' , 'WindDirect(degrees)' , 'MWindSpeed(km/h)' ] from pandas.plotting import scatter_matrix scatter_matrix ( data_taken_for_train [ num_vars ], figsize = ( 20 , 15 )) plt . show () Plotting MaxTemp(\u2103), MinTemp(\u2103), MCloud(%), MWindSpeed(km/h) against speed. plt . figure ( figsize = ( 10 , 8 )) sns . set_style ( 'darkgrid' ) sns . jointplot ( y = 'speed' , x = 'MaxTemp(\u2103)' , data = data_taken_for_train ) sns . set_style ( 'darkgrid' ) sns . jointplot ( y = 'speed' , x = 'MinTemp(\u2103)' , data = data_taken_for_train ) sns . set_style ( 'darkgrid' ) sns . jointplot ( y = 'speed' , x = 'MCloud(%)' , data = data_taken_for_train ) sns . set_style ( 'darkgrid' ) sns . jointplot ( y = 'speed' , x = 'MWindSpeed(km/h)' , data = data_taken_for_train ) plt . show () <Figure size 720x576 with 0 Axes> Plotting traffic speed over mean cloud %. plt . figure ( figsize = ( 35 , 10 )) sns . barplot ( x = 'MCloud(%)' , y = 'speed' , data = data_taken_for_train ) plt . show () Analysing the correlation matrix of the numerical variables. plt . figure ( figsize = ( 20 , 10 )) sns . heatmap ( data_taken_for_train . corr (), annot = True ) plt . title ( 'Correlation' ) plt . show () Prepare the data for machine learning \u00b6 data_taken_for_train . columns Index(['year', 'month', 'day', 'time', 'speed', 'IsWeekend', 'IsHoliday', 'hour_modify', 'MaxTemp(\u2103)', 'MinTemp(\u2103)', 'MCloud(%)', 'TRainfall(mm)_num', '#hRedVisi(h)', 'WindDirect(degrees)', 'MWindSpeed(km/h)'], dtype='object') data_taken_for_train .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day time speed IsWeekend IsHoliday hour_modify MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm)_num #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) 0 2017 1 1 0 43.002930 1 1 Late_Night 20.8 18.4 72 0 0 60 34.2 1 2017 1 1 1 46.118696 1 1 Late_Night 20.8 18.4 72 0 0 60 34.2 2 2017 1 1 2 44.294158 1 1 Late_Night 20.8 18.4 72 0 0 60 34.2 3 2017 1 1 3 41.067468 1 1 Late_Night 20.8 18.4 72 0 0 60 34.2 4 2017 1 1 4 46.448653 1 1 Early_Morning 20.8 18.4 72 0 0 60 34.2 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 14001 2018 12 31 12 19.865269 0 0 Afternoon 15.6 11.8 77 0 0 360 26.8 14002 2018 12 31 15 17.820375 0 0 Afternoon 15.6 11.8 77 0 0 360 26.8 14003 2018 12 31 16 12.501851 0 0 Evening 15.6 11.8 77 0 0 360 26.8 14004 2018 12 31 18 15.979319 0 0 Evening 15.6 11.8 77 0 0 360 26.8 14005 2018 12 31 20 40.594183 0 0 Night 15.6 11.8 77 0 0 360 26.8 14006 rows \u00d7 15 columns target = [ 'speed' ] cat_vars = [ 'IsWeekend' , 'IsHoliday' , 'hour_modify' ] num_vars = [ 'year' , 'month' , 'day' , 'time' , 'MaxTemp(\u2103)' , 'MinTemp(\u2103)' , 'MCloud(%)' , 'TRainfall(mm)_num' , '#hRedVisi(h)' , 'WindDirect(degrees)' , 'MWindSpeed(km/h)' ] # Creating pipeline to transform data numeric_transformer = Pipeline ( steps = [ ( 'scaler' , StandardScaler ())]) categorical_transformer = Pipeline ( steps = [ ( 'oneHot' , OneHotEncoder ())]) preprocessor = ColumnTransformer ( transformers = [ ( 'num' , numeric_transformer , num_vars ), ( 'cat' , categorical_transformer , cat_vars )]) data_transformed = preprocessor . fit_transform ( data_taken_for_train ) print ( data_transformed . shape ) (14006, 21) # Split the data into trainset and testset. # direction 1 X_train , X_test , y_train , y_test = train_test_split ( data_transformed , data_taken_for_train [ 'speed' ], test_size = 0.15 , random_state = 50 ) print ( 'Shape of X_train: ' , str ( X_train . shape )); print ( 'Shape of y_train: ' , str ( y_train . shape )) print ( 'Shape of X_test: ' , str ( X_test . shape )); print ( 'Shape of y_test: ' , str ( y_test . shape )) Shape of X_train: (11905, 21) Shape of y_train: (11905,) Shape of X_test: (2101, 21) Shape of y_test: (2101,) Train and fine tune models \u00b6 Regression task. # 1. train a XGBoostRegressor tscv = TimeSeriesSplit ( n_splits = 3 ) model = XGBRegressor () param_grid = { 'nthread' :[ 4 , 6 , 8 ], 'objective' :[ 'reg:squarederror' ], 'learning_rate' :[ . 03 , 0.05 , . 07 ], 'max_depth' :[ 5 , 6 , 7 , 8 ], 'min_child_weight' :[ 4 ], 'subsample' :[ 0.7 ], 'colsample_bytree' :[ 0.7 ], 'n_estimators' :[ 500 , 700 ]} GridSearch = GridSearchCV ( estimator = model , param_grid = param_grid , cv = tscv , n_jobs = 2 , verbose = 10 ) GridSearch . fit ( X_train , y_train ) y_pred = GridSearch . predict ( X_test ) Fitting 3 folds for each of 72 candidates, totalling 216 fits [Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers. [Parallel(n_jobs=2)]: Done 1 tasks | elapsed: 4.9s [Parallel(n_jobs=2)]: Done 4 tasks | elapsed: 15.0s [Parallel(n_jobs=2)]: Done 9 tasks | elapsed: 33.8s [Parallel(n_jobs=2)]: Done 14 tasks | elapsed: 56.9s [Parallel(n_jobs=2)]: Done 21 tasks | elapsed: 1.5min [Parallel(n_jobs=2)]: Done 28 tasks | elapsed: 2.0min [Parallel(n_jobs=2)]: Done 37 tasks | elapsed: 2.8min [Parallel(n_jobs=2)]: Done 46 tasks | elapsed: 3.6min [Parallel(n_jobs=2)]: Done 57 tasks | elapsed: 4.9min [Parallel(n_jobs=2)]: Done 68 tasks | elapsed: 6.1min [Parallel(n_jobs=2)]: Done 81 tasks | elapsed: 7.2min [Parallel(n_jobs=2)]: Done 94 tasks | elapsed: 8.1min [Parallel(n_jobs=2)]: Done 109 tasks | elapsed: 9.5min [Parallel(n_jobs=2)]: Done 124 tasks | elapsed: 11.0min [Parallel(n_jobs=2)]: Done 141 tasks | elapsed: 13.0min [Parallel(n_jobs=2)]: Done 158 tasks | elapsed: 14.3min [Parallel(n_jobs=2)]: Done 177 tasks | elapsed: 15.8min [Parallel(n_jobs=2)]: Done 196 tasks | elapsed: 17.7min [Parallel(n_jobs=2)]: Done 216 out of 216 | elapsed: 20.1min finished # XGBoostRegressor for direction 2 GridSearch_d2 = GridSearchCV ( estimator = model , param_grid = param_grid , cv = tscv , n_jobs = 2 ) GridSearch_d2 . fit ( X_2017 , y_2017 ) y_pred_2018 = GridSearch_d2 . predict ( X_2018 ) # 2. train a RandomForestRegressor model2 = RandomForestRegressor () param_grid2 = { 'n_estimators' :[ 10 , 50 , 100 , 1000 ], 'max_features' :[ 1 , 2 , 3 ] } GridSearch2 = GridSearchCV ( estimator = model2 , param_grid = param_grid2 , cv = tscv , n_jobs = 2 , verbose = 10 ) GridSearch2 . fit ( X_train , y_train ) y_pred2 = GridSearch2 . predict ( X_test ) Fitting 3 folds for each of 12 candidates, totalling 36 fits [Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers. [Parallel(n_jobs=2)]: Done 1 tasks | elapsed: 0.1s [Parallel(n_jobs=2)]: Batch computation too fast (0.1413s.) Setting batch_size=2. [Parallel(n_jobs=2)]: Done 4 tasks | elapsed: 0.6s [Parallel(n_jobs=2)]: Batch computation too slow (3.4045s.) Setting batch_size=1. [Parallel(n_jobs=2)]: Done 14 tasks | elapsed: 12.6s [Parallel(n_jobs=2)]: Done 19 tasks | elapsed: 18.3s [Parallel(n_jobs=2)]: Done 27 tasks | elapsed: 41.9s [Parallel(n_jobs=2)]: Done 36 out of 36 | elapsed: 1.3min finished Model evaluation \u00b6 MSE_xgb = mean_squared_error ( y_pred , y_test ) # 10.7063378519613 MSE_rf = mean_squared_error ( y_pred2 , y_test ) print ( 'MSE for XGBoost is ' + str ( MSE_xgb )) print ( 'MSE for RandomForest is ' + str ( MSE_rf )) MSE for XGBoost is 10.874016698999894 MSE for RandomForest is 13.34623796947232 XGBoost performs better than RandomForest in this regression task. We use XGBoost for the following prediction. Performance is not so good in direction 2 as in direction 1. Maybe the proportion of testset is too large in this way and undermined model performace. Therefore, I choose to use XGBoost built in direction 1 to predict test.csv. Prediction on test data \u00b6 # Prepare for prediction testset = test . copy () testset . drop ( 'id' , axis = 1 , inplace = True ) testset [ 'year' ] = testset [ 'date' ] . apply ( lambda y : int ( y . split ()[ 0 ] . split ( '/' )[ 2 ])) testset [ 'month' ] = testset [ 'date' ] . apply ( lambda m : int ( m . split ()[ 0 ] . split ( '/' )[ 1 ])) testset [ 'day' ] = testset [ 'date' ] . apply ( lambda d : int ( d . split ()[ 0 ] . split ( '/' )[ 0 ])) testset [ 'time' ] = testset [ 'date' ] . apply ( lambda t : int ( t . split ()[ 1 ] . split ( ':' )[ 0 ])) testset [ 'datetime' ] = testset [ 'date' ] . apply ( lambda x : datetime . datetime ( int ( x . split ()[ 0 ] . split ( '/' )[ 2 ]), int ( x . split ()[ 0 ] . split ( '/' )[ 1 ]), int ( x . split ()[ 0 ] . split ( '/' )[ 0 ]), int ( x . split ()[ 1 ] . split ( ':' )[ 0 ]) ) ) testset [ 'IsWeekend' ] = testset [ 'datetime' ] . apply ( lambda x : 0 if x . weekday () in [ 0 , 1 , 2 , 3 , 4 ] else 1 ) testset [ 'IsHoliday' ] = testset [ 'datetime' ] . apply ( lambda x : 1 if ( x . date () . strftime ( '%Y-%m- %d ' ) in [ '2017-01-02' , '2017-01-28' , '2017-01-30' , '2017-01-31' , '2017-04-04' , '2017-04-14' , '2017-04-15' , '2017-04-17' , '2017-05-01' , '2017-05-03' , '2017-05-30' , '2017-07-01' , '2017-10-02' , '2017-10-05' , '2017-10-28' , '2017-12-25' , '2017-12-26' , '2018-01-01' , '2018-02-16' , '2018-02-17' , '2018-02-19' , '2018-03-30' , '2018-03-31' , '2018-04-02' , '2018-04-05' , '2018-05-01' , '2018-05-22' , '2018-06-18' , '2018-07-02' , '2018-09-25' , '2018-10-01' , '2018-10-17' , '2018-12-25' , '2018-12-26' ]) or ( x . weekday () in [ 6 ]) else 0 ) testset [ 'hour_modify' ] = testset [ 'time' ] . apply ( hour_modify ) testset [ 'common' ] = testset [ 'datetime' ] . apply ( lambda x : x . date ()) testdata = pd . merge ( testset [[ 'datetime' , 'year' , 'month' , 'day' , 'time' , 'IsWeekend' , 'IsHoliday' , 'hour_modify' , 'common' ]], weather [[ 'MaxTemp(\u2103)' , 'MinTemp(\u2103)' , 'MCloud(%)' , 'TRainfall(mm)_num' , '#hRedVisi(h)' , 'WindDirect(degrees)' , 'MWindSpeed(km/h)' , 'common' ]], on = 'common' , how = 'left' ) testdata . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } datetime year month day time IsWeekend IsHoliday hour_modify common MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm)_num #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) 0 2018-01-01 02:00:00 2018 1 1 2 0 1 Late_Night 2018-01-01 19.0 16.3 75 0 21 70 28.4 1 2018-01-01 05:00:00 2018 1 1 5 0 1 Early_Morning 2018-01-01 19.0 16.3 75 0 21 70 28.4 2 2018-01-01 07:00:00 2018 1 1 7 0 1 Early_Morning 2018-01-01 19.0 16.3 75 0 21 70 28.4 3 2018-01-01 08:00:00 2018 1 1 8 0 1 Morning 2018-01-01 19.0 16.3 75 0 21 70 28.4 4 2018-01-01 10:00:00 2018 1 1 10 0 1 Morning 2018-01-01 19.0 16.3 75 0 21 70 28.4 # Prediction test_transformed = preprocessor . fit_transform ( testdata ) y_test_pred = GridSearch . predict ( test_transformed ) print ( len ( y_test_pred )) test_transformed . shape [ 0 ] 3504 3504 # Write into test.casv and save as submission_20710754.csv pred = pd . DataFrame ({ 'speed' : list ( y_test_pred )}, columns = [ 'speed' ]) submission = pd . concat ([ test . drop ( 'date' , axis = 1 ), pred ], axis = 1 ) submission . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id speed 0 0 47.546001 1 1 47.211971 2 2 39.180874 3 3 32.459953 4 4 40.378597 submission . shape (3504, 2) submission . to_csv ( 'submission_20720230.csv' , index = False )","title":"Msbd5001-fall2020"},{"location":"MSBD5001/project/msbd5001-fall2020/#task-description","text":"Use the train data which contains 2017 and part of 2018 average traffic speed of a major road in Hong Kong and their corresponding timestamp to predict speed in 2018. The total work includes data integration, feature engineering, model training and final prediction. Task is roughly categorized as regression, so I tried a collection of regressors like RandomForestRegressor or XGBoostRegressor and selected the best one. For model construction, I mainly tried two ways. One is built on randomly spliting trainset and testset from train.csv and use the model to predict test.csv. Another is built on data in 2017, evaluate the model on 2018 in train.csv, and use the final model trained to predict test.csv. Performance evaluation mainly includes mse. All of the decision is based on entensive comparison. My exploration of the data is reported as follows. Outline * Take a brief look at data * Extract useful features from train data * Merge weather data from other sources * Plot analysis * Prepare the data for machine learning * Train models * Fine tune models * Model evaluation * Prediction on test data # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ): for filename in filenames : print ( os . path . join ( dirname , filename )) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session # Import necessary libraries import requests from bs4 import BeautifulSoup import datetime import matplotlib.pyplot as plt import seaborn as sns from sklearn.pipeline import Pipeline from sklearn.preprocessing import LabelBinarizer , OneHotEncoder , StandardScaler from sklearn.compose import ColumnTransformer from sklearn.model_selection import train_test_split from sklearn.model_selection import TimeSeriesSplit from sklearn.model_selection import GridSearchCV from xgboost import XGBRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error import warnings warnings . filterwarnings ( \"ignore\" ) from google.colab import drive drive . mount ( '/content/drive' ) Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).","title":"Task description"},{"location":"MSBD5001/project/msbd5001-fall2020/#take-a-brief-look-at-data","text":"# Importing dataset from csv to data frame train = pd . read_csv ( '/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/train.csv' ) test = pd . read_csv ( '/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/test.csv' ) train . head ( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id date speed 0 0 1/1/2017 0:00 43.002930 1 1 1/1/2017 1:00 46.118696 2 2 1/1/2017 2:00 44.294158 3 3 1/1/2017 3:00 41.067468 4 4 1/1/2017 4:00 46.448653 5 5 1/1/2017 5:00 46.797766 6 6 1/1/2017 6:00 44.404925 7 7 1/1/2017 7:00 45.255897 8 8 1/1/2017 8:00 45.680859 9 9 1/1/2017 9:00 48.435676 print ( train . shape ) print ( test . shape ) (14006, 3) (3504, 2) train . dtypes id int64 date object speed float64 dtype: object train . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 14006 entries, 0 to 14005 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 14006 non-null int64 1 date 14006 non-null object 2 speed 14006 non-null float64 dtypes: float64(1), int64(1), object(1) memory usage: 328.4+ KB train . isna () . value_counts () id date speed False False False 14006 dtype: int64 No Na in data. train . drop ( 'id' , axis = 1 ) . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } speed count 14006.000000 mean 32.779118 std 13.573813 min 2.573417 25% 19.301089 50% 36.580595 75% 45.877665 max 53.161286 Speed is around (32.78 \u00b1 13.57). print ( \"max date : \" + train . date . max ()) print ( \"min date : \" + train . date . min ()) max date : 9/9/2018 8:00 min date : 1/1/2017 0:00 trainset = train . copy () trainset . drop ( 'id' , axis = 1 , inplace = True )","title":"Take a brief look at data"},{"location":"MSBD5001/project/msbd5001-fall2020/#extract-useful-features-from-train-data","text":"# Split datetime trainset [ 'year' ] = trainset [ 'date' ] . apply ( lambda y : int ( y . split ()[ 0 ] . split ( '/' )[ 2 ])) trainset [ 'month' ] = trainset [ 'date' ] . apply ( lambda m : int ( m . split ()[ 0 ] . split ( '/' )[ 1 ])) trainset [ 'day' ] = trainset [ 'date' ] . apply ( lambda d : int ( d . split ()[ 0 ] . split ( '/' )[ 0 ])) trainset [ 'time' ] = trainset [ 'date' ] . apply ( lambda t : int ( t . split ()[ 1 ] . split ( ':' )[ 0 ])) trainset [ 'datetime' ] = trainset [ 'date' ] . apply ( lambda x : datetime . datetime ( int ( x . split ()[ 0 ] . split ( '/' )[ 2 ]), int ( x . split ()[ 0 ] . split ( '/' )[ 1 ]), int ( x . split ()[ 0 ] . split ( '/' )[ 0 ]), int ( x . split ()[ 1 ] . split ( ':' )[ 0 ]) ) ) # Whether a day is weekend or not trainset [ 'IsWeekend' ] = trainset [ 'datetime' ] . apply ( lambda x : 0 if x . weekday () in [ 0 , 1 , 2 , 3 , 4 ] else 1 ) # Whether a day is Hong Kong General Holidays trainset [ 'IsHoliday' ] = trainset [ 'datetime' ] . apply ( lambda x : 1 if ( x . date () . strftime ( '%Y-%m- %d ' ) in [ '2017-01-02' , '2017-01-28' , '2017-01-30' , '2017-01-31' , '2017-04-04' , '2017-04-14' , '2017-04-15' , '2017-04-17' , '2017-05-01' , '2017-05-03' , '2017-05-30' , '2017-07-01' , '2017-10-02' , '2017-10-05' , '2017-10-28' , '2017-12-25' , '2017-12-26' , '2018-01-01' , '2018-02-16' , '2018-02-17' , '2018-02-19' , '2018-03-30' , '2018-03-31' , '2018-04-02' , '2018-04-05' , '2018-05-01' , '2018-05-22' , '2018-06-18' , '2018-07-02' , '2018-09-25' , '2018-10-01' , '2018-10-17' , '2018-12-25' , '2018-12-26' ]) or ( x . weekday () in [ 6 ]) else 0 ) Hong Kong General Holiday information is taken from https://www.gov.hk/en/about/abouthk/holiday/2018.htm # Categorizing hours to different time periods like morning, afternoon, etc. def hour_modify ( x ): Early_Morning = [ 4 , 5 , 6 , 7 ] Morning = [ 8 , 9 , 10 , 11 ] Afternoon = [ 12 , 13 , 14 , 15 ] Evening = [ 16 , 17 , 18 , 19 ] Night = [ 20 , 21 , 22 , 23 ] Late_Night = [ 0 , 1 , 2 , 3 ] if x in Early_Morning : return 'Early_Morning' elif x in Morning : return 'Morning' elif x in Afternoon : return 'Afternoon' elif x in Evening : return 'Evening' elif x in Night : return 'Night' else : return 'Late_Night' trainset [ 'hour_modify' ] = trainset [ 'time' ] . apply ( hour_modify ) trainset . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date speed year month day time datetime IsWeekend IsHoliday hour_modify 0 1/1/2017 0:00 43.002930 2017 1 1 0 2017-01-01 00:00:00 1 1 Late_Night 1 1/1/2017 1:00 46.118696 2017 1 1 1 2017-01-01 01:00:00 1 1 Late_Night 2 1/1/2017 2:00 44.294158 2017 1 1 2 2017-01-01 02:00:00 1 1 Late_Night 3 1/1/2017 3:00 41.067468 2017 1 1 3 2017-01-01 03:00:00 1 1 Late_Night 4 1/1/2017 4:00 46.448653 2017 1 1 4 2017-01-01 04:00:00 1 1 Early_Morning","title":"Extract useful features from train data"},{"location":"MSBD5001/project/msbd5001-fall2020/#merge-weather-data-from-other-sources","text":"As is known to all, the traffic speed is largely related to weather like visibility, amount of rainfall. And quite often an accident is likely to occur in a foggy or snowy weather, which would also affect speed in traffic. Broadly, I collected daily weather data from HONG KONG OBSERVATORY. For collection, I crawlled the data from their website as follows. def crawl_weather (): YEAR = [ '2017' , '2018' ] MONTH = [ '01' , '02' , '03' , '04' , '05' , '06' , '07' , '08' , '09' , '10' , '11' , '12' ] URL = 'https://www.hko.gov.hk/en/wxinfo/pastwx/metob' wr = pd . DataFrame ({}, columns = [ 'year' , 'month' , 'day' , 'MPressure(hPa)' , 'MaxTemp(\u2103)' , 'MinTemp(\u2103)' , 'MCloud(%)' , 'TRainfall(mm)' , '#hRedVisi(h)' , 'WindDirect(degrees)' , 'MWindSpeed(km/h)' ], index = range ( 365 * 2 )) cnt = 0 for yr in YEAR : for mon in MONTH : url = URL + yr + mon + '.htm' ymhtml = requests . get ( url ) soup = BeautifulSoup ( ymhtml . text , 'lxml' ) tbls = soup . find_all ( 'table' ) tbl0 = tbls [ 0 ] . find_all ( 'tr' ) tbl1 = tbls [ 1 ] . find_all ( 'tr' ) for tr1 , tr2 in zip ( tbl0 [ 2 : - 3 ], tbl1 [ 1 : - 3 ]): wr . iloc [ cnt , 0 ] = yr wr . iloc [ cnt , 1 ] = mon tds1 = tr1 . find_all ( 'td' ) tds2 = tr2 . find_all ( 'td' ) wr . iloc [ cnt , 2 ] = tds1 [ 0 ] . contents [ 0 ] wr . iloc [ cnt , 3 ] = tds1 [ 1 ] . contents [ 0 ] wr . iloc [ cnt , 4 ] = tds1 [ 2 ] . contents [ 0 ] wr . iloc [ cnt , 5 ] = tds1 [ 4 ] . contents [ 0 ] wr . iloc [ cnt , 6 ] = tds1 [ 7 ] . contents [ 0 ] wr . iloc [ cnt , 7 ] = tds1 [ 8 ] . contents [ 0 ] wr . iloc [ cnt , 8 ] = tds2 [ 1 ] . contents [ 0 ] wr . iloc [ cnt , 9 ] = tds2 [ 5 ] . contents [ 0 ] wr . iloc [ cnt , 10 ] = tds2 [ 6 ] . contents [ 0 ] cnt += 1 wr . to_csv ( '../input/weather/wr.csv' ) For connection reason on kaggle, I can only run the crawl_weather() above locally and upload csv file. weather = pd . read_csv ( \"/content/drive/MyDrive/courses/HKUST/MSBD5001/project/data/individual project/wr.csv\" ) weather . drop ( \"Unnamed: 0\" , axis = 1 , inplace = True ) weather [ 'year' ] . apply ( lambda t : int ( t )) weather [ 'month' ] . apply ( lambda t : int ( t )) weather [ 'day' ] . apply ( lambda t : int ( t )) weather . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day MPressure(hPa) MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm) #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) 0 2017 1 1 1021.7 20.8 18.4 72 - 0 60 34.2 1 2017 1 2 1020.2 23.3 18.4 28 - 0 70 17.7 2 2017 1 3 1019.8 21.3 18.9 56 - 5 70 26.1 3 2017 1 4 1018.7 21.7 18.7 51 - 0 70 27.7 4 2017 1 5 1016.9 23.4 18.9 61 - 0 40 14.3 I extracted these necessary information in weather table. Column information is listed as follows: * year: Year of weather * month: Month of weather * day: Day of weather * MPressure(hPa): Mean Pressure in hPa * MaxTemp(\u2103): Maximum air temperature in deg. C of a day * MinTemp(\u2103): Minimum air temperature in deg. C of a day * MCloud(%): Mean amount of cloud (%) of a day * TRainfall(mm): Total rainfall (mm) of a day * #hRedVisi(h): Number of hours of reduced visibility# (hours) * WindDirect(degrees): Prevailing wind direction (degrees) * MWindSpeed(km/h): Mean wind speed (km/h) weather . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 730 entries, 0 to 729 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 year 730 non-null int64 1 month 730 non-null int64 2 day 730 non-null int64 3 MPressure(hPa) 730 non-null float64 4 MaxTemp(\u2103) 730 non-null float64 5 MinTemp(\u2103) 730 non-null float64 6 MCloud(%) 730 non-null int64 7 TRainfall(mm) 730 non-null object 8 #hRedVisi(h) 730 non-null int64 9 WindDirect(degrees) 730 non-null int64 10 MWindSpeed(km/h) 730 non-null float64 dtypes: float64(4), int64(6), object(1) memory usage: 62.9+ KB weather . drop ([ 'year' , 'month' , 'day' ], axis = 1 ) . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MPressure(hPa) MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) count 730.000000 730.000000 730.000000 730.000000 730.000000 730.000000 730.000000 mean 1012.883562 26.564110 22.012877 70.315068 1.032877 140.671233 23.796849 std 6.504514 5.354513 5.088792 22.101572 2.924569 110.679229 10.542179 min 990.900000 10.600000 6.800000 4.000000 0.000000 10.000000 4.000000 25% 1008.400000 22.700000 17.900000 59.000000 0.000000 60.000000 15.900000 50% 1013.400000 27.400000 23.000000 79.000000 0.000000 80.000000 23.600000 75% 1017.500000 31.100000 26.400000 88.000000 0.000000 220.000000 30.650000 max 1028.200000 36.600000 29.800000 100.000000 21.000000 360.000000 101.300000 weather [ 'TRainfall(mm)' ] . value_counts () - 297 Trace 153 0.2 17 0.1 15 0.3 13 ... 2.1 1 8.5 1 14.4 1 13.5 1 9.7 1 Name: TRainfall(mm), Length: 174, dtype: int64 # Clean the Na weather [ 'TRainfall(mm)_num' ] = weather [ 'TRainfall(mm)' ] . apply ( lambda x : 0 if x in [ '-' , 'Trace' ] else x ) Although cloud coverage, rainfall, and visibility vary from time to time in a day, consequently having an influence on speed, we assume they do not change too much within a day's range. Then we can merge weather table onto train table. trainset [ 'common' ] = trainset [ 'datetime' ] . apply ( lambda x : x . date ()) def addup ( x ): return datetime . datetime ( x [ 0 ], x [ 1 ], x [ 2 ]) . date () weather [ 'common' ] = weather . apply ( addup , axis = 1 ) data = pd . merge ( trainset [[ 'datetime' , 'year' , 'month' , 'day' , 'time' , 'speed' , 'IsWeekend' , 'IsHoliday' , 'hour_modify' , 'common' ]], weather [[ 'MaxTemp(\u2103)' , 'MinTemp(\u2103)' , 'MCloud(%)' , 'TRainfall(mm)_num' , '#hRedVisi(h)' , 'WindDirect(degrees)' , 'MWindSpeed(km/h)' , 'common' ]], on = 'common' , how = 'left' ) data . sample ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } datetime year month day time speed IsWeekend IsHoliday hour_modify common MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm)_num #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) 13441 2018-11-19 19:00:00 2018 11 19 19 24.992212 0 0 Evening 2018-11-19 25.8 22.0 78 0 2 10 21.5 6787 2017-10-11 05:00:00 2017 10 11 5 47.357381 0 0 Early_Morning 2017-10-11 32.5 28.3 43 0.2 0 70 31.9 10969 2018-05-31 00:00:00 2018 5 31 0 45.111513 0 0 Late_Night 2018-05-31 34.8 28.9 57 0 0 230 20.2 2458 2017-04-13 19:00:00 2017 4 13 19 13.613956 0 0 Evening 2017-04-13 21.5 18.8 86 0 0 40 22.8 2926 2017-05-03 07:00:00 2017 5 3 7 40.589572 0 1 Early_Morning 2017-05-03 31.3 25.6 79 0 0 130 12.2 6448 2017-09-27 02:00:00 2017 9 27 2 49.145292 0 0 Late_Night 2017-09-27 33.0 27.7 34 0 1 200 6.2 11853 2018-07-29 10:00:00 2018 7 29 10 38.890240 1 1 Morning 2018-07-29 34.3 27.9 41 0 0 200 9.0 7206 2017-10-28 16:00:00 2017 10 28 16 35.552712 1 1 Evening 2017-10-28 28.0 22.5 21 0 0 360 18.5 874 2017-02-06 19:00:00 2017 2 6 19 33.605201 0 0 Evening 2017-02-06 19.7 16.9 80 0 3 70 38.5 12781 2018-10-02 23:00:00 2018 10 2 23 43.320265 0 0 Night 2018-10-02 30.4 25.3 33 0 0 80 24.8 3414 2017-05-23 15:00:00 2017 5 23 15 22.593669 0 0 Afternoon 2017-05-23 28.5 24.6 86 4.1 1 100 15.9 5404 2017-08-14 14:00:00 2017 8 14 14 17.242391 0 0 Afternoon 2017-08-14 32.5 28.8 58 0 0 240 23.6 6675 2017-10-06 13:00:00 2017 10 6 13 22.288706 0 0 Afternoon 2017-10-06 31.1 27.4 83 0.2 0 70 36.4 4080 2017-06-20 09:00:00 2017 6 20 9 16.305993 0 0 Morning 2017-06-20 28.2 25.2 88 24.8 0 240 13.9 1939 2017-03-23 04:00:00 2017 3 23 4 48.648829 0 0 Early_Morning 2017-03-23 24.6 19.0 83 0 6 50 16.3 5128 2017-08-03 02:00:00 2017 8 3 2 45.672921 0 0 Late_Night 2017-08-03 29.8 25.3 90 66.7 0 70 6.2 10234 2018-04-11 07:00:00 2018 4 11 7 29.919863 0 0 Early_Morning 2018-04-11 27.6 22.5 75 0 0 130 9.9 10740 2018-05-16 03:00:00 2018 5 16 3 48.558058 0 0 Late_Night 2018-05-16 32.2 26.1 46 0 0 150 12.6 1991 2017-03-25 08:00:00 2017 3 25 8 27.639233 1 0 Morning 2017-03-25 23.4 16.5 82 0 2 20 23.1 13536 2018-11-27 07:00:00 2018 11 27 7 24.001901 0 0 Early_Morning 2018-11-27 22.5 19.0 89 16.3 0 40 25.9","title":"Merge weather data from other sources"},{"location":"MSBD5001/project/msbd5001-fall2020/#plot-analysis","text":"Univariate analysis data_taken_for_train = data . drop ([ 'datetime' , 'common' ], axis = 1 ) data_taken_for_train . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day time speed IsWeekend IsHoliday hour_modify MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm)_num #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) 0 2017 1 1 0 43.002930 1 1 Late_Night 20.8 18.4 72 0 0 60 34.2 1 2017 1 1 1 46.118696 1 1 Late_Night 20.8 18.4 72 0 0 60 34.2 2 2017 1 1 2 44.294158 1 1 Late_Night 20.8 18.4 72 0 0 60 34.2 3 2017 1 1 3 41.067468 1 1 Late_Night 20.8 18.4 72 0 0 60 34.2 4 2017 1 1 4 46.448653 1 1 Early_Morning 20.8 18.4 72 0 0 60 34.2 plt . figure ( figsize = ( 6 , 4 )) sns . boxplot ( 'speed' , data = data_taken_for_train , orient = 'h' , palette = \"Set3\" , linewidth = 2.5 ) plt . show () No significant outlier. No need to eliminate data. Traffic speed across months. tmp_data = data_taken_for_train . groupby ( 'month' ) . aggregate ({ 'speed' : 'mean' }) plt . figure ( figsize = ( 8 , 6 )) sns . lineplot ( x = tmp_data . index , y = tmp_data . speed , data = tmp_data , palette = \"Set2\" ) plt . show () Count on different hour stage. plt . figure ( figsize = ( 8 , 6 )) sns . countplot ( y = 'hour_modify' , data = data_taken_for_train , palette = [ \"#7fcdbb\" , \"#edf8b1\" , \"#fc9272\" , \"#fee0d2\" , \"#bcbddc\" , \"#efedf5\" ]) plt . show () data_taken_for_train . hist ( bins = 50 , figsize = ( 20 , 15 )) plt . show () Speed is high in rush hour, and speed in the afternoon is higher than morning. There seems some positive correlation between max/ min temperature, mean cloud coverage and speed. And negative correlation between wind mean wind speed and speed. Some left skewed in number of hours of reduced visibility# (hours) and some right skewed in mean cloud coverage (%). Categorical variables are almost balanced. Bivariate analysis Plotting relationship between speed, MaxTemp, MinTemp, MCloud, #hRedVisi(h), WindDirect, MWindSpeed. num_vars = [ 'speed' , 'MaxTemp(\u2103)' , 'MinTemp(\u2103)' , 'MCloud(%)' , '#hRedVisi(h)' , 'WindDirect(degrees)' , 'MWindSpeed(km/h)' ] from pandas.plotting import scatter_matrix scatter_matrix ( data_taken_for_train [ num_vars ], figsize = ( 20 , 15 )) plt . show () Plotting MaxTemp(\u2103), MinTemp(\u2103), MCloud(%), MWindSpeed(km/h) against speed. plt . figure ( figsize = ( 10 , 8 )) sns . set_style ( 'darkgrid' ) sns . jointplot ( y = 'speed' , x = 'MaxTemp(\u2103)' , data = data_taken_for_train ) sns . set_style ( 'darkgrid' ) sns . jointplot ( y = 'speed' , x = 'MinTemp(\u2103)' , data = data_taken_for_train ) sns . set_style ( 'darkgrid' ) sns . jointplot ( y = 'speed' , x = 'MCloud(%)' , data = data_taken_for_train ) sns . set_style ( 'darkgrid' ) sns . jointplot ( y = 'speed' , x = 'MWindSpeed(km/h)' , data = data_taken_for_train ) plt . show () <Figure size 720x576 with 0 Axes> Plotting traffic speed over mean cloud %. plt . figure ( figsize = ( 35 , 10 )) sns . barplot ( x = 'MCloud(%)' , y = 'speed' , data = data_taken_for_train ) plt . show () Analysing the correlation matrix of the numerical variables. plt . figure ( figsize = ( 20 , 10 )) sns . heatmap ( data_taken_for_train . corr (), annot = True ) plt . title ( 'Correlation' ) plt . show ()","title":"Plot analysis"},{"location":"MSBD5001/project/msbd5001-fall2020/#prepare-the-data-for-machine-learning","text":"data_taken_for_train . columns Index(['year', 'month', 'day', 'time', 'speed', 'IsWeekend', 'IsHoliday', 'hour_modify', 'MaxTemp(\u2103)', 'MinTemp(\u2103)', 'MCloud(%)', 'TRainfall(mm)_num', '#hRedVisi(h)', 'WindDirect(degrees)', 'MWindSpeed(km/h)'], dtype='object') data_taken_for_train .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day time speed IsWeekend IsHoliday hour_modify MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm)_num #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) 0 2017 1 1 0 43.002930 1 1 Late_Night 20.8 18.4 72 0 0 60 34.2 1 2017 1 1 1 46.118696 1 1 Late_Night 20.8 18.4 72 0 0 60 34.2 2 2017 1 1 2 44.294158 1 1 Late_Night 20.8 18.4 72 0 0 60 34.2 3 2017 1 1 3 41.067468 1 1 Late_Night 20.8 18.4 72 0 0 60 34.2 4 2017 1 1 4 46.448653 1 1 Early_Morning 20.8 18.4 72 0 0 60 34.2 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 14001 2018 12 31 12 19.865269 0 0 Afternoon 15.6 11.8 77 0 0 360 26.8 14002 2018 12 31 15 17.820375 0 0 Afternoon 15.6 11.8 77 0 0 360 26.8 14003 2018 12 31 16 12.501851 0 0 Evening 15.6 11.8 77 0 0 360 26.8 14004 2018 12 31 18 15.979319 0 0 Evening 15.6 11.8 77 0 0 360 26.8 14005 2018 12 31 20 40.594183 0 0 Night 15.6 11.8 77 0 0 360 26.8 14006 rows \u00d7 15 columns target = [ 'speed' ] cat_vars = [ 'IsWeekend' , 'IsHoliday' , 'hour_modify' ] num_vars = [ 'year' , 'month' , 'day' , 'time' , 'MaxTemp(\u2103)' , 'MinTemp(\u2103)' , 'MCloud(%)' , 'TRainfall(mm)_num' , '#hRedVisi(h)' , 'WindDirect(degrees)' , 'MWindSpeed(km/h)' ] # Creating pipeline to transform data numeric_transformer = Pipeline ( steps = [ ( 'scaler' , StandardScaler ())]) categorical_transformer = Pipeline ( steps = [ ( 'oneHot' , OneHotEncoder ())]) preprocessor = ColumnTransformer ( transformers = [ ( 'num' , numeric_transformer , num_vars ), ( 'cat' , categorical_transformer , cat_vars )]) data_transformed = preprocessor . fit_transform ( data_taken_for_train ) print ( data_transformed . shape ) (14006, 21) # Split the data into trainset and testset. # direction 1 X_train , X_test , y_train , y_test = train_test_split ( data_transformed , data_taken_for_train [ 'speed' ], test_size = 0.15 , random_state = 50 ) print ( 'Shape of X_train: ' , str ( X_train . shape )); print ( 'Shape of y_train: ' , str ( y_train . shape )) print ( 'Shape of X_test: ' , str ( X_test . shape )); print ( 'Shape of y_test: ' , str ( y_test . shape )) Shape of X_train: (11905, 21) Shape of y_train: (11905,) Shape of X_test: (2101, 21) Shape of y_test: (2101,)","title":"Prepare the data for machine learning"},{"location":"MSBD5001/project/msbd5001-fall2020/#train-and-fine-tune-models","text":"Regression task. # 1. train a XGBoostRegressor tscv = TimeSeriesSplit ( n_splits = 3 ) model = XGBRegressor () param_grid = { 'nthread' :[ 4 , 6 , 8 ], 'objective' :[ 'reg:squarederror' ], 'learning_rate' :[ . 03 , 0.05 , . 07 ], 'max_depth' :[ 5 , 6 , 7 , 8 ], 'min_child_weight' :[ 4 ], 'subsample' :[ 0.7 ], 'colsample_bytree' :[ 0.7 ], 'n_estimators' :[ 500 , 700 ]} GridSearch = GridSearchCV ( estimator = model , param_grid = param_grid , cv = tscv , n_jobs = 2 , verbose = 10 ) GridSearch . fit ( X_train , y_train ) y_pred = GridSearch . predict ( X_test ) Fitting 3 folds for each of 72 candidates, totalling 216 fits [Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers. [Parallel(n_jobs=2)]: Done 1 tasks | elapsed: 4.9s [Parallel(n_jobs=2)]: Done 4 tasks | elapsed: 15.0s [Parallel(n_jobs=2)]: Done 9 tasks | elapsed: 33.8s [Parallel(n_jobs=2)]: Done 14 tasks | elapsed: 56.9s [Parallel(n_jobs=2)]: Done 21 tasks | elapsed: 1.5min [Parallel(n_jobs=2)]: Done 28 tasks | elapsed: 2.0min [Parallel(n_jobs=2)]: Done 37 tasks | elapsed: 2.8min [Parallel(n_jobs=2)]: Done 46 tasks | elapsed: 3.6min [Parallel(n_jobs=2)]: Done 57 tasks | elapsed: 4.9min [Parallel(n_jobs=2)]: Done 68 tasks | elapsed: 6.1min [Parallel(n_jobs=2)]: Done 81 tasks | elapsed: 7.2min [Parallel(n_jobs=2)]: Done 94 tasks | elapsed: 8.1min [Parallel(n_jobs=2)]: Done 109 tasks | elapsed: 9.5min [Parallel(n_jobs=2)]: Done 124 tasks | elapsed: 11.0min [Parallel(n_jobs=2)]: Done 141 tasks | elapsed: 13.0min [Parallel(n_jobs=2)]: Done 158 tasks | elapsed: 14.3min [Parallel(n_jobs=2)]: Done 177 tasks | elapsed: 15.8min [Parallel(n_jobs=2)]: Done 196 tasks | elapsed: 17.7min [Parallel(n_jobs=2)]: Done 216 out of 216 | elapsed: 20.1min finished # XGBoostRegressor for direction 2 GridSearch_d2 = GridSearchCV ( estimator = model , param_grid = param_grid , cv = tscv , n_jobs = 2 ) GridSearch_d2 . fit ( X_2017 , y_2017 ) y_pred_2018 = GridSearch_d2 . predict ( X_2018 ) # 2. train a RandomForestRegressor model2 = RandomForestRegressor () param_grid2 = { 'n_estimators' :[ 10 , 50 , 100 , 1000 ], 'max_features' :[ 1 , 2 , 3 ] } GridSearch2 = GridSearchCV ( estimator = model2 , param_grid = param_grid2 , cv = tscv , n_jobs = 2 , verbose = 10 ) GridSearch2 . fit ( X_train , y_train ) y_pred2 = GridSearch2 . predict ( X_test ) Fitting 3 folds for each of 12 candidates, totalling 36 fits [Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers. [Parallel(n_jobs=2)]: Done 1 tasks | elapsed: 0.1s [Parallel(n_jobs=2)]: Batch computation too fast (0.1413s.) Setting batch_size=2. [Parallel(n_jobs=2)]: Done 4 tasks | elapsed: 0.6s [Parallel(n_jobs=2)]: Batch computation too slow (3.4045s.) Setting batch_size=1. [Parallel(n_jobs=2)]: Done 14 tasks | elapsed: 12.6s [Parallel(n_jobs=2)]: Done 19 tasks | elapsed: 18.3s [Parallel(n_jobs=2)]: Done 27 tasks | elapsed: 41.9s [Parallel(n_jobs=2)]: Done 36 out of 36 | elapsed: 1.3min finished","title":"Train and fine tune models"},{"location":"MSBD5001/project/msbd5001-fall2020/#model-evaluation","text":"MSE_xgb = mean_squared_error ( y_pred , y_test ) # 10.7063378519613 MSE_rf = mean_squared_error ( y_pred2 , y_test ) print ( 'MSE for XGBoost is ' + str ( MSE_xgb )) print ( 'MSE for RandomForest is ' + str ( MSE_rf )) MSE for XGBoost is 10.874016698999894 MSE for RandomForest is 13.34623796947232 XGBoost performs better than RandomForest in this regression task. We use XGBoost for the following prediction. Performance is not so good in direction 2 as in direction 1. Maybe the proportion of testset is too large in this way and undermined model performace. Therefore, I choose to use XGBoost built in direction 1 to predict test.csv.","title":"Model evaluation"},{"location":"MSBD5001/project/msbd5001-fall2020/#prediction-on-test-data","text":"# Prepare for prediction testset = test . copy () testset . drop ( 'id' , axis = 1 , inplace = True ) testset [ 'year' ] = testset [ 'date' ] . apply ( lambda y : int ( y . split ()[ 0 ] . split ( '/' )[ 2 ])) testset [ 'month' ] = testset [ 'date' ] . apply ( lambda m : int ( m . split ()[ 0 ] . split ( '/' )[ 1 ])) testset [ 'day' ] = testset [ 'date' ] . apply ( lambda d : int ( d . split ()[ 0 ] . split ( '/' )[ 0 ])) testset [ 'time' ] = testset [ 'date' ] . apply ( lambda t : int ( t . split ()[ 1 ] . split ( ':' )[ 0 ])) testset [ 'datetime' ] = testset [ 'date' ] . apply ( lambda x : datetime . datetime ( int ( x . split ()[ 0 ] . split ( '/' )[ 2 ]), int ( x . split ()[ 0 ] . split ( '/' )[ 1 ]), int ( x . split ()[ 0 ] . split ( '/' )[ 0 ]), int ( x . split ()[ 1 ] . split ( ':' )[ 0 ]) ) ) testset [ 'IsWeekend' ] = testset [ 'datetime' ] . apply ( lambda x : 0 if x . weekday () in [ 0 , 1 , 2 , 3 , 4 ] else 1 ) testset [ 'IsHoliday' ] = testset [ 'datetime' ] . apply ( lambda x : 1 if ( x . date () . strftime ( '%Y-%m- %d ' ) in [ '2017-01-02' , '2017-01-28' , '2017-01-30' , '2017-01-31' , '2017-04-04' , '2017-04-14' , '2017-04-15' , '2017-04-17' , '2017-05-01' , '2017-05-03' , '2017-05-30' , '2017-07-01' , '2017-10-02' , '2017-10-05' , '2017-10-28' , '2017-12-25' , '2017-12-26' , '2018-01-01' , '2018-02-16' , '2018-02-17' , '2018-02-19' , '2018-03-30' , '2018-03-31' , '2018-04-02' , '2018-04-05' , '2018-05-01' , '2018-05-22' , '2018-06-18' , '2018-07-02' , '2018-09-25' , '2018-10-01' , '2018-10-17' , '2018-12-25' , '2018-12-26' ]) or ( x . weekday () in [ 6 ]) else 0 ) testset [ 'hour_modify' ] = testset [ 'time' ] . apply ( hour_modify ) testset [ 'common' ] = testset [ 'datetime' ] . apply ( lambda x : x . date ()) testdata = pd . merge ( testset [[ 'datetime' , 'year' , 'month' , 'day' , 'time' , 'IsWeekend' , 'IsHoliday' , 'hour_modify' , 'common' ]], weather [[ 'MaxTemp(\u2103)' , 'MinTemp(\u2103)' , 'MCloud(%)' , 'TRainfall(mm)_num' , '#hRedVisi(h)' , 'WindDirect(degrees)' , 'MWindSpeed(km/h)' , 'common' ]], on = 'common' , how = 'left' ) testdata . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } datetime year month day time IsWeekend IsHoliday hour_modify common MaxTemp(\u2103) MinTemp(\u2103) MCloud(%) TRainfall(mm)_num #hRedVisi(h) WindDirect(degrees) MWindSpeed(km/h) 0 2018-01-01 02:00:00 2018 1 1 2 0 1 Late_Night 2018-01-01 19.0 16.3 75 0 21 70 28.4 1 2018-01-01 05:00:00 2018 1 1 5 0 1 Early_Morning 2018-01-01 19.0 16.3 75 0 21 70 28.4 2 2018-01-01 07:00:00 2018 1 1 7 0 1 Early_Morning 2018-01-01 19.0 16.3 75 0 21 70 28.4 3 2018-01-01 08:00:00 2018 1 1 8 0 1 Morning 2018-01-01 19.0 16.3 75 0 21 70 28.4 4 2018-01-01 10:00:00 2018 1 1 10 0 1 Morning 2018-01-01 19.0 16.3 75 0 21 70 28.4 # Prediction test_transformed = preprocessor . fit_transform ( testdata ) y_test_pred = GridSearch . predict ( test_transformed ) print ( len ( y_test_pred )) test_transformed . shape [ 0 ] 3504 3504 # Write into test.casv and save as submission_20710754.csv pred = pd . DataFrame ({ 'speed' : list ( y_test_pred )}, columns = [ 'speed' ]) submission = pd . concat ([ test . drop ( 'date' , axis = 1 ), pred ], axis = 1 ) submission . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id speed 0 0 47.546001 1 1 47.211971 2 2 39.180874 3 3 32.459953 4 4 40.378597 submission . shape (3504, 2) submission . to_csv ( 'submission_20720230.csv' , index = False )","title":"Prediction on test data"},{"location":"MSBD5003/Azure%20Cloud/","text":"Define cloud \u00b6 On Demand Scalable Multi-Tenant Self service Reliability Utility Based Subscription Public cloud \u00b6 Providers let clients access the cloud via Internet Made available to the general public Private cloud \u00b6 The cloud is used solely by an organization (e.g. HKUST, Facebook, HSBC) May reside in-house or off-premise Secure, dedicated infrastructure with the benefits of on-demand provisioning Not burdened by network bandwidth and availability issues and security threats associated with public clouds. Greater control, security, and resilience. Hybrid cloud \u00b6 Composed of multiple clouds (private, public, etc.) that remain independent entities, but interoperate using standard or proprietary protocols Banks, hospitals, government Infrastructure-as-a-Service \u00b6 Providers give you the computing infrastructure made available as a service. You get \u201cbare-metal\u201d machines. Providers manage a large pool of resources, and use virtualization to dynamically allocate Customers \u201crent\u201d these physical resources to customize their own infrastructure Full control of OS, storage, applications, and some networking components (e.g., firewalls) IaaS \u00b6 Netflix rents thousands of servers, terabytes of storage from Amazon Web Services (AWS) Develop and deploy specialized software for transcoding, storage, streaming, analytics, etc. on top of it Is able to support tens of millions of connected devices, used by 40+ million users from 40+ countries Example: Amazon EC2 Platform-as-a-service PaaS \u00b6 Providers give you a software platform, or middleware, where applications run You develop and maintain and deploy your own software on top of the platform The hardware needed for running the software is automatically managed by the platform. You can\u2019t explicitly ask for resources. Example: AWS lambda SaaS \u00b6 Providers give you a piece of software/application. They take care of updating, and maintaining it. You simply use the software through the Internet. Example: Office 365 Why the cloud \u00b6 Pay only for what you use Easy/fast deployment to end users Monthly payments Encourages standard systems Requires less in-house staff, costs Virtual Machines \u00b6 Infrastructure as a Service (IaaS) Provides an operating system, storage, and networking User needs to maintain the software on the VM Virtualization \u00b6 Virtualization is an enabling technology for IaaS Cloud Suppose an IaaS provider owns a large cluster and wants to provision cloud services for its users Virtualization is a broad term. It can be applied to all types of resources (CPU, memory, network, etc.) Allows one computer to \u201clook like\u201d multiple computers, doing multiple jobs, by sharing the resources of a single machine across multiple environments. Old model: A server for every application. Big disadvantage: low utilization New model: Physical resources are virtualized. improved utilization Resource pooling \u00b6 The provider\u2019s resources are pooled to serve consumers using a multi-tenant model, with different physical and virtual resources dynamically allocated according to consumer demand. Location independence: the customer generally has no control or knowledge over the exact location of the provided resources but may be able to specify location at a higher level of abstraction (e.g., country, state, or datacenter). Advantage for providers: efficiency in utilization","title":"Azure cloud"},{"location":"MSBD5003/Azure%20Cloud/#define-cloud","text":"On Demand Scalable Multi-Tenant Self service Reliability Utility Based Subscription","title":"Define cloud"},{"location":"MSBD5003/Azure%20Cloud/#public-cloud","text":"Providers let clients access the cloud via Internet Made available to the general public","title":"Public cloud"},{"location":"MSBD5003/Azure%20Cloud/#private-cloud","text":"The cloud is used solely by an organization (e.g. HKUST, Facebook, HSBC) May reside in-house or off-premise Secure, dedicated infrastructure with the benefits of on-demand provisioning Not burdened by network bandwidth and availability issues and security threats associated with public clouds. Greater control, security, and resilience.","title":"Private cloud"},{"location":"MSBD5003/Azure%20Cloud/#hybrid-cloud","text":"Composed of multiple clouds (private, public, etc.) that remain independent entities, but interoperate using standard or proprietary protocols Banks, hospitals, government","title":"Hybrid cloud"},{"location":"MSBD5003/Azure%20Cloud/#infrastructure-as-a-service","text":"Providers give you the computing infrastructure made available as a service. You get \u201cbare-metal\u201d machines. Providers manage a large pool of resources, and use virtualization to dynamically allocate Customers \u201crent\u201d these physical resources to customize their own infrastructure Full control of OS, storage, applications, and some networking components (e.g., firewalls)","title":"Infrastructure-as-a-Service"},{"location":"MSBD5003/Azure%20Cloud/#iaas","text":"Netflix rents thousands of servers, terabytes of storage from Amazon Web Services (AWS) Develop and deploy specialized software for transcoding, storage, streaming, analytics, etc. on top of it Is able to support tens of millions of connected devices, used by 40+ million users from 40+ countries Example: Amazon EC2","title":"IaaS"},{"location":"MSBD5003/Azure%20Cloud/#platform-as-a-service-paas","text":"Providers give you a software platform, or middleware, where applications run You develop and maintain and deploy your own software on top of the platform The hardware needed for running the software is automatically managed by the platform. You can\u2019t explicitly ask for resources. Example: AWS lambda","title":"Platform-as-a-service PaaS"},{"location":"MSBD5003/Azure%20Cloud/#saas","text":"Providers give you a piece of software/application. They take care of updating, and maintaining it. You simply use the software through the Internet. Example: Office 365","title":"SaaS"},{"location":"MSBD5003/Azure%20Cloud/#why-the-cloud","text":"Pay only for what you use Easy/fast deployment to end users Monthly payments Encourages standard systems Requires less in-house staff, costs","title":"Why the cloud"},{"location":"MSBD5003/Azure%20Cloud/#virtual-machines","text":"Infrastructure as a Service (IaaS) Provides an operating system, storage, and networking User needs to maintain the software on the VM","title":"Virtual Machines"},{"location":"MSBD5003/Azure%20Cloud/#virtualization","text":"Virtualization is an enabling technology for IaaS Cloud Suppose an IaaS provider owns a large cluster and wants to provision cloud services for its users Virtualization is a broad term. It can be applied to all types of resources (CPU, memory, network, etc.) Allows one computer to \u201clook like\u201d multiple computers, doing multiple jobs, by sharing the resources of a single machine across multiple environments. Old model: A server for every application. Big disadvantage: low utilization New model: Physical resources are virtualized. improved utilization","title":"Virtualization"},{"location":"MSBD5003/Azure%20Cloud/#resource-pooling","text":"The provider\u2019s resources are pooled to serve consumers using a multi-tenant model, with different physical and virtual resources dynamically allocated according to consumer demand. Location independence: the customer generally has no control or knowledge over the exact location of the provided resources but may be able to specify location at a higher level of abstraction (e.g., country, state, or datacenter). Advantage for providers: efficiency in utilization","title":"Resource pooling"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/","text":"Big Data Definition \u00b6 Big Data is data whose scale, complexity, and speed require new architecture, techniques, algorithms, and analytics to manage it and extract value and hidden knowledge from it. Old model: Few companies are generating data, all others are consuming data New model: all of us are generating and consuming data at the same time Socail media Scientific instruments Mobile devices Sensors 3 vs \u00b6 Since the CPU techonology's improvement getting slower these years, the only way to scale up is distributed computation. Volume \u00b6 Data volume is increasing exponentially Velocity ( Streaming data ) \u00b6 Data is being generated fast and need to be processed fast Static data Streaming data Online Data Analytics Late decisions \ud83e\udc7a missing opportunities Variety \u00b6 Various formats, types, and structures Numerical, text, images, audio, video, sequences, time series, social media data, multi-dim arrays, etc\u2026 A single application can be generating/collecting many types of data The frustration of paraller programming \u00b6 Race conditions Deadlock Hard to debug: Race conditions and deadlocks are nondeterministic Most programming languages are low-level The programmer needs to manage shared memory and/or communication OpenMP is a good step forward, but still difficult for most programmers Programs written for multi-cores do not easily carry over to clusters The structure spectrum \u00b6 Structured (schema-first) Relational database -- Semi-structured(Schema-later) Documents XML -- Unstucture data (MongoDB) Structured data \u00b6 Modify the rows is easy but modify the column costs a lot. The relational data model is the most used data model Every relation has a schema defining each columns' type The programmer must statically specify the schema Semi-structured Data \u00b6 Json object consists of a collection name: value pairs, seperated by commas. Each value can be - A string - A number - A boolean - null - An array - a JSON object How to handle big data \u00b6 Race conditions ( use locks ) The frustration of parallel programming \u00b6 Hard to debug How to migrate from one archetecture to another Cloud comuting \u00b6 Dynamic provisioning Scalability Elasticity Mapreduce \u00b6 Map: Takes raw input and produces a key, value pair Reduce: Takes data with same key and produces outputs Shuffling and sorting - Hidden phase between mappers and reducers - Groups all key value pairs Example; Inverted index \u00b6 Map For each (url, doc) pair Emit (keyword, url) for each keyword in doc Reduce For each keyword, output (keyword, list of urls) Example: translate this SQL query into map reduce \u00b6 SELECT AuthorName FROM Authors , Books WHERE Authors . AuthorID = Books . AuthorID AND Books . Date > 1980 Answer: For each record in the \u2018Authors\u2019 table: Map: Emit (AuthorID, AuthorName) For each record in the \u2018Books\u2019 table: Map: Emit (AuthorID, Date) Reduce: For each AuthorID, if Date>1980, output AuthorName Where do we store data \u00b6 Distributed store. - I/O is a bottleneck - Building a high-end supercomputer is very costly - Storing all data in one place adds the risk of hardware failures Target environment \u00b6 Thousands of computers Distributed Failures are the norm Files are huge, but not many greater than 100M, usually multi-gigabyte Read/write characteristics (write-once, read-many) Files are mutated by appending Once written, files are typically only read Large streaming reads and small random reads are typical I/O bandwidth is more important than latency GFS design decisions \u00b6 Files stored into chunks Reliability through replication Single master to coordinate access, keep metadata HDFS \u00b6 an open-source implementation for Google's MapReduce and GFS Clean and simple programming abstraction Automatic parallelization & distribution Fault tolerance and automatic recovery Hadoop architecture \u00b6 Single master node, many slave nodes Distributed file system (HDFS) Execution engine (MapReduce) HDFS details \u00b6 Name node: Maintains metadata info about files Maps a filename to a set of blocks Maps a block to the data nodes where it resides replication engine for blocks Datanode Store data Files are divided into blocks Communicates with name nodes through periodic heartbeat Replication engine Upon detecting a DataNode failure, will choose new DataNode for replicas. Balance disk usage Balance communication traffic to DataNodes M data cells and n parity cells Storage efficiency = \\(\\frac{m}{m+n}\\) Namenode failure \u00b6 1) What is the Single point of failure in Hadoop v1? The single point of failure in Hadoop v1 is NameNode. If NameNode gets fail the whole Hadoop cluster will not work. Actually, there will not any data loss only the cluster work will be shut down, because NameNode is only the point of contact to all DataNodes and if the NameNode fails all communication will stop. 2) What are the available solutions to handle single point of failure in Hadoop 1? To handle the single point of failure, we can use another setup configuration which can backup NameNode metadata. If the primary NameNode will fail our setup can switch to secondary (backup) and no any type to shutdown will happen for Hadoop cluster. 3) How Single point of failure issue has been addressed in Hadoop 2? HDFS High Availability of Namenode is introduced with Hadoop 2. In this two separate machines are getting configured as NameNodes, where one NameNode always in working state and anther is in standby. Working Name node handling all clients request in the cluster where standby is behaving as the slave and maintaining enough state to provide a fast failover on Working Name node. On worker ailure \u00b6 Detect failure via periodic heartbeats Workers send heartbeat messages (ping) periodically to the master node Re-execute completed and in-progress map tasks Re-execute in-progress reduce tasks Task completion committed through maste Mapreduce: A major step backwards \u00b6 Mapreduce may be a good idea for writing certain types of computations A giant step backward in the programming paradigm for large-scale data intensive applications A sub-cptimal implementation, in that it uses brute force instead of indexing Missing most of features that are routinely included in current DMBS Not novel at all","title":"Big data and hdfs"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#big-data-definition","text":"Big Data is data whose scale, complexity, and speed require new architecture, techniques, algorithms, and analytics to manage it and extract value and hidden knowledge from it. Old model: Few companies are generating data, all others are consuming data New model: all of us are generating and consuming data at the same time Socail media Scientific instruments Mobile devices Sensors","title":"Big Data Definition"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#3-vs","text":"Since the CPU techonology's improvement getting slower these years, the only way to scale up is distributed computation.","title":"3 vs"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#volume","text":"Data volume is increasing exponentially","title":"Volume"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#velocity-streaming-data","text":"Data is being generated fast and need to be processed fast Static data Streaming data Online Data Analytics Late decisions \ud83e\udc7a missing opportunities","title":"Velocity ( Streaming data )"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#variety","text":"Various formats, types, and structures Numerical, text, images, audio, video, sequences, time series, social media data, multi-dim arrays, etc\u2026 A single application can be generating/collecting many types of data","title":"Variety"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#the-frustration-of-paraller-programming","text":"Race conditions Deadlock Hard to debug: Race conditions and deadlocks are nondeterministic Most programming languages are low-level The programmer needs to manage shared memory and/or communication OpenMP is a good step forward, but still difficult for most programmers Programs written for multi-cores do not easily carry over to clusters","title":"The frustration of paraller programming"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#the-structure-spectrum","text":"Structured (schema-first) Relational database -- Semi-structured(Schema-later) Documents XML -- Unstucture data (MongoDB)","title":"The structure spectrum"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#structured-data","text":"Modify the rows is easy but modify the column costs a lot. The relational data model is the most used data model Every relation has a schema defining each columns' type The programmer must statically specify the schema","title":"Structured data"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#semi-structured-data","text":"Json object consists of a collection name: value pairs, seperated by commas. Each value can be - A string - A number - A boolean - null - An array - a JSON object","title":"Semi-structured Data"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#how-to-handle-big-data","text":"Race conditions ( use locks )","title":"How to handle big data"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#the-frustration-of-parallel-programming","text":"Hard to debug How to migrate from one archetecture to another","title":"The frustration of parallel programming"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#cloud-comuting","text":"Dynamic provisioning Scalability Elasticity","title":"Cloud comuting"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#mapreduce","text":"Map: Takes raw input and produces a key, value pair Reduce: Takes data with same key and produces outputs Shuffling and sorting - Hidden phase between mappers and reducers - Groups all key value pairs","title":"Mapreduce"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#example-inverted-index","text":"Map For each (url, doc) pair Emit (keyword, url) for each keyword in doc Reduce For each keyword, output (keyword, list of urls)","title":"Example; Inverted index"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#example-translate-this-sql-query-into-map-reduce","text":"SELECT AuthorName FROM Authors , Books WHERE Authors . AuthorID = Books . AuthorID AND Books . Date > 1980 Answer: For each record in the \u2018Authors\u2019 table: Map: Emit (AuthorID, AuthorName) For each record in the \u2018Books\u2019 table: Map: Emit (AuthorID, Date) Reduce: For each AuthorID, if Date>1980, output AuthorName","title":"Example: translate this SQL query into map reduce"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#where-do-we-store-data","text":"Distributed store. - I/O is a bottleneck - Building a high-end supercomputer is very costly - Storing all data in one place adds the risk of hardware failures","title":"Where do we store data"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#target-environment","text":"Thousands of computers Distributed Failures are the norm Files are huge, but not many greater than 100M, usually multi-gigabyte Read/write characteristics (write-once, read-many) Files are mutated by appending Once written, files are typically only read Large streaming reads and small random reads are typical I/O bandwidth is more important than latency","title":"Target environment"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#gfs-design-decisions","text":"Files stored into chunks Reliability through replication Single master to coordinate access, keep metadata","title":"GFS design decisions"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#hdfs","text":"an open-source implementation for Google's MapReduce and GFS Clean and simple programming abstraction Automatic parallelization & distribution Fault tolerance and automatic recovery","title":"HDFS"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#hadoop-architecture","text":"Single master node, many slave nodes Distributed file system (HDFS) Execution engine (MapReduce)","title":"Hadoop architecture"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#hdfs-details","text":"Name node: Maintains metadata info about files Maps a filename to a set of blocks Maps a block to the data nodes where it resides replication engine for blocks Datanode Store data Files are divided into blocks Communicates with name nodes through periodic heartbeat Replication engine Upon detecting a DataNode failure, will choose new DataNode for replicas. Balance disk usage Balance communication traffic to DataNodes M data cells and n parity cells Storage efficiency = \\(\\frac{m}{m+n}\\)","title":"HDFS details"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#namenode-failure","text":"1) What is the Single point of failure in Hadoop v1? The single point of failure in Hadoop v1 is NameNode. If NameNode gets fail the whole Hadoop cluster will not work. Actually, there will not any data loss only the cluster work will be shut down, because NameNode is only the point of contact to all DataNodes and if the NameNode fails all communication will stop. 2) What are the available solutions to handle single point of failure in Hadoop 1? To handle the single point of failure, we can use another setup configuration which can backup NameNode metadata. If the primary NameNode will fail our setup can switch to secondary (backup) and no any type to shutdown will happen for Hadoop cluster. 3) How Single point of failure issue has been addressed in Hadoop 2? HDFS High Availability of Namenode is introduced with Hadoop 2. In this two separate machines are getting configured as NameNodes, where one NameNode always in working state and anther is in standby. Working Name node handling all clients request in the cluster where standby is behaving as the slave and maintaining enough state to provide a fast failover on Working Name node.","title":"Namenode failure"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#on-worker-ailure","text":"Detect failure via periodic heartbeats Workers send heartbeat messages (ping) periodically to the master node Re-execute completed and in-progress map tasks Re-execute in-progress reduce tasks Task completion committed through maste","title":"On worker ailure"},{"location":"MSBD5003/Big%20Data%20and%20HDFS/#mapreduce-a-major-step-backwards","text":"Mapreduce may be a good idea for writing certain types of computations A giant step backward in the programming paradigm for large-scale data intensive applications A sub-cptimal implementation, in that it uses brute force instead of indexing Missing most of features that are routinely included in current DMBS Not novel at all","title":"Mapreduce: A major step backwards"},{"location":"MSBD5003/Lecture3/","text":"!pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 69kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 40.7MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=afa56cc6a9466c8bd467ecaf305826a28ca15fb5f31ee4f848525e71d4193f60 Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 Example: Linear-time selection \u00b6 Problem: \u00b6 \u2014 Input: an array A of n numbers (unordered), and k \u2014 Output: the k-th smallest number (counting from 0) Algorithm \u00b6 \\(x=A[0]\\) partition A into \\(A[0..mid-1] < A[mid] = x < A[mid+1..n-1]\\) if \\(mid =k\\) then return \\(x\\) if \\(k<mid\\) then \\(A= A[O..mid-1]\\) if k > mid then \\(A = A[mid+1,n-1], k= k\u2014 mid-1\\) gotostep 1 Key-value Pairs \u00b6 While most Spark operations work on RDDs containing any type of objects, a few special operations are only available on RDDs of key-value pairs. In Python, these operations work on RDDs containing built-in Python tuples such as (1, 2). Simply create such tuples and then call your desired operation. For example, the following code uses the reduceByKey operation on key-value pairs to count how many times each line of text occurs in a file: lines = sc . textFile ( \"README.md\" ) pairs = lines . map ( lambda s : ( s , 1 )) counts = pairs . reduceByKey ( lambda a , b : a + b ) We could also use counts.sortByKey() , for example, to sort the pairs alphabetically, and finally counts.collect() to bring them back to the driver program as a list of objects. PMI \u00b6 PMI (pointwise mutual information) is a measure of association used in information theory and statistics. Given a list of pairs (x, y) \\[pmi(x, y) = log\\frac{p(x,y)}{p(x)p(y}\\] where - \\(p(x)\\) : probability of x - \\(p(y)\\) : probability of y - \\(p(x,y)\\) : joint probability Example: p(x=0) = 0.8, p(x=1)=0.2, p(y=0)=0.25, p(y=1)=0.75 pmi(x=0;y=0) = \u22121 pmi(x=0;y=1) = 0.222392 pmi(x=1;y=0) = 1.584963 pmi(x=1;y=1) = -1.584963 Example notebook see: note book in class/PMI","title":"Lecture3"},{"location":"MSBD5003/Lecture3/#example-linear-time-selection","text":"","title":"Example: Linear-time selection"},{"location":"MSBD5003/Lecture3/#problem","text":"\u2014 Input: an array A of n numbers (unordered), and k \u2014 Output: the k-th smallest number (counting from 0)","title":"Problem:"},{"location":"MSBD5003/Lecture3/#algorithm","text":"\\(x=A[0]\\) partition A into \\(A[0..mid-1] < A[mid] = x < A[mid+1..n-1]\\) if \\(mid =k\\) then return \\(x\\) if \\(k<mid\\) then \\(A= A[O..mid-1]\\) if k > mid then \\(A = A[mid+1,n-1], k= k\u2014 mid-1\\) gotostep 1","title":"Algorithm"},{"location":"MSBD5003/Lecture3/#key-value-pairs","text":"While most Spark operations work on RDDs containing any type of objects, a few special operations are only available on RDDs of key-value pairs. In Python, these operations work on RDDs containing built-in Python tuples such as (1, 2). Simply create such tuples and then call your desired operation. For example, the following code uses the reduceByKey operation on key-value pairs to count how many times each line of text occurs in a file: lines = sc . textFile ( \"README.md\" ) pairs = lines . map ( lambda s : ( s , 1 )) counts = pairs . reduceByKey ( lambda a , b : a + b ) We could also use counts.sortByKey() , for example, to sort the pairs alphabetically, and finally counts.collect() to bring them back to the driver program as a list of objects.","title":"Key-value Pairs"},{"location":"MSBD5003/Lecture3/#pmi","text":"PMI (pointwise mutual information) is a measure of association used in information theory and statistics. Given a list of pairs (x, y) \\[pmi(x, y) = log\\frac{p(x,y)}{p(x)p(y}\\] where - \\(p(x)\\) : probability of x - \\(p(y)\\) : probability of y - \\(p(x,y)\\) : joint probability Example: p(x=0) = 0.8, p(x=1)=0.2, p(y=0)=0.25, p(y=1)=0.75 pmi(x=0;y=0) = \u22121 pmi(x=0;y=1) = 0.222392 pmi(x=1;y=0) = 1.584963 pmi(x=1;y=1) = -1.584963 Example notebook see: note book in class/PMI","title":"PMI"},{"location":"MSBD5003/Lecture7/","text":"!pip install pyspark Code at internal.ipynb Data Partitioning \u00b6 RDDs are stored in partitions. When performing computations on RDDs, these partitions can be operated on in parallel. You get better parallelism when the partitions are balanced. When RDDs are first created, the partitions are balanced. However, partitions may get out of balance after certain transformations. Hash Partitioning \u00b6 We can view the contents of each partition: - e.g., prime.glom().collect()[1][0:4] - We see that it hashed all numbers x such that x mod 8 = 1 to partition #1 In general, hash partitioning allocates tuple (k, v) to partition p where - p = k.hashCode() % numPartitions Usually works well but be aware of bad inputs! Shuffle \u00b6 Spark uses shuffles to implement wide dependencies - Examples: reduceByKey, repartition, coalesce, join (on RDDs not partitioned using the same partitioner) Spark generates sets of tasks - map tasks to organize the data, and a set of reduce tasks to group/aggregate it. Internally, Spark builds a hash table within each task to perform the grouping. If the hash table is too large, Spark will spill these tables to disk, incurring the additional overhead of disk I/O RDDs resulting from shuffles are automatically cached. Range partitioning \u00b6 For data types that have or ordering defined - Examples: Int, Char, String, \u2026 - Internally, Spark samples the data so as to produce more balanced partitions. - Used by default after sorting Example: - An RDD with keys [8, 96, 240, 400, 401, 800], - Number of partitions: 4 - In this case, hash partitioning distributes the keys as follows among the partitions: - partition 0: [8, 96, 240, 400, 800] - partition 1: [401] - partition 2: [] - partition 3: [] Range partitioning would improve the partitioning significantly Partitioner inheritance \u00b6 Operations on Pair RDDs that hold to (and propagate) a partitioner: mapValues (if parent has a partitioner) flatMapValues (if parent has a partitioner) filter (if parent has a partitioner)","title":"Lecture7"},{"location":"MSBD5003/Lecture7/#data-partitioning","text":"RDDs are stored in partitions. When performing computations on RDDs, these partitions can be operated on in parallel. You get better parallelism when the partitions are balanced. When RDDs are first created, the partitions are balanced. However, partitions may get out of balance after certain transformations.","title":"Data Partitioning"},{"location":"MSBD5003/Lecture7/#hash-partitioning","text":"We can view the contents of each partition: - e.g., prime.glom().collect()[1][0:4] - We see that it hashed all numbers x such that x mod 8 = 1 to partition #1 In general, hash partitioning allocates tuple (k, v) to partition p where - p = k.hashCode() % numPartitions Usually works well but be aware of bad inputs!","title":"Hash Partitioning"},{"location":"MSBD5003/Lecture7/#shuffle","text":"Spark uses shuffles to implement wide dependencies - Examples: reduceByKey, repartition, coalesce, join (on RDDs not partitioned using the same partitioner) Spark generates sets of tasks - map tasks to organize the data, and a set of reduce tasks to group/aggregate it. Internally, Spark builds a hash table within each task to perform the grouping. If the hash table is too large, Spark will spill these tables to disk, incurring the additional overhead of disk I/O RDDs resulting from shuffles are automatically cached.","title":"Shuffle"},{"location":"MSBD5003/Lecture7/#range-partitioning","text":"For data types that have or ordering defined - Examples: Int, Char, String, \u2026 - Internally, Spark samples the data so as to produce more balanced partitions. - Used by default after sorting Example: - An RDD with keys [8, 96, 240, 400, 401, 800], - Number of partitions: 4 - In this case, hash partitioning distributes the keys as follows among the partitions: - partition 0: [8, 96, 240, 400, 800] - partition 1: [401] - partition 2: [] - partition 3: [] Range partitioning would improve the partitioning significantly","title":"Range partitioning"},{"location":"MSBD5003/Lecture7/#partitioner-inheritance","text":"Operations on Pair RDDs that hold to (and propagate) a partitioner: mapValues (if parent has a partitioner) flatMapValues (if parent has a partitioner) filter (if parent has a partitioner)","title":"Partitioner inheritance"},{"location":"MSBD5003/Lecture8/","text":"Algorithm Design \u00b6 !pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 63kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 38.8MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=c2149ee5f1b68200a671dfab446e477b77a733c24b319c6e977cd2618ae1a79e Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 Divide-and-Conquer \u00b6 Classical D&C - Divide problem into 2 parts - Recursively solve each part - Combine the results together D&C under big data systems Divide problem into \ufffc partitions, where (ideally) \ufffc is the number of executors in the system Solve the problem on each partition Combine the results together Example: sum(), reduce() Prefix sums \u00b6 Input: Sequence x of n elements, binary associative operator + Output: Sequence y of n elements, with\u2028yk = x1 + ... + xk Example: x = [1, 4, 3, 5, 6, 7, 0, 1] y = [1, 5, 8, 13, 19, 26, 26, 27] Algorithm: Compute sum for each partition Compute the prefix sums of the \ufffc sums Compute prefix sums in each partition Time: O(2n) from pyspark.context import SparkContext sc = SparkContext . getOrCreate () x = [ 1, 4, 3, 5, 6, 7, 0, 1 ] rdd = sc . parallelize ( x , 4 ). cache () def f ( i ) : yield sum ( i ) sums = rdd . mapPartitions ( f ). collect () print ( sums ) for i in range ( 1 , len ( sums )) : sums [ i ] += sums [ i - 1 ] print ( sums ) def g ( index , iter ) : global sums if index == 0 : s = 0 else : s = sums [ index - 1 ] for i in iter : s += i yield s prefix_sums = rdd . mapPartitionsWithIndex ( g ) print ( prefix_sums . collect ()) [5, 8, 13, 1] [5, 13, 26, 27] [1, 5, 8, 13, 19, 26, 26, 27] Given a sequence of integers, check whether these numbers are monotonically decreasing. x = [ 1, 3, 5, 6, 7, 8, 3 ] rdd = sc . parallelize ( x , 4 ). cache () def f ( it ) : first = next ( it ) last = first increasing = True for i in it : if i < last : increasing = False last = i yield increasing , first , last results = rdd . mapPartitions ( f ). collect () print ( results ) increasing = True if results [ 0 ][ 0 ] == False : increasing = False else : for i in range ( 1 , len ( results )) : if results [ i ][ 0 ] == False or results [ i ][ 1 ] < results [ i - 1 ][ 2 ] : increasing = False print ( increasing ) [(True, 1, 1), (True, 3, 5), (True, 6, 7), (False, 8, 3)] False Maximum sub array \u00b6 Level 1: Naively: 2 executors are working, all others idle time = \ufffc \\(O(n/2)\\) Smarter: \ufffc \\(L_m\\) and \\(R_m\\) \ufffc can be found by the prefix-sum algorithm Can use all executors, time =$O(n/p) \ufffc Level 2: We have 4 subarrays, and solve two prefix-sums for each subarray Each subarray has size \ufffc, and we make sure that each has the same number of partitions Time = \ufffc \\(O(n / p\\) Level 3: Time = \ufffc \\(O(n / p)\\) Stop recursion when each subarray is one partition. Total time: \ufffc \\(O(\\frac{n}{p}\\times logp)\\)","title":"Lecture8"},{"location":"MSBD5003/Lecture8/#algorithm-design","text":"!pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 63kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 38.8MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=c2149ee5f1b68200a671dfab446e477b77a733c24b319c6e977cd2618ae1a79e Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1","title":"Algorithm Design"},{"location":"MSBD5003/Lecture8/#divide-and-conquer","text":"Classical D&C - Divide problem into 2 parts - Recursively solve each part - Combine the results together D&C under big data systems Divide problem into \ufffc partitions, where (ideally) \ufffc is the number of executors in the system Solve the problem on each partition Combine the results together Example: sum(), reduce()","title":"Divide-and-Conquer"},{"location":"MSBD5003/Lecture8/#prefix-sums","text":"Input: Sequence x of n elements, binary associative operator + Output: Sequence y of n elements, with\u2028yk = x1 + ... + xk Example: x = [1, 4, 3, 5, 6, 7, 0, 1] y = [1, 5, 8, 13, 19, 26, 26, 27] Algorithm: Compute sum for each partition Compute the prefix sums of the \ufffc sums Compute prefix sums in each partition Time: O(2n) from pyspark.context import SparkContext sc = SparkContext . getOrCreate () x = [ 1, 4, 3, 5, 6, 7, 0, 1 ] rdd = sc . parallelize ( x , 4 ). cache () def f ( i ) : yield sum ( i ) sums = rdd . mapPartitions ( f ). collect () print ( sums ) for i in range ( 1 , len ( sums )) : sums [ i ] += sums [ i - 1 ] print ( sums ) def g ( index , iter ) : global sums if index == 0 : s = 0 else : s = sums [ index - 1 ] for i in iter : s += i yield s prefix_sums = rdd . mapPartitionsWithIndex ( g ) print ( prefix_sums . collect ()) [5, 8, 13, 1] [5, 13, 26, 27] [1, 5, 8, 13, 19, 26, 26, 27] Given a sequence of integers, check whether these numbers are monotonically decreasing. x = [ 1, 3, 5, 6, 7, 8, 3 ] rdd = sc . parallelize ( x , 4 ). cache () def f ( it ) : first = next ( it ) last = first increasing = True for i in it : if i < last : increasing = False last = i yield increasing , first , last results = rdd . mapPartitions ( f ). collect () print ( results ) increasing = True if results [ 0 ][ 0 ] == False : increasing = False else : for i in range ( 1 , len ( results )) : if results [ i ][ 0 ] == False or results [ i ][ 1 ] < results [ i - 1 ][ 2 ] : increasing = False print ( increasing ) [(True, 1, 1), (True, 3, 5), (True, 6, 7), (False, 8, 3)] False","title":"Prefix sums"},{"location":"MSBD5003/Lecture8/#maximum-sub-array","text":"Level 1: Naively: 2 executors are working, all others idle time = \ufffc \\(O(n/2)\\) Smarter: \ufffc \\(L_m\\) and \\(R_m\\) \ufffc can be found by the prefix-sum algorithm Can use all executors, time =$O(n/p) \ufffc Level 2: We have 4 subarrays, and solve two prefix-sums for each subarray Each subarray has size \ufffc, and we make sure that each has the same number of partitions Time = \ufffc \\(O(n / p\\) Level 3: Time = \ufffc \\(O(n / p)\\) Stop recursion when each subarray is one partition. Total time: \ufffc \\(O(\\frac{n}{p}\\times logp)\\)","title":"Maximum sub array"},{"location":"MSBD5003/MLib/","text":"Mlib \u00b6 Spark's scalable machine learning library ML Pipelines \u00b6 Inspired by scikit-learn DataFrame Pipeline componens: - Transformer - Estimator Parameters Transformers \u00b6 Converts one dataframe to another Must implement a method transform() Examples: Model: - DataFrame[id: int, feature_vector: Vector] => DataFrame[id: int, label: string] Feature transformer: - DataFrame[id: int, text: string] => DataFrame[id: int, feature_vector: Vector] Estimators \u00b6 Input: DataFrame Output: Model Must implement a method fit() Example: - LogisticRegression is an Estimator. - Calling fit() trains a LogisticRegressionModel, which is a Model (hence also a Transformer). Paramters \u00b6 Both transformers and estimators can have parameters Set parameters: lr = LogisticRegression() lr.setMaxIter(10) Pass a ParamMap to fit() or transform(). Pass A ParamMap is a set of (parameter, value) pairs.","title":"Mlib"},{"location":"MSBD5003/MLib/#mlib","text":"Spark's scalable machine learning library","title":"Mlib"},{"location":"MSBD5003/MLib/#ml-pipelines","text":"Inspired by scikit-learn DataFrame Pipeline componens: - Transformer - Estimator Parameters","title":"ML Pipelines"},{"location":"MSBD5003/MLib/#transformers","text":"Converts one dataframe to another Must implement a method transform() Examples: Model: - DataFrame[id: int, feature_vector: Vector] => DataFrame[id: int, label: string] Feature transformer: - DataFrame[id: int, text: string] => DataFrame[id: int, feature_vector: Vector]","title":"Transformers"},{"location":"MSBD5003/MLib/#estimators","text":"Input: DataFrame Output: Model Must implement a method fit() Example: - LogisticRegression is an Estimator. - Calling fit() trains a LogisticRegressionModel, which is a Model (hence also a Transformer).","title":"Estimators"},{"location":"MSBD5003/MLib/#paramters","text":"Both transformers and estimators can have parameters Set parameters: lr = LogisticRegression() lr.setMaxIter(10) Pass a ParamMap to fit() or transform(). Pass A ParamMap is a set of (parameter, value) pairs.","title":"Paramters"},{"location":"MSBD5003/Spark%20SQL/","text":"SQL \u00b6 Tables explained \u00b6 The schema of a table is the table name and its attributes: A tuple = a record = row A table = a set of tuples Operators \u00b6 Like operator \u00b6 SELECT * FROM Products WHERE PName LIKE \u2018 % gizmo % \u2019 s LIKE p: pattern matching on strings p may contain two special symbols: - % = any sequence of characters - _ = any single character Distinct \u00b6 SELECT DISTINCT category FROM Product Order \u00b6 SELECT pname , price , manufacturer FROM Product WHERE category = \u2018 gizmo \u2019 AND price > 50 ORDER BY price , pname join \u00b6 support we have a table with following schema drop table if exists product , location ; create table location ( id int primary key auto_increment , location text not null ); create table product ( id int primary key auto_increment , name text , location_id int , foreign key ( location_id ) references location ( id ) ); insert into location ( location ) values ( 'Shenzhen' ); insert into location ( location ) values ( 'Shanghai' ); insert into location ( location ) values ( 'Beijing' ); insert into product ( name , location_id ) values ( 'iPad' , 1 ); insert into product ( name , location_id ) values ( 'iPhone' , 2 ); insert into product ( name , location_id ) values ( 'iMac' , 3 ); First way \u00b6 select name from product inner join location l on product . location_id = l . id where location = 'Shenzhen' ; Second way \u00b6 select name from product , location l where location_id = l . id and location = 'Shenzhen' ; Outter join \u00b6 iPad Shenzhen iPhone Shanghai iMac Beijing Null Hangzhou Inner join \u00b6 iPad Shenzhen iPhone Shanghai iMac Beijing Aggregation \u00b6 sum, count, min, max, avg SELECT avg ( price ) FROM Product WHERE maker = \u201c Toyota \u201d COUNT applies to duplicates, unless otherwise stated: Having \u00b6 SELECT product , Sum ( price * quantity ) FROM Purchase WHERE date > \u2018 10 / 1 / 2005 \u2019 GROUP BY product HAVING Sum ( quantity ) > 30 PySpark SQL \u00b6 RDD can be used to achieve the same functionality. What are the benefits of using DataFrames? Readability Flexibility Columnar storage Catalyst optimizer Plan Optimization \u00b6 Tree transformations \u00b6 Catalyst Rules \u00b6 Pattern matching functions that transform subtrees into specific structures. Multiple patterns in the same transform call. May take multiple batches to reach a fixed point. transform can contain arbitrary Scala code. Rule-based optimization \u00b6 Cost-based optimization \u00b6 Currently only used for choosing join algorithms - Big table joining big table: shuffle - Small table joining big table: broadcast the small table Pyspark demo \u00b6 !pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 67kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 27.6MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=1dcf7073a6e4895948c28fb6dcc6c2d41885f6288af4226ea8e4e78f09e44494 Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 from pyspark.sql import Row from pyspark.context import SparkContext from pyspark.sql.session import SparkSession sc = SparkContext . getOrCreate () spark = SparkSession ( sc ) row = Row(name='Alice', age=11) row.name, row.age ('Alice', 11) Using . to get attribute or using python dict function row = Row(name='Alice', age=11, count=1) print(f\"Row.count: {row.count}\") print(f\"Row['count']: {row['count']}\") Row.count: <built-in method count of Row object at 0x7fd3091ecaf0> Row['count']: 1 Load csv df = spark.read.csv('/content/drive/MyDrive/courses/HKUST/MSBD5003/data/building.csv', header=True, inferSchema=True) Create rdd from dataframe dfrdd = df.rdd dfrdd.take(3) [Row(BuildingID=1, BuildingMgr='M1', BuildingAge=25, HVACproduct='AC1000', Country='USA'), Row(BuildingID=2, BuildingMgr='M2', BuildingAge=27, HVACproduct='FN39TG', Country='France'), Row(BuildingID=3, BuildingMgr='M3', BuildingAge=28, HVACproduct='JDNS77', Country='Brazil')] Select \u00b6 from pyspark.sql.functions import * df . select ( 'BuildingID' , 'Country' ) . show () +----------+------------+ |BuildingID| Country| +----------+------------+ | 1| USA| | 2| France| | 3| Brazil| | 4| Finland| | 5| Hong Kong| | 6| Singapore| | 7|South Africa| | 8| Australia| | 9| Mexico| | 10| China| | 11| Belgium| | 12| Finland| | 13|Saudi Arabia| | 14| Germany| | 15| Israel| | 16| Turkey| | 17| Egypt| | 18| Indonesia| | 19| Canada| | 20| Argentina| +----------+------------+ Using where clause lit: Creates a Column of literal value. df.where(\"Country<'USA'\").select('BuildingID', lit('OK')).show() +----------+---+ |BuildingID| OK| +----------+---+ | 2| OK| | 3| OK| | 4| OK| | 5| OK| | 6| OK| | 7| OK| | 8| OK| | 9| OK| | 10| OK| | 11| OK| | 12| OK| | 13| OK| | 14| OK| | 15| OK| | 16| OK| | 17| OK| | 18| OK| | 19| OK| | 20| OK| +----------+---+ RawSQL to spark \u00b6 dfCustomer = spark.read.csv('/content/drive/MyDrive/courses/HKUST/MSBD5003/data/Customer.csv', header=True, inferSchema=True) dfProduct = spark.read.csv('/content/drive/MyDrive/courses/HKUST/MSBD5003/data/Product.csv', header=True, inferSchema=True) dfDetail = spark.read.csv('/content/drive/MyDrive/courses/HKUST/MSBD5003/data/SalesOrderDetail.csv', header=True, inferSchema=True) dfHeader = spark.read.csv('/content/drive/MyDrive/courses/HKUST/MSBD5003/data/SalesOrderHeader.csv', header=True, inferSchema=True) SELECT ProductID , Name , ListPrice FROM Product WHERE Color = 'black' dfProduct.filter(\"Color = 'Black'\")\\ .select('ProductID', 'Name', 'ListPrice')\\ .show(truncate=False) +---------+-----------------------------+---------+ |ProductID|Name |ListPrice| +---------+-----------------------------+---------+ |680 |HL Road Frame - Black, 58 |1431.5 | |708 |Sport-100 Helmet, Black |34.99 | |722 |LL Road Frame - Black, 58 |337.22 | |723 |LL Road Frame - Black, 60 |337.22 | |724 |LL Road Frame - Black, 62 |337.22 | |736 |LL Road Frame - Black, 44 |337.22 | |737 |LL Road Frame - Black, 48 |337.22 | |738 |LL Road Frame - Black, 52 |337.22 | |743 |HL Mountain Frame - Black, 42|1349.6 | |744 |HL Mountain Frame - Black, 44|1349.6 | |745 |HL Mountain Frame - Black, 48|1349.6 | |746 |HL Mountain Frame - Black, 46|1349.6 | |747 |HL Mountain Frame - Black, 38|1349.6 | |765 |Road-650 Black, 58 |782.99 | |766 |Road-650 Black, 60 |782.99 | |767 |Road-650 Black, 62 |782.99 | |768 |Road-650 Black, 44 |782.99 | |769 |Road-650 Black, 48 |782.99 | |770 |Road-650 Black, 52 |782.99 | |775 |Mountain-100 Black, 38 |3374.99 | +---------+-----------------------------+---------+ only showing top 20 rows dfProduct.where(dfProduct.Color=='Black') \\ .select(dfProduct.ProductID, dfProduct['Name'], (dfProduct['ListPrice'] * 2).alias('Double price')) \\ .show(truncate=False) +---------+-----------------------------+------------+ |ProductID|Name |Double price| +---------+-----------------------------+------------+ |680 |HL Road Frame - Black, 58 |2863.0 | |708 |Sport-100 Helmet, Black |69.98 | |722 |LL Road Frame - Black, 58 |674.44 | |723 |LL Road Frame - Black, 60 |674.44 | |724 |LL Road Frame - Black, 62 |674.44 | |736 |LL Road Frame - Black, 44 |674.44 | |737 |LL Road Frame - Black, 48 |674.44 | |738 |LL Road Frame - Black, 52 |674.44 | |743 |HL Mountain Frame - Black, 42|2699.2 | |744 |HL Mountain Frame - Black, 44|2699.2 | |745 |HL Mountain Frame - Black, 48|2699.2 | |746 |HL Mountain Frame - Black, 46|2699.2 | |747 |HL Mountain Frame - Black, 38|2699.2 | |765 |Road-650 Black, 58 |1565.98 | |766 |Road-650 Black, 60 |1565.98 | |767 |Road-650 Black, 62 |1565.98 | |768 |Road-650 Black, 44 |1565.98 | |769 |Road-650 Black, 48 |1565.98 | |770 |Road-650 Black, 52 |1565.98 | |775 |Mountain-100 Black, 38 |6749.98 | +---------+-----------------------------+------------+ only showing top 20 rows dfProduct.where(dfProduct.ListPrice * 2 > 100) \\ .select(dfProduct.ProductID, dfProduct['Name'], dfProduct.ListPrice * 2) \\ .show(truncate=False) +---------+-------------------------+---------------+ |ProductID|Name |(ListPrice * 2)| +---------+-------------------------+---------------+ |680 |HL Road Frame - Black, 58|2863.0 | |706 |HL Road Frame - Red, 58 |2863.0 | |717 |HL Road Frame - Red, 62 |2863.0 | |718 |HL Road Frame - Red, 44 |2863.0 | |719 |HL Road Frame - Red, 48 |2863.0 | |720 |HL Road Frame - Red, 52 |2863.0 | |721 |HL Road Frame - Red, 56 |2863.0 | |722 |LL Road Frame - Black, 58|674.44 | |723 |LL Road Frame - Black, 60|674.44 | |724 |LL Road Frame - Black, 62|674.44 | |725 |LL Road Frame - Red, 44 |674.44 | |726 |LL Road Frame - Red, 48 |674.44 | |727 |LL Road Frame - Red, 52 |674.44 | |728 |LL Road Frame - Red, 58 |674.44 | |729 |LL Road Frame - Red, 60 |674.44 | |730 |LL Road Frame - Red, 62 |674.44 | |731 |ML Road Frame - Red, 44 |1189.66 | |732 |ML Road Frame - Red, 48 |1189.66 | |733 |ML Road Frame - Red, 52 |1189.66 | |734 |ML Road Frame - Red, 58 |1189.66 | +---------+-------------------------+---------------+ only showing top 20 rows SELECT ProductID , Name , ListPrice FROM Product WHERE Color = 'black' ORDER BY ProductID dfProduct.filter(\"Color = 'Black'\")\\ .select('ProductID', 'Name', 'ListPrice')\\ .orderBy('ListPrice')\\ .show(truncate=False) +---------+--------------------------+---------+ |ProductID|Name |ListPrice| +---------+--------------------------+---------+ |860 |Half-Finger Gloves, L |24.49 | |859 |Half-Finger Gloves, M |24.49 | |858 |Half-Finger Gloves, S |24.49 | |708 |Sport-100 Helmet, Black |34.99 | |862 |Full-Finger Gloves, M |37.99 | |861 |Full-Finger Gloves, S |37.99 | |863 |Full-Finger Gloves, L |37.99 | |841 |Men's Sports Shorts, S |59.99 | |849 |Men's Sports Shorts, M |59.99 | |851 |Men's Sports Shorts, XL |59.99 | |850 |Men's Sports Shorts, L |59.99 | |815 |LL Mountain Front Wheel |60.745 | |868 |Women's Mountain Shorts, M|69.99 | |869 |Women's Mountain Shorts, L|69.99 | |867 |Women's Mountain Shorts, S|69.99 | |853 |Women's Tights, M |74.99 | |854 |Women's Tights, L |74.99 | |852 |Women's Tights, S |74.99 | |818 |LL Road Front Wheel |85.565 | |823 |LL Mountain Rear Wheel |87.745 | +---------+--------------------------+---------+ only showing top 20 rows Find all orders and details on black product, return the product SalesOrderID, SalesOrderDetailID, Name, UnitPrice, and OrderQty SELECT SalesOrderID , SalesOrderDetailID , Name , UnitPrice , OrderQty FROM SalesLT . SalesOrderDetail , SalesLT . Product WHERE SalesOrderDetail . ProductID = Product . ProductID AND Color = 'Black' SELECT SalesOrderID , SalesOrderDetailID , Name , UnitPrice , OrderQty FROM SalesLT . SalesOrderDetail JOIN SalesLT . Product ON SalesOrderDetail . ProductID = Product . ProductID WHERE Color = 'Black' dfDetail.join(dfProduct, 'ProductID') \\ .filter(\"Color='Black'\")\\ .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty') \\ .show() +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71797| 111045|LL Road Frame - B...| 202.332| 3| | 71783| 110730|LL Road Frame - B...| 202.332| 6| | 71938| 113297|LL Road Frame - B...| 202.332| 3| | 71915| 113090|LL Road Frame - B...| 202.332| 2| | 71815| 111451|LL Road Frame - B...| 202.332| 1| | 71797| 111044|LL Road Frame - B...| 202.332| 1| | 71783| 110710|LL Road Frame - B...| 202.332| 4| | 71936| 113260|HL Mountain Frame...| 809.76| 4| | 71899| 112937|HL Mountain Frame...| 809.76| 1| | 71845| 112137|HL Mountain Frame...| 809.76| 2| | 71832| 111806|HL Mountain Frame...| 809.76| 4| | 71780| 110622|HL Mountain Frame...| 809.76| 1| | 71936| 113235|HL Mountain Frame...| 809.76| 4| | 71845| 112134|HL Mountain Frame...| 809.76| 3| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows Move filter to after select query = dfDetail . join ( dfProduct , 'ProductID' ) \\ . select ( 'SalesOrderID' , 'SalesOrderDetailID' , 'Name' , 'UnitPrice' , 'OrderQty' ) \\ . filter ( \"Color='Black'\" ) \\ query . show () +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71797| 111045|LL Road Frame - B...| 202.332| 3| | 71783| 110730|LL Road Frame - B...| 202.332| 6| | 71938| 113297|LL Road Frame - B...| 202.332| 3| | 71915| 113090|LL Road Frame - B...| 202.332| 2| | 71815| 111451|LL Road Frame - B...| 202.332| 1| | 71797| 111044|LL Road Frame - B...| 202.332| 1| | 71783| 110710|LL Road Frame - B...| 202.332| 4| | 71936| 113260|HL Mountain Frame...| 809.76| 4| | 71899| 112937|HL Mountain Frame...| 809.76| 1| | 71845| 112137|HL Mountain Frame...| 809.76| 2| | 71832| 111806|HL Mountain Frame...| 809.76| 4| | 71780| 110622|HL Mountain Frame...| 809.76| 1| | 71936| 113235|HL Mountain Frame...| 809.76| 4| | 71845| 112134|HL Mountain Frame...| 809.76| 3| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows Query plan query.explain() == Physical Plan == *(2) Project [SalesOrderID#183, SalesOrderDetailID#184, Name#134, UnitPrice#187, OrderQty#185] +- *(2) BroadcastHashJoin [ProductID#186], [ProductID#133], Inner, BuildLeft :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[3, int, true] as bigint))), [id=#485] : +- *(1) Project [SalesOrderID#183, SalesOrderDetailID#184, OrderQty#185, ProductID#186, UnitPrice#187] : +- *(1) Filter isnotnull(ProductID#186) : +- FileScan csv [SalesOrderID#183,SalesOrderDetailID#184,OrderQty#185,ProductID#186,UnitPrice#187] Batched: false, DataFilters: [isnotnull(ProductID#186)], Format: CSV, Location: InMemoryFileIndex[file:/content/drive/MyDrive/courses/HKUST/MSBD5003/data/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double> +- *(2) Project [ProductID#133, Name#134] +- *(2) Filter ((isnotnull(Color#136) AND (Color#136 = Black)) AND isnotnull(ProductID#133)) +- FileScan csv [ProductID#133,Name#134,Color#136] Batched: false, DataFilters: [isnotnull(Color#136), (Color#136 = Black), isnotnull(ProductID#133)], Format: CSV, Location: InMemoryFileIndex[file:/content/drive/MyDrive/courses/HKUST/MSBD5003/data/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string> SparkSQL performs optimization depending on whether intermediate dataframe are cached or not: d1 = dfDetail.join(dfProduct, 'ProductID') \\ .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty') d1.persist() DataFrame[SalesOrderID: int, SalesOrderDetailID: int, Name: string, UnitPrice: double, OrderQty: int] d2 = d1.filter(\"Color = 'Black'\") #d2 = d1.filter(\"OrderQty >= 10\") d2.explain() == Physical Plan == *(2) Project [SalesOrderID#183, SalesOrderDetailID#184, Name#134, UnitPrice#187, OrderQty#185] +- *(2) BroadcastHashJoin [ProductID#186], [ProductID#133], Inner, BuildLeft :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[3, int, true] as bigint))), [id=#564] : +- *(1) Project [SalesOrderID#183, SalesOrderDetailID#184, OrderQty#185, ProductID#186, UnitPrice#187] : +- *(1) Filter isnotnull(ProductID#186) : +- FileScan csv [SalesOrderID#183,SalesOrderDetailID#184,OrderQty#185,ProductID#186,UnitPrice#187] Batched: false, DataFilters: [isnotnull(ProductID#186)], Format: CSV, Location: InMemoryFileIndex[file:/content/drive/MyDrive/courses/HKUST/MSBD5003/data/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double> +- *(2) Project [ProductID#133, Name#134] +- *(2) Filter ((isnotnull(Color#136) AND (Color#136 = Black)) AND isnotnull(ProductID#133)) +- FileScan csv [ProductID#133,Name#134,Color#136] Batched: false, DataFilters: [isnotnull(Color#136), (Color#136 = Black), isnotnull(ProductID#133)], Format: CSV, Location: InMemoryFileIndex[file:/content/drive/MyDrive/courses/HKUST/MSBD5003/data/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string> Error! Since color doesn't exists in the result d1 = dfDetail.join(dfProduct, 'ProductID') \\ .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty') d1.write.csv('temp.csv', mode = 'overwrite', header = True) d2 = spark.read.csv('temp.csv', header = True, inferSchema = True) d2.filter(\"Color = 'Black'\").show() --------------------------------------------------------------------------- AnalysisException Traceback (most recent call last) <ipython-input-26-30770f7af6e5> in <module>() 2 d1.write.csv('temp.csv', mode = 'overwrite', header = True) 3 d2 = spark.read.csv('temp.csv', header = True, inferSchema = True) ----> 4 d2.filter(\"Color = 'Black'\").show() /usr/local/lib/python3.6/dist-packages/pyspark/sql/dataframe.py in filter(self, condition) 1457 \"\"\" 1458 if isinstance(condition, basestring): -> 1459 jdf = self._jdf.filter(condition) 1460 elif isinstance(condition, Column): 1461 jdf = self._jdf.filter(condition._jc) /usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py in __call__(self, *args) 1303 answer = self.gateway_client.send_command(command) 1304 return_value = get_return_value( -> 1305 answer, self.gateway_client, self.target_id, self.name) 1306 1307 for temp_arg in temp_args: /usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py in deco(*a, **kw) 132 # Hide where the exception came from that shows a non-Pythonic 133 # JVM exception message. --> 134 raise_from(converted) 135 else: 136 raise /usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py in raise_from(e) AnalysisException: cannot resolve '`Color`' given input columns: [Name, OrderQty, SalesOrderDetailID, SalesOrderID, UnitPrice]; line 1 pos 0; 'Filter ('Color = Black) +- Relation[SalesOrderID#959,SalesOrderDetailID#960,Name#961,UnitPrice#962,OrderQty#963] csv Find all orders that include at least one black product, return the product SalesOrderID, Name, UnitPrice, and OrderQty SELECT DISTINCT SalesOrderID FROM SalesLT . SalesOrderDetail JOIN SalesLT . Product ON SalesOrderDetail . ProductID = Product . ProductID WHERE Color = 'Black' dfDetail.join(dfProduct.filter(\"Color='Black'\"), 'ProductID') \\ .select('SalesOrderID') \\ .distinct() \\ .show() +------------+ |SalesOrderID| +------------+ | 71902| | 71832| | 71915| | 71831| | 71898| | 71935| | 71938| | 71845| | 71783| | 71815| | 71936| | 71863| | 71780| | 71782| | 71899| | 71784| | 71797| +------------+ SELECT COUNT ( DISTINCT Color ) FROM SalesLT . Product It's 1 more than standard SQL. In standard SQL, COUNT() does not count NULLs. dfProduct.select('Color').distinct().count() 10 Find the total price of each order, return SalesOrderID and total price (column name should be \u2018totalprice\u2019) SELECT SalesOrderID , SUM ( UnitPrice * OrderQty * ( 1 - UnitPriceDiscount )) AS TotalPrice FROM SalesLT . SalesOrderDetail GROUP BY SalesOrderID dfDetail.select('*', (dfDetail.UnitPrice * dfDetail.OrderQty * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\ .groupBy('SalesOrderID').sum('netprice') \\ .withColumnRenamed('sum(netprice)', 'TotalPrice')\\ .show() +------------+------------------+ |SalesOrderID| TotalPrice| +------------+------------------+ | 71867| 858.9| | 71902|59894.209199999976| | 71832| 28950.678108| | 71915|1732.8899999999999| | 71946| 31.584| | 71895|221.25600000000003| | 71816|2847.4079999999994| | 71831| 1712.946| | 71923| 96.108824| | 71858|11528.844000000001| | 71917| 37.758| | 71897| 10585.05| | 71885| 524.664| | 71856|500.30400000000003| | 71898| 53248.69200000002| | 71774| 713.796| | 71796| 47848.02600000001| | 71935|5533.8689079999995| | 71938| 74205.228| | 71845| 34118.5356| +------------+------------------+ only showing top 20 rows Find the total price of each order where the total price > 10000 SELECT SalesOrderID , SUM ( UnitPrice * OrderQty * ( 1 - UnitPriceDiscount )) AS TotalPrice FROM SalesLT . SalesOrderDetail GROUP BY SalesOrderID HAVING SUM ( UnitPrice * OrderQty * ( 1 - UnitPriceDiscount )) > 10000 dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\ .groupBy('SalesOrderID').sum('netprice') \\ .withColumnRenamed('sum(netprice)', 'TotalPrice')\\ .where('TotalPrice > 10000')\\ .show() +------------+------------------+ |SalesOrderID| TotalPrice| +------------+------------------+ | 71902|59894.209199999976| | 71832| 28950.678108| | 71858|11528.844000000001| | 71897| 10585.05| | 71898| 53248.69200000002| | 71796| 47848.02600000001| | 71938| 74205.228| | 71845| 34118.5356| | 71783| 65683.367986| | 71936| 79589.61602399996| | 71780|29923.007999999998| | 71782| 33319.98600000001| | 71784| 89869.27631400003| | 71797| 65123.46341800001| +------------+------------------+ Find the total price on the black products of each order where the total price > 10000 SELECT SalesOrderID , SUM ( UnitPrice * OrderQty * ( 1 - UnitPriceDiscount )) AS TotalPrice FROM SalesLT . SalesOrderDetail , SalesLT . Product WHERE SalesLT . SalesOrderDetail . ProductID = SalesLT . Product . ProductID AND Color = 'Black' GROUP BY SalesOrderID HAVING SUM ( UnitPrice * OrderQty * ( 1 - UnitPriceDiscount )) > 10000 dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\ .join(dfProduct, 'ProductID') \\ .where(\"Color = 'Black'\")\\ .groupBy('SalesOrderID').sum('netprice') \\ .withColumnRenamed('sum(netprice)', 'TotalPrice')\\ .where('TotalPrice > 10000')\\ .show() +------------+------------------+ |SalesOrderID| TotalPrice| +------------+------------------+ | 71902|26677.883999999995| | 71832| 16883.748108| | 71938| 33824.448| | 71845| 18109.836| | 71783|15524.117476000003| | 71936| 44490.290424| | 71780| 16964.322| | 71797| 27581.613792| +------------+------------------+ For each customer, find the total quantity of black products bought. Report CustomerID, FirstName, LastName, and total quantity select saleslt . customer . customerid , FirstName , LastName , sum ( orderqty ) from saleslt . customer left outer join ( saleslt . salesorderheader join saleslt . salesorderdetail on saleslt . salesorderdetail . salesorderid = saleslt . salesorderheader . salesorderid join saleslt . product on saleslt . product . productid = saleslt . salesorderdetail . productid and color = 'black' ) on saleslt . customer . customerid = saleslt . salesorderheader . customerid group by saleslt . customer . customerid , FirstName , LastName order by sum ( orderqty ) desc d1 = dfDetail.join(dfProduct, 'ProductID')\\ .where('Color = \"Black\"') \\ .join(dfHeader, 'SalesOrderID')\\ .groupBy('CustomerID').sum('OrderQty') dfCustomer.join(d1, 'CustomerID', 'left_outer')\\ .select('CustomerID', 'FirstName', 'LastName', 'sum(OrderQty)')\\ .orderBy('sum(OrderQty)', ascending=False)\\ .show() +----------+------------+------------+-------------+ |CustomerID| FirstName| LastName|sum(OrderQty)| +----------+------------+------------+-------------+ | 30050| Krishna|Sunkammurali| 89| | 29796| Jon| Grande| 65| | 29957| Kevin| Liu| 62| | 29929| Jeffrey| Kurtz| 46| | 29546| Christopher| Beck| 45| | 29922| Pamala| Kotc| 34| | 30113| Raja| Venugopal| 34| | 29938| Frank| Campbell| 29| | 29736| Terry| Eminhizer| 23| | 29485| Catherine| Abel| 10| | 30019| Matthew| Miller| 9| | 29932| Rebecca| Laszlo| 7| | 29975| Walter| Mays| 5| | 29638| Rosmarie| Carroll| 2| | 30089|Michael John| Troyer| 1| | 29531| Cory| Booth| 1| | 29568| Donald| Blanton| 1| | 137| Gytis| Barzdukas| null| | 29834| Cheryl| Herring| null| | 451| John| Emory| null| +----------+------------+------------+-------------+ only showing top 20 rows Run SQL Query \u00b6 df.createOrReplaceTempView('HVAC') spark.sql('SELECT * FROM HVAC WHERE BuildingAge >= 10').show() +----------+-----------+-----------+-----------+------------+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country| +----------+-----------+-----------+-----------+------------+ | 1| M1| 25| AC1000| USA| | 2| M2| 27| FN39TG| France| | 3| M3| 28| JDNS77| Brazil| | 4| M4| 17| GG1919| Finland| | 7| M7| 13| FN39TG|South Africa| | 8| M8| 25| JDNS77| Australia| | 9| M9| 11| GG1919| Mexico| | 10| M10| 23| ACMAX22| China| | 11| M11| 14| AC1000| Belgium| | 12| M12| 26| FN39TG| Finland| | 13| M13| 25| JDNS77|Saudi Arabia| | 14| M14| 17| GG1919| Germany| | 15| M15| 19| ACMAX22| Israel| | 16| M16| 23| AC1000| Turkey| | 17| M17| 11| FN39TG| Egypt| | 18| M18| 25| JDNS77| Indonesia| | 19| M19| 14| GG1919| Canada| | 20| M20| 19| ACMAX22| Argentina| +----------+-----------+-----------+-----------+------------+ Mix DF with SQL df.where('BuildingAge >= 10').createOrReplaceTempView('OldBuildings') spark.sql('SELECT HVACproduct, COUNT(*) FROM OldBuildings GROUP BY HVACproduct').show() +-----------+--------+ |HVACproduct|count(1)| +-----------+--------+ | ACMAX22| 3| | AC1000| 3| | JDNS77| 4| | FN39TG| 4| | GG1919| 4| +-----------+--------+ d1 = spark.sql('SELECT * FROM HVAC WHERE BuildingAge >= 10') d1.groupBy('HVACproduct').count().show() +-----------+-----+ |HVACproduct|count| +-----------+-----+ | ACMAX22| 3| | AC1000| 3| | JDNS77| 4| | FN39TG| 4| | GG1919| 4| +-----------+-----+ User Defined Function from pyspark.sql.functions import udf from pyspark.sql.types import IntegerType slen = udf ( lambda s : len ( s ) + 2 , IntegerType ()) df . select ( '*' , slen ( df [ 'Country' ]) . alias ( 'slen' )) . show () +----------+-----------+-----------+-----------+------------+----+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country|slen| +----------+-----------+-----------+-----------+------------+----+ | 1| M1| 25| AC1000| USA| 5| | 2| M2| 27| FN39TG| France| 8| | 3| M3| 28| JDNS77| Brazil| 8| | 4| M4| 17| GG1919| Finland| 9| | 5| M5| 3| ACMAX22| Hong Kong| 11| | 6| M6| 9| AC1000| Singapore| 11| | 7| M7| 13| FN39TG|South Africa| 14| | 8| M8| 25| JDNS77| Australia| 11| | 9| M9| 11| GG1919| Mexico| 8| | 10| M10| 23| ACMAX22| China| 7| | 11| M11| 14| AC1000| Belgium| 9| | 12| M12| 26| FN39TG| Finland| 9| | 13| M13| 25| JDNS77|Saudi Arabia| 14| | 14| M14| 17| GG1919| Germany| 9| | 15| M15| 19| ACMAX22| Israel| 8| | 16| M16| 23| AC1000| Turkey| 8| | 17| M17| 11| FN39TG| Egypt| 7| | 18| M18| 25| JDNS77| Indonesia| 11| | 19| M19| 14| GG1919| Canada| 8| | 20| M20| 19| ACMAX22| Argentina| 11| +----------+-----------+-----------+-----------+------------+----+ spark.udf.register('slen', lambda s: len(s), IntegerType()) spark.sql('SELECT *, slen(Country) AS slen FROM HVAC').show() +----------+-----------+-----------+-----------+------------+----+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country|slen| +----------+-----------+-----------+-----------+------------+----+ | 1| M1| 25| AC1000| USA| 3| | 2| M2| 27| FN39TG| France| 6| | 3| M3| 28| JDNS77| Brazil| 6| | 4| M4| 17| GG1919| Finland| 7| | 5| M5| 3| ACMAX22| Hong Kong| 9| | 6| M6| 9| AC1000| Singapore| 9| | 7| M7| 13| FN39TG|South Africa| 12| | 8| M8| 25| JDNS77| Australia| 9| | 9| M9| 11| GG1919| Mexico| 6| | 10| M10| 23| ACMAX22| China| 5| | 11| M11| 14| AC1000| Belgium| 7| | 12| M12| 26| FN39TG| Finland| 7| | 13| M13| 25| JDNS77|Saudi Arabia| 12| | 14| M14| 17| GG1919| Germany| 7| | 15| M15| 19| ACMAX22| Israel| 6| | 16| M16| 23| AC1000| Turkey| 6| | 17| M17| 11| FN39TG| Egypt| 5| | 18| M18| 25| JDNS77| Indonesia| 9| | 19| M19| 14| GG1919| Canada| 6| | 20| M20| 19| ACMAX22| Argentina| 9| +----------+-----------+-----------+-----------+------------+----+ Convert between RDD and Dataframe \u00b6 From Dataframe to RDD \u00b6 rdd = df.rdd rdd.take(10) [Row(BuildingID=1, BuildingMgr='M1', BuildingAge=25, HVACproduct='AC1000', Country='USA'), Row(BuildingID=2, BuildingMgr='M2', BuildingAge=27, HVACproduct='FN39TG', Country='France'), Row(BuildingID=3, BuildingMgr='M3', BuildingAge=28, HVACproduct='JDNS77', Country='Brazil'), Row(BuildingID=4, BuildingMgr='M4', BuildingAge=17, HVACproduct='GG1919', Country='Finland'), Row(BuildingID=5, BuildingMgr='M5', BuildingAge=3, HVACproduct='ACMAX22', Country='Hong Kong'), Row(BuildingID=6, BuildingMgr='M6', BuildingAge=9, HVACproduct='AC1000', Country='Singapore'), Row(BuildingID=7, BuildingMgr='M7', BuildingAge=13, HVACproduct='FN39TG', Country='South Africa'), Row(BuildingID=8, BuildingMgr='M8', BuildingAge=25, HVACproduct='JDNS77', Country='Australia'), Row(BuildingID=9, BuildingMgr='M9', BuildingAge=11, HVACproduct='GG1919', Country='Mexico'), Row(BuildingID=10, BuildingMgr='M10', BuildingAge=23, HVACproduct='ACMAX22', Country='China')] From rdd to Dataframe df = spark.createDataFrame(rdd) df.printSchema() df.show() root |-- BuildingID: long (nullable = true) |-- BuildingMgr: string (nullable = true) |-- BuildingAge: long (nullable = true) |-- HVACproduct: string (nullable = true) |-- Country: string (nullable = true) +----------+-----------+-----------+-----------+------------+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country| +----------+-----------+-----------+-----------+------------+ | 1| M1| 25| AC1000| USA| | 2| M2| 27| FN39TG| France| | 3| M3| 28| JDNS77| Brazil| | 4| M4| 17| GG1919| Finland| | 5| M5| 3| ACMAX22| Hong Kong| | 6| M6| 9| AC1000| Singapore| | 7| M7| 13| FN39TG|South Africa| | 8| M8| 25| JDNS77| Australia| | 9| M9| 11| GG1919| Mexico| | 10| M10| 23| ACMAX22| China| | 11| M11| 14| AC1000| Belgium| | 12| M12| 26| FN39TG| Finland| | 13| M13| 25| JDNS77|Saudi Arabia| | 14| M14| 17| GG1919| Germany| | 15| M15| 19| ACMAX22| Israel| | 16| M16| 23| AC1000| Turkey| | 17| M17| 11| FN39TG| Egypt| | 18| M18| 25| JDNS77| Indonesia| | 19| M19| 14| GG1919| Canada| | 20| M20| 19| ACMAX22| Argentina| +----------+-----------+-----------+-----------+------------+ Note: DataFrames are stored using columnar storage with compression RDDs are stored using row storage without compression The RDD view of DataFrame just provides an interface, the Row objects are constructed on the fly and do not necessarily represent the internal storage format of the data Closure in Dataframes \u00b6 data = range(10) df = spark.createDataFrame(zip(data, data)) df.printSchema() df.show() root |-- _1: long (nullable = true) |-- _2: long (nullable = true) +---+---+ | _1| _2| +---+---+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| | 5| 5| | 6| 6| | 7| 7| | 8| 8| | 9| 9| +---+---+ The 'closure' behaviour in RDD doesn't seem to exist for DataFrames because of the Catalyst optimizer # using Dataframe x = 5 df1 = df.filter(df._1 < x) df1.show() x = 3 df1.show() +---+---+ | _1| _2| +---+---+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| +---+---+ +---+---+ | _1| _2| +---+---+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| +---+---+ When using rdd, the a will be calculated twice and the x's value changes will finally impact the result # Using rdd rdd = sc.parallelize(range(10)) x = 5 a = rdd.filter(lambda z: z < x) print(a.take(10)) x = 3 print(a.take(10)) [0, 1, 2, 3, 4] [0, 1, 2] def f(): return x/2 x = 5 df1 = df.select(df._1 * 2 + f() + 1 + 1) df1.explain() df1.show() == Physical Plan == *(1) Project [(((cast((_1#1540L * 2) as double) + 2.5) + 1.0) + 1.0) AS ((((_1 * 2) + 2.5) + 1) + 1)#1573] +- *(1) Scan ExistingRDD[_1#1540L,_2#1541L] +----------------------------+ |((((_1 * 2) + 2.5) + 1) + 1)| +----------------------------+ | 4.5| | 6.5| | 8.5| | 10.5| | 12.5| | 14.5| | 16.5| | 18.5| | 20.5| | 22.5| +----------------------------+ counter = 0 def increment_counter ( x ): global counter counter += 1 df . foreach ( increment_counter ) print ( counter ) 0","title":"Spark sql"},{"location":"MSBD5003/Spark%20SQL/#sql","text":"","title":"SQL"},{"location":"MSBD5003/Spark%20SQL/#tables-explained","text":"The schema of a table is the table name and its attributes: A tuple = a record = row A table = a set of tuples","title":"Tables explained"},{"location":"MSBD5003/Spark%20SQL/#operators","text":"","title":"Operators"},{"location":"MSBD5003/Spark%20SQL/#like-operator","text":"SELECT * FROM Products WHERE PName LIKE \u2018 % gizmo % \u2019 s LIKE p: pattern matching on strings p may contain two special symbols: - % = any sequence of characters - _ = any single character","title":"Like operator"},{"location":"MSBD5003/Spark%20SQL/#distinct","text":"SELECT DISTINCT category FROM Product","title":"Distinct"},{"location":"MSBD5003/Spark%20SQL/#order","text":"SELECT pname , price , manufacturer FROM Product WHERE category = \u2018 gizmo \u2019 AND price > 50 ORDER BY price , pname","title":"Order"},{"location":"MSBD5003/Spark%20SQL/#join","text":"support we have a table with following schema drop table if exists product , location ; create table location ( id int primary key auto_increment , location text not null ); create table product ( id int primary key auto_increment , name text , location_id int , foreign key ( location_id ) references location ( id ) ); insert into location ( location ) values ( 'Shenzhen' ); insert into location ( location ) values ( 'Shanghai' ); insert into location ( location ) values ( 'Beijing' ); insert into product ( name , location_id ) values ( 'iPad' , 1 ); insert into product ( name , location_id ) values ( 'iPhone' , 2 ); insert into product ( name , location_id ) values ( 'iMac' , 3 );","title":"join"},{"location":"MSBD5003/Spark%20SQL/#first-way","text":"select name from product inner join location l on product . location_id = l . id where location = 'Shenzhen' ;","title":"First way"},{"location":"MSBD5003/Spark%20SQL/#second-way","text":"select name from product , location l where location_id = l . id and location = 'Shenzhen' ;","title":"Second way"},{"location":"MSBD5003/Spark%20SQL/#outter-join","text":"iPad Shenzhen iPhone Shanghai iMac Beijing Null Hangzhou","title":"Outter join"},{"location":"MSBD5003/Spark%20SQL/#inner-join","text":"iPad Shenzhen iPhone Shanghai iMac Beijing","title":"Inner join"},{"location":"MSBD5003/Spark%20SQL/#aggregation","text":"sum, count, min, max, avg SELECT avg ( price ) FROM Product WHERE maker = \u201c Toyota \u201d COUNT applies to duplicates, unless otherwise stated:","title":"Aggregation"},{"location":"MSBD5003/Spark%20SQL/#having","text":"SELECT product , Sum ( price * quantity ) FROM Purchase WHERE date > \u2018 10 / 1 / 2005 \u2019 GROUP BY product HAVING Sum ( quantity ) > 30","title":"Having"},{"location":"MSBD5003/Spark%20SQL/#pyspark-sql","text":"RDD can be used to achieve the same functionality. What are the benefits of using DataFrames? Readability Flexibility Columnar storage Catalyst optimizer","title":"PySpark SQL"},{"location":"MSBD5003/Spark%20SQL/#plan-optimization","text":"","title":"Plan Optimization"},{"location":"MSBD5003/Spark%20SQL/#tree-transformations","text":"","title":"Tree transformations"},{"location":"MSBD5003/Spark%20SQL/#catalyst-rules","text":"Pattern matching functions that transform subtrees into specific structures. Multiple patterns in the same transform call. May take multiple batches to reach a fixed point. transform can contain arbitrary Scala code.","title":"Catalyst Rules"},{"location":"MSBD5003/Spark%20SQL/#rule-based-optimization","text":"","title":"Rule-based optimization"},{"location":"MSBD5003/Spark%20SQL/#cost-based-optimization","text":"Currently only used for choosing join algorithms - Big table joining big table: shuffle - Small table joining big table: broadcast the small table","title":"Cost-based optimization"},{"location":"MSBD5003/Spark%20SQL/#pyspark-demo","text":"!pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 67kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 27.6MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=1dcf7073a6e4895948c28fb6dcc6c2d41885f6288af4226ea8e4e78f09e44494 Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 from pyspark.sql import Row from pyspark.context import SparkContext from pyspark.sql.session import SparkSession sc = SparkContext . getOrCreate () spark = SparkSession ( sc ) row = Row(name='Alice', age=11) row.name, row.age ('Alice', 11) Using . to get attribute or using python dict function row = Row(name='Alice', age=11, count=1) print(f\"Row.count: {row.count}\") print(f\"Row['count']: {row['count']}\") Row.count: <built-in method count of Row object at 0x7fd3091ecaf0> Row['count']: 1 Load csv df = spark.read.csv('/content/drive/MyDrive/courses/HKUST/MSBD5003/data/building.csv', header=True, inferSchema=True) Create rdd from dataframe dfrdd = df.rdd dfrdd.take(3) [Row(BuildingID=1, BuildingMgr='M1', BuildingAge=25, HVACproduct='AC1000', Country='USA'), Row(BuildingID=2, BuildingMgr='M2', BuildingAge=27, HVACproduct='FN39TG', Country='France'), Row(BuildingID=3, BuildingMgr='M3', BuildingAge=28, HVACproduct='JDNS77', Country='Brazil')]","title":"Pyspark demo"},{"location":"MSBD5003/Spark%20SQL/#select","text":"from pyspark.sql.functions import * df . select ( 'BuildingID' , 'Country' ) . show () +----------+------------+ |BuildingID| Country| +----------+------------+ | 1| USA| | 2| France| | 3| Brazil| | 4| Finland| | 5| Hong Kong| | 6| Singapore| | 7|South Africa| | 8| Australia| | 9| Mexico| | 10| China| | 11| Belgium| | 12| Finland| | 13|Saudi Arabia| | 14| Germany| | 15| Israel| | 16| Turkey| | 17| Egypt| | 18| Indonesia| | 19| Canada| | 20| Argentina| +----------+------------+ Using where clause lit: Creates a Column of literal value. df.where(\"Country<'USA'\").select('BuildingID', lit('OK')).show() +----------+---+ |BuildingID| OK| +----------+---+ | 2| OK| | 3| OK| | 4| OK| | 5| OK| | 6| OK| | 7| OK| | 8| OK| | 9| OK| | 10| OK| | 11| OK| | 12| OK| | 13| OK| | 14| OK| | 15| OK| | 16| OK| | 17| OK| | 18| OK| | 19| OK| | 20| OK| +----------+---+","title":"Select"},{"location":"MSBD5003/Spark%20SQL/#rawsql-to-spark","text":"dfCustomer = spark.read.csv('/content/drive/MyDrive/courses/HKUST/MSBD5003/data/Customer.csv', header=True, inferSchema=True) dfProduct = spark.read.csv('/content/drive/MyDrive/courses/HKUST/MSBD5003/data/Product.csv', header=True, inferSchema=True) dfDetail = spark.read.csv('/content/drive/MyDrive/courses/HKUST/MSBD5003/data/SalesOrderDetail.csv', header=True, inferSchema=True) dfHeader = spark.read.csv('/content/drive/MyDrive/courses/HKUST/MSBD5003/data/SalesOrderHeader.csv', header=True, inferSchema=True) SELECT ProductID , Name , ListPrice FROM Product WHERE Color = 'black' dfProduct.filter(\"Color = 'Black'\")\\ .select('ProductID', 'Name', 'ListPrice')\\ .show(truncate=False) +---------+-----------------------------+---------+ |ProductID|Name |ListPrice| +---------+-----------------------------+---------+ |680 |HL Road Frame - Black, 58 |1431.5 | |708 |Sport-100 Helmet, Black |34.99 | |722 |LL Road Frame - Black, 58 |337.22 | |723 |LL Road Frame - Black, 60 |337.22 | |724 |LL Road Frame - Black, 62 |337.22 | |736 |LL Road Frame - Black, 44 |337.22 | |737 |LL Road Frame - Black, 48 |337.22 | |738 |LL Road Frame - Black, 52 |337.22 | |743 |HL Mountain Frame - Black, 42|1349.6 | |744 |HL Mountain Frame - Black, 44|1349.6 | |745 |HL Mountain Frame - Black, 48|1349.6 | |746 |HL Mountain Frame - Black, 46|1349.6 | |747 |HL Mountain Frame - Black, 38|1349.6 | |765 |Road-650 Black, 58 |782.99 | |766 |Road-650 Black, 60 |782.99 | |767 |Road-650 Black, 62 |782.99 | |768 |Road-650 Black, 44 |782.99 | |769 |Road-650 Black, 48 |782.99 | |770 |Road-650 Black, 52 |782.99 | |775 |Mountain-100 Black, 38 |3374.99 | +---------+-----------------------------+---------+ only showing top 20 rows dfProduct.where(dfProduct.Color=='Black') \\ .select(dfProduct.ProductID, dfProduct['Name'], (dfProduct['ListPrice'] * 2).alias('Double price')) \\ .show(truncate=False) +---------+-----------------------------+------------+ |ProductID|Name |Double price| +---------+-----------------------------+------------+ |680 |HL Road Frame - Black, 58 |2863.0 | |708 |Sport-100 Helmet, Black |69.98 | |722 |LL Road Frame - Black, 58 |674.44 | |723 |LL Road Frame - Black, 60 |674.44 | |724 |LL Road Frame - Black, 62 |674.44 | |736 |LL Road Frame - Black, 44 |674.44 | |737 |LL Road Frame - Black, 48 |674.44 | |738 |LL Road Frame - Black, 52 |674.44 | |743 |HL Mountain Frame - Black, 42|2699.2 | |744 |HL Mountain Frame - Black, 44|2699.2 | |745 |HL Mountain Frame - Black, 48|2699.2 | |746 |HL Mountain Frame - Black, 46|2699.2 | |747 |HL Mountain Frame - Black, 38|2699.2 | |765 |Road-650 Black, 58 |1565.98 | |766 |Road-650 Black, 60 |1565.98 | |767 |Road-650 Black, 62 |1565.98 | |768 |Road-650 Black, 44 |1565.98 | |769 |Road-650 Black, 48 |1565.98 | |770 |Road-650 Black, 52 |1565.98 | |775 |Mountain-100 Black, 38 |6749.98 | +---------+-----------------------------+------------+ only showing top 20 rows dfProduct.where(dfProduct.ListPrice * 2 > 100) \\ .select(dfProduct.ProductID, dfProduct['Name'], dfProduct.ListPrice * 2) \\ .show(truncate=False) +---------+-------------------------+---------------+ |ProductID|Name |(ListPrice * 2)| +---------+-------------------------+---------------+ |680 |HL Road Frame - Black, 58|2863.0 | |706 |HL Road Frame - Red, 58 |2863.0 | |717 |HL Road Frame - Red, 62 |2863.0 | |718 |HL Road Frame - Red, 44 |2863.0 | |719 |HL Road Frame - Red, 48 |2863.0 | |720 |HL Road Frame - Red, 52 |2863.0 | |721 |HL Road Frame - Red, 56 |2863.0 | |722 |LL Road Frame - Black, 58|674.44 | |723 |LL Road Frame - Black, 60|674.44 | |724 |LL Road Frame - Black, 62|674.44 | |725 |LL Road Frame - Red, 44 |674.44 | |726 |LL Road Frame - Red, 48 |674.44 | |727 |LL Road Frame - Red, 52 |674.44 | |728 |LL Road Frame - Red, 58 |674.44 | |729 |LL Road Frame - Red, 60 |674.44 | |730 |LL Road Frame - Red, 62 |674.44 | |731 |ML Road Frame - Red, 44 |1189.66 | |732 |ML Road Frame - Red, 48 |1189.66 | |733 |ML Road Frame - Red, 52 |1189.66 | |734 |ML Road Frame - Red, 58 |1189.66 | +---------+-------------------------+---------------+ only showing top 20 rows SELECT ProductID , Name , ListPrice FROM Product WHERE Color = 'black' ORDER BY ProductID dfProduct.filter(\"Color = 'Black'\")\\ .select('ProductID', 'Name', 'ListPrice')\\ .orderBy('ListPrice')\\ .show(truncate=False) +---------+--------------------------+---------+ |ProductID|Name |ListPrice| +---------+--------------------------+---------+ |860 |Half-Finger Gloves, L |24.49 | |859 |Half-Finger Gloves, M |24.49 | |858 |Half-Finger Gloves, S |24.49 | |708 |Sport-100 Helmet, Black |34.99 | |862 |Full-Finger Gloves, M |37.99 | |861 |Full-Finger Gloves, S |37.99 | |863 |Full-Finger Gloves, L |37.99 | |841 |Men's Sports Shorts, S |59.99 | |849 |Men's Sports Shorts, M |59.99 | |851 |Men's Sports Shorts, XL |59.99 | |850 |Men's Sports Shorts, L |59.99 | |815 |LL Mountain Front Wheel |60.745 | |868 |Women's Mountain Shorts, M|69.99 | |869 |Women's Mountain Shorts, L|69.99 | |867 |Women's Mountain Shorts, S|69.99 | |853 |Women's Tights, M |74.99 | |854 |Women's Tights, L |74.99 | |852 |Women's Tights, S |74.99 | |818 |LL Road Front Wheel |85.565 | |823 |LL Mountain Rear Wheel |87.745 | +---------+--------------------------+---------+ only showing top 20 rows Find all orders and details on black product, return the product SalesOrderID, SalesOrderDetailID, Name, UnitPrice, and OrderQty SELECT SalesOrderID , SalesOrderDetailID , Name , UnitPrice , OrderQty FROM SalesLT . SalesOrderDetail , SalesLT . Product WHERE SalesOrderDetail . ProductID = Product . ProductID AND Color = 'Black' SELECT SalesOrderID , SalesOrderDetailID , Name , UnitPrice , OrderQty FROM SalesLT . SalesOrderDetail JOIN SalesLT . Product ON SalesOrderDetail . ProductID = Product . ProductID WHERE Color = 'Black' dfDetail.join(dfProduct, 'ProductID') \\ .filter(\"Color='Black'\")\\ .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty') \\ .show() +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71797| 111045|LL Road Frame - B...| 202.332| 3| | 71783| 110730|LL Road Frame - B...| 202.332| 6| | 71938| 113297|LL Road Frame - B...| 202.332| 3| | 71915| 113090|LL Road Frame - B...| 202.332| 2| | 71815| 111451|LL Road Frame - B...| 202.332| 1| | 71797| 111044|LL Road Frame - B...| 202.332| 1| | 71783| 110710|LL Road Frame - B...| 202.332| 4| | 71936| 113260|HL Mountain Frame...| 809.76| 4| | 71899| 112937|HL Mountain Frame...| 809.76| 1| | 71845| 112137|HL Mountain Frame...| 809.76| 2| | 71832| 111806|HL Mountain Frame...| 809.76| 4| | 71780| 110622|HL Mountain Frame...| 809.76| 1| | 71936| 113235|HL Mountain Frame...| 809.76| 4| | 71845| 112134|HL Mountain Frame...| 809.76| 3| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows Move filter to after select query = dfDetail . join ( dfProduct , 'ProductID' ) \\ . select ( 'SalesOrderID' , 'SalesOrderDetailID' , 'Name' , 'UnitPrice' , 'OrderQty' ) \\ . filter ( \"Color='Black'\" ) \\ query . show () +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71797| 111045|LL Road Frame - B...| 202.332| 3| | 71783| 110730|LL Road Frame - B...| 202.332| 6| | 71938| 113297|LL Road Frame - B...| 202.332| 3| | 71915| 113090|LL Road Frame - B...| 202.332| 2| | 71815| 111451|LL Road Frame - B...| 202.332| 1| | 71797| 111044|LL Road Frame - B...| 202.332| 1| | 71783| 110710|LL Road Frame - B...| 202.332| 4| | 71936| 113260|HL Mountain Frame...| 809.76| 4| | 71899| 112937|HL Mountain Frame...| 809.76| 1| | 71845| 112137|HL Mountain Frame...| 809.76| 2| | 71832| 111806|HL Mountain Frame...| 809.76| 4| | 71780| 110622|HL Mountain Frame...| 809.76| 1| | 71936| 113235|HL Mountain Frame...| 809.76| 4| | 71845| 112134|HL Mountain Frame...| 809.76| 3| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows Query plan query.explain() == Physical Plan == *(2) Project [SalesOrderID#183, SalesOrderDetailID#184, Name#134, UnitPrice#187, OrderQty#185] +- *(2) BroadcastHashJoin [ProductID#186], [ProductID#133], Inner, BuildLeft :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[3, int, true] as bigint))), [id=#485] : +- *(1) Project [SalesOrderID#183, SalesOrderDetailID#184, OrderQty#185, ProductID#186, UnitPrice#187] : +- *(1) Filter isnotnull(ProductID#186) : +- FileScan csv [SalesOrderID#183,SalesOrderDetailID#184,OrderQty#185,ProductID#186,UnitPrice#187] Batched: false, DataFilters: [isnotnull(ProductID#186)], Format: CSV, Location: InMemoryFileIndex[file:/content/drive/MyDrive/courses/HKUST/MSBD5003/data/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double> +- *(2) Project [ProductID#133, Name#134] +- *(2) Filter ((isnotnull(Color#136) AND (Color#136 = Black)) AND isnotnull(ProductID#133)) +- FileScan csv [ProductID#133,Name#134,Color#136] Batched: false, DataFilters: [isnotnull(Color#136), (Color#136 = Black), isnotnull(ProductID#133)], Format: CSV, Location: InMemoryFileIndex[file:/content/drive/MyDrive/courses/HKUST/MSBD5003/data/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string> SparkSQL performs optimization depending on whether intermediate dataframe are cached or not: d1 = dfDetail.join(dfProduct, 'ProductID') \\ .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty') d1.persist() DataFrame[SalesOrderID: int, SalesOrderDetailID: int, Name: string, UnitPrice: double, OrderQty: int] d2 = d1.filter(\"Color = 'Black'\") #d2 = d1.filter(\"OrderQty >= 10\") d2.explain() == Physical Plan == *(2) Project [SalesOrderID#183, SalesOrderDetailID#184, Name#134, UnitPrice#187, OrderQty#185] +- *(2) BroadcastHashJoin [ProductID#186], [ProductID#133], Inner, BuildLeft :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[3, int, true] as bigint))), [id=#564] : +- *(1) Project [SalesOrderID#183, SalesOrderDetailID#184, OrderQty#185, ProductID#186, UnitPrice#187] : +- *(1) Filter isnotnull(ProductID#186) : +- FileScan csv [SalesOrderID#183,SalesOrderDetailID#184,OrderQty#185,ProductID#186,UnitPrice#187] Batched: false, DataFilters: [isnotnull(ProductID#186)], Format: CSV, Location: InMemoryFileIndex[file:/content/drive/MyDrive/courses/HKUST/MSBD5003/data/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double> +- *(2) Project [ProductID#133, Name#134] +- *(2) Filter ((isnotnull(Color#136) AND (Color#136 = Black)) AND isnotnull(ProductID#133)) +- FileScan csv [ProductID#133,Name#134,Color#136] Batched: false, DataFilters: [isnotnull(Color#136), (Color#136 = Black), isnotnull(ProductID#133)], Format: CSV, Location: InMemoryFileIndex[file:/content/drive/MyDrive/courses/HKUST/MSBD5003/data/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string> Error! Since color doesn't exists in the result d1 = dfDetail.join(dfProduct, 'ProductID') \\ .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty') d1.write.csv('temp.csv', mode = 'overwrite', header = True) d2 = spark.read.csv('temp.csv', header = True, inferSchema = True) d2.filter(\"Color = 'Black'\").show() --------------------------------------------------------------------------- AnalysisException Traceback (most recent call last) <ipython-input-26-30770f7af6e5> in <module>() 2 d1.write.csv('temp.csv', mode = 'overwrite', header = True) 3 d2 = spark.read.csv('temp.csv', header = True, inferSchema = True) ----> 4 d2.filter(\"Color = 'Black'\").show() /usr/local/lib/python3.6/dist-packages/pyspark/sql/dataframe.py in filter(self, condition) 1457 \"\"\" 1458 if isinstance(condition, basestring): -> 1459 jdf = self._jdf.filter(condition) 1460 elif isinstance(condition, Column): 1461 jdf = self._jdf.filter(condition._jc) /usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py in __call__(self, *args) 1303 answer = self.gateway_client.send_command(command) 1304 return_value = get_return_value( -> 1305 answer, self.gateway_client, self.target_id, self.name) 1306 1307 for temp_arg in temp_args: /usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py in deco(*a, **kw) 132 # Hide where the exception came from that shows a non-Pythonic 133 # JVM exception message. --> 134 raise_from(converted) 135 else: 136 raise /usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py in raise_from(e) AnalysisException: cannot resolve '`Color`' given input columns: [Name, OrderQty, SalesOrderDetailID, SalesOrderID, UnitPrice]; line 1 pos 0; 'Filter ('Color = Black) +- Relation[SalesOrderID#959,SalesOrderDetailID#960,Name#961,UnitPrice#962,OrderQty#963] csv Find all orders that include at least one black product, return the product SalesOrderID, Name, UnitPrice, and OrderQty SELECT DISTINCT SalesOrderID FROM SalesLT . SalesOrderDetail JOIN SalesLT . Product ON SalesOrderDetail . ProductID = Product . ProductID WHERE Color = 'Black' dfDetail.join(dfProduct.filter(\"Color='Black'\"), 'ProductID') \\ .select('SalesOrderID') \\ .distinct() \\ .show() +------------+ |SalesOrderID| +------------+ | 71902| | 71832| | 71915| | 71831| | 71898| | 71935| | 71938| | 71845| | 71783| | 71815| | 71936| | 71863| | 71780| | 71782| | 71899| | 71784| | 71797| +------------+ SELECT COUNT ( DISTINCT Color ) FROM SalesLT . Product It's 1 more than standard SQL. In standard SQL, COUNT() does not count NULLs. dfProduct.select('Color').distinct().count() 10 Find the total price of each order, return SalesOrderID and total price (column name should be \u2018totalprice\u2019) SELECT SalesOrderID , SUM ( UnitPrice * OrderQty * ( 1 - UnitPriceDiscount )) AS TotalPrice FROM SalesLT . SalesOrderDetail GROUP BY SalesOrderID dfDetail.select('*', (dfDetail.UnitPrice * dfDetail.OrderQty * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\ .groupBy('SalesOrderID').sum('netprice') \\ .withColumnRenamed('sum(netprice)', 'TotalPrice')\\ .show() +------------+------------------+ |SalesOrderID| TotalPrice| +------------+------------------+ | 71867| 858.9| | 71902|59894.209199999976| | 71832| 28950.678108| | 71915|1732.8899999999999| | 71946| 31.584| | 71895|221.25600000000003| | 71816|2847.4079999999994| | 71831| 1712.946| | 71923| 96.108824| | 71858|11528.844000000001| | 71917| 37.758| | 71897| 10585.05| | 71885| 524.664| | 71856|500.30400000000003| | 71898| 53248.69200000002| | 71774| 713.796| | 71796| 47848.02600000001| | 71935|5533.8689079999995| | 71938| 74205.228| | 71845| 34118.5356| +------------+------------------+ only showing top 20 rows Find the total price of each order where the total price > 10000 SELECT SalesOrderID , SUM ( UnitPrice * OrderQty * ( 1 - UnitPriceDiscount )) AS TotalPrice FROM SalesLT . SalesOrderDetail GROUP BY SalesOrderID HAVING SUM ( UnitPrice * OrderQty * ( 1 - UnitPriceDiscount )) > 10000 dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\ .groupBy('SalesOrderID').sum('netprice') \\ .withColumnRenamed('sum(netprice)', 'TotalPrice')\\ .where('TotalPrice > 10000')\\ .show() +------------+------------------+ |SalesOrderID| TotalPrice| +------------+------------------+ | 71902|59894.209199999976| | 71832| 28950.678108| | 71858|11528.844000000001| | 71897| 10585.05| | 71898| 53248.69200000002| | 71796| 47848.02600000001| | 71938| 74205.228| | 71845| 34118.5356| | 71783| 65683.367986| | 71936| 79589.61602399996| | 71780|29923.007999999998| | 71782| 33319.98600000001| | 71784| 89869.27631400003| | 71797| 65123.46341800001| +------------+------------------+ Find the total price on the black products of each order where the total price > 10000 SELECT SalesOrderID , SUM ( UnitPrice * OrderQty * ( 1 - UnitPriceDiscount )) AS TotalPrice FROM SalesLT . SalesOrderDetail , SalesLT . Product WHERE SalesLT . SalesOrderDetail . ProductID = SalesLT . Product . ProductID AND Color = 'Black' GROUP BY SalesOrderID HAVING SUM ( UnitPrice * OrderQty * ( 1 - UnitPriceDiscount )) > 10000 dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\ .join(dfProduct, 'ProductID') \\ .where(\"Color = 'Black'\")\\ .groupBy('SalesOrderID').sum('netprice') \\ .withColumnRenamed('sum(netprice)', 'TotalPrice')\\ .where('TotalPrice > 10000')\\ .show() +------------+------------------+ |SalesOrderID| TotalPrice| +------------+------------------+ | 71902|26677.883999999995| | 71832| 16883.748108| | 71938| 33824.448| | 71845| 18109.836| | 71783|15524.117476000003| | 71936| 44490.290424| | 71780| 16964.322| | 71797| 27581.613792| +------------+------------------+ For each customer, find the total quantity of black products bought. Report CustomerID, FirstName, LastName, and total quantity select saleslt . customer . customerid , FirstName , LastName , sum ( orderqty ) from saleslt . customer left outer join ( saleslt . salesorderheader join saleslt . salesorderdetail on saleslt . salesorderdetail . salesorderid = saleslt . salesorderheader . salesorderid join saleslt . product on saleslt . product . productid = saleslt . salesorderdetail . productid and color = 'black' ) on saleslt . customer . customerid = saleslt . salesorderheader . customerid group by saleslt . customer . customerid , FirstName , LastName order by sum ( orderqty ) desc d1 = dfDetail.join(dfProduct, 'ProductID')\\ .where('Color = \"Black\"') \\ .join(dfHeader, 'SalesOrderID')\\ .groupBy('CustomerID').sum('OrderQty') dfCustomer.join(d1, 'CustomerID', 'left_outer')\\ .select('CustomerID', 'FirstName', 'LastName', 'sum(OrderQty)')\\ .orderBy('sum(OrderQty)', ascending=False)\\ .show() +----------+------------+------------+-------------+ |CustomerID| FirstName| LastName|sum(OrderQty)| +----------+------------+------------+-------------+ | 30050| Krishna|Sunkammurali| 89| | 29796| Jon| Grande| 65| | 29957| Kevin| Liu| 62| | 29929| Jeffrey| Kurtz| 46| | 29546| Christopher| Beck| 45| | 29922| Pamala| Kotc| 34| | 30113| Raja| Venugopal| 34| | 29938| Frank| Campbell| 29| | 29736| Terry| Eminhizer| 23| | 29485| Catherine| Abel| 10| | 30019| Matthew| Miller| 9| | 29932| Rebecca| Laszlo| 7| | 29975| Walter| Mays| 5| | 29638| Rosmarie| Carroll| 2| | 30089|Michael John| Troyer| 1| | 29531| Cory| Booth| 1| | 29568| Donald| Blanton| 1| | 137| Gytis| Barzdukas| null| | 29834| Cheryl| Herring| null| | 451| John| Emory| null| +----------+------------+------------+-------------+ only showing top 20 rows","title":"RawSQL to spark"},{"location":"MSBD5003/Spark%20SQL/#run-sql-query","text":"df.createOrReplaceTempView('HVAC') spark.sql('SELECT * FROM HVAC WHERE BuildingAge >= 10').show() +----------+-----------+-----------+-----------+------------+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country| +----------+-----------+-----------+-----------+------------+ | 1| M1| 25| AC1000| USA| | 2| M2| 27| FN39TG| France| | 3| M3| 28| JDNS77| Brazil| | 4| M4| 17| GG1919| Finland| | 7| M7| 13| FN39TG|South Africa| | 8| M8| 25| JDNS77| Australia| | 9| M9| 11| GG1919| Mexico| | 10| M10| 23| ACMAX22| China| | 11| M11| 14| AC1000| Belgium| | 12| M12| 26| FN39TG| Finland| | 13| M13| 25| JDNS77|Saudi Arabia| | 14| M14| 17| GG1919| Germany| | 15| M15| 19| ACMAX22| Israel| | 16| M16| 23| AC1000| Turkey| | 17| M17| 11| FN39TG| Egypt| | 18| M18| 25| JDNS77| Indonesia| | 19| M19| 14| GG1919| Canada| | 20| M20| 19| ACMAX22| Argentina| +----------+-----------+-----------+-----------+------------+ Mix DF with SQL df.where('BuildingAge >= 10').createOrReplaceTempView('OldBuildings') spark.sql('SELECT HVACproduct, COUNT(*) FROM OldBuildings GROUP BY HVACproduct').show() +-----------+--------+ |HVACproduct|count(1)| +-----------+--------+ | ACMAX22| 3| | AC1000| 3| | JDNS77| 4| | FN39TG| 4| | GG1919| 4| +-----------+--------+ d1 = spark.sql('SELECT * FROM HVAC WHERE BuildingAge >= 10') d1.groupBy('HVACproduct').count().show() +-----------+-----+ |HVACproduct|count| +-----------+-----+ | ACMAX22| 3| | AC1000| 3| | JDNS77| 4| | FN39TG| 4| | GG1919| 4| +-----------+-----+ User Defined Function from pyspark.sql.functions import udf from pyspark.sql.types import IntegerType slen = udf ( lambda s : len ( s ) + 2 , IntegerType ()) df . select ( '*' , slen ( df [ 'Country' ]) . alias ( 'slen' )) . show () +----------+-----------+-----------+-----------+------------+----+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country|slen| +----------+-----------+-----------+-----------+------------+----+ | 1| M1| 25| AC1000| USA| 5| | 2| M2| 27| FN39TG| France| 8| | 3| M3| 28| JDNS77| Brazil| 8| | 4| M4| 17| GG1919| Finland| 9| | 5| M5| 3| ACMAX22| Hong Kong| 11| | 6| M6| 9| AC1000| Singapore| 11| | 7| M7| 13| FN39TG|South Africa| 14| | 8| M8| 25| JDNS77| Australia| 11| | 9| M9| 11| GG1919| Mexico| 8| | 10| M10| 23| ACMAX22| China| 7| | 11| M11| 14| AC1000| Belgium| 9| | 12| M12| 26| FN39TG| Finland| 9| | 13| M13| 25| JDNS77|Saudi Arabia| 14| | 14| M14| 17| GG1919| Germany| 9| | 15| M15| 19| ACMAX22| Israel| 8| | 16| M16| 23| AC1000| Turkey| 8| | 17| M17| 11| FN39TG| Egypt| 7| | 18| M18| 25| JDNS77| Indonesia| 11| | 19| M19| 14| GG1919| Canada| 8| | 20| M20| 19| ACMAX22| Argentina| 11| +----------+-----------+-----------+-----------+------------+----+ spark.udf.register('slen', lambda s: len(s), IntegerType()) spark.sql('SELECT *, slen(Country) AS slen FROM HVAC').show() +----------+-----------+-----------+-----------+------------+----+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country|slen| +----------+-----------+-----------+-----------+------------+----+ | 1| M1| 25| AC1000| USA| 3| | 2| M2| 27| FN39TG| France| 6| | 3| M3| 28| JDNS77| Brazil| 6| | 4| M4| 17| GG1919| Finland| 7| | 5| M5| 3| ACMAX22| Hong Kong| 9| | 6| M6| 9| AC1000| Singapore| 9| | 7| M7| 13| FN39TG|South Africa| 12| | 8| M8| 25| JDNS77| Australia| 9| | 9| M9| 11| GG1919| Mexico| 6| | 10| M10| 23| ACMAX22| China| 5| | 11| M11| 14| AC1000| Belgium| 7| | 12| M12| 26| FN39TG| Finland| 7| | 13| M13| 25| JDNS77|Saudi Arabia| 12| | 14| M14| 17| GG1919| Germany| 7| | 15| M15| 19| ACMAX22| Israel| 6| | 16| M16| 23| AC1000| Turkey| 6| | 17| M17| 11| FN39TG| Egypt| 5| | 18| M18| 25| JDNS77| Indonesia| 9| | 19| M19| 14| GG1919| Canada| 6| | 20| M20| 19| ACMAX22| Argentina| 9| +----------+-----------+-----------+-----------+------------+----+","title":"Run SQL Query"},{"location":"MSBD5003/Spark%20SQL/#convert-between-rdd-and-dataframe","text":"","title":"Convert between RDD and Dataframe"},{"location":"MSBD5003/Spark%20SQL/#from-dataframe-to-rdd","text":"rdd = df.rdd rdd.take(10) [Row(BuildingID=1, BuildingMgr='M1', BuildingAge=25, HVACproduct='AC1000', Country='USA'), Row(BuildingID=2, BuildingMgr='M2', BuildingAge=27, HVACproduct='FN39TG', Country='France'), Row(BuildingID=3, BuildingMgr='M3', BuildingAge=28, HVACproduct='JDNS77', Country='Brazil'), Row(BuildingID=4, BuildingMgr='M4', BuildingAge=17, HVACproduct='GG1919', Country='Finland'), Row(BuildingID=5, BuildingMgr='M5', BuildingAge=3, HVACproduct='ACMAX22', Country='Hong Kong'), Row(BuildingID=6, BuildingMgr='M6', BuildingAge=9, HVACproduct='AC1000', Country='Singapore'), Row(BuildingID=7, BuildingMgr='M7', BuildingAge=13, HVACproduct='FN39TG', Country='South Africa'), Row(BuildingID=8, BuildingMgr='M8', BuildingAge=25, HVACproduct='JDNS77', Country='Australia'), Row(BuildingID=9, BuildingMgr='M9', BuildingAge=11, HVACproduct='GG1919', Country='Mexico'), Row(BuildingID=10, BuildingMgr='M10', BuildingAge=23, HVACproduct='ACMAX22', Country='China')] From rdd to Dataframe df = spark.createDataFrame(rdd) df.printSchema() df.show() root |-- BuildingID: long (nullable = true) |-- BuildingMgr: string (nullable = true) |-- BuildingAge: long (nullable = true) |-- HVACproduct: string (nullable = true) |-- Country: string (nullable = true) +----------+-----------+-----------+-----------+------------+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country| +----------+-----------+-----------+-----------+------------+ | 1| M1| 25| AC1000| USA| | 2| M2| 27| FN39TG| France| | 3| M3| 28| JDNS77| Brazil| | 4| M4| 17| GG1919| Finland| | 5| M5| 3| ACMAX22| Hong Kong| | 6| M6| 9| AC1000| Singapore| | 7| M7| 13| FN39TG|South Africa| | 8| M8| 25| JDNS77| Australia| | 9| M9| 11| GG1919| Mexico| | 10| M10| 23| ACMAX22| China| | 11| M11| 14| AC1000| Belgium| | 12| M12| 26| FN39TG| Finland| | 13| M13| 25| JDNS77|Saudi Arabia| | 14| M14| 17| GG1919| Germany| | 15| M15| 19| ACMAX22| Israel| | 16| M16| 23| AC1000| Turkey| | 17| M17| 11| FN39TG| Egypt| | 18| M18| 25| JDNS77| Indonesia| | 19| M19| 14| GG1919| Canada| | 20| M20| 19| ACMAX22| Argentina| +----------+-----------+-----------+-----------+------------+ Note: DataFrames are stored using columnar storage with compression RDDs are stored using row storage without compression The RDD view of DataFrame just provides an interface, the Row objects are constructed on the fly and do not necessarily represent the internal storage format of the data","title":"From Dataframe to RDD"},{"location":"MSBD5003/Spark%20SQL/#closure-in-dataframes","text":"data = range(10) df = spark.createDataFrame(zip(data, data)) df.printSchema() df.show() root |-- _1: long (nullable = true) |-- _2: long (nullable = true) +---+---+ | _1| _2| +---+---+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| | 5| 5| | 6| 6| | 7| 7| | 8| 8| | 9| 9| +---+---+ The 'closure' behaviour in RDD doesn't seem to exist for DataFrames because of the Catalyst optimizer # using Dataframe x = 5 df1 = df.filter(df._1 < x) df1.show() x = 3 df1.show() +---+---+ | _1| _2| +---+---+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| +---+---+ +---+---+ | _1| _2| +---+---+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| +---+---+ When using rdd, the a will be calculated twice and the x's value changes will finally impact the result # Using rdd rdd = sc.parallelize(range(10)) x = 5 a = rdd.filter(lambda z: z < x) print(a.take(10)) x = 3 print(a.take(10)) [0, 1, 2, 3, 4] [0, 1, 2] def f(): return x/2 x = 5 df1 = df.select(df._1 * 2 + f() + 1 + 1) df1.explain() df1.show() == Physical Plan == *(1) Project [(((cast((_1#1540L * 2) as double) + 2.5) + 1.0) + 1.0) AS ((((_1 * 2) + 2.5) + 1) + 1)#1573] +- *(1) Scan ExistingRDD[_1#1540L,_2#1541L] +----------------------------+ |((((_1 * 2) + 2.5) + 1) + 1)| +----------------------------+ | 4.5| | 6.5| | 8.5| | 10.5| | 12.5| | 14.5| | 16.5| | 18.5| | 20.5| | 22.5| +----------------------------+ counter = 0 def increment_counter ( x ): global counter counter += 1 df . foreach ( increment_counter ) print ( counter ) 0","title":"Closure in Dataframes"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/","text":"Spark basics and RDD \u00b6 !pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 66kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 45.5MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=4cb1ed4d5de7d731528bf244d817e4b9dd399567b667d0ffe949093308211f45 Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 from pyspark.context import SparkContext sc = SparkContext . getOrCreate () fruits = sc.textFile('/content/drive/MyDrive/courses/HKUST/MSBD5003/data/fruits.txt', 4) Why is Map/Reduce bad? \u00b6 Programming model too restricted Iterative jobs involve a lot of disk I/O Why is spark \u00b6 Fast and Expressive Cluster Computing Engine Compatible with Apache Hadoop Efficient General execution graphs In-memory storage Usable - Rich APIs in Java, scala, python - Interactive shell Hadoop Spark Storage Disk only In-memory or on Disk Operations Map/Reduce Many transformation and actions, including Map and Reduce Execution model Batch Batch, interactive, Streaming Languages Java Scala, Java, R, and Python RDD Resilient Distributed Datasets \u00b6 A real or virtual file consisting of records Partitioned into partitions Created through deterministic transformations on: Data in persistent storage Other RDDs RDDs do not need to be materialized Users can control two other aspects: - Persistence - Partitioning (e.g. key of the record) Programmer specifies number of partitions for an RDD (Default value used if unspecified) more partitions: more parallelism but also more overhead Lineage Graph \u00b6 Fault Tolerance Mechanism RDD has enough information about how it was derived from to compute its partitions from data in stable storage. If a partition of errors is lost, Spark rebuilds it by applying a filter on only the corresponding partition of lines. Partitions can be recomputed in parallel on different nodes, without having to roll back the whole program. Spark recomputes transformations \u00b6 lines = sc . textFile ( \"...\" , 4 ) comments = lines . filter ( isComment ) print ( lines . count (), comments . count ()) lines = sc . textFile ( \"...\" , 4 ) lines . cache () comments = lines . filter ( isComment ) print ( lines . count (), comments . count ()) Execution is pipelined and parallel. No need to store intermediate results. Lazy execution allows optimization. RDD has enough information about how it was rderived from to compute its partitions from data in stable storage. Example: If a partition of errors is lost, Spark rebuilds it by applying a filter on only the corresponding partition of lines. Partitions can be recomputed in parallel on different nodes, without having to roll back the whole program. RDD Persistence \u00b6 Make an RDD persist using persist() or cache() Different storage levels, default is MEMORY_ONLY Allows faster reuse and fault recovery Spark also automatically persists some intermediate data in shuffle operations Spark automatically drops out old data partitions using LRU policy. You can also unpersist() an RDD manually. RDD Basics \u00b6 lines = sc . textFile ( \"README.md\" ) lineLengths = lines . map ( lambda s : len ( s )) totalLength = lineLengths . reduce ( lambda a , b : a + b ) The first line defines a base RDD from an external file. This dataset is not loaded in memory or otherwise acted on: lines is merely a pointer to the file. The second line defines lineLengths as the result of a map transformation. Again, lineLengths is not immediately computed, due to laziness. Finally, we run reduce, which is an action. At this point Spark breaks the computation into tasks to run on separate machines, and each machine runs both its part of the map and a local reduction, returning only its answer to the driver program. Where code runs \u00b6 Most Python code runs in driver, except for code passed to transformations. Transformations run at executors, actions run at executors and driver. Closure \u00b6 A task\u2019s closure is those variables and methods which must be visible for the executor to perform its computations on the RDD. Functions that run on RDDs at executors Any global variables used by those executors The variables within the closure sent to each executor are copies. This closure is serialized and sent to each executor from the driver when an action is invoked. counter = 0 rdd = sc . parallelize ( range ( 10 )) # Wrong : Don ' t do this !! def increment_counter ( x ): global counter counter += x print ( rdd . collect ()) rdd . foreach ( increment_counter ) print ( f \"Counter: {counter}\" ) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] Counter: 0 closure and persistence # RDD variables are references A = sc . parallelize ( range ( 10 )) B = A . map ( lambda x : x * 2 ) A = B . map ( lambda x : x + 1 ) A . take ( 10 ) [1, 3, 5, 7, 9, 11, 13, 15, 17, 19] A = sc . parallelize ( range ( 10 )) x = 5 B = A . filter ( lambda z : z < x ) # B . cache () print ( B . count ()) x = 3 print ( B . count ()) 5 3 A = sc . parallelize ( range ( 10 )) x = 5 B = A . filter ( lambda z : z < x ) # B . cache () B . unpersist () # print ( B . take ( 10 )) print ( B . collect ()) x = 3 # print ( B . take ( 10 )) print ( B . collect ()) # collect () doesn ' t always re - collect data - bad design ! # Always use take () instead of collect () [0, 1, 2, 3, 4] [0, 1, 2, 3, 4] Accumulators \u00b6 Accumulators are variables that are only \u201cadded\u201d to through an associative and commutative operation. Created from an initial value v by calling SparkContext.accumulator(v). Tasks running on a cluster can then add to it using the add method or the += operator Only the driver program can read the accumulator\u2019s value, using its value method. Note: Accumulators do not change the lazy evaluation model of Spark. If they are being updated within an operation on an RDD, their value is only updated once that RDD is computed as part of an action. Consequently, accumulator updates are not guaranteed to be executed when made within a lazy transformation like map(). Update in transformations may be applied more than once if tasks or job stages are re-executed. See example. Suggestion: Avoid using accumulators whenever possible. Use reduce() instead. rdd = sc . parallelize ( range ( 10 )) accum = sc . accumulator ( 0 ) def g ( x ): global accum accum += x a = rdd . foreach ( g ) print ( f \"Accumulator value: {accum.value}\" ) Accumulator value: 45 rdd = sc . parallelize ( range ( 10 )) accum = sc . accumulator ( 0 ) def g ( x ): global accum accum += x return x * x a = rdd . map ( g ) print ( f \"Accumulator value: {accum.value}\" ) a . cache () tmp = a . count () print ( f \"Accumulator value: {accum.value}\" ) print ( f \"Reduce: {rdd.reduce(lambda x, y: x+y)}\" ) # If we uncomment cache , then the value will become to 90 since # accumulator will be executed twice tmp = a . count () print ( f \"Accumulator value: {accum.value}\" ) print ( f \"Reduce: {rdd.reduce(lambda x, y: x+y)}\" ) Accumulator value: 0 Accumulator value: 45 Reduce: 45 Accumulator value: 45 Reduce: 45 Broadcast variables \u00b6 Broadcast variables allow the programmer to keep a read-only variable cached on each machine (not each task) More efficient than sending closures to tasks # Using join products = sc . parallelize ([( 1 , \"Apple\" ), ( 2 , \"Orange\" ), ( 3 , \"TV\" ), ( 5 , \"Computer\" )]) trans = sc . parallelize ([( 1 , ( 134 , \"OK\" )), ( 3 , ( 34 , \"OK\" )), ( 5 , ( 162 , \"Error\" )), ( 1 , ( 135 , \"OK\" )), ( 2 , ( 53 , \"OK\" )), ( 1 , ( 45 , \"OK\" ))]) print ( trans . join ( products ). take ( 20 )) [(1, ((134, 'OK'), 'Apple')), (1, ((135, 'OK'), 'Apple')), (1, ((45, 'OK'), 'Apple')), (5, ((162, 'Error'), 'Computer')), (2, ((53, 'OK'), 'Orange')), (3, ((34, 'OK'), 'TV'))] # Using broadcast products = { 1 : \"Apple\" , 2 : \"Orange\" , 3 : \"TV\" , 5 : \"Computer\" } trans = sc . parallelize ([( 1 , ( 134 , \"OK\" )), ( 3 , ( 34 , \"OK\" )), ( 5 , ( 162 , \"Error\" )), ( 1 , ( 135 , \"OK\" )), ( 2 , ( 53 , \"OK\" )), ( 1 , ( 45 , \"OK\" ))]) broadcasted_products = sc . broadcast ( products ) results = trans . map ( lambda x : ( x [ 0 ], broadcasted_products . value [ x [ 0 ]], x [ 1 ])) # results = trans . map ( lambda x : ( x [ 0 ], products [ x [ 0 ]], x [ 1 ])) print ( results . take ( 20 )) [(1, 'Apple', (134, 'OK')), (3, 'TV', (34, 'OK')), (5, 'Computer', (162, 'Error')), (1, 'Apple', (135, 'OK')), (2, 'Orange', (53, 'OK')), (1, 'Apple', (45, 'OK'))] RDD Operations \u00b6 Map \u00b6 fruitsReversed = fruits . map ( lambda fruit : fruit [ ::- 1 ]) fruitsReversed . collect () ['elppa', 'ananab', 'nolem yranac', 'parg', 'nomel', 'egnaro', 'elppaenip', 'yrrebwarts'] Filter \u00b6 shortFruits = fruits.filter(lambda fruit: len(fruit) <= 5) shortFruits.collect() ['apple', 'grap', 'lemon'] FlatMap \u00b6 characters = fruits.flatMap(lambda fruit: list(fruit)) print(characters.collect()) ['a', 'p', 'p', 'l', 'e', 'b', 'a', 'n', 'a', 'n', 'a', 'c', 'a', 'n', 'a', 'r', 'y', ' ', 'm', 'e', 'l', 'o', 'n', 'g', 'r', 'a', 'p', 'l', 'e', 'm', 'o', 'n', 'o', 'r', 'a', 'n', 'g', 'e', 'p', 'i', 'n', 'e', 'a', 'p', 'p', 'l', 'e', 's', 't', 'r', 'a', 'w', 'b', 'e', 'r', 'r', 'y'] Union \u00b6 new_fruits = fruits.union(fruits) new_fruits.collect() ['apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry', 'apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry'] Intersection \u00b6 new_fruits = fruits.intersection(fruits) new_fruits.collect() ['orange', 'pineapple', 'canary melon', 'lemon', 'banana', 'apple', 'grap', 'strawberry'] RDD Actions \u00b6 collect \u00b6 take \u00b6 first3Fruits = fruits.take(3) print(first3Fruits) ['apple', 'banana', 'canary melon'] count \u00b6 fruits.count() 8 reduce \u00b6 fruits.map(lambda fruit: set(fruit)).reduce(lambda x, y: x.union(y)) {' ', 'a', 'b', 'c', 'e', 'g', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'w', 'y'} PageRank \u00b6 import re from operator import add def computeContribs ( urls , rank ): # Calculates URL contributions to the rank of other URLs. num_urls = len ( urls ) for url in urls : yield ( url , rank / num_urls ) def parseNeighbors ( urls ): # Parses a urls pair string into urls pair.\"\"\" parts = urls . split ( ' ' ) return parts [ 0 ], parts [ 1 ] # Loads in input file. It should be in format of: # URL neighbor URL # URL neighbor URL # URL neighbor URL # ... # The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/* lines = sc . textFile ( \"/content/drive/MyDrive/courses/HKUST/MSBD5003/homeworks/hw2/pagerank_data.txt\" , 2 ) # lines = sc.textFile(\"../data/dblp.in\", 5) numOfIterations = 10 # Loads all URLs from input file and initialize their neighbors. links = lines . map ( lambda urls : parseNeighbors ( urls )) \\ . groupByKey () # Loads all URLs with other URL(s) link to from input file # and initialize ranks of them to one. ranks = links . mapValues ( lambda neighbors : 1.0 ) print ( 'ranks' , ranks . collect ()) print ( 'links' , links . collect ()) # Calculates and updates URL ranks continuously using PageRank algorithm. for iteration in range ( numOfIterations ): # Calculates URL contributions to the rank of other URLs. contribs = links . join ( ranks ) \\ . flatMap ( lambda url_urls_rank : computeContribs ( url_urls_rank [ 1 ][ 0 ], url_urls_rank [ 1 ][ 1 ])) # After the join, each element in the RDD is of the form # (url, (list of neighbor urls, rank)) # Re-calculates URL ranks based on neighbor contributions. # ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15) ranks = contribs . reduceByKey ( add ) . map ( lambda t : ( t [ 0 ], t [ 1 ] * 0.85 + 0.15 )) print ( ranks . top ( 5 , lambda x : x [ 1 ])) ranks [('1', 1.0), ('4', 1.0), ('2', 1.0), ('3', 1.0)] links [('1', <pyspark.resultiterable.ResultIterable object at 0x7f6ea9564710>), ('4', <pyspark.resultiterable.ResultIterable object at 0x7f6ea95647f0>), ('2', <pyspark.resultiterable.ResultIterable object at 0x7f6ea95647b8>), ('3', <pyspark.resultiterable.ResultIterable object at 0x7f6ea9564828>)] [('1', 1.2981882732854677), ('4', 0.9999999999999998), ('3', 0.9999999999999998), ('2', 0.7018117267145316)] K-means clustering \u00b6 import numpy as np def parseVector ( line ): return np . array ([ float ( x ) for x in line . split ()]) def closestPoint ( p , centers ): bestIndex = 0 closest = float ( \"+inf\" ) for i in range ( len ( centers )): tempDist = np . sum (( p - centers [ i ]) ** 2 ) if tempDist < closest : closest = tempDist bestIndex = i return bestIndex # The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_data.txt lines = sc . textFile ( '/content/drive/MyDrive/courses/HKUST/MSBD5003/data/kmeans_data.txt' , 5 ) # The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_bigdata.txt # lines = sc.textFile('../data/kmeans_bigdata.txt', 5) # lines is an RDD of strings K = 3 convergeDist = 0.01 # terminate algorithm when the total distance from old center to new centers is less than this value data = lines . map ( parseVector ) . cache () # data is an RDD of arrays kCenters = data . takeSample ( False , K , 1 ) # intial centers as a list of arrays tempDist = 1.0 # total distance from old centers to new centers while tempDist > convergeDist : closest = data . map ( lambda p : ( closestPoint ( p , kCenters ), ( p , 1 ))) # for each point in data, find its closest center # closest is an RDD of tuples (index of closest center, (point, 1)) pointStats = closest . reduceByKey ( lambda p1 , p2 : ( p1 [ 0 ] + p2 [ 0 ], p1 [ 1 ] + p2 [ 1 ])) # pointStats is an RDD of tuples (index of center, # (array of sums of coordinates, total number of points assigned)) newCenters = pointStats . map ( lambda st : ( st [ 0 ], st [ 1 ][ 0 ] / st [ 1 ][ 1 ])) . collect () # compute the new centers tempDist = sum ( np . sum (( kCenters [ i ] - p ) ** 2 ) for ( i , p ) in newCenters ) # compute the total disctance from old centers to new centers for ( i , p ) in newCenters : kCenters [ i ] = p print ( \"Final centers: \" , kCenters ) Final centers: [array([0.1 , 0.33333333, 0.23333333]), array([9.05, 3.05, 4.65]), array([9.2, 2.2, 9.2])] Linear-time selection \u00b6 Problem: \u00b6 \u2014 Input: an array A of n numbers (unordered), and k \u2014 Output: the k-th smallest number (counting from 0) Algorithm \u00b6 \\(x=A[0]\\) partition A into \\(A[0..mid-1] < A[mid] = x < A[mid+1..n-1]\\) if \\(mid =k\\) then return \\(x\\) if \\(k<mid\\) then \\(A= A[O..mid-1]\\) if k > mid then \\(A = A[mid+1,n-1], k= k\u2014 mid-1\\) gotostep 1 # Linear - time selection data = [ 34 , 67 , 21 , 56 , 47 , 89 , 12 , 44 , 74 , 43 , 26 ] A = sc . parallelize ( data , 2 ) k = 4 while True : x = A . first () A1 = A . filter ( lambda z : z < x ) A2 = A . filter ( lambda z : z > x ) A1 . cache () A2 . cache () mid = A1 . count () if mid == k : print ( x ) break if k < mid : A = A1 else : A = A2 k = k - mid - 1 43 Key-value Pairs \u00b6 While most Spark operations work on RDDs containing any type of objects, a few special operations are only available on RDDs of key-value pairs. In Python, these operations work on RDDs containing built-in Python tuples such as (1, 2). Simply create such tuples and then call your desired operation. For example, the following code uses the reduceByKey operation on key-value pairs to count how many times each line of text occurs in a file: lines = sc . textFile ( \"README.md\" ) pairs = lines . map ( lambda s : ( s , 1 )) counts = pairs . reduceByKey ( lambda a , b : a + b ) We could also use counts.sortByKey() , for example, to sort the pairs alphabetically, and finally counts.collect() to bring them back to the driver program as a list of objects. PMI \u00b6 PMI (pointwise mutual information) is a measure of association used in information theory and statistics. Given a list of pairs (x, y) \\[pmi(x, y) = log\\frac{p(x,y)}{p(x)p(y}\\] where - \\(p(x)\\) : probability of x - \\(p(y)\\) : probability of y - \\(p(x,y)\\) : joint probability Example: p(x=0) = 0.8, p(x=1)=0.2, p(y=0)=0.25, p(y=1)=0.75 pmi(x=0;y=0) = \u22121 pmi(x=0;y=1) = 0.222392 pmi(x=1;y=0) = 1.584963 pmi(x=1;y=1) = -1.584963 lines = sc.textFile('/content/drive/MyDrive/courses/HKUST/MSBD5003/data/adj_noun_pairs.txt', 4) # Converting lines into word pairs. # Data is dirty: some lines have more than 2 words, so filter them out. pairs = lines.map(lambda l: tuple(l.split())).filter(lambda p: len(p)==2) pairs.cache() pairs.take(5) [('early', 'radical'), ('french', 'revolution'), ('pejorative', 'way'), ('violent', 'means'), ('positive', 'label')] N = pairs.count() N 3162674 # Compute the frequency of each pair. # Ignore pairs that not frequent enough pair_freqs = pairs.map(lambda p: (p,1)).reduceByKey(lambda f1, f2: f1 + f2) \\ .filter(lambda pf: pf[1] >= 100) pair_freqs.take(5) [(('human', 'society'), 154), (('16th', 'century'), 950), (('first', 'man'), 166), (('civil', 'war'), 2236), (('social', 'class'), 155)] # Computing the frequencies of the adjectives and the nouns a_freqs = pairs.map(lambda p: (p[0],1)).reduceByKey(lambda x,y: x+y) n_freqs = pairs.map(lambda p: (p[1],1)).reduceByKey(lambda x,y: x+y) # Broadcasting the adjective and noun frequencies. #a_dict = a_freqs.collectAsMap() #a_dict = sc.parallelize(a_dict).map(lambda x: x) n_dict = sc.broadcast(n_freqs.collectAsMap()) a_dict = sc.broadcast(a_freqs.collectAsMap()) a_dict.value['violent'] 1191 from math import * # Computing the PMI for a pair. def pmi_score ( pair_freq ): w1 , w2 = pair_freq [ 0 ] f = pair_freq [ 1 ] pmi = log ( float ( f ) * N / ( a_dict . value [ w1 ] * n_dict . value [ w2 ]), 2 ) return pmi , ( w1 , w2 ) # Computing the PMI for all pairs. scored_pairs = pair_freqs.map(pmi_score) # Printing the most strongly associated pairs. scored_pairs.top(10) [(14.41018838546462, ('magna', 'carta')), (13.071365888694997, ('polish-lithuanian', 'Commonwealth')), (12.990597616733414, ('nitrous', 'oxide')), (12.64972604311254, ('latter-day', 'Saints')), (12.50658937509916, ('stainless', 'steel')), (12.482331020687814, ('pave', 'runway')), (12.19140721768055, ('corporal', 'punishment')), (12.183248694293388, ('capital', 'punishment')), (12.147015483562537, ('rush', 'yard')), (12.109945794428935, ('globular', 'cluster'))]","title":"Spark basics and rdd"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#spark-basics-and-rdd","text":"!pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 66kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 45.5MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=4cb1ed4d5de7d731528bf244d817e4b9dd399567b667d0ffe949093308211f45 Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 from pyspark.context import SparkContext sc = SparkContext . getOrCreate () fruits = sc.textFile('/content/drive/MyDrive/courses/HKUST/MSBD5003/data/fruits.txt', 4)","title":"Spark basics and RDD"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#why-is-mapreduce-bad","text":"Programming model too restricted Iterative jobs involve a lot of disk I/O","title":"Why is Map/Reduce bad?"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#why-is-spark","text":"Fast and Expressive Cluster Computing Engine Compatible with Apache Hadoop Efficient General execution graphs In-memory storage Usable - Rich APIs in Java, scala, python - Interactive shell Hadoop Spark Storage Disk only In-memory or on Disk Operations Map/Reduce Many transformation and actions, including Map and Reduce Execution model Batch Batch, interactive, Streaming Languages Java Scala, Java, R, and Python","title":"Why is spark"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#rdd-resilient-distributed-datasets","text":"A real or virtual file consisting of records Partitioned into partitions Created through deterministic transformations on: Data in persistent storage Other RDDs RDDs do not need to be materialized Users can control two other aspects: - Persistence - Partitioning (e.g. key of the record) Programmer specifies number of partitions for an RDD (Default value used if unspecified) more partitions: more parallelism but also more overhead","title":"RDD Resilient Distributed Datasets"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#lineage-graph","text":"Fault Tolerance Mechanism RDD has enough information about how it was derived from to compute its partitions from data in stable storage. If a partition of errors is lost, Spark rebuilds it by applying a filter on only the corresponding partition of lines. Partitions can be recomputed in parallel on different nodes, without having to roll back the whole program.","title":"Lineage Graph"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#spark-recomputes-transformations","text":"lines = sc . textFile ( \"...\" , 4 ) comments = lines . filter ( isComment ) print ( lines . count (), comments . count ()) lines = sc . textFile ( \"...\" , 4 ) lines . cache () comments = lines . filter ( isComment ) print ( lines . count (), comments . count ()) Execution is pipelined and parallel. No need to store intermediate results. Lazy execution allows optimization. RDD has enough information about how it was rderived from to compute its partitions from data in stable storage. Example: If a partition of errors is lost, Spark rebuilds it by applying a filter on only the corresponding partition of lines. Partitions can be recomputed in parallel on different nodes, without having to roll back the whole program.","title":"Spark recomputes transformations"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#rdd-persistence","text":"Make an RDD persist using persist() or cache() Different storage levels, default is MEMORY_ONLY Allows faster reuse and fault recovery Spark also automatically persists some intermediate data in shuffle operations Spark automatically drops out old data partitions using LRU policy. You can also unpersist() an RDD manually.","title":"RDD Persistence"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#rdd-basics","text":"lines = sc . textFile ( \"README.md\" ) lineLengths = lines . map ( lambda s : len ( s )) totalLength = lineLengths . reduce ( lambda a , b : a + b ) The first line defines a base RDD from an external file. This dataset is not loaded in memory or otherwise acted on: lines is merely a pointer to the file. The second line defines lineLengths as the result of a map transformation. Again, lineLengths is not immediately computed, due to laziness. Finally, we run reduce, which is an action. At this point Spark breaks the computation into tasks to run on separate machines, and each machine runs both its part of the map and a local reduction, returning only its answer to the driver program.","title":"RDD Basics"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#where-code-runs","text":"Most Python code runs in driver, except for code passed to transformations. Transformations run at executors, actions run at executors and driver.","title":"Where code runs"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#closure","text":"A task\u2019s closure is those variables and methods which must be visible for the executor to perform its computations on the RDD. Functions that run on RDDs at executors Any global variables used by those executors The variables within the closure sent to each executor are copies. This closure is serialized and sent to each executor from the driver when an action is invoked. counter = 0 rdd = sc . parallelize ( range ( 10 )) # Wrong : Don ' t do this !! def increment_counter ( x ): global counter counter += x print ( rdd . collect ()) rdd . foreach ( increment_counter ) print ( f \"Counter: {counter}\" ) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] Counter: 0 closure and persistence # RDD variables are references A = sc . parallelize ( range ( 10 )) B = A . map ( lambda x : x * 2 ) A = B . map ( lambda x : x + 1 ) A . take ( 10 ) [1, 3, 5, 7, 9, 11, 13, 15, 17, 19] A = sc . parallelize ( range ( 10 )) x = 5 B = A . filter ( lambda z : z < x ) # B . cache () print ( B . count ()) x = 3 print ( B . count ()) 5 3 A = sc . parallelize ( range ( 10 )) x = 5 B = A . filter ( lambda z : z < x ) # B . cache () B . unpersist () # print ( B . take ( 10 )) print ( B . collect ()) x = 3 # print ( B . take ( 10 )) print ( B . collect ()) # collect () doesn ' t always re - collect data - bad design ! # Always use take () instead of collect () [0, 1, 2, 3, 4] [0, 1, 2, 3, 4]","title":"Closure"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#accumulators","text":"Accumulators are variables that are only \u201cadded\u201d to through an associative and commutative operation. Created from an initial value v by calling SparkContext.accumulator(v). Tasks running on a cluster can then add to it using the add method or the += operator Only the driver program can read the accumulator\u2019s value, using its value method. Note: Accumulators do not change the lazy evaluation model of Spark. If they are being updated within an operation on an RDD, their value is only updated once that RDD is computed as part of an action. Consequently, accumulator updates are not guaranteed to be executed when made within a lazy transformation like map(). Update in transformations may be applied more than once if tasks or job stages are re-executed. See example. Suggestion: Avoid using accumulators whenever possible. Use reduce() instead. rdd = sc . parallelize ( range ( 10 )) accum = sc . accumulator ( 0 ) def g ( x ): global accum accum += x a = rdd . foreach ( g ) print ( f \"Accumulator value: {accum.value}\" ) Accumulator value: 45 rdd = sc . parallelize ( range ( 10 )) accum = sc . accumulator ( 0 ) def g ( x ): global accum accum += x return x * x a = rdd . map ( g ) print ( f \"Accumulator value: {accum.value}\" ) a . cache () tmp = a . count () print ( f \"Accumulator value: {accum.value}\" ) print ( f \"Reduce: {rdd.reduce(lambda x, y: x+y)}\" ) # If we uncomment cache , then the value will become to 90 since # accumulator will be executed twice tmp = a . count () print ( f \"Accumulator value: {accum.value}\" ) print ( f \"Reduce: {rdd.reduce(lambda x, y: x+y)}\" ) Accumulator value: 0 Accumulator value: 45 Reduce: 45 Accumulator value: 45 Reduce: 45","title":"Accumulators"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#broadcast-variables","text":"Broadcast variables allow the programmer to keep a read-only variable cached on each machine (not each task) More efficient than sending closures to tasks # Using join products = sc . parallelize ([( 1 , \"Apple\" ), ( 2 , \"Orange\" ), ( 3 , \"TV\" ), ( 5 , \"Computer\" )]) trans = sc . parallelize ([( 1 , ( 134 , \"OK\" )), ( 3 , ( 34 , \"OK\" )), ( 5 , ( 162 , \"Error\" )), ( 1 , ( 135 , \"OK\" )), ( 2 , ( 53 , \"OK\" )), ( 1 , ( 45 , \"OK\" ))]) print ( trans . join ( products ). take ( 20 )) [(1, ((134, 'OK'), 'Apple')), (1, ((135, 'OK'), 'Apple')), (1, ((45, 'OK'), 'Apple')), (5, ((162, 'Error'), 'Computer')), (2, ((53, 'OK'), 'Orange')), (3, ((34, 'OK'), 'TV'))] # Using broadcast products = { 1 : \"Apple\" , 2 : \"Orange\" , 3 : \"TV\" , 5 : \"Computer\" } trans = sc . parallelize ([( 1 , ( 134 , \"OK\" )), ( 3 , ( 34 , \"OK\" )), ( 5 , ( 162 , \"Error\" )), ( 1 , ( 135 , \"OK\" )), ( 2 , ( 53 , \"OK\" )), ( 1 , ( 45 , \"OK\" ))]) broadcasted_products = sc . broadcast ( products ) results = trans . map ( lambda x : ( x [ 0 ], broadcasted_products . value [ x [ 0 ]], x [ 1 ])) # results = trans . map ( lambda x : ( x [ 0 ], products [ x [ 0 ]], x [ 1 ])) print ( results . take ( 20 )) [(1, 'Apple', (134, 'OK')), (3, 'TV', (34, 'OK')), (5, 'Computer', (162, 'Error')), (1, 'Apple', (135, 'OK')), (2, 'Orange', (53, 'OK')), (1, 'Apple', (45, 'OK'))]","title":"Broadcast variables"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#rdd-operations","text":"","title":"RDD Operations"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#map","text":"fruitsReversed = fruits . map ( lambda fruit : fruit [ ::- 1 ]) fruitsReversed . collect () ['elppa', 'ananab', 'nolem yranac', 'parg', 'nomel', 'egnaro', 'elppaenip', 'yrrebwarts']","title":"Map"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#filter","text":"shortFruits = fruits.filter(lambda fruit: len(fruit) <= 5) shortFruits.collect() ['apple', 'grap', 'lemon']","title":"Filter"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#flatmap","text":"characters = fruits.flatMap(lambda fruit: list(fruit)) print(characters.collect()) ['a', 'p', 'p', 'l', 'e', 'b', 'a', 'n', 'a', 'n', 'a', 'c', 'a', 'n', 'a', 'r', 'y', ' ', 'm', 'e', 'l', 'o', 'n', 'g', 'r', 'a', 'p', 'l', 'e', 'm', 'o', 'n', 'o', 'r', 'a', 'n', 'g', 'e', 'p', 'i', 'n', 'e', 'a', 'p', 'p', 'l', 'e', 's', 't', 'r', 'a', 'w', 'b', 'e', 'r', 'r', 'y']","title":"FlatMap"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#union","text":"new_fruits = fruits.union(fruits) new_fruits.collect() ['apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry', 'apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry']","title":"Union"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#intersection","text":"new_fruits = fruits.intersection(fruits) new_fruits.collect() ['orange', 'pineapple', 'canary melon', 'lemon', 'banana', 'apple', 'grap', 'strawberry']","title":"Intersection"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#rdd-actions","text":"","title":"RDD Actions"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#collect","text":"","title":"collect"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#take","text":"first3Fruits = fruits.take(3) print(first3Fruits) ['apple', 'banana', 'canary melon']","title":"take"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#count","text":"fruits.count() 8","title":"count"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#reduce","text":"fruits.map(lambda fruit: set(fruit)).reduce(lambda x, y: x.union(y)) {' ', 'a', 'b', 'c', 'e', 'g', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'w', 'y'}","title":"reduce"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#pagerank","text":"import re from operator import add def computeContribs ( urls , rank ): # Calculates URL contributions to the rank of other URLs. num_urls = len ( urls ) for url in urls : yield ( url , rank / num_urls ) def parseNeighbors ( urls ): # Parses a urls pair string into urls pair.\"\"\" parts = urls . split ( ' ' ) return parts [ 0 ], parts [ 1 ] # Loads in input file. It should be in format of: # URL neighbor URL # URL neighbor URL # URL neighbor URL # ... # The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/* lines = sc . textFile ( \"/content/drive/MyDrive/courses/HKUST/MSBD5003/homeworks/hw2/pagerank_data.txt\" , 2 ) # lines = sc.textFile(\"../data/dblp.in\", 5) numOfIterations = 10 # Loads all URLs from input file and initialize their neighbors. links = lines . map ( lambda urls : parseNeighbors ( urls )) \\ . groupByKey () # Loads all URLs with other URL(s) link to from input file # and initialize ranks of them to one. ranks = links . mapValues ( lambda neighbors : 1.0 ) print ( 'ranks' , ranks . collect ()) print ( 'links' , links . collect ()) # Calculates and updates URL ranks continuously using PageRank algorithm. for iteration in range ( numOfIterations ): # Calculates URL contributions to the rank of other URLs. contribs = links . join ( ranks ) \\ . flatMap ( lambda url_urls_rank : computeContribs ( url_urls_rank [ 1 ][ 0 ], url_urls_rank [ 1 ][ 1 ])) # After the join, each element in the RDD is of the form # (url, (list of neighbor urls, rank)) # Re-calculates URL ranks based on neighbor contributions. # ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15) ranks = contribs . reduceByKey ( add ) . map ( lambda t : ( t [ 0 ], t [ 1 ] * 0.85 + 0.15 )) print ( ranks . top ( 5 , lambda x : x [ 1 ])) ranks [('1', 1.0), ('4', 1.0), ('2', 1.0), ('3', 1.0)] links [('1', <pyspark.resultiterable.ResultIterable object at 0x7f6ea9564710>), ('4', <pyspark.resultiterable.ResultIterable object at 0x7f6ea95647f0>), ('2', <pyspark.resultiterable.ResultIterable object at 0x7f6ea95647b8>), ('3', <pyspark.resultiterable.ResultIterable object at 0x7f6ea9564828>)] [('1', 1.2981882732854677), ('4', 0.9999999999999998), ('3', 0.9999999999999998), ('2', 0.7018117267145316)]","title":"PageRank"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#k-means-clustering","text":"import numpy as np def parseVector ( line ): return np . array ([ float ( x ) for x in line . split ()]) def closestPoint ( p , centers ): bestIndex = 0 closest = float ( \"+inf\" ) for i in range ( len ( centers )): tempDist = np . sum (( p - centers [ i ]) ** 2 ) if tempDist < closest : closest = tempDist bestIndex = i return bestIndex # The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_data.txt lines = sc . textFile ( '/content/drive/MyDrive/courses/HKUST/MSBD5003/data/kmeans_data.txt' , 5 ) # The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_bigdata.txt # lines = sc.textFile('../data/kmeans_bigdata.txt', 5) # lines is an RDD of strings K = 3 convergeDist = 0.01 # terminate algorithm when the total distance from old center to new centers is less than this value data = lines . map ( parseVector ) . cache () # data is an RDD of arrays kCenters = data . takeSample ( False , K , 1 ) # intial centers as a list of arrays tempDist = 1.0 # total distance from old centers to new centers while tempDist > convergeDist : closest = data . map ( lambda p : ( closestPoint ( p , kCenters ), ( p , 1 ))) # for each point in data, find its closest center # closest is an RDD of tuples (index of closest center, (point, 1)) pointStats = closest . reduceByKey ( lambda p1 , p2 : ( p1 [ 0 ] + p2 [ 0 ], p1 [ 1 ] + p2 [ 1 ])) # pointStats is an RDD of tuples (index of center, # (array of sums of coordinates, total number of points assigned)) newCenters = pointStats . map ( lambda st : ( st [ 0 ], st [ 1 ][ 0 ] / st [ 1 ][ 1 ])) . collect () # compute the new centers tempDist = sum ( np . sum (( kCenters [ i ] - p ) ** 2 ) for ( i , p ) in newCenters ) # compute the total disctance from old centers to new centers for ( i , p ) in newCenters : kCenters [ i ] = p print ( \"Final centers: \" , kCenters ) Final centers: [array([0.1 , 0.33333333, 0.23333333]), array([9.05, 3.05, 4.65]), array([9.2, 2.2, 9.2])]","title":"K-means clustering"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#linear-time-selection","text":"","title":"Linear-time selection"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#problem","text":"\u2014 Input: an array A of n numbers (unordered), and k \u2014 Output: the k-th smallest number (counting from 0)","title":"Problem:"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#algorithm","text":"\\(x=A[0]\\) partition A into \\(A[0..mid-1] < A[mid] = x < A[mid+1..n-1]\\) if \\(mid =k\\) then return \\(x\\) if \\(k<mid\\) then \\(A= A[O..mid-1]\\) if k > mid then \\(A = A[mid+1,n-1], k= k\u2014 mid-1\\) gotostep 1 # Linear - time selection data = [ 34 , 67 , 21 , 56 , 47 , 89 , 12 , 44 , 74 , 43 , 26 ] A = sc . parallelize ( data , 2 ) k = 4 while True : x = A . first () A1 = A . filter ( lambda z : z < x ) A2 = A . filter ( lambda z : z > x ) A1 . cache () A2 . cache () mid = A1 . count () if mid == k : print ( x ) break if k < mid : A = A1 else : A = A2 k = k - mid - 1 43","title":"Algorithm"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#key-value-pairs","text":"While most Spark operations work on RDDs containing any type of objects, a few special operations are only available on RDDs of key-value pairs. In Python, these operations work on RDDs containing built-in Python tuples such as (1, 2). Simply create such tuples and then call your desired operation. For example, the following code uses the reduceByKey operation on key-value pairs to count how many times each line of text occurs in a file: lines = sc . textFile ( \"README.md\" ) pairs = lines . map ( lambda s : ( s , 1 )) counts = pairs . reduceByKey ( lambda a , b : a + b ) We could also use counts.sortByKey() , for example, to sort the pairs alphabetically, and finally counts.collect() to bring them back to the driver program as a list of objects.","title":"Key-value Pairs"},{"location":"MSBD5003/Spark%20basics%20and%20RDD/#pmi","text":"PMI (pointwise mutual information) is a measure of association used in information theory and statistics. Given a list of pairs (x, y) \\[pmi(x, y) = log\\frac{p(x,y)}{p(x)p(y}\\] where - \\(p(x)\\) : probability of x - \\(p(y)\\) : probability of y - \\(p(x,y)\\) : joint probability Example: p(x=0) = 0.8, p(x=1)=0.2, p(y=0)=0.25, p(y=1)=0.75 pmi(x=0;y=0) = \u22121 pmi(x=0;y=1) = 0.222392 pmi(x=1;y=0) = 1.584963 pmi(x=1;y=1) = -1.584963 lines = sc.textFile('/content/drive/MyDrive/courses/HKUST/MSBD5003/data/adj_noun_pairs.txt', 4) # Converting lines into word pairs. # Data is dirty: some lines have more than 2 words, so filter them out. pairs = lines.map(lambda l: tuple(l.split())).filter(lambda p: len(p)==2) pairs.cache() pairs.take(5) [('early', 'radical'), ('french', 'revolution'), ('pejorative', 'way'), ('violent', 'means'), ('positive', 'label')] N = pairs.count() N 3162674 # Compute the frequency of each pair. # Ignore pairs that not frequent enough pair_freqs = pairs.map(lambda p: (p,1)).reduceByKey(lambda f1, f2: f1 + f2) \\ .filter(lambda pf: pf[1] >= 100) pair_freqs.take(5) [(('human', 'society'), 154), (('16th', 'century'), 950), (('first', 'man'), 166), (('civil', 'war'), 2236), (('social', 'class'), 155)] # Computing the frequencies of the adjectives and the nouns a_freqs = pairs.map(lambda p: (p[0],1)).reduceByKey(lambda x,y: x+y) n_freqs = pairs.map(lambda p: (p[1],1)).reduceByKey(lambda x,y: x+y) # Broadcasting the adjective and noun frequencies. #a_dict = a_freqs.collectAsMap() #a_dict = sc.parallelize(a_dict).map(lambda x: x) n_dict = sc.broadcast(n_freqs.collectAsMap()) a_dict = sc.broadcast(a_freqs.collectAsMap()) a_dict.value['violent'] 1191 from math import * # Computing the PMI for a pair. def pmi_score ( pair_freq ): w1 , w2 = pair_freq [ 0 ] f = pair_freq [ 1 ] pmi = log ( float ( f ) * N / ( a_dict . value [ w1 ] * n_dict . value [ w2 ]), 2 ) return pmi , ( w1 , w2 ) # Computing the PMI for all pairs. scored_pairs = pair_freqs.map(pmi_score) # Printing the most strongly associated pairs. scored_pairs.top(10) [(14.41018838546462, ('magna', 'carta')), (13.071365888694997, ('polish-lithuanian', 'Commonwealth')), (12.990597616733414, ('nitrous', 'oxide')), (12.64972604311254, ('latter-day', 'Saints')), (12.50658937509916, ('stainless', 'steel')), (12.482331020687814, ('pave', 'runway')), (12.19140721768055, ('corporal', 'punishment')), (12.183248694293388, ('capital', 'punishment')), (12.147015483562537, ('rush', 'yard')), (12.109945794428935, ('globular', 'cluster'))]","title":"PMI"},{"location":"MSBD5003/homeworks/HW4/","text":"! pip install networkx Requirement already satisfied: networkx in /opt/conda/lib/python3.8/site-packages (2.5) Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.8/site-packages (from networkx) (4.4.2) Import dependencies \u00b6 import os os . environ [ 'PYSPARK_SUBMIT_ARGS' ] = '--packages graphframes:graphframes:0.8.1-spark3.0-s_2.12 pyspark-shell' from pyspark.context import SparkContext from pyspark.sql.session import SparkSession from pyspark.sql.types import Row from graphframes import * import networkx as nx import matplotlib.pyplot as plt sc = SparkContext . getOrCreate () spark = SparkSession ( sc ) def PlotGraph ( edge_list ): Gplot = nx . DiGraph () for row in edge_list . select ( 'src' , 'dst' ) . take ( 1000 ): Gplot . add_edge ( row [ 'src' ], row [ 'dst' ]) # plt.subplot(121) # plt.figure(figsize=(10, 10)) nx . draw ( Gplot , with_labels = True , font_weight = 'bold' ) v = spark . createDataFrame ([ ( \"a\" , \"Alice\" , 34 ), ( \"b\" , \"Bob\" , 36 ), ( \"c\" , \"Charlie\" , 37 ), ( \"d\" , \"David\" , 29 ), ( \"e\" , \"Esther\" , 32 ), ( \"f\" , \"Fanny\" , 38 ), ( \"g\" , \"Gabby\" , 60 ) ], [ \"id\" , \"name\" , \"age\" ]) # Edges DataFrame e = spark . createDataFrame ([ ( \"a\" , \"b\" , \"friend\" ), ( \"b\" , \"c\" , \"follow\" ), ( \"c\" , \"b\" , \"follow\" ), ( \"f\" , \"c\" , \"follow\" ), ( \"e\" , \"f\" , \"follow\" ), ( \"e\" , \"d\" , \"friend\" ), ( \"d\" , \"a\" , \"friend\" ), ( \"a\" , \"e\" , \"friend\" ), ( \"g\" , \"e\" , \"follow\" ) ], [ \"src\" , \"dst\" , \"relationship\" ]) # Create a GraphFrame g = GraphFrame ( v , e ) g . vertices . show () g . edges . show () +---+-------+---+ | id| name|age| +---+-------+---+ | a| Alice| 34| | b| Bob| 36| | c|Charlie| 37| | d| David| 29| | e| Esther| 32| | f| Fanny| 38| | g| Gabby| 60| +---+-------+---+ +---+---+------------+ |src|dst|relationship| +---+---+------------+ | a| b| friend| | b| c| follow| | c| b| follow| | f| c| follow| | e| f| follow| | e| d| friend| | d| a| friend| | a| e| friend| | g| e| follow| +---+---+------------+ PlotGraph ( g . edges ) Question 1 \u00b6 Write code to perform the following tasks using GraphFrames: Find Alice's two-hop neighbors' names, regardless of the edge type. friends = g . find ( \"(a)-[]->(b);(b)-[]->(c)\" ) . filter ( \"a.name='Alice'\" ) friends . select ( 'c' ) . collect () [Row(c=Row(id='f', name='Fanny', age=38)), Row(c=Row(id='d', name='David', age=29)), Row(c=Row(id='c', name='Charlie', age=37))] Question 2 \u00b6 Redo the previous question, but exclude Alice's two-hop neighbors who have an edge back to Alice. friends = g . find ( \"(a)-[]->(b);(b)-[]->(c);(c)-[]->(a)\" ) . filter ( \"a.name='Alice'\" ) friends . select ( 'c' ) . collect () [Row(c=Row(id='d', name='David', age=29))] Question 3 \u00b6 Find all people who follow Charlie. Hint: Use AND in SQL, or (..) & (..) in DataFrame boolean expressions. followers = g . find ( \"(a)-[e]->(b)\" ) . filter ( \"b.name='Charlie' AND e.relationship='follow' \" ) followers . select ( 'a' ) . show () +--------------+ | a| +--------------+ |[f, Fanny, 38]| | [b, Bob, 36]| +--------------+ Question 4 \u00b6 Find all people who are being followed by at least 2 people. g . edges . filter ( g . edges . relationship == 'follow' ) . groupby ( 'dst' ) . count () . filter ( \"count>=2\" ) . show () +---+-----+ |dst|count| +---+-----+ | c| 2| +---+-----+ Question 5 \u00b6 Create a queue of 10 RDDs using this data set and feed it into a Spark Streaming program. Your Spark Streaming algorithm should maintain a state that keeps track of the longest noun seen so far associated with each distinct adjective. After each RDD, print any 5 adjectives and their associated longest nouns, as well as the longest noun associated with the adjective 'good'. Note that not every line in the data set contains exactly two words, so make sure to clean the data as they are fed into the streaming program. The skeleton code is provided below: from pyspark.streaming import StreamingContext ssc = StreamingContext ( sc , 5 ) # Provide a checkpointing directory. Required for stateful transformations ssc . checkpoint ( \"checkpoint\" ) numPartitions = 8 rdd = sc . textFile ( '../data/adj_noun_pairs.txt' , numPartitions ) rddQueue = rdd . randomSplit ([ 1 ] * 10 , 123 ) lines = ssc . queueStream ( rddQueue ) # FILL IN YOUR CODE ssc . start () ssc . awaitTermination ( 50 ) ssc . stop ( False ) print ( \"Finished\" ) from pyspark.streaming import StreamingContext ssc = StreamingContext ( sc , 5 ) # Provide a checkpointing directory. Required for stateful transformations ssc . checkpoint ( \"checkpoint\" ) numPartitions = 8 rdd = sc . textFile ( 'adj_noun_pairs.txt' , numPartitions ) rddQueue = rdd . randomSplit ([ 1 ] * 10 , 123 ) lines = ssc . queueStream ( rddQueue ) ssc . start () ssc . awaitTermination ( 10 ) ssc . stop ( False ) print ( \"Finished\" ) --------------------------------------------------------------------------- IllegalArgumentException Traceback (most recent call last) <ipython-input-49-043ad3d9c502> in <module> 12 13 ---> 14 ssc.start() 15 ssc.awaitTermination(10) 16 ssc.stop(False) /usr/local/spark/python/pyspark/streaming/context.py in start(self) 177 Start the execution of the streams. 178 \"\"\" --> 179 self._jssc.start() 180 StreamingContext._activeContext = self 181 /usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args) 1302 1303 answer = self.gateway_client.send_command(command) -> 1304 return_value = get_return_value( 1305 answer, self.gateway_client, self.target_id, self.name) 1306 /usr/local/spark/python/pyspark/sql/utils.py in deco(*a, **kw) 132 # Hide where the exception came from that shows a non-Pythonic 133 # JVM exception message. --> 134 raise_from(converted) 135 else: 136 raise /usr/local/spark/python/pyspark/sql/utils.py in raise_from(e) IllegalArgumentException: requirement failed: No output operations registered, so nothing to execute","title":"Hw4"},{"location":"MSBD5003/homeworks/HW4/#import-dependencies","text":"import os os . environ [ 'PYSPARK_SUBMIT_ARGS' ] = '--packages graphframes:graphframes:0.8.1-spark3.0-s_2.12 pyspark-shell' from pyspark.context import SparkContext from pyspark.sql.session import SparkSession from pyspark.sql.types import Row from graphframes import * import networkx as nx import matplotlib.pyplot as plt sc = SparkContext . getOrCreate () spark = SparkSession ( sc ) def PlotGraph ( edge_list ): Gplot = nx . DiGraph () for row in edge_list . select ( 'src' , 'dst' ) . take ( 1000 ): Gplot . add_edge ( row [ 'src' ], row [ 'dst' ]) # plt.subplot(121) # plt.figure(figsize=(10, 10)) nx . draw ( Gplot , with_labels = True , font_weight = 'bold' ) v = spark . createDataFrame ([ ( \"a\" , \"Alice\" , 34 ), ( \"b\" , \"Bob\" , 36 ), ( \"c\" , \"Charlie\" , 37 ), ( \"d\" , \"David\" , 29 ), ( \"e\" , \"Esther\" , 32 ), ( \"f\" , \"Fanny\" , 38 ), ( \"g\" , \"Gabby\" , 60 ) ], [ \"id\" , \"name\" , \"age\" ]) # Edges DataFrame e = spark . createDataFrame ([ ( \"a\" , \"b\" , \"friend\" ), ( \"b\" , \"c\" , \"follow\" ), ( \"c\" , \"b\" , \"follow\" ), ( \"f\" , \"c\" , \"follow\" ), ( \"e\" , \"f\" , \"follow\" ), ( \"e\" , \"d\" , \"friend\" ), ( \"d\" , \"a\" , \"friend\" ), ( \"a\" , \"e\" , \"friend\" ), ( \"g\" , \"e\" , \"follow\" ) ], [ \"src\" , \"dst\" , \"relationship\" ]) # Create a GraphFrame g = GraphFrame ( v , e ) g . vertices . show () g . edges . show () +---+-------+---+ | id| name|age| +---+-------+---+ | a| Alice| 34| | b| Bob| 36| | c|Charlie| 37| | d| David| 29| | e| Esther| 32| | f| Fanny| 38| | g| Gabby| 60| +---+-------+---+ +---+---+------------+ |src|dst|relationship| +---+---+------------+ | a| b| friend| | b| c| follow| | c| b| follow| | f| c| follow| | e| f| follow| | e| d| friend| | d| a| friend| | a| e| friend| | g| e| follow| +---+---+------------+ PlotGraph ( g . edges )","title":"Import dependencies"},{"location":"MSBD5003/homeworks/HW4/#question-1","text":"Write code to perform the following tasks using GraphFrames: Find Alice's two-hop neighbors' names, regardless of the edge type. friends = g . find ( \"(a)-[]->(b);(b)-[]->(c)\" ) . filter ( \"a.name='Alice'\" ) friends . select ( 'c' ) . collect () [Row(c=Row(id='f', name='Fanny', age=38)), Row(c=Row(id='d', name='David', age=29)), Row(c=Row(id='c', name='Charlie', age=37))]","title":"Question 1"},{"location":"MSBD5003/homeworks/HW4/#question-2","text":"Redo the previous question, but exclude Alice's two-hop neighbors who have an edge back to Alice. friends = g . find ( \"(a)-[]->(b);(b)-[]->(c);(c)-[]->(a)\" ) . filter ( \"a.name='Alice'\" ) friends . select ( 'c' ) . collect () [Row(c=Row(id='d', name='David', age=29))]","title":"Question 2"},{"location":"MSBD5003/homeworks/HW4/#question-3","text":"Find all people who follow Charlie. Hint: Use AND in SQL, or (..) & (..) in DataFrame boolean expressions. followers = g . find ( \"(a)-[e]->(b)\" ) . filter ( \"b.name='Charlie' AND e.relationship='follow' \" ) followers . select ( 'a' ) . show () +--------------+ | a| +--------------+ |[f, Fanny, 38]| | [b, Bob, 36]| +--------------+","title":"Question 3"},{"location":"MSBD5003/homeworks/HW4/#question-4","text":"Find all people who are being followed by at least 2 people. g . edges . filter ( g . edges . relationship == 'follow' ) . groupby ( 'dst' ) . count () . filter ( \"count>=2\" ) . show () +---+-----+ |dst|count| +---+-----+ | c| 2| +---+-----+","title":"Question 4"},{"location":"MSBD5003/homeworks/HW4/#question-5","text":"Create a queue of 10 RDDs using this data set and feed it into a Spark Streaming program. Your Spark Streaming algorithm should maintain a state that keeps track of the longest noun seen so far associated with each distinct adjective. After each RDD, print any 5 adjectives and their associated longest nouns, as well as the longest noun associated with the adjective 'good'. Note that not every line in the data set contains exactly two words, so make sure to clean the data as they are fed into the streaming program. The skeleton code is provided below: from pyspark.streaming import StreamingContext ssc = StreamingContext ( sc , 5 ) # Provide a checkpointing directory. Required for stateful transformations ssc . checkpoint ( \"checkpoint\" ) numPartitions = 8 rdd = sc . textFile ( '../data/adj_noun_pairs.txt' , numPartitions ) rddQueue = rdd . randomSplit ([ 1 ] * 10 , 123 ) lines = ssc . queueStream ( rddQueue ) # FILL IN YOUR CODE ssc . start () ssc . awaitTermination ( 50 ) ssc . stop ( False ) print ( \"Finished\" ) from pyspark.streaming import StreamingContext ssc = StreamingContext ( sc , 5 ) # Provide a checkpointing directory. Required for stateful transformations ssc . checkpoint ( \"checkpoint\" ) numPartitions = 8 rdd = sc . textFile ( 'adj_noun_pairs.txt' , numPartitions ) rddQueue = rdd . randomSplit ([ 1 ] * 10 , 123 ) lines = ssc . queueStream ( rddQueue ) ssc . start () ssc . awaitTermination ( 10 ) ssc . stop ( False ) print ( \"Finished\" ) --------------------------------------------------------------------------- IllegalArgumentException Traceback (most recent call last) <ipython-input-49-043ad3d9c502> in <module> 12 13 ---> 14 ssc.start() 15 ssc.awaitTermination(10) 16 ssc.stop(False) /usr/local/spark/python/pyspark/streaming/context.py in start(self) 177 Start the execution of the streams. 178 \"\"\" --> 179 self._jssc.start() 180 StreamingContext._activeContext = self 181 /usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args) 1302 1303 answer = self.gateway_client.send_command(command) -> 1304 return_value = get_return_value( 1305 answer, self.gateway_client, self.target_id, self.name) 1306 /usr/local/spark/python/pyspark/sql/utils.py in deco(*a, **kw) 132 # Hide where the exception came from that shows a non-Pythonic 133 # JVM exception message. --> 134 raise_from(converted) 135 else: 136 raise /usr/local/spark/python/pyspark/sql/utils.py in raise_from(e) IllegalArgumentException: requirement failed: No output operations registered, so nothing to execute","title":"Question 5"},{"location":"MSBD5003/homeworks/hw1/","text":"!pip install pyspark import requests from pyspark.context import SparkContext r = requests . get ( 'https://www.cse.ust.hk/msbd5003/data/fruits.txt' ) open ( 'fruits.txt' , 'wb' ) . write ( r . content ) sc = SparkContext . getOrCreate () Question 1 \u00b6 The following piece of code computes the frequencies of the words in a text file: from operator import add lines = sc . textFile ( 'README.md' ) counts = lines . flatMap ( lambda x : x . split ()) \\ . map ( lambda x : ( x , 1 )) \\ . reduceByKey ( add ) Add one line to find the most frequent word. Output this word and its frequency. Hint: Use sortBy(), reduce(), or max() from operator import add lines = sc . textFile ( 'sample_data/README.md' ) counts = lines . flatMap ( lambda x : x . split ()) \\ . map ( lambda x : ( x , 1 )) \\ . reduceByKey ( add ) \\ . max ( key = lambda x : x [ 1 ]) print ( counts ) ('is', 4) Question 2 \u00b6 Modify the word count example above, so that we only count the frequencies of those words consisting of 5 or more characters. from operator import add lines = sc . textFile ( 'sample_data/README.md' ) counts = lines . flatMap ( lambda x : x . split ()) \\ . map ( lambda x : ( x , 1 )) \\ . reduceByKey ( add ) \\ . filter ( lambda x : len ( x [ 0 ]) >= 5 ) print ( counts . take ( 10 )) [('directory', 1), ('datasets', 1), ('`california_housing_data*.csv`', 1), ('housing', 1), ('https://developers.google.com/machine-learning/crash-course/california-housing-data-description', 1), ('`mnist_*.csv`', 1), (\"[Anscombe's\", 1), ('originally', 1), ('Anscombe,', 1), (\"'Graphs\", 1)] Question 3 \u00b6 Consider the following piece of code: A = sc . parallelize ( range ( 1 , 100 )) t = 50 B = A . filter ( lambda x : x < t ) print ( B . count ()) t = 10 C = B . filter ( lambda x : x > t ) print ( C . count ()) What's its output? (Yes, you can just run it.) A = sc.parallelize(range(1, 100)) t = 50 B = A.filter(lambda x: x < t) print(B.count()) t = 10 C = B.filter(lambda x: x > t) print(C.count()) 49 0 Question 4 \u00b6 The intent of the code above is to get all numbers below 50 from A and put them into B, and then get all numbers above 10 from B and put them into C. Fix the code so that it produces the desired behavior, by adding one line of code. You are not allowed to change the existing code. A = sc.parallelize(range(1, 100)) t = 50 B = A.filter(lambda x: x < t) B.cache() print(B.count()) t = 10 C = B.filter(lambda x: x > t) print(C.count()) 49 39 Question 5 \u00b6 Modify the PMI example by sending a_dict and n_dict inside the closure. Do not use broadcast variables. By changing broadcast variable n_dict = sc.broadcast(n_freqs.collectAsMap()) a_dict = sc.broadcast(a_freqs.collectAsMap()) to global variable n_dict = n_freqs . collectAsMap () a_dict = a_freqs . collectAsMap () Question 6 \u00b6 The following code creates an RDD with 4 partitions: partition 0, 1, 2, and 3. A = sc . parallelize ( range ( 100 ), 4 ) For each item in the RDD, add its partition number to it, and write the results to another RDD, i.e., the resulting RDD should contain: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102] def f ( splitIndex , iterator ): for i in iterator : yield i + splitIndex A = sc . parallelize ( range ( 100 ), 4 ) A . mapPartitionsWithIndex ( f ). collect ()","title":"Hw1"},{"location":"MSBD5003/homeworks/hw1/#question-1","text":"The following piece of code computes the frequencies of the words in a text file: from operator import add lines = sc . textFile ( 'README.md' ) counts = lines . flatMap ( lambda x : x . split ()) \\ . map ( lambda x : ( x , 1 )) \\ . reduceByKey ( add ) Add one line to find the most frequent word. Output this word and its frequency. Hint: Use sortBy(), reduce(), or max() from operator import add lines = sc . textFile ( 'sample_data/README.md' ) counts = lines . flatMap ( lambda x : x . split ()) \\ . map ( lambda x : ( x , 1 )) \\ . reduceByKey ( add ) \\ . max ( key = lambda x : x [ 1 ]) print ( counts ) ('is', 4)","title":"Question 1"},{"location":"MSBD5003/homeworks/hw1/#question-2","text":"Modify the word count example above, so that we only count the frequencies of those words consisting of 5 or more characters. from operator import add lines = sc . textFile ( 'sample_data/README.md' ) counts = lines . flatMap ( lambda x : x . split ()) \\ . map ( lambda x : ( x , 1 )) \\ . reduceByKey ( add ) \\ . filter ( lambda x : len ( x [ 0 ]) >= 5 ) print ( counts . take ( 10 )) [('directory', 1), ('datasets', 1), ('`california_housing_data*.csv`', 1), ('housing', 1), ('https://developers.google.com/machine-learning/crash-course/california-housing-data-description', 1), ('`mnist_*.csv`', 1), (\"[Anscombe's\", 1), ('originally', 1), ('Anscombe,', 1), (\"'Graphs\", 1)]","title":"Question 2"},{"location":"MSBD5003/homeworks/hw1/#question-3","text":"Consider the following piece of code: A = sc . parallelize ( range ( 1 , 100 )) t = 50 B = A . filter ( lambda x : x < t ) print ( B . count ()) t = 10 C = B . filter ( lambda x : x > t ) print ( C . count ()) What's its output? (Yes, you can just run it.) A = sc.parallelize(range(1, 100)) t = 50 B = A.filter(lambda x: x < t) print(B.count()) t = 10 C = B.filter(lambda x: x > t) print(C.count()) 49 0","title":"Question 3"},{"location":"MSBD5003/homeworks/hw1/#question-4","text":"The intent of the code above is to get all numbers below 50 from A and put them into B, and then get all numbers above 10 from B and put them into C. Fix the code so that it produces the desired behavior, by adding one line of code. You are not allowed to change the existing code. A = sc.parallelize(range(1, 100)) t = 50 B = A.filter(lambda x: x < t) B.cache() print(B.count()) t = 10 C = B.filter(lambda x: x > t) print(C.count()) 49 39","title":"Question 4"},{"location":"MSBD5003/homeworks/hw1/#question-5","text":"Modify the PMI example by sending a_dict and n_dict inside the closure. Do not use broadcast variables. By changing broadcast variable n_dict = sc.broadcast(n_freqs.collectAsMap()) a_dict = sc.broadcast(a_freqs.collectAsMap()) to global variable n_dict = n_freqs . collectAsMap () a_dict = a_freqs . collectAsMap ()","title":"Question 5"},{"location":"MSBD5003/homeworks/hw1/#question-6","text":"The following code creates an RDD with 4 partitions: partition 0, 1, 2, and 3. A = sc . parallelize ( range ( 100 ), 4 ) For each item in the RDD, add its partition number to it, and write the results to another RDD, i.e., the resulting RDD should contain: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102] def f ( splitIndex , iterator ): for i in iterator : yield i + splitIndex A = sc . parallelize ( range ( 100 ), 4 ) A . mapPartitionsWithIndex ( f ). collect ()","title":"Question 6"},{"location":"MSBD5003/homeworks/hw2/","text":"!pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 62kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 38.1MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=579f4f3993e28b1ba393be6ffc555dc3c75b4997f72e1ad9bd8ec34f37910075 Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 from pyspark.context import SparkContext from pyspark.sql.session import SparkSession sc = SparkContext . getOrCreate () spark = SparkSession ( sc ) df = spark.read.csv('/content/drive/My Drive/\u8bfe\u7a0b/HKUST/MSBD5003/homeworks/hw2/sales.csv', header=True, inferSchema=True) df.show() +----------------+--------+-----+------------+-----------------+--------------------+--------------+--------------+----------------+----------------+-----------+----------+ |Transaction_date| Product|Price|Payment_Type| Name| City| State| Country| Account_Created| Last_Login| Latitude| Longitude| +----------------+--------+-----+------------+-----------------+--------------------+--------------+--------------+----------------+----------------+-----------+----------+ | 01/02/2009 6:17|Product1| 1200| Mastercard| carolina| Basildon| England|United Kingdom| 01/02/2009 6:00| 01/02/2009 6:08| 51.5|-1.1166667| | 01/02/2009 4:53|Product1| 1200| Visa| Betina|Parkville ...| MO| United States| 01/02/2009 4:42| 01/02/2009 7:49| 39.195| -94.68194| |01/02/2009 13:08|Product1| 1200| Mastercard|Federica e Andrea|Astoria ...| OR| United States|01/01/2009 16:21|01/03/2009 12:32| 46.18806| -123.83| |01/03/2009 14:44|Product1| 1200| Visa| Gouya| Echuca| Victoria| Australia| 9/25/05 21:13|01/03/2009 14:22|-36.1333333| 144.75| |01/04/2009 12:56|Product2| 3600| Visa| Gerd W |Cahaba Heights ...| AL| United States| 11/15/08 15:47|01/04/2009 12:45| 33.52056| -86.8025| |01/04/2009 13:19|Product1| 1200| Visa| LAURENCE|Mickleton ...| NJ| United States| 9/24/08 15:19|01/04/2009 13:04| 39.79| -75.23806| |01/04/2009 20:11|Product1| 1200| Mastercard| Fleur|Peoria ...| IL| United States| 01/03/2009 9:38|01/04/2009 19:45| 40.69361| -89.58889| |01/02/2009 20:09|Product1| 1200| Mastercard| adam|Martin ...| TN| United States|01/02/2009 17:43|01/04/2009 20:01| 36.34333| -88.85028| |01/04/2009 13:17|Product1| 1200| Mastercard| Renee Elisabeth| Tel Aviv| Tel Aviv| Israel|01/04/2009 13:03|01/04/2009 22:10| 32.0666667|34.7666667| |01/04/2009 14:11|Product1| 1200| Visa| Aidan| Chatou| Ile-de-France| France| 06/03/2008 4:22| 01/05/2009 1:17| 48.8833333| 2.15| | 01/05/2009 2:42|Product1| 1200| Diners| Stacy|New York ...| NY| United States| 01/05/2009 2:23| 01/05/2009 4:59| 40.71417| -74.00639| | 01/05/2009 5:39|Product1| 1200| Amex| Heidi| Eindhoven| Noord-Brabant| Netherlands| 01/05/2009 4:55| 01/05/2009 8:15| 51.45| 5.4666667| | 01/02/2009 9:16|Product1| 1200| Mastercard| Sean |Shavano Park ...| TX| United States| 01/02/2009 8:32| 01/05/2009 9:05| 29.42389| -98.49333| |01/05/2009 10:08|Product1| 1200| Visa| Georgia|Eagle ...| ID| United States|11/11/2008 15:53|01/05/2009 10:05| 43.69556|-116.35306| |01/02/2009 14:18|Product1| 1200| Visa| Richard|Riverside ...| NJ| United States|12/09/2008 12:07|01/05/2009 11:01| 40.03222| -74.95778| | 01/04/2009 1:05|Product1| 1200| Diners| Leanne| Julianstown| Meath| Ireland| 01/04/2009 0:00|01/05/2009 13:36| 53.6772222|-6.3191667| |01/05/2009 11:37|Product1| 1200| Visa| Janet| Ottawa| Ontario| Canada| 01/05/2009 9:35|01/05/2009 19:24| 45.4166667| -75.7| | 01/06/2009 5:02|Product1| 1200| Diners| barbara| Hyderabad|Andhra Pradesh| India| 01/06/2009 2:41| 01/06/2009 7:52| 17.3833333|78.4666667| | 01/06/2009 7:45|Product2| 3600| Visa| Sabine| London| England|United Kingdom| 01/06/2009 7:00| 01/06/2009 9:17| 51.52721| 0.14559| | 01/02/2009 7:35|Product1| 1200| Diners| Hani|Salt Lake City ...| UT| United States| 12/30/08 5:44|01/06/2009 10:52| 40.76083|-111.89028| +----------------+--------+-----+------------+-----------------+--------------------+--------------+--------------+----------------+----------------+-----------+----------+ only showing top 20 rows Question 1 \u00b6 Find all distinct countries. Hint: use select(), distinct() countries = df.select(\"Country\").distinct() countries.collect() [Row(Country='Sweden'), Row(Country='Jersey'), Row(Country='Malaysia'), Row(Country='Turkey'), Row(Country='Germany'), Row(Country='France'), Row(Country='Belgium'), Row(Country='Finland'), Row(Country='United States'), Row(Country='India'), Row(Country='Kuwait'), Row(Country='Malta'), Row(Country='Italy'), Row(Country='Norway'), Row(Country='Spain'), Row(Country='Denmark'), Row(Country='Ireland'), Row(Country='Israel'), Row(Country='Iceland'), Row(Country='South Korea'), Row(Country='Switzerland'), Row(Country='United Arab Emirates'), Row(Country='Canada'), Row(Country='Brazil'), Row(Country='Luxembourg'), Row(Country='New Zealand'), Row(Country='Australia'), Row(Country='Austria'), Row(Country='South Africa'), Row(Country='Bahrain'), Row(Country='Hungary'), Row(Country='United Kingdom'), Row(Country='Moldova'), Row(Country='Netherlands')] Question 2 \u00b6 Find the Name and Price of sales records in Brazil. Hint: use filter(). df.select(\"Country\", \"Price\").filter(\"Country = 'Brazil' \").show() +-------+-----+ |Country|Price| +-------+-----+ | Brazil| 1200| | Brazil| 7500| +-------+-----+ Question 3 \u00b6 For each country, find the total Price. Hint: Use groupBy() df.groupBy('Country').sum('Price').withColumnRenamed('sum(Price)', \"Total Price\").show() +-------------+-----------+ | Country|Total Price| +-------------+-----------+ | Sweden| 8400| | Jersey| 1200| | Malaysia| 1200| | Turkey| 2400| | Germany| 22800| | France| 30300| | Belgium| 3600| | Finland| 1200| |United States| 350350| | India| 2400| | Kuwait| 1200| | Malta| 3600| | Italy| 2400| | Norway| 12000| | Spain| 2400| | Denmark| 8400| | Ireland| 29100| | Israel| 1200| | Iceland| 1200| | South Korea| 1200| +-------------+-----------+ only showing top 20 rows Question 4 \u00b6 List countries by their total Price in descending order. Hint: Use orderBy() df.groupBy('Country').sum('Price').withColumnRenamed('sum(Price)', \"Total Price\").orderBy('Total Price', ascending=False).show() +--------------------+-----------+ | Country|Total Price| +--------------------+-----------+ | United States| 350350| | United Kingdom| 63600| | Canada| 42000| | France| 30300| | Ireland| 29100| | Germany| 22800| | Australia| 22800| | Switzerland| 19200| | Netherlands| 14400| | Norway| 12000| | Brazil| 8700| | Denmark| 8400| | Sweden| 8400| | Austria| 3600| | South Africa| 3600| | Malta| 3600| | Belgium| 3600| |United Arab Emirates| 3600| | Turkey| 2400| | New Zealand| 2400| +--------------------+-----------+ only showing top 20 rows Question 5 \u00b6 Redo Question 3, but replace the country names by their IDs. Hint: Use join() df2 = spark.read.csv('/content/drive/My Drive/\u8bfe\u7a0b/HKUST/MSBD5003/homeworks/hw2/countries.csv', header=True, inferSchema=True) df2.show() +--------------+---+ | Country| ID| +--------------+---+ |United Kingdom| 1| | United States| 2| | Australia| 3| | Israel| 4| | France| 5| | Netherlands| 6| | Ireland| 7| | Canada| 8| | India| 9| | South Africa| 10| | Finland| 11| | Switzerland| 12| | Denmark| 13| | Belgium| 14| | Sweden| 15| | Norway| 16| | Luxembourg| 17| | Italy| 18| | Germany| 19| | Moldova| 20| +--------------+---+ only showing top 20 rows df.join(df2, 'Country').groupBy('ID').sum('Price').withColumnRenamed('sum(Price)', \"Total Price\").show() +---+-----------+ | ID|Total Price| +---+-----------+ | 31| 1200| | 34| 2400| | 28| 3600| | 26| 3600| | 27| 1200| | 12| 19200| | 22| 3600| | 1| 63600| | 13| 8400| | 6| 14400| | 16| 12000| | 3| 22800| | 20| 1200| | 5| 30300| | 19| 22800| | 15| 8400| | 9| 2400| | 17| 1200| | 4| 1200| | 8| 42000| +---+-----------+ only showing top 20 rows Question 6 \u00b6 Rewrite the PageRank example using DataFrame API. Here is a skeleton of the code. Your job is to fill in the missing part. The data files can be downloaded at: from pyspark.sql.functions import * numOfIterations = 10 lines = spark . read . text ( \"pagerank_data.txt\" ) # You can also test your program on the follow larger data set: # lines = spark.read.text(\"dblp.in\") a = lines . select ( split ( lines [ 0 ], ' ' )) links = a . select ( a [ 0 ][ 0 ] . alias ( 'src' ), a [ 0 ][ 1 ] . alias ( 'dst' )) outdegrees = links . groupBy ( 'src' ) . count () ranks = outdegrees . select ( 'src' , lit ( 1 ) . alias ( 'rank' )) for iteration in range ( numOfIterations ): # FILL IN THIS PART ranks . orderBy ( desc ( 'rank' )) . show () lines = spark.read.text(\"/content/drive/My Drive/\u8bfe\u7a0b/HKUST/MSBD5003/homeworks/hw2/pagerank_data.txt\") lines.show() +-----+ |value| +-----+ | 1 2| | 1 3| | 2 3| | 3 4| | 4 1| | 2 1| +-----+ from pyspark.sql.functions import * from pyspark.sql.types import FloatType from operator import add numOfIterations = 10 # You can also test your program on the follow larger data set: # lines = spark.read.text(\"dblp.in\") a = lines . select ( split ( lines [ 0 ], ' ' )) links = a . select ( a [ 0 ][ 0 ] . alias ( 'src' ), a [ 0 ][ 1 ] . alias ( 'dst' )) outdegrees = links . groupBy ( 'src' ) . count () ranks = outdegrees . select ( 'src' , lit ( 1 ) . alias ( 'rank' )) for iteration in range ( numOfIterations ): contribs = links . join ( ranks , 'src' ) . join ( outdegrees , 'src' ) . withColumnRenamed ( 'count' , 'outdegrees' ) contribs = contribs . select ( 'src' , 'dst' ,( contribs . rank / contribs . outdegrees ) . alias ( 'rank' )) . groupBy ( 'dst' ) . sum () . withColumnRenamed ( 'dst' , 'src' ) . withColumnRenamed ( 'sum(rank)' , 'rank' ) ranks = contribs . select ( 'src' , ( contribs . rank * 0.85 + 0.15 ) . alias ( 'rank' )) ranks . orderBy ( desc ( 'rank' )) . show () +---+------------------+ |src| rank| +---+------------------+ | 1|1.2981882732854677| | 4|0.9999999999999998| | 3|0.9999999999999998| | 2|0.7018117267145316| +---+------------------+","title":"Hw2"},{"location":"MSBD5003/homeworks/hw2/#question-1","text":"Find all distinct countries. Hint: use select(), distinct() countries = df.select(\"Country\").distinct() countries.collect() [Row(Country='Sweden'), Row(Country='Jersey'), Row(Country='Malaysia'), Row(Country='Turkey'), Row(Country='Germany'), Row(Country='France'), Row(Country='Belgium'), Row(Country='Finland'), Row(Country='United States'), Row(Country='India'), Row(Country='Kuwait'), Row(Country='Malta'), Row(Country='Italy'), Row(Country='Norway'), Row(Country='Spain'), Row(Country='Denmark'), Row(Country='Ireland'), Row(Country='Israel'), Row(Country='Iceland'), Row(Country='South Korea'), Row(Country='Switzerland'), Row(Country='United Arab Emirates'), Row(Country='Canada'), Row(Country='Brazil'), Row(Country='Luxembourg'), Row(Country='New Zealand'), Row(Country='Australia'), Row(Country='Austria'), Row(Country='South Africa'), Row(Country='Bahrain'), Row(Country='Hungary'), Row(Country='United Kingdom'), Row(Country='Moldova'), Row(Country='Netherlands')]","title":"Question 1"},{"location":"MSBD5003/homeworks/hw2/#question-2","text":"Find the Name and Price of sales records in Brazil. Hint: use filter(). df.select(\"Country\", \"Price\").filter(\"Country = 'Brazil' \").show() +-------+-----+ |Country|Price| +-------+-----+ | Brazil| 1200| | Brazil| 7500| +-------+-----+","title":"Question 2"},{"location":"MSBD5003/homeworks/hw2/#question-3","text":"For each country, find the total Price. Hint: Use groupBy() df.groupBy('Country').sum('Price').withColumnRenamed('sum(Price)', \"Total Price\").show() +-------------+-----------+ | Country|Total Price| +-------------+-----------+ | Sweden| 8400| | Jersey| 1200| | Malaysia| 1200| | Turkey| 2400| | Germany| 22800| | France| 30300| | Belgium| 3600| | Finland| 1200| |United States| 350350| | India| 2400| | Kuwait| 1200| | Malta| 3600| | Italy| 2400| | Norway| 12000| | Spain| 2400| | Denmark| 8400| | Ireland| 29100| | Israel| 1200| | Iceland| 1200| | South Korea| 1200| +-------------+-----------+ only showing top 20 rows","title":"Question 3"},{"location":"MSBD5003/homeworks/hw2/#question-4","text":"List countries by their total Price in descending order. Hint: Use orderBy() df.groupBy('Country').sum('Price').withColumnRenamed('sum(Price)', \"Total Price\").orderBy('Total Price', ascending=False).show() +--------------------+-----------+ | Country|Total Price| +--------------------+-----------+ | United States| 350350| | United Kingdom| 63600| | Canada| 42000| | France| 30300| | Ireland| 29100| | Germany| 22800| | Australia| 22800| | Switzerland| 19200| | Netherlands| 14400| | Norway| 12000| | Brazil| 8700| | Denmark| 8400| | Sweden| 8400| | Austria| 3600| | South Africa| 3600| | Malta| 3600| | Belgium| 3600| |United Arab Emirates| 3600| | Turkey| 2400| | New Zealand| 2400| +--------------------+-----------+ only showing top 20 rows","title":"Question 4"},{"location":"MSBD5003/homeworks/hw2/#question-5","text":"Redo Question 3, but replace the country names by their IDs. Hint: Use join() df2 = spark.read.csv('/content/drive/My Drive/\u8bfe\u7a0b/HKUST/MSBD5003/homeworks/hw2/countries.csv', header=True, inferSchema=True) df2.show() +--------------+---+ | Country| ID| +--------------+---+ |United Kingdom| 1| | United States| 2| | Australia| 3| | Israel| 4| | France| 5| | Netherlands| 6| | Ireland| 7| | Canada| 8| | India| 9| | South Africa| 10| | Finland| 11| | Switzerland| 12| | Denmark| 13| | Belgium| 14| | Sweden| 15| | Norway| 16| | Luxembourg| 17| | Italy| 18| | Germany| 19| | Moldova| 20| +--------------+---+ only showing top 20 rows df.join(df2, 'Country').groupBy('ID').sum('Price').withColumnRenamed('sum(Price)', \"Total Price\").show() +---+-----------+ | ID|Total Price| +---+-----------+ | 31| 1200| | 34| 2400| | 28| 3600| | 26| 3600| | 27| 1200| | 12| 19200| | 22| 3600| | 1| 63600| | 13| 8400| | 6| 14400| | 16| 12000| | 3| 22800| | 20| 1200| | 5| 30300| | 19| 22800| | 15| 8400| | 9| 2400| | 17| 1200| | 4| 1200| | 8| 42000| +---+-----------+ only showing top 20 rows","title":"Question 5"},{"location":"MSBD5003/homeworks/hw2/#question-6","text":"Rewrite the PageRank example using DataFrame API. Here is a skeleton of the code. Your job is to fill in the missing part. The data files can be downloaded at: from pyspark.sql.functions import * numOfIterations = 10 lines = spark . read . text ( \"pagerank_data.txt\" ) # You can also test your program on the follow larger data set: # lines = spark.read.text(\"dblp.in\") a = lines . select ( split ( lines [ 0 ], ' ' )) links = a . select ( a [ 0 ][ 0 ] . alias ( 'src' ), a [ 0 ][ 1 ] . alias ( 'dst' )) outdegrees = links . groupBy ( 'src' ) . count () ranks = outdegrees . select ( 'src' , lit ( 1 ) . alias ( 'rank' )) for iteration in range ( numOfIterations ): # FILL IN THIS PART ranks . orderBy ( desc ( 'rank' )) . show () lines = spark.read.text(\"/content/drive/My Drive/\u8bfe\u7a0b/HKUST/MSBD5003/homeworks/hw2/pagerank_data.txt\") lines.show() +-----+ |value| +-----+ | 1 2| | 1 3| | 2 3| | 3 4| | 4 1| | 2 1| +-----+ from pyspark.sql.functions import * from pyspark.sql.types import FloatType from operator import add numOfIterations = 10 # You can also test your program on the follow larger data set: # lines = spark.read.text(\"dblp.in\") a = lines . select ( split ( lines [ 0 ], ' ' )) links = a . select ( a [ 0 ][ 0 ] . alias ( 'src' ), a [ 0 ][ 1 ] . alias ( 'dst' )) outdegrees = links . groupBy ( 'src' ) . count () ranks = outdegrees . select ( 'src' , lit ( 1 ) . alias ( 'rank' )) for iteration in range ( numOfIterations ): contribs = links . join ( ranks , 'src' ) . join ( outdegrees , 'src' ) . withColumnRenamed ( 'count' , 'outdegrees' ) contribs = contribs . select ( 'src' , 'dst' ,( contribs . rank / contribs . outdegrees ) . alias ( 'rank' )) . groupBy ( 'dst' ) . sum () . withColumnRenamed ( 'dst' , 'src' ) . withColumnRenamed ( 'sum(rank)' , 'rank' ) ranks = contribs . select ( 'src' , ( contribs . rank * 0.85 + 0.15 ) . alias ( 'rank' )) ranks . orderBy ( desc ( 'rank' )) . show () +---+------------------+ |src| rank| +---+------------------+ | 1|1.2981882732854677| | 4|0.9999999999999998| | 3|0.9999999999999998| | 2|0.7018117267145316| +---+------------------+","title":"Question 6"},{"location":"MSBD5003/homeworks/hw3/","text":"!pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 65kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 37.0MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=edc89a8f53022da82822d1b1bfb5ff85b57d72c5c4f9530d935fa927d268c34f Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 from pyspark.context import SparkContext from pyspark.sql.session import SparkSession sc = SparkContext . getOrCreate () spark = SparkSession ( sc ) Question 1 \u00b6 Load it into spark and use divide-and-conquer to find the first (adj, noun) pair in which the noun is 'unification'. Print the corresponding adjective. The skeleton code is provided below. One solution is to use filter() to find all pairs where the noun is 'unification', and then report the first one. This is inefficient. The better idea is to find, in parallel, the first such pair in each partition (if one exists), and then find the first partition that returns such a pair. numPartitions = 10 lines = sc . textFile ( path_to_file , numPartitions ) pairs = lines . map ( lambda l : tuple ( l . split ())) . filter ( lambda p : len ( p ) == 2 ) pairs . cache () # FILL IN YOUR CODE HERE numPartitions = 10 def find_word ( iterator ): for adj , noun in iterator : if noun == \"unification\" : yield ( adj , noun ) break path_to_file = \"/content/drive/My Drive/courses/HKUST/MSBD5003/data/adj_noun_pairs.txt\" lines = sc . textFile ( path_to_file , numPartitions ) pairs = lines . map ( lambda l : tuple ( l . split ())). filter ( lambda p : len ( p ) == 2 ) pairs . cache () words = pairs . mapPartitions ( find_word ). collect () print ( words [ 0 ][ 0 ]) several Answer \u00b6 numPartitions = 10 def find_word ( iterator ): for adj , noun in iterator : if noun == \"unification\" : yield ( adj , noun ) break path_to_file = \"/content/drive/My Drive/courses/HKUST/MSBD5003/data/adj_noun_pairs.txt\" lines = sc . textFile ( path_to_file , numPartitions ) pairs = lines . map ( lambda l : tuple ( l . split ())) . filter ( lambda p : len ( p ) == 2 ) pairs . cache () words = pairs . mapPartitions ( find_word ) . collect () print ( words [ 0 ][ 0 ]) Question 2 \u00b6 Design a parallel divide-and-conquer algorithm for the following problem: Given two strings of equal length, compare them lexicographically. Output '<', '=', or '>', depending on the comparison result. The skeleton code is provided below. Your code should run on all partitions of the rdd in parallel. x = 'abcccbcbcacaccacaabb' y = 'abcccbcccacaccacaabb' numPartitions = 4 rdd = sc . parallelize ( zip ( x , y ), numPartitions ) # FILL IN YOUR CODE HERE ord(\"A\") 65 x = 'abcccbcbcacaccacaabb' y = 'abcccbcccacaccacaabb' def get_sum_by_partitions ( iterator ): sum_1 = 0 sum_2 = 0 for a , b in iterator : sum_1 += ord ( a ) sum_2 += ord ( b ) yield ( sum_1 , sum_2 ) numPartitions = 4 rdd = sc . parallelize ( zip ( x , y ), numPartitions ) sums = rdd . mapPartitions ( get_sum_by_partitions ). collect () sum_1 = 0 sum_2 = 0 for s_1 , s_2 in sums : sum_1 += s_1 sum_2 += s_2 if sum_1 < sum_2 : print ( \"<\" ) elif sum_1 == sum_2 : print ( \"=\" ) else : print ( \">\" ) > Answer \u00b6 x = 'abcccbcbcacaccacaabb' y = 'abcccbcccacaccacaabb' def get_sum_by_partitions ( iterator ): sum_1 = 0 sum_2 = 0 for a , b in iterator : sum_1 += ord ( a ) sum_2 += ord ( b ) yield ( sum_1 , sum_2 ) numPartitions = 4 rdd = sc . parallelize ( zip ( x , y ), numPartitions ) sums = rdd . mapPartitions ( get_sum_by_partitions ) . collect () sum_1 = 0 sum_2 = 0 for s_1 , s_2 in sums : sum_1 += s_1 sum_2 += s_2 if sum_1 < sum_2 : print ( \"<\" ) elif sum_1 == sum_2 : print ( \"=\" ) else : print ( \">\" )","title":"Hw3"},{"location":"MSBD5003/homeworks/hw3/#question-1","text":"Load it into spark and use divide-and-conquer to find the first (adj, noun) pair in which the noun is 'unification'. Print the corresponding adjective. The skeleton code is provided below. One solution is to use filter() to find all pairs where the noun is 'unification', and then report the first one. This is inefficient. The better idea is to find, in parallel, the first such pair in each partition (if one exists), and then find the first partition that returns such a pair. numPartitions = 10 lines = sc . textFile ( path_to_file , numPartitions ) pairs = lines . map ( lambda l : tuple ( l . split ())) . filter ( lambda p : len ( p ) == 2 ) pairs . cache () # FILL IN YOUR CODE HERE numPartitions = 10 def find_word ( iterator ): for adj , noun in iterator : if noun == \"unification\" : yield ( adj , noun ) break path_to_file = \"/content/drive/My Drive/courses/HKUST/MSBD5003/data/adj_noun_pairs.txt\" lines = sc . textFile ( path_to_file , numPartitions ) pairs = lines . map ( lambda l : tuple ( l . split ())). filter ( lambda p : len ( p ) == 2 ) pairs . cache () words = pairs . mapPartitions ( find_word ). collect () print ( words [ 0 ][ 0 ]) several","title":"Question 1"},{"location":"MSBD5003/homeworks/hw3/#answer","text":"numPartitions = 10 def find_word ( iterator ): for adj , noun in iterator : if noun == \"unification\" : yield ( adj , noun ) break path_to_file = \"/content/drive/My Drive/courses/HKUST/MSBD5003/data/adj_noun_pairs.txt\" lines = sc . textFile ( path_to_file , numPartitions ) pairs = lines . map ( lambda l : tuple ( l . split ())) . filter ( lambda p : len ( p ) == 2 ) pairs . cache () words = pairs . mapPartitions ( find_word ) . collect () print ( words [ 0 ][ 0 ])","title":"Answer"},{"location":"MSBD5003/homeworks/hw3/#question-2","text":"Design a parallel divide-and-conquer algorithm for the following problem: Given two strings of equal length, compare them lexicographically. Output '<', '=', or '>', depending on the comparison result. The skeleton code is provided below. Your code should run on all partitions of the rdd in parallel. x = 'abcccbcbcacaccacaabb' y = 'abcccbcccacaccacaabb' numPartitions = 4 rdd = sc . parallelize ( zip ( x , y ), numPartitions ) # FILL IN YOUR CODE HERE ord(\"A\") 65 x = 'abcccbcbcacaccacaabb' y = 'abcccbcccacaccacaabb' def get_sum_by_partitions ( iterator ): sum_1 = 0 sum_2 = 0 for a , b in iterator : sum_1 += ord ( a ) sum_2 += ord ( b ) yield ( sum_1 , sum_2 ) numPartitions = 4 rdd = sc . parallelize ( zip ( x , y ), numPartitions ) sums = rdd . mapPartitions ( get_sum_by_partitions ). collect () sum_1 = 0 sum_2 = 0 for s_1 , s_2 in sums : sum_1 += s_1 sum_2 += s_2 if sum_1 < sum_2 : print ( \"<\" ) elif sum_1 == sum_2 : print ( \"=\" ) else : print ( \">\" ) >","title":"Question 2"},{"location":"MSBD5003/homeworks/hw3/#answer_1","text":"x = 'abcccbcbcacaccacaabb' y = 'abcccbcccacaccacaabb' def get_sum_by_partitions ( iterator ): sum_1 = 0 sum_2 = 0 for a , b in iterator : sum_1 += ord ( a ) sum_2 += ord ( b ) yield ( sum_1 , sum_2 ) numPartitions = 4 rdd = sc . parallelize ( zip ( x , y ), numPartitions ) sums = rdd . mapPartitions ( get_sum_by_partitions ) . collect () sum_1 = 0 sum_2 = 0 for s_1 , s_2 in sums : sum_1 += s_1 sum_2 += s_2 if sum_1 < sum_2 : print ( \"<\" ) elif sum_1 == sum_2 : print ( \"=\" ) else : print ( \">\" )","title":"Answer"},{"location":"MSBD5003/notebooks%20in%20class/DC/","text":"! pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 63kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 47.6MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=bd505a7a219c64c9cf329fc493b951b33584b8631e2c6893141869ec65dcc4d0 Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 from pyspark.context import SparkContext from pyspark.sql.session import SparkSession sc = SparkContext . getOrCreate () spark = SparkSession ( sc ) Prefix Sums \u00b6 x = [ 1 , 4 , 3 , 5 , 6 , 7 , 0 , 1 ] rdd = sc . parallelize ( x , 4 ) . cache () def f ( iterator ): yield sum ( iterator ) sums = rdd . mapPartitions ( f ) . collect () print ( sums ) for i in range ( 1 , len ( sums )): sums [ i ] += sums [ i - 1 ] print ( sums ) def g ( index , iterator ): global sums if index == 0 : s = 0 else : s = sums [ index - 1 ] for i in iterator : s += i yield s prefix_sums = rdd . mapPartitionsWithIndex ( g ) print ( prefix_sums . collect ()) [5, 8, 13, 1] [5, 13, 26, 27] [1, 5, 8, 13, 19, 26, 26, 27] Monotocity checking \u00b6 x = [ 1 , 3 , 4 , 5 , 7 , 3 , 10 , 14 , 16 , 20 , 21 , 24 , 24 , 26 , 27 , 30 ] rdd = sc . parallelize ( x , 4 ) . cache () def f ( it ): first = next ( it ) last = first increasing = True for i in it : if i < last : increasing = False last = i yield increasing , first , last results = rdd . mapPartitions ( f ) . collect () print ( results ) increasing = True if results [ 0 ][ 0 ] == False : increasing = False else : for i in range ( 1 , len ( results )): if results [ i ][ 0 ] == False or results [ i ][ 1 ] < results [ i - 1 ][ 2 ]: increasing = False if increasing : print ( \"Monotone\" ) else : print ( \"Not monotone\" ) [(True, 1, 5), (False, 7, 14), (True, 16, 24), (True, 24, 30)] Not monotone Maximum Subarray Problem \u00b6 # Classical divide and conquer algorithm A = [ - 3 , 2 , 1 , - 4 , 5 , 2 , - 1 , 3 , - 1 ] def MaxSubarray ( A , p , r ): if p == r : return A [ p ] q = ( p + r ) // 2 M1 = MaxSubarray ( A , p , q ) M2 = MaxSubarray ( A , q + 1 , r ) Lm = - float ( 'inf' ) Rm = Lm V = 0 for i in range ( q , p - 1 , - 1 ): V += A [ i ] if V > Lm : Lm = V V = 0 for i in range ( q + 1 , r + 1 ): V += A [ i ] if V > Rm : Rm = V return max ( M1 , M2 , Lm + Rm ) print ( MaxSubarray ( A , 0 , len ( A ) - 1 )) 9 # Linear-time algorithm # Written in a way so that we can call it for each partition def linear_time ( it ): Vmax = - float ( 'inf' ) V = 0 for Ai in it : V += Ai if V > Vmax : Vmax = V if V < 0 : V = 0 yield Vmax print ( next ( linear_time ( A ))) 9 # The Spark algorithm: def compute_sum ( it ): yield sum ( it ) def compute_LmRm ( index , it ): Lm = - float ( 'inf' ) Rm = - float ( 'inf' ) L = sums [ index ] R = 0 for Ai in it : L -= Ai R += Ai if L > Lm : Lm = L if R > Rm : Rm = R yield ( Lm , Rm ) num_partitions = 4 rdd = sc . parallelize ( A , num_partitions ) . cache () sums = rdd . mapPartitions ( compute_sum ) . collect () print ( sums ) LmRms = rdd . mapPartitionsWithIndex ( compute_LmRm ) . collect () print ( LmRms ) best = max ( rdd . mapPartitions ( linear_time ) . collect ()) for i in range ( num_partitions - 1 ): for j in range ( i + 1 , num_partitions ): x = LmRms [ i ][ 0 ] + sum ( sums [ i + 1 : j ]) + LmRms [ j ][ 1 ] if x > best : best = x print ( best ) [-1, -3, 7, 1] [(2, -1), (0, 1), (2, 7), (2, 2)] 9","title":"Dc"},{"location":"MSBD5003/notebooks%20in%20class/DC/#prefix-sums","text":"x = [ 1 , 4 , 3 , 5 , 6 , 7 , 0 , 1 ] rdd = sc . parallelize ( x , 4 ) . cache () def f ( iterator ): yield sum ( iterator ) sums = rdd . mapPartitions ( f ) . collect () print ( sums ) for i in range ( 1 , len ( sums )): sums [ i ] += sums [ i - 1 ] print ( sums ) def g ( index , iterator ): global sums if index == 0 : s = 0 else : s = sums [ index - 1 ] for i in iterator : s += i yield s prefix_sums = rdd . mapPartitionsWithIndex ( g ) print ( prefix_sums . collect ()) [5, 8, 13, 1] [5, 13, 26, 27] [1, 5, 8, 13, 19, 26, 26, 27]","title":"Prefix Sums"},{"location":"MSBD5003/notebooks%20in%20class/DC/#monotocity-checking","text":"x = [ 1 , 3 , 4 , 5 , 7 , 3 , 10 , 14 , 16 , 20 , 21 , 24 , 24 , 26 , 27 , 30 ] rdd = sc . parallelize ( x , 4 ) . cache () def f ( it ): first = next ( it ) last = first increasing = True for i in it : if i < last : increasing = False last = i yield increasing , first , last results = rdd . mapPartitions ( f ) . collect () print ( results ) increasing = True if results [ 0 ][ 0 ] == False : increasing = False else : for i in range ( 1 , len ( results )): if results [ i ][ 0 ] == False or results [ i ][ 1 ] < results [ i - 1 ][ 2 ]: increasing = False if increasing : print ( \"Monotone\" ) else : print ( \"Not monotone\" ) [(True, 1, 5), (False, 7, 14), (True, 16, 24), (True, 24, 30)] Not monotone","title":"Monotocity checking"},{"location":"MSBD5003/notebooks%20in%20class/DC/#maximum-subarray-problem","text":"# Classical divide and conquer algorithm A = [ - 3 , 2 , 1 , - 4 , 5 , 2 , - 1 , 3 , - 1 ] def MaxSubarray ( A , p , r ): if p == r : return A [ p ] q = ( p + r ) // 2 M1 = MaxSubarray ( A , p , q ) M2 = MaxSubarray ( A , q + 1 , r ) Lm = - float ( 'inf' ) Rm = Lm V = 0 for i in range ( q , p - 1 , - 1 ): V += A [ i ] if V > Lm : Lm = V V = 0 for i in range ( q + 1 , r + 1 ): V += A [ i ] if V > Rm : Rm = V return max ( M1 , M2 , Lm + Rm ) print ( MaxSubarray ( A , 0 , len ( A ) - 1 )) 9 # Linear-time algorithm # Written in a way so that we can call it for each partition def linear_time ( it ): Vmax = - float ( 'inf' ) V = 0 for Ai in it : V += Ai if V > Vmax : Vmax = V if V < 0 : V = 0 yield Vmax print ( next ( linear_time ( A ))) 9 # The Spark algorithm: def compute_sum ( it ): yield sum ( it ) def compute_LmRm ( index , it ): Lm = - float ( 'inf' ) Rm = - float ( 'inf' ) L = sums [ index ] R = 0 for Ai in it : L -= Ai R += Ai if L > Lm : Lm = L if R > Rm : Rm = R yield ( Lm , Rm ) num_partitions = 4 rdd = sc . parallelize ( A , num_partitions ) . cache () sums = rdd . mapPartitions ( compute_sum ) . collect () print ( sums ) LmRms = rdd . mapPartitionsWithIndex ( compute_LmRm ) . collect () print ( LmRms ) best = max ( rdd . mapPartitions ( linear_time ) . collect ()) for i in range ( num_partitions - 1 ): for j in range ( i + 1 , num_partitions ): x = LmRms [ i ][ 0 ] + sum ( sums [ i + 1 : j ]) + LmRms [ j ][ 1 ] if x > best : best = x print ( best ) [-1, -3, 7, 1] [(2, -1), (0, 1), (2, 7), (2, 2)] 9","title":"Maximum Subarray Problem"},{"location":"MSBD5003/notebooks%20in%20class/PMI/","text":"PMI \u00b6 PMI (pointwise mutual information) is a measure of association used in information theory and statistics. Given a list of pairs (x, y) \\[pmi(x, y) = log\\frac{p(x,y)}{p(x)p(y}\\] where - \\(p(x)\\) : probability of x - \\(p(y)\\) : probability of y - \\(p(x,y)\\) : joint probability Example: p(x=0) = 0.8, p(x=1)=0.2, p(y=0)=0.25, p(y=1)=0.75 pmi(x=0;y=0) = \u22121 pmi(x=0;y=1) = 0.222392 pmi(x=1;y=0) = 1.584963 pmi(x=1;y=1) = -1.584963 Example notebook see: note book in class/PMI ! pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 70kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 44.6MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=ae3121fc30af19c4ec22b0beb2c7452d103f59dc5ad06c6fa21a5b108cdbf54a Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 import requests from pyspark.context import SparkContext r = requests . get ( 'https://www.cse.ust.hk/msbd5003/data/adj_noun_pairs.txt' ) open ( 'adj_noun_pairs.txt' , 'wb' ) . write ( r . content ) sc = SparkContext . getOrCreate () # Data file at https://www.cse.ust.hk/msbd5003/data lines = sc . textFile ( 'adj_noun_pairs.txt' ) lines . count () 3162692 lines . getNumPartitions () 2 lines . take ( 5 ) ['early radical', 'french revolution', 'pejorative way', 'violent means', 'positive label'] # Converting lines into word pairs. # Data is dirty: some lines have more than 2 words, so filter them out. pairs = lines . map ( lambda l : tuple ( l . split ())) . filter ( lambda p : len ( p ) == 2 ) pairs . cache () PythonRDD[4] at RDD at PythonRDD.scala:53 pairs . take ( 5 ) [('early', 'radical'), ('french', 'revolution'), ('pejorative', 'way'), ('violent', 'means'), ('positive', 'label')] N = pairs . count () N 3162674 # Compute the frequency of each pair. # Ignore pairs that not frequent enough pair_freqs = pairs . map ( lambda p : ( p , 1 )) . reduceByKey ( lambda f1 , f2 : f1 + f2 ) \\ . filter ( lambda pf : pf [ 1 ] >= 100 ) pair_freqs . take ( 5 ) [(('political', 'philosophy'), 160), (('human', 'society'), 154), (('16th', 'century'), 950), (('first', 'man'), 166), (('same', 'time'), 2744)] # Computing the frequencies of the adjectives and the nouns a_freqs = pairs . map ( lambda p : ( p [ 0 ], 1 )) . reduceByKey ( lambda x , y : x + y ) n_freqs = pairs . map ( lambda p : ( p [ 1 ], 1 )) . reduceByKey ( lambda x , y : x + y ) a_freqs . take ( 5 ) [('violent', 1191), ('positive', 2302), ('self-defined', 3), ('political', 15935), ('differ', 381)] n_freqs . count () 106333 # Broadcasting the adjective and noun frequencies. #a_dict = a_freqs.collectAsMap() #a_dict = sc.parallelize(a_dict).map(lambda x: x) n_dict = sc . broadcast ( n_freqs . collectAsMap ()) a_dict = sc . broadcast ( a_freqs . collectAsMap ()) a_dict . value [ 'violent' ] 1191 from math import * # Computing the PMI for a pair. def pmi_score ( pair_freq ): w1 , w2 = pair_freq [ 0 ] f = pair_freq [ 1 ] pmi = log ( float ( f ) * N / ( a_dict . value [ w1 ] * n_dict . value [ w2 ]), 2 ) return pmi , ( w1 , w2 ) # Computing the PMI for all pairs. scored_pairs = pair_freqs . map ( pmi_score ) # Printing the most strongly associated pairs. scored_pairs . top ( 10 ) [(14.41018838546462, ('magna', 'carta')), (13.071365888694997, ('polish-lithuanian', 'Commonwealth')), (12.990597616733414, ('nitrous', 'oxide')), (12.64972604311254, ('latter-day', 'Saints')), (12.50658937509916, ('stainless', 'steel')), (12.482331020687814, ('pave', 'runway')), (12.19140721768055, ('corporal', 'punishment')), (12.183248694293388, ('capital', 'punishment')), (12.147015483562537, ('rush', 'yard')), (12.109945794428935, ('globular', 'cluster'))] Another way n_dict = n_freqs . collectAsMap () a_dict = a_freqs . collectAsMap () from math import * # Computing the PMI for a pair. def pmi_score ( pair_freq ): w1 , w2 = pair_freq [ 0 ] f = pair_freq [ 1 ] pmi = log ( float ( f ) * N / ( a_dict [ w1 ] * n_dict [ w2 ]), 2 ) return pmi , ( w1 , w2 ) scored_pairs = pair_freqs . map ( pmi_score ) scored_pairs . top ( 10 ) [(14.41018838546462, ('magna', 'carta')), (13.071365888694997, ('polish-lithuanian', 'Commonwealth')), (12.990597616733414, ('nitrous', 'oxide')), (12.64972604311254, ('latter-day', 'Saints')), (12.50658937509916, ('stainless', 'steel')), (12.482331020687814, ('pave', 'runway')), (12.19140721768055, ('corporal', 'punishment')), (12.183248694293388, ('capital', 'punishment')), (12.147015483562537, ('rush', 'yard')), (12.109945794428935, ('globular', 'cluster'))]","title":"Pmi"},{"location":"MSBD5003/notebooks%20in%20class/PMI/#pmi","text":"PMI (pointwise mutual information) is a measure of association used in information theory and statistics. Given a list of pairs (x, y) \\[pmi(x, y) = log\\frac{p(x,y)}{p(x)p(y}\\] where - \\(p(x)\\) : probability of x - \\(p(y)\\) : probability of y - \\(p(x,y)\\) : joint probability Example: p(x=0) = 0.8, p(x=1)=0.2, p(y=0)=0.25, p(y=1)=0.75 pmi(x=0;y=0) = \u22121 pmi(x=0;y=1) = 0.222392 pmi(x=1;y=0) = 1.584963 pmi(x=1;y=1) = -1.584963 Example notebook see: note book in class/PMI ! pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 70kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 44.6MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=ae3121fc30af19c4ec22b0beb2c7452d103f59dc5ad06c6fa21a5b108cdbf54a Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 import requests from pyspark.context import SparkContext r = requests . get ( 'https://www.cse.ust.hk/msbd5003/data/adj_noun_pairs.txt' ) open ( 'adj_noun_pairs.txt' , 'wb' ) . write ( r . content ) sc = SparkContext . getOrCreate () # Data file at https://www.cse.ust.hk/msbd5003/data lines = sc . textFile ( 'adj_noun_pairs.txt' ) lines . count () 3162692 lines . getNumPartitions () 2 lines . take ( 5 ) ['early radical', 'french revolution', 'pejorative way', 'violent means', 'positive label'] # Converting lines into word pairs. # Data is dirty: some lines have more than 2 words, so filter them out. pairs = lines . map ( lambda l : tuple ( l . split ())) . filter ( lambda p : len ( p ) == 2 ) pairs . cache () PythonRDD[4] at RDD at PythonRDD.scala:53 pairs . take ( 5 ) [('early', 'radical'), ('french', 'revolution'), ('pejorative', 'way'), ('violent', 'means'), ('positive', 'label')] N = pairs . count () N 3162674 # Compute the frequency of each pair. # Ignore pairs that not frequent enough pair_freqs = pairs . map ( lambda p : ( p , 1 )) . reduceByKey ( lambda f1 , f2 : f1 + f2 ) \\ . filter ( lambda pf : pf [ 1 ] >= 100 ) pair_freqs . take ( 5 ) [(('political', 'philosophy'), 160), (('human', 'society'), 154), (('16th', 'century'), 950), (('first', 'man'), 166), (('same', 'time'), 2744)] # Computing the frequencies of the adjectives and the nouns a_freqs = pairs . map ( lambda p : ( p [ 0 ], 1 )) . reduceByKey ( lambda x , y : x + y ) n_freqs = pairs . map ( lambda p : ( p [ 1 ], 1 )) . reduceByKey ( lambda x , y : x + y ) a_freqs . take ( 5 ) [('violent', 1191), ('positive', 2302), ('self-defined', 3), ('political', 15935), ('differ', 381)] n_freqs . count () 106333 # Broadcasting the adjective and noun frequencies. #a_dict = a_freqs.collectAsMap() #a_dict = sc.parallelize(a_dict).map(lambda x: x) n_dict = sc . broadcast ( n_freqs . collectAsMap ()) a_dict = sc . broadcast ( a_freqs . collectAsMap ()) a_dict . value [ 'violent' ] 1191 from math import * # Computing the PMI for a pair. def pmi_score ( pair_freq ): w1 , w2 = pair_freq [ 0 ] f = pair_freq [ 1 ] pmi = log ( float ( f ) * N / ( a_dict . value [ w1 ] * n_dict . value [ w2 ]), 2 ) return pmi , ( w1 , w2 ) # Computing the PMI for all pairs. scored_pairs = pair_freqs . map ( pmi_score ) # Printing the most strongly associated pairs. scored_pairs . top ( 10 ) [(14.41018838546462, ('magna', 'carta')), (13.071365888694997, ('polish-lithuanian', 'Commonwealth')), (12.990597616733414, ('nitrous', 'oxide')), (12.64972604311254, ('latter-day', 'Saints')), (12.50658937509916, ('stainless', 'steel')), (12.482331020687814, ('pave', 'runway')), (12.19140721768055, ('corporal', 'punishment')), (12.183248694293388, ('capital', 'punishment')), (12.147015483562537, ('rush', 'yard')), (12.109945794428935, ('globular', 'cluster'))] Another way n_dict = n_freqs . collectAsMap () a_dict = a_freqs . collectAsMap () from math import * # Computing the PMI for a pair. def pmi_score ( pair_freq ): w1 , w2 = pair_freq [ 0 ] f = pair_freq [ 1 ] pmi = log ( float ( f ) * N / ( a_dict [ w1 ] * n_dict [ w2 ]), 2 ) return pmi , ( w1 , w2 ) scored_pairs = pair_freqs . map ( pmi_score ) scored_pairs . top ( 10 ) [(14.41018838546462, ('magna', 'carta')), (13.071365888694997, ('polish-lithuanian', 'Commonwealth')), (12.990597616733414, ('nitrous', 'oxide')), (12.64972604311254, ('latter-day', 'Saints')), (12.50658937509916, ('stainless', 'steel')), (12.482331020687814, ('pave', 'runway')), (12.19140721768055, ('corporal', 'punishment')), (12.183248694293388, ('capital', 'punishment')), (12.147015483562537, ('rush', 'yard')), (12.109945794428935, ('globular', 'cluster'))]","title":"PMI"},{"location":"MSBD5003/notebooks%20in%20class/graph/","text":"! pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 38kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 51.8MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=d4a7bf9d9286fd24237188aa6c7e6fce433cbe8062ab1b5448e6c06d960012fe Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 ! pip install graphframes Collecting graphframes Downloading https://files.pythonhosted.org/packages/0b/27/c7c7e1ced2fe9a905f865dd91faaec2ac8a8e313f511678c8ec92a41a153/graphframes-0.6-py2.py3-none-any.whl Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from graphframes) (1.18.5) Collecting nose \u001b[?25l Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 15.3MB/s \u001b[?25hInstalling collected packages: nose, graphframes Successfully installed graphframes-0.6 nose-1.3.7 ! wget https : // dl . bintray . com / spark - packages / maven / graphframes / graphframes / 0.8 . 0 - spark3 . 0 - s_2 . 12 / graphframes - 0.8 . 0 - spark3 . 0 - s_2 . 12. jar --2020-12-02 15:45:29-- https://dl.bintray.com/spark-packages/maven/graphframes/graphframes/0.8.0-spark3.0-s_2.12/graphframes-0.8.0-spark3.0-s_2.12.jar Resolving dl.bintray.com (dl.bintray.com)... 3.122.43.129, 35.157.127.85 Connecting to dl.bintray.com (dl.bintray.com)|3.122.43.129|:443... connected. HTTP request sent, awaiting response... 302 Location: https://d29vzk4ow07wi7.cloudfront.net/b62d4bb1c4fdd74c9ce5aa4adee520a7a4375c2de73487381644e5220c67c1dd?response-content-disposition=attachment%3Bfilename%3D%22graphframes-0.8.0-spark3.0-s_2.12.jar%22&Policy=eyJTdGF0ZW1lbnQiOiBbeyJSZXNvdXJjZSI6Imh0dHAqOi8vZDI5dnprNG93MDd3aTcuY2xvdWRmcm9udC5uZXQvYjYyZDRiYjFjNGZkZDc0YzljZTVhYTRhZGVlNTIwYTdhNDM3NWMyZGU3MzQ4NzM4MTY0NGU1MjIwYzY3YzFkZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0JmaWxlbmFtZSUzRCUyMmdyYXBoZnJhbWVzLTAuOC4wLXNwYXJrMy4wLXNfMi4xMi5qYXIlMjIiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2MDY5MjQ2NDl9LCJJcEFkZHJlc3MiOnsiQVdTOlNvdXJjZUlwIjoiMC4wLjAuMC8wIn19fV19&Signature=dJQvqo-UV~oRF0t93Qw-YSGpDE1pvQCuS0t~kNdiC2cDsUoJ5CdWB-RinPlpEX5TY8-oIQBnpefd7ljNKnJ5t7L1ae4ZcIQLfqBdvJdMK7AcKzytxy3cc17j7Zc80hbTMYVkhThFSJS0Loz6fvPyedCqfjI8G66Mrp46VgSpTBCqHxF0bKusZuM4w82M9d-iLmYELnyDNPHTbLIAjlMh24CcxETAKmI~AN-pZPjaGz6YMc9rFuyROe8FE4p2B5jbmjzo5LB0AHNdJll~GXtqGFKPsdJavvoCVDqbAdyJxL3XtGZdMLwbSHO6WMhbJRetQc5mEgqSLPsrNXrCvD2Q-g__&Key-Pair-Id=APKAIFKFWOMXM2UMTSFA [following] --2020-12-02 15:45:29-- https://d29vzk4ow07wi7.cloudfront.net/b62d4bb1c4fdd74c9ce5aa4adee520a7a4375c2de73487381644e5220c67c1dd?response-content-disposition=attachment%3Bfilename%3D%22graphframes-0.8.0-spark3.0-s_2.12.jar%22&Policy=eyJTdGF0ZW1lbnQiOiBbeyJSZXNvdXJjZSI6Imh0dHAqOi8vZDI5dnprNG93MDd3aTcuY2xvdWRmcm9udC5uZXQvYjYyZDRiYjFjNGZkZDc0YzljZTVhYTRhZGVlNTIwYTdhNDM3NWMyZGU3MzQ4NzM4MTY0NGU1MjIwYzY3YzFkZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0JmaWxlbmFtZSUzRCUyMmdyYXBoZnJhbWVzLTAuOC4wLXNwYXJrMy4wLXNfMi4xMi5qYXIlMjIiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2MDY5MjQ2NDl9LCJJcEFkZHJlc3MiOnsiQVdTOlNvdXJjZUlwIjoiMC4wLjAuMC8wIn19fV19&Signature=dJQvqo-UV~oRF0t93Qw-YSGpDE1pvQCuS0t~kNdiC2cDsUoJ5CdWB-RinPlpEX5TY8-oIQBnpefd7ljNKnJ5t7L1ae4ZcIQLfqBdvJdMK7AcKzytxy3cc17j7Zc80hbTMYVkhThFSJS0Loz6fvPyedCqfjI8G66Mrp46VgSpTBCqHxF0bKusZuM4w82M9d-iLmYELnyDNPHTbLIAjlMh24CcxETAKmI~AN-pZPjaGz6YMc9rFuyROe8FE4p2B5jbmjzo5LB0AHNdJll~GXtqGFKPsdJavvoCVDqbAdyJxL3XtGZdMLwbSHO6WMhbJRetQc5mEgqSLPsrNXrCvD2Q-g__&Key-Pair-Id=APKAIFKFWOMXM2UMTSFA Resolving d29vzk4ow07wi7.cloudfront.net (d29vzk4ow07wi7.cloudfront.net)... 54.240.168.145, 54.240.168.82, 54.240.168.77, ... Connecting to d29vzk4ow07wi7.cloudfront.net (d29vzk4ow07wi7.cloudfront.net)|54.240.168.145|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 243265 (238K) [multipart/form-data] Saving to: \u2018graphframes-0.8.0-spark3.0-s_2.12.jar\u2019 graphframes-0.8.0-s 100%[===================>] 237.56K 563KB/s in 0.4s 2020-12-02 15:45:31 (563 KB/s) - \u2018graphframes-0.8.0-spark3.0-s_2.12.jar\u2019 saved [243265/243265] os . environ [ 'PYSPARK_SUBMIT_ARGS' ] = '--packages graphframes:graphframes:0.8.1-spark3.0-s_2.12 pyspark-shell' --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-15-4d1ba1edf8bb> in <module>() ----> 1 os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages graphframes:graphframes:0.8.1-spark3.0-s_2.12 pyspark-shell' NameError: name 'os' is not defined from pyspark.context import SparkContext from pyspark.sql.session import SparkSession from pyspark.sql.types import Row sc = SparkContext . getOrCreate () spark = SparkSession ( sc ) from graphframes import * sc . addPyFile ( \"https://dl.bintray.com/spark-packages/maven/graphframes/graphframes/0.8.0-spark3.0-s_2.12/graphframes-0.8.0-spark3.0-s_2.12.jar\" ) # Vertics DataFrame v = spark . createDataFrame ([ ( \"a\" , \"Alice\" , 34 ), ( \"b\" , \"Bob\" , 36 ), ( \"c\" , \"Charlie\" , 37 ), ( \"d\" , \"David\" , 29 ), ( \"e\" , \"Esther\" , 32 ), ( \"f\" , \"Fanny\" , 38 ), ( \"g\" , \"Gabby\" , 60 ) ], [ \"id\" , \"name\" , \"age\" ]) # Edges DataFrame e = spark . createDataFrame ([ ( \"a\" , \"b\" , \"friend\" ), ( \"b\" , \"c\" , \"follow\" ), ( \"c\" , \"b\" , \"follow\" ), ( \"f\" , \"c\" , \"follow\" ), ( \"e\" , \"f\" , \"follow\" ), ( \"e\" , \"d\" , \"friend\" ), ( \"d\" , \"a\" , \"friend\" ), ( \"a\" , \"e\" , \"friend\" ), ( \"g\" , \"e\" , \"follow\" ) ], [ \"src\" , \"dst\" , \"relationship\" ]) # Create a GraphFrame g = GraphFrame ( v , e ) g . vertices . show () g . edges . show () --------------------------------------------------------------------------- Py4JJavaError Traceback (most recent call last) <ipython-input-14-eccf3cb921e3> in <module>() 24 25 # Create a GraphFrame ---> 26 g = GraphFrame(v, e) 27 28 g.vertices.show() /usr/local/lib/python3.6/dist-packages/graphframes/graphframe.py in __init__(self, v, e) 63 self._sqlContext = v.sql_ctx 64 self._sc = self._sqlContext._sc ---> 65 self._jvm_gf_api = _java_api(self._sc) 66 67 self.ID = self._jvm_gf_api.ID() /usr/local/lib/python3.6/dist-packages/graphframes/graphframe.py in _java_api(jsc) 36 def _java_api(jsc): 37 javaClassName = \"org.graphframes.GraphFramePythonAPI\" ---> 38 return jsc._jvm.Thread.currentThread().getContextClassLoader().loadClass(javaClassName) \\ 39 .newInstance() 40 /usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py in __call__(self, *args) 1303 answer = self.gateway_client.send_command(command) 1304 return_value = get_return_value( -> 1305 answer, self.gateway_client, self.target_id, self.name) 1306 1307 for temp_arg in temp_args: /usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py in deco(*a, **kw) 126 def deco(*a, **kw): 127 try: --> 128 return f(*a, **kw) 129 except py4j.protocol.Py4JJavaError as e: 130 converted = convert_exception(e.java_exception) /usr/local/lib/python3.6/dist-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name) 326 raise Py4JJavaError( 327 \"An error occurred while calling {0}{1}{2}.\\n\". --> 328 format(target_id, \".\", name), value) 329 else: 330 raise Py4JError( Py4JJavaError: An error occurred while calling o148.loadClass. : java.lang.ClassNotFoundException: org.graphframes.GraphFramePythonAPI at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:471) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at py4j.Gateway.invoke(Gateway.java:282) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:238) at java.base/java.lang.Thread.run(Thread.java:834) 3 # g.vertices and g.edges are just DataFrames # You can use any DataFrame API on them g . edges . filter ( \"src = 'a'\" ) . show () +---+---+------------+ |src|dst|relationship| +---+---+------------+ | a| b| friend| | a| e| friend| +---+---+------------+ g . edges . filter ( \"src = 'a'\" ) . count () 2 # Count the number of followers of c. # This queries the edge DataFrame. print ( g . edges . filter ( \"relationship = 'follow' and dst = 'c'\" ) . count ()) 2 # A GraphFrame has additional attributes g . outDegrees . show () +---+---------+ | id|outDegree| +---+---------+ | g| 1| | f| 1| | e| 2| | d| 1| | c| 1| | b| 1| | a| 2| +---+---------+ g . inDegrees . show () +---+--------+ | id|inDegree| +---+--------+ | f| 1| | e| 2| | d| 1| | c| 2| | b| 2| | a| 1| +---+--------+ g . inDegrees . explain () == Physical Plan == *(2) HashAggregate(keys=[dst#45], functions=[count(1)]) +- Exchange hashpartitioning(dst#45, 200), true, [id=#171] +- *(1) HashAggregate(keys=[dst#45], functions=[partial_count(1)]) +- *(1) Project [dst#45] +- *(1) Scan ExistingRDD[src#44,dst#45,relationship#46] myInDegrees = g . edges . groupBy ( 'dst' ) . count () \\ . withColumnRenamed ( 'dst' , 'id' ) . withColumnRenamed ( 'count' , 'inDegree' ) myInDegrees . show () +---+--------+ | id|inDegree| +---+--------+ | f| 1| | e| 2| | d| 1| | c| 2| | b| 2| | a| 1| +---+--------+ myInDegrees . explain () == Physical Plan == *(2) HashAggregate(keys=[dst#45], functions=[count(1)]) +- Exchange hashpartitioning(dst#45, 200), true, [id=#218] +- *(1) HashAggregate(keys=[dst#45], functions=[partial_count(1)]) +- *(1) Project [dst#45] +- *(1) Scan ExistingRDD[src#44,dst#45,relationship#46] print ( g . inDegrees . storageLevel ) Serialized 1x Replicated g . inDegrees . cache () DataFrame[id: string, inDegree: int] print ( g . inDegrees . storageLevel ) Disk Memory Deserialized 1x Replicated print ( g . vertices . storageLevel ) Serialized 1x Replicated g . cache () GraphFrame(v:[id: string, name: string ... 1 more field], e:[src: string, dst: string ... 1 more field]) print ( g . vertices . storageLevel ) print ( g . edges . storageLevel ) Disk Memory Deserialized 1x Replicated Disk Memory Deserialized 1x Replicated # A triplet view of the graph g . triplets . show () +----------------+--------------+----------------+ | src| edge| dst| +----------------+--------------+----------------+ | [e, Esther, 32]|[e, f, follow]| [f, Fanny, 38]| | [g, Gabby, 60]|[g, e, follow]| [e, Esther, 32]| | [a, Alice, 34]|[a, e, friend]| [e, Esther, 32]| | [e, Esther, 32]|[e, d, friend]| [d, David, 29]| | [f, Fanny, 38]|[f, c, follow]|[c, Charlie, 37]| | [b, Bob, 36]|[b, c, follow]|[c, Charlie, 37]| |[c, Charlie, 37]|[c, b, follow]| [b, Bob, 36]| | [a, Alice, 34]|[a, b, friend]| [b, Bob, 36]| | [d, David, 29]|[d, a, friend]| [a, Alice, 34]| +----------------+--------------+----------------+ g . triplets . explain () == Physical Plan == *(9) Project [src#217, edge#215, dst#219] +- *(9) SortMergeJoin [edge#215.dst], [dst#219.id], Inner :- *(6) Sort [edge#215.dst ASC NULLS FIRST], false, 0 : +- Exchange hashpartitioning(edge#215.dst, 200), true, [id=#312] : +- *(5) SortMergeJoin [edge#215.src], [src#217.id], Inner : :- *(2) Sort [edge#215.src ASC NULLS FIRST], false, 0 : : +- Exchange hashpartitioning(edge#215.src, 200), true, [id=#297] : : +- *(1) Project [struct(src, src#44, dst, dst#45, relationship, relationship#46) AS edge#215] : : +- InMemoryTableScan [dst#45, relationship#46, src#44] : : +- InMemoryRelation [src#44, dst#45, relationship#46], StorageLevel(disk, memory, deserialized, 1 replicas) : : +- *(1) Scan ExistingRDD[src#44,dst#45,relationship#46] : +- *(4) Sort [src#217.id ASC NULLS FIRST], false, 0 : +- Exchange hashpartitioning(src#217.id, 200), true, [id=#305] : +- *(3) Project [struct(id, id#38, name, name#39, age, age#40L) AS src#217] : +- InMemoryTableScan [age#40L, id#38, name#39] : +- InMemoryRelation [id#38, name#39, age#40L], StorageLevel(disk, memory, deserialized, 1 replicas) : +- *(1) Scan ExistingRDD[id#38,name#39,age#40L] +- *(8) Sort [dst#219.id ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(dst#219.id, 200), true, [id=#320] +- *(7) Project [struct(id, id#38, name, name#39, age, age#40L) AS dst#219] +- InMemoryTableScan [age#40L, id#38, name#39] +- InMemoryRelation [id#38, name#39, age#40L], StorageLevel(disk, memory, deserialized, 1 replicas) +- *(1) Scan ExistingRDD[id#38,name#39,age#40L] Motif Finding \u00b6 # Search for pairs of vertices with edges in both directions between them. motifs = g . find ( \"(a)-[]->(b); (b)-[]->(a)\" ) . filter ( 'a.id < b.id' ) motifs . show () +------------+----------------+ | a| b| +------------+----------------+ |[b, Bob, 36]|[c, Charlie, 37]| +------------+----------------+ # Find triangles triangles = g . find ( \"(a)-[]->(b); (b)-[]->(c); (c)-[]->(a)\" ) triangles = triangles . filter ( \"a.id < b.id AND a.id < c.id\" ) triangles . show () +--------------+---------------+--------------+ | a| b| c| +--------------+---------------+--------------+ |[a, Alice, 34]|[e, Esther, 32]|[d, David, 29]| +--------------+---------------+--------------+ triangles . explain () == Physical Plan == *(6) Project [a#630, b#632, c#657] +- *(6) BroadcastHashJoin [c#657.id, a#630.id], [__tmp-6526019406657860729#687.src, __tmp-6526019406657860729#687.dst], Inner, BuildRight :- *(6) Project [a#630, b#632, c#657] : +- *(6) BroadcastHashJoin [__tmp-430217833014886237#655.dst], [c#657.id], Inner, BuildRight, (a#630.id < c#657.id) : :- *(6) BroadcastHashJoin [b#632.id], [__tmp-430217833014886237#655.src], Inner, BuildRight : : :- *(6) Project [a#630, b#632] : : : +- *(6) BroadcastHashJoin [__tmp-1043886091038848698#628.dst], [b#632.id], Inner, BuildRight, (a#630.id < b#632.id) : : : :- *(6) BroadcastHashJoin [__tmp-1043886091038848698#628.src], [a#630.id], Inner, BuildRight : : : : :- *(6) Project [struct(src, src#44, dst, dst#45, relationship, relationship#46) AS __tmp-1043886091038848698#628] : : : : : +- InMemoryTableScan [dst#45, relationship#46, src#44] : : : : : +- InMemoryRelation [src#44, dst#45, relationship#46], StorageLevel(disk, memory, deserialized, 1 replicas) : : : : : +- *(1) Scan ExistingRDD[src#44,dst#45,relationship#46] : : : : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, struct<id:string,name:string,age:bigint>, false].id)), [id=#628] : : : : +- *(1) Project [struct(id, id#38, name, name#39, age, age#40L) AS a#630] : : : : +- InMemoryTableScan [age#40L, id#38, name#39] : : : : +- InMemoryRelation [id#38, name#39, age#40L], StorageLevel(disk, memory, deserialized, 1 replicas) : : : : +- *(1) Scan ExistingRDD[id#38,name#39,age#40L] : : : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, struct<id:string,name:string,age:bigint>, false].id)), [id=#634] : : : +- *(2) Project [struct(id, id#38, name, name#39, age, age#40L) AS b#632] : : : +- InMemoryTableScan [age#40L, id#38, name#39] : : : +- InMemoryRelation [id#38, name#39, age#40L], StorageLevel(disk, memory, deserialized, 1 replicas) : : : +- *(1) Scan ExistingRDD[id#38,name#39,age#40L] : : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, struct<src:string,dst:string,relationship:string>, false].src)), [id=#641] : : +- *(3) Project [struct(src, src#44, dst, dst#45, relationship, relationship#46) AS __tmp-430217833014886237#655] : : +- InMemoryTableScan [dst#45, relationship#46, src#44] : : +- InMemoryRelation [src#44, dst#45, relationship#46], StorageLevel(disk, memory, deserialized, 1 replicas) : : +- *(1) Scan ExistingRDD[src#44,dst#45,relationship#46] : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, struct<id:string,name:string,age:bigint>, false].id)), [id=#647] : +- *(4) Project [struct(id, id#38, name, name#39, age, age#40L) AS c#657] : +- InMemoryTableScan [age#40L, id#38, name#39] : +- InMemoryRelation [id#38, name#39, age#40L], StorageLevel(disk, memory, deserialized, 1 replicas) : +- *(1) Scan ExistingRDD[id#38,name#39,age#40L] +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, struct<src:string,dst:string,relationship:string>, false].src, input[0, struct<src:string,dst:string,relationship:string>, false].dst)), [id=#654] +- *(5) Project [struct(src, src#44, dst, dst#45, relationship, relationship#46) AS __tmp-6526019406657860729#687] +- InMemoryTableScan [dst#45, relationship#46, src#44] +- InMemoryRelation [src#44, dst#45, relationship#46], StorageLevel(disk, memory, deserialized, 1 replicas) +- *(1) Scan ExistingRDD[src#44,dst#45,relationship#46] # Negation oneway = g . find ( \"(a)-[]->(b); !(b)-[]->(a)\" ) oneway . show () +---------------+----------------+ | a| b| +---------------+----------------+ | [a, Alice, 34]| [e, Esther, 32]| |[e, Esther, 32]| [d, David, 29]| | [a, Alice, 34]| [b, Bob, 36]| | [g, Gabby, 60]| [e, Esther, 32]| |[e, Esther, 32]| [f, Fanny, 38]| | [f, Fanny, 38]|[c, Charlie, 37]| | [d, David, 29]| [a, Alice, 34]| +---------------+----------------+ # Find vertices without incoming edges: g . find ( \"!()-[]->(a)\" ) . show () +--------------+ | a| +--------------+ |[g, Gabby, 60]| +--------------+ # More meaningful queries can be expressed by applying filters. # Question: where is this filter applied? g . find ( \"(a)-[e]->(b); (b)-[]->(a)\" ) . filter ( \"b.age > 36\" ) . show () +------------+--------------+----------------+ | a| e| b| +------------+--------------+----------------+ |[b, Bob, 36]|[b, c, follow]|[c, Charlie, 37]| +------------+--------------+----------------+ +------------+ |relationship| +------------+ | follow| +------------+ g . find ( \"(a)-[]->(b); (b)-[]->(a)\" ) . filter ( \"b.age > 36\" ) . explain () == Physical Plan == *(4) Project [a#2584, b#2586] +- *(4) BroadcastHashJoin [b#2586.id, a#2584.id], [__tmp2506060614762666678#2609.src, __tmp2506060614762666678#2609.dst], Inner, BuildRight :- *(4) Project [a#2584, b#2586] : +- *(4) BroadcastHashJoin [__tmp-3851898762290097694#2582.dst], [b#2586.id], Inner, BuildRight : :- *(4) BroadcastHashJoin [__tmp-3851898762290097694#2582.src], [a#2584.id], Inner, BuildRight : : :- *(4) Project [struct(src, src#44, dst, dst#45, relationship, relationship#46) AS __tmp-3851898762290097694#2582] : : : +- InMemoryTableScan [dst#45, relationship#46, src#44] : : : +- InMemoryRelation [src#44, dst#45, relationship#46], StorageLevel(disk, memory, deserialized, 1 replicas) : : : +- *(1) Scan ExistingRDD[src#44,dst#45,relationship#46] : : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, struct<id:string,name:string,age:bigint>, false].id)), [id=#1356] : : +- *(1) Project [struct(id, id#38, name, name#39, age, age#40L) AS a#2584] : : +- InMemoryTableScan [age#40L, id#38, name#39] : : +- InMemoryRelation [id#38, name#39, age#40L], StorageLevel(disk, memory, deserialized, 1 replicas) : : +- *(1) Scan ExistingRDD[id#38,name#39,age#40L] : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, struct<id:string,name:string,age:bigint>, false].id)), [id=#1363] : +- *(2) Project [struct(id, id#38, name, name#39, age, age#40L) AS b#2586] : +- *(2) Filter (isnotnull(age#40L) AND (age#40L > 36)) : +- InMemoryTableScan [age#40L, id#38, name#39], [isnotnull(age#40L), (age#40L > 36)] : +- InMemoryRelation [id#38, name#39, age#40L], StorageLevel(disk, memory, deserialized, 1 replicas) : +- *(1) Scan ExistingRDD[id#38,name#39,age#40L] +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, struct<src:string,dst:string,relationship:string>, false].src, input[0, struct<src:string,dst:string,relationship:string>, false].dst)), [id=#1370] +- *(3) Project [struct(src, src#44, dst, dst#45, relationship, relationship#46) AS __tmp2506060614762666678#2609] +- InMemoryTableScan [dst#45, relationship#46, src#44] +- InMemoryRelation [src#44, dst#45, relationship#46], StorageLevel(disk, memory, deserialized, 1 replicas) +- *(1) Scan ExistingRDD[src#44,dst#45,relationship#46] # Find chains of 4 vertices such that at least 2 of the 3 edges are \"friend\" relationships. # The when function is similar to the CASE WHEN in SQL chain4 = g . find ( \"(a)-[e1]->(b); (b)-[e2]->(c); (c)-[e3]->(d)\" ) . where ( 'a!=d AND a!=c AND b!=d' ) friendTo1 = lambda e : when ( e [ 'relationship' ] == 'friend' , 1 ) . otherwise ( 0 ) chain4 . select ( '*' , friendTo1 ( chain4 [ 'e1' ]) . alias ( 'f1' ), \\ friendTo1 ( chain4 [ 'e2' ]) . alias ( 'f2' ), \\ friendTo1 ( chain4 [ 'e3' ]) . alias ( 'f3' )) \\ . where ( 'f1 + f2 + f3 >= 2' ) . select ( 'a' , 'b' , 'c' , 'd' ) . show () +---------------+--------------+---------------+--------------+---------------+--------------+----------------+---+---+---+ | a| e1| b| e2| c| e3| d| f1| f2| f3| +---------------+--------------+---------------+--------------+---------------+--------------+----------------+---+---+---+ | [d, David, 29]|[d, a, friend]| [a, Alice, 34]|[a, e, friend]|[e, Esther, 32]|[e, f, follow]| [f, Fanny, 38]| 1| 1| 0| | [d, David, 29]|[d, a, friend]| [a, Alice, 34]|[a, b, friend]| [b, Bob, 36]|[b, c, follow]|[c, Charlie, 37]| 1| 1| 0| |[e, Esther, 32]|[e, d, friend]| [d, David, 29]|[d, a, friend]| [a, Alice, 34]|[a, b, friend]| [b, Bob, 36]| 1| 1| 1| | [g, Gabby, 60]|[g, e, follow]|[e, Esther, 32]|[e, d, friend]| [d, David, 29]|[d, a, friend]| [a, Alice, 34]| 0| 1| 1| +---------------+--------------+---------------+--------------+---------------+--------------+----------------+---+---+---+ Subgraphs \u00b6 # Select subgraph of users older than 30, and relationships of type \"friend\". # Drop isolated vertices (users) which are not contained in any edges (relationships). g1 = g . filterVertices ( \"age > 30\" ) . filterEdges ( \"relationship = 'friend'\" ) \\ . dropIsolatedVertices () g1 . vertices . show () g1 . edges . show () +---+------+---+ | id| name|age| +---+------+---+ | e|Esther| 32| | b| Bob| 36| | a| Alice| 34| +---+------+---+ +---+---+------------+ |src|dst|relationship| +---+---+------------+ | a| e| friend| | a| b| friend| +---+---+------------+ # Select subgraph based on edges \"e\" of type \"follow\" # pointing from a younger user \"a\" to an older user \"b\". paths = g . find ( \"(a)-[e]->(b)\" ) \\ . filter ( \"e.relationship = 'follow'\" ) \\ . filter ( \"a.age < b.age\" ) # \"paths\" contains vertex info. Extract the edges. e2 = paths . select ( \"e.*\" ) # Construct the subgraph g2 = GraphFrame ( g . vertices , e2 ) . dropIsolatedVertices () g2 . vertices . show () g2 . edges . show () +---+-------+---+ | id| name|age| +---+-------+---+ | f| Fanny| 38| | e| Esther| 32| | c|Charlie| 37| | b| Bob| 36| +---+-------+---+ +---+---+------------+ |src|dst|relationship| +---+---+------------+ | e| f| follow| | b| c| follow| +---+---+------------+ BFS \u00b6 # Starting vertex is 'a' layers = [ g . vertices . select ( 'id' ) . where ( \"id = 'a'\" )] visited = layers [ 0 ] while layers [ - 1 ] . count () > 0 : # From the current layer, get all the one-hop neighbors d1 = layers [ - 1 ] . join ( g . edges , layers [ - 1 ][ 'id' ] == g . edges [ 'src' ]) # Rename the column as 'id', and remove visited verices and duplicates d2 = d1 . select ( d1 [ 'dst' ] . alias ( 'id' )) \\ . subtract ( visited ) . distinct () . cache () layers += [ d2 ] visited = visited . union ( layers [ - 1 ]) . cache () layers [ 0 ] . show () +---+ | id| +---+ | a| +---+ layers [ 1 ] . show () +---+ | id| +---+ | e| | b| +---+ layers [ 2 ] . show () +---+ | id| +---+ | f| | d| | c| +---+ layers [ 3 ] . show () +---+ | id| +---+ +---+ # GraphFrames provides own BFS: paths = g . bfs ( \"id = 'a'\" , \"age > 36\" ) paths . show () +--------------+--------------+---------------+--------------+----------------+ | from| e0| v1| e1| to| +--------------+--------------+---------------+--------------+----------------+ |[a, Alice, 34]|[a, b, friend]| [b, Bob, 36]|[b, c, follow]|[c, Charlie, 37]| |[a, Alice, 34]|[a, e, friend]|[e, Esther, 32]|[e, f, follow]| [f, Fanny, 38]| +--------------+--------------+---------------+--------------+----------------+ List Ranking \u00b6 # -1 denotes end of list data = [( 0 , 5 ), ( 1 , 0 ), ( 3 , 4 ), ( 4 , 6 ), ( 5 , - 1 ), ( 6 , 1 )] e = spark . createDataFrame ( data , [ 'src' , 'dst' ]) v = e . select ( col ( 'src' ) . alias ( 'id' ), when ( e . dst == - 1 , 0 ) . otherwise ( 1 ) . alias ( 'd' )) v1 = spark . createDataFrame ([( - 1 , 0 )], [ 'id' , 'd' ]) v = v . union ( v1 ) v . show () e . show () +---+---+ | id| d| +---+---+ | 0| 1| | 1| 1| | 3| 1| | 4| 1| | 5| 0| | 6| 1| | -1| 0| +---+---+ +---+---+ |src|dst| +---+---+ | 0| 5| | 1| 0| | 3| 4| | 4| 6| | 5| -1| | 6| 1| +---+---+ while e . filter ( 'dst != -1' ) . count () > 0 : g = GraphFrame ( v , e ) g . cache () v = g . triplets . select ( col ( 'src.id' ) . alias ( 'id' ), ( col ( 'src.d' ) + col ( 'dst.d' )) . alias ( 'd' )) \\ . union ( v1 ) e = g . find ( '(a)-[]->(b); (b)-[]->(c)' ) \\ . select ( col ( 'a.id' ) . alias ( 'src' ), col ( 'c.id' ) . alias ( 'dst' )) \\ . union ( e . filter ( 'dst = -1' )) v . show () +---+---+ | id| d| +---+---+ | 0| 1| | 1| 2| | 3| 5| | 4| 4| | 5| 0| | 6| 3| | -1| 0| +---+---+ Message passing via AggregateMessages \u00b6 from pyspark.sql.functions import coalesce , col , lit , sum , when , min , max from graphframes.lib import AggregateMessages as AM # AggregateMessages has the following members: src, dst, edge, msg # For each user, sum the ages of the adjacent users. agg = g . aggregateMessages ( sum ( AM . msg ) . alias ( \"summedAges\" ), sendToSrc = AM . dst [ 'age' ], sendToDst = AM . src [ 'age' ]) agg . show () +---+----------+ | id|summedAges| +---+----------+ | g| 32| | f| 69| | e| 161| | d| 66| | c| 110| | b| 108| | a| 97| +---+----------+ The Pregel Model for Graph Computation \u00b6 # Pagerank in the Pregel model from pyspark.sql.functions import coalesce , col , lit , sum , when , min from graphframes.lib import Pregel # Need to set up a directory for Pregel computation sc . setCheckpointDir ( \"checkpoint\" ) ''' Use builder pattern to describe the operations. Call run() to start a run. It returns a DataFrame of vertices from the last iteration. When a run starts, it expands the vertices DataFrame using column expressions defined by withVertexColumn(). Those additional vertex properties can be changed during Pregel iterations. In each Pregel iteration, there are three phases: * Given each edge triplet, generate messages and specify target vertices to send, described by sendMsgToDst() and sendMsgToSrc(). * Aggregate messages by target vertex IDs, described by aggMsgs(). * Update additional vertex properties based on aggregated messages and states from previous iteration, described by withVertexColumn(). ''' v = g . outDegrees g = GraphFrame ( v , e ) ranks = g . pregel \\ . setMaxIter ( 5 ) \\ . sendMsgToDst ( Pregel . src ( \"rank\" ) / Pregel . src ( \"outDegree\" )) \\ . aggMsgs ( sum ( Pregel . msg ())) \\ . withVertexColumn ( \"rank\" , lit ( 1.0 ), \\ coalesce ( Pregel . msg (), lit ( 0.0 )) * lit ( 0.85 ) + lit ( 0.15 )) \\ . run () ranks . show () # pyspark.sql.functions.coalesce(*cols): Returns the first column that is not null. # Not to be confused with spark.sql.coalesce(numPartitions) +---+---------+-------------------+ | id|outDegree| rank| +---+---------+-------------------+ | g| 1| 0.15| | f| 1|0.41104330078124995| | e| 2| 0.5032932031249999| | d| 1|0.41104330078124995| | c| 1| 2.780783203124999| | b| 1| 2.2680220312499997| | a| 2| 0.4758149609375| +---+---------+-------------------+ # BFS in the Pregel model g = GraphFrame ( v , e ) dist = g . pregel \\ . sendMsgToDst ( when ( Pregel . src ( 'active' ), Pregel . src ( 'd' ) + 1 )) \\ . aggMsgs ( min ( Pregel . msg ())) \\ . withVertexColumn ( 'd' , when ( v [ 'id' ] == 'a' , 0 ) . otherwise ( 99999 ), \\ when ( Pregel . msg () < col ( 'd' ), Pregel . msg ()) . otherwise ( col ( 'd' ))) \\ . withVertexColumn ( 'active' , when ( v [ 'id' ] == 'a' , True ) . otherwise ( False ), \\ when ( Pregel . msg () < col ( 'd' ), True ) . otherwise ( False )) \\ . run () dist . show () +---+---------+-----+------+ | id|outDegree| d|active| +---+---------+-----+------+ | g| 1|99999| false| | f| 1| 2| false| | e| 2| 1| false| | d| 1| 2| false| | c| 1| 2| false| | b| 1| 1| false| | a| 2| 0| false| +---+---------+-----+------+","title":"Graph"},{"location":"MSBD5003/notebooks%20in%20class/graph/#motif-finding","text":"# Search for pairs of vertices with edges in both directions between them. motifs = g . find ( \"(a)-[]->(b); (b)-[]->(a)\" ) . filter ( 'a.id < b.id' ) motifs . show () +------------+----------------+ | a| b| +------------+----------------+ |[b, Bob, 36]|[c, Charlie, 37]| +------------+----------------+ # Find triangles triangles = g . find ( \"(a)-[]->(b); (b)-[]->(c); (c)-[]->(a)\" ) triangles = triangles . filter ( \"a.id < b.id AND a.id < c.id\" ) triangles . show () +--------------+---------------+--------------+ | a| b| c| +--------------+---------------+--------------+ |[a, Alice, 34]|[e, Esther, 32]|[d, David, 29]| +--------------+---------------+--------------+ triangles . explain () == Physical Plan == *(6) Project [a#630, b#632, c#657] +- *(6) BroadcastHashJoin [c#657.id, a#630.id], [__tmp-6526019406657860729#687.src, __tmp-6526019406657860729#687.dst], Inner, BuildRight :- *(6) Project [a#630, b#632, c#657] : +- *(6) BroadcastHashJoin [__tmp-430217833014886237#655.dst], [c#657.id], Inner, BuildRight, (a#630.id < c#657.id) : :- *(6) BroadcastHashJoin [b#632.id], [__tmp-430217833014886237#655.src], Inner, BuildRight : : :- *(6) Project [a#630, b#632] : : : +- *(6) BroadcastHashJoin [__tmp-1043886091038848698#628.dst], [b#632.id], Inner, BuildRight, (a#630.id < b#632.id) : : : :- *(6) BroadcastHashJoin [__tmp-1043886091038848698#628.src], [a#630.id], Inner, BuildRight : : : : :- *(6) Project [struct(src, src#44, dst, dst#45, relationship, relationship#46) AS __tmp-1043886091038848698#628] : : : : : +- InMemoryTableScan [dst#45, relationship#46, src#44] : : : : : +- InMemoryRelation [src#44, dst#45, relationship#46], StorageLevel(disk, memory, deserialized, 1 replicas) : : : : : +- *(1) Scan ExistingRDD[src#44,dst#45,relationship#46] : : : : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, struct<id:string,name:string,age:bigint>, false].id)), [id=#628] : : : : +- *(1) Project [struct(id, id#38, name, name#39, age, age#40L) AS a#630] : : : : +- InMemoryTableScan [age#40L, id#38, name#39] : : : : +- InMemoryRelation [id#38, name#39, age#40L], StorageLevel(disk, memory, deserialized, 1 replicas) : : : : +- *(1) Scan ExistingRDD[id#38,name#39,age#40L] : : : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, struct<id:string,name:string,age:bigint>, false].id)), [id=#634] : : : +- *(2) Project [struct(id, id#38, name, name#39, age, age#40L) AS b#632] : : : +- InMemoryTableScan [age#40L, id#38, name#39] : : : +- InMemoryRelation [id#38, name#39, age#40L], StorageLevel(disk, memory, deserialized, 1 replicas) : : : +- *(1) Scan ExistingRDD[id#38,name#39,age#40L] : : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, struct<src:string,dst:string,relationship:string>, false].src)), [id=#641] : : +- *(3) Project [struct(src, src#44, dst, dst#45, relationship, relationship#46) AS __tmp-430217833014886237#655] : : +- InMemoryTableScan [dst#45, relationship#46, src#44] : : +- InMemoryRelation [src#44, dst#45, relationship#46], StorageLevel(disk, memory, deserialized, 1 replicas) : : +- *(1) Scan ExistingRDD[src#44,dst#45,relationship#46] : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, struct<id:string,name:string,age:bigint>, false].id)), [id=#647] : +- *(4) Project [struct(id, id#38, name, name#39, age, age#40L) AS c#657] : +- InMemoryTableScan [age#40L, id#38, name#39] : +- InMemoryRelation [id#38, name#39, age#40L], StorageLevel(disk, memory, deserialized, 1 replicas) : +- *(1) Scan ExistingRDD[id#38,name#39,age#40L] +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, struct<src:string,dst:string,relationship:string>, false].src, input[0, struct<src:string,dst:string,relationship:string>, false].dst)), [id=#654] +- *(5) Project [struct(src, src#44, dst, dst#45, relationship, relationship#46) AS __tmp-6526019406657860729#687] +- InMemoryTableScan [dst#45, relationship#46, src#44] +- InMemoryRelation [src#44, dst#45, relationship#46], StorageLevel(disk, memory, deserialized, 1 replicas) +- *(1) Scan ExistingRDD[src#44,dst#45,relationship#46] # Negation oneway = g . find ( \"(a)-[]->(b); !(b)-[]->(a)\" ) oneway . show () +---------------+----------------+ | a| b| +---------------+----------------+ | [a, Alice, 34]| [e, Esther, 32]| |[e, Esther, 32]| [d, David, 29]| | [a, Alice, 34]| [b, Bob, 36]| | [g, Gabby, 60]| [e, Esther, 32]| |[e, Esther, 32]| [f, Fanny, 38]| | [f, Fanny, 38]|[c, Charlie, 37]| | [d, David, 29]| [a, Alice, 34]| +---------------+----------------+ # Find vertices without incoming edges: g . find ( \"!()-[]->(a)\" ) . show () +--------------+ | a| +--------------+ |[g, Gabby, 60]| +--------------+ # More meaningful queries can be expressed by applying filters. # Question: where is this filter applied? g . find ( \"(a)-[e]->(b); (b)-[]->(a)\" ) . filter ( \"b.age > 36\" ) . show () +------------+--------------+----------------+ | a| e| b| +------------+--------------+----------------+ |[b, Bob, 36]|[b, c, follow]|[c, Charlie, 37]| +------------+--------------+----------------+ +------------+ |relationship| +------------+ | follow| +------------+ g . find ( \"(a)-[]->(b); (b)-[]->(a)\" ) . filter ( \"b.age > 36\" ) . explain () == Physical Plan == *(4) Project [a#2584, b#2586] +- *(4) BroadcastHashJoin [b#2586.id, a#2584.id], [__tmp2506060614762666678#2609.src, __tmp2506060614762666678#2609.dst], Inner, BuildRight :- *(4) Project [a#2584, b#2586] : +- *(4) BroadcastHashJoin [__tmp-3851898762290097694#2582.dst], [b#2586.id], Inner, BuildRight : :- *(4) BroadcastHashJoin [__tmp-3851898762290097694#2582.src], [a#2584.id], Inner, BuildRight : : :- *(4) Project [struct(src, src#44, dst, dst#45, relationship, relationship#46) AS __tmp-3851898762290097694#2582] : : : +- InMemoryTableScan [dst#45, relationship#46, src#44] : : : +- InMemoryRelation [src#44, dst#45, relationship#46], StorageLevel(disk, memory, deserialized, 1 replicas) : : : +- *(1) Scan ExistingRDD[src#44,dst#45,relationship#46] : : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, struct<id:string,name:string,age:bigint>, false].id)), [id=#1356] : : +- *(1) Project [struct(id, id#38, name, name#39, age, age#40L) AS a#2584] : : +- InMemoryTableScan [age#40L, id#38, name#39] : : +- InMemoryRelation [id#38, name#39, age#40L], StorageLevel(disk, memory, deserialized, 1 replicas) : : +- *(1) Scan ExistingRDD[id#38,name#39,age#40L] : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, struct<id:string,name:string,age:bigint>, false].id)), [id=#1363] : +- *(2) Project [struct(id, id#38, name, name#39, age, age#40L) AS b#2586] : +- *(2) Filter (isnotnull(age#40L) AND (age#40L > 36)) : +- InMemoryTableScan [age#40L, id#38, name#39], [isnotnull(age#40L), (age#40L > 36)] : +- InMemoryRelation [id#38, name#39, age#40L], StorageLevel(disk, memory, deserialized, 1 replicas) : +- *(1) Scan ExistingRDD[id#38,name#39,age#40L] +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, struct<src:string,dst:string,relationship:string>, false].src, input[0, struct<src:string,dst:string,relationship:string>, false].dst)), [id=#1370] +- *(3) Project [struct(src, src#44, dst, dst#45, relationship, relationship#46) AS __tmp2506060614762666678#2609] +- InMemoryTableScan [dst#45, relationship#46, src#44] +- InMemoryRelation [src#44, dst#45, relationship#46], StorageLevel(disk, memory, deserialized, 1 replicas) +- *(1) Scan ExistingRDD[src#44,dst#45,relationship#46] # Find chains of 4 vertices such that at least 2 of the 3 edges are \"friend\" relationships. # The when function is similar to the CASE WHEN in SQL chain4 = g . find ( \"(a)-[e1]->(b); (b)-[e2]->(c); (c)-[e3]->(d)\" ) . where ( 'a!=d AND a!=c AND b!=d' ) friendTo1 = lambda e : when ( e [ 'relationship' ] == 'friend' , 1 ) . otherwise ( 0 ) chain4 . select ( '*' , friendTo1 ( chain4 [ 'e1' ]) . alias ( 'f1' ), \\ friendTo1 ( chain4 [ 'e2' ]) . alias ( 'f2' ), \\ friendTo1 ( chain4 [ 'e3' ]) . alias ( 'f3' )) \\ . where ( 'f1 + f2 + f3 >= 2' ) . select ( 'a' , 'b' , 'c' , 'd' ) . show () +---------------+--------------+---------------+--------------+---------------+--------------+----------------+---+---+---+ | a| e1| b| e2| c| e3| d| f1| f2| f3| +---------------+--------------+---------------+--------------+---------------+--------------+----------------+---+---+---+ | [d, David, 29]|[d, a, friend]| [a, Alice, 34]|[a, e, friend]|[e, Esther, 32]|[e, f, follow]| [f, Fanny, 38]| 1| 1| 0| | [d, David, 29]|[d, a, friend]| [a, Alice, 34]|[a, b, friend]| [b, Bob, 36]|[b, c, follow]|[c, Charlie, 37]| 1| 1| 0| |[e, Esther, 32]|[e, d, friend]| [d, David, 29]|[d, a, friend]| [a, Alice, 34]|[a, b, friend]| [b, Bob, 36]| 1| 1| 1| | [g, Gabby, 60]|[g, e, follow]|[e, Esther, 32]|[e, d, friend]| [d, David, 29]|[d, a, friend]| [a, Alice, 34]| 0| 1| 1| +---------------+--------------+---------------+--------------+---------------+--------------+----------------+---+---+---+","title":"Motif Finding"},{"location":"MSBD5003/notebooks%20in%20class/graph/#subgraphs","text":"# Select subgraph of users older than 30, and relationships of type \"friend\". # Drop isolated vertices (users) which are not contained in any edges (relationships). g1 = g . filterVertices ( \"age > 30\" ) . filterEdges ( \"relationship = 'friend'\" ) \\ . dropIsolatedVertices () g1 . vertices . show () g1 . edges . show () +---+------+---+ | id| name|age| +---+------+---+ | e|Esther| 32| | b| Bob| 36| | a| Alice| 34| +---+------+---+ +---+---+------------+ |src|dst|relationship| +---+---+------------+ | a| e| friend| | a| b| friend| +---+---+------------+ # Select subgraph based on edges \"e\" of type \"follow\" # pointing from a younger user \"a\" to an older user \"b\". paths = g . find ( \"(a)-[e]->(b)\" ) \\ . filter ( \"e.relationship = 'follow'\" ) \\ . filter ( \"a.age < b.age\" ) # \"paths\" contains vertex info. Extract the edges. e2 = paths . select ( \"e.*\" ) # Construct the subgraph g2 = GraphFrame ( g . vertices , e2 ) . dropIsolatedVertices () g2 . vertices . show () g2 . edges . show () +---+-------+---+ | id| name|age| +---+-------+---+ | f| Fanny| 38| | e| Esther| 32| | c|Charlie| 37| | b| Bob| 36| +---+-------+---+ +---+---+------------+ |src|dst|relationship| +---+---+------------+ | e| f| follow| | b| c| follow| +---+---+------------+","title":"Subgraphs"},{"location":"MSBD5003/notebooks%20in%20class/graph/#bfs","text":"# Starting vertex is 'a' layers = [ g . vertices . select ( 'id' ) . where ( \"id = 'a'\" )] visited = layers [ 0 ] while layers [ - 1 ] . count () > 0 : # From the current layer, get all the one-hop neighbors d1 = layers [ - 1 ] . join ( g . edges , layers [ - 1 ][ 'id' ] == g . edges [ 'src' ]) # Rename the column as 'id', and remove visited verices and duplicates d2 = d1 . select ( d1 [ 'dst' ] . alias ( 'id' )) \\ . subtract ( visited ) . distinct () . cache () layers += [ d2 ] visited = visited . union ( layers [ - 1 ]) . cache () layers [ 0 ] . show () +---+ | id| +---+ | a| +---+ layers [ 1 ] . show () +---+ | id| +---+ | e| | b| +---+ layers [ 2 ] . show () +---+ | id| +---+ | f| | d| | c| +---+ layers [ 3 ] . show () +---+ | id| +---+ +---+ # GraphFrames provides own BFS: paths = g . bfs ( \"id = 'a'\" , \"age > 36\" ) paths . show () +--------------+--------------+---------------+--------------+----------------+ | from| e0| v1| e1| to| +--------------+--------------+---------------+--------------+----------------+ |[a, Alice, 34]|[a, b, friend]| [b, Bob, 36]|[b, c, follow]|[c, Charlie, 37]| |[a, Alice, 34]|[a, e, friend]|[e, Esther, 32]|[e, f, follow]| [f, Fanny, 38]| +--------------+--------------+---------------+--------------+----------------+","title":"BFS"},{"location":"MSBD5003/notebooks%20in%20class/graph/#list-ranking","text":"# -1 denotes end of list data = [( 0 , 5 ), ( 1 , 0 ), ( 3 , 4 ), ( 4 , 6 ), ( 5 , - 1 ), ( 6 , 1 )] e = spark . createDataFrame ( data , [ 'src' , 'dst' ]) v = e . select ( col ( 'src' ) . alias ( 'id' ), when ( e . dst == - 1 , 0 ) . otherwise ( 1 ) . alias ( 'd' )) v1 = spark . createDataFrame ([( - 1 , 0 )], [ 'id' , 'd' ]) v = v . union ( v1 ) v . show () e . show () +---+---+ | id| d| +---+---+ | 0| 1| | 1| 1| | 3| 1| | 4| 1| | 5| 0| | 6| 1| | -1| 0| +---+---+ +---+---+ |src|dst| +---+---+ | 0| 5| | 1| 0| | 3| 4| | 4| 6| | 5| -1| | 6| 1| +---+---+ while e . filter ( 'dst != -1' ) . count () > 0 : g = GraphFrame ( v , e ) g . cache () v = g . triplets . select ( col ( 'src.id' ) . alias ( 'id' ), ( col ( 'src.d' ) + col ( 'dst.d' )) . alias ( 'd' )) \\ . union ( v1 ) e = g . find ( '(a)-[]->(b); (b)-[]->(c)' ) \\ . select ( col ( 'a.id' ) . alias ( 'src' ), col ( 'c.id' ) . alias ( 'dst' )) \\ . union ( e . filter ( 'dst = -1' )) v . show () +---+---+ | id| d| +---+---+ | 0| 1| | 1| 2| | 3| 5| | 4| 4| | 5| 0| | 6| 3| | -1| 0| +---+---+","title":"List Ranking"},{"location":"MSBD5003/notebooks%20in%20class/graph/#message-passing-via-aggregatemessages","text":"from pyspark.sql.functions import coalesce , col , lit , sum , when , min , max from graphframes.lib import AggregateMessages as AM # AggregateMessages has the following members: src, dst, edge, msg # For each user, sum the ages of the adjacent users. agg = g . aggregateMessages ( sum ( AM . msg ) . alias ( \"summedAges\" ), sendToSrc = AM . dst [ 'age' ], sendToDst = AM . src [ 'age' ]) agg . show () +---+----------+ | id|summedAges| +---+----------+ | g| 32| | f| 69| | e| 161| | d| 66| | c| 110| | b| 108| | a| 97| +---+----------+","title":"Message passing via AggregateMessages"},{"location":"MSBD5003/notebooks%20in%20class/graph/#the-pregel-model-for-graph-computation","text":"# Pagerank in the Pregel model from pyspark.sql.functions import coalesce , col , lit , sum , when , min from graphframes.lib import Pregel # Need to set up a directory for Pregel computation sc . setCheckpointDir ( \"checkpoint\" ) ''' Use builder pattern to describe the operations. Call run() to start a run. It returns a DataFrame of vertices from the last iteration. When a run starts, it expands the vertices DataFrame using column expressions defined by withVertexColumn(). Those additional vertex properties can be changed during Pregel iterations. In each Pregel iteration, there are three phases: * Given each edge triplet, generate messages and specify target vertices to send, described by sendMsgToDst() and sendMsgToSrc(). * Aggregate messages by target vertex IDs, described by aggMsgs(). * Update additional vertex properties based on aggregated messages and states from previous iteration, described by withVertexColumn(). ''' v = g . outDegrees g = GraphFrame ( v , e ) ranks = g . pregel \\ . setMaxIter ( 5 ) \\ . sendMsgToDst ( Pregel . src ( \"rank\" ) / Pregel . src ( \"outDegree\" )) \\ . aggMsgs ( sum ( Pregel . msg ())) \\ . withVertexColumn ( \"rank\" , lit ( 1.0 ), \\ coalesce ( Pregel . msg (), lit ( 0.0 )) * lit ( 0.85 ) + lit ( 0.15 )) \\ . run () ranks . show () # pyspark.sql.functions.coalesce(*cols): Returns the first column that is not null. # Not to be confused with spark.sql.coalesce(numPartitions) +---+---------+-------------------+ | id|outDegree| rank| +---+---------+-------------------+ | g| 1| 0.15| | f| 1|0.41104330078124995| | e| 2| 0.5032932031249999| | d| 1|0.41104330078124995| | c| 1| 2.780783203124999| | b| 1| 2.2680220312499997| | a| 2| 0.4758149609375| +---+---------+-------------------+ # BFS in the Pregel model g = GraphFrame ( v , e ) dist = g . pregel \\ . sendMsgToDst ( when ( Pregel . src ( 'active' ), Pregel . src ( 'd' ) + 1 )) \\ . aggMsgs ( min ( Pregel . msg ())) \\ . withVertexColumn ( 'd' , when ( v [ 'id' ] == 'a' , 0 ) . otherwise ( 99999 ), \\ when ( Pregel . msg () < col ( 'd' ), Pregel . msg ()) . otherwise ( col ( 'd' ))) \\ . withVertexColumn ( 'active' , when ( v [ 'id' ] == 'a' , True ) . otherwise ( False ), \\ when ( Pregel . msg () < col ( 'd' ), True ) . otherwise ( False )) \\ . run () dist . show () +---+---------+-----+------+ | id|outDegree| d|active| +---+---------+-----+------+ | g| 1|99999| false| | f| 1| 2| false| | e| 2| 1| false| | d| 1| 2| false| | c| 1| 2| false| | b| 1| 1| false| | a| 2| 0| false| +---+---------+-----+------+","title":"The Pregel Model for Graph Computation"},{"location":"MSBD5003/notebooks%20in%20class/internal/","text":"3 from pyspark.context import SparkContext sc = SparkContext . getOrCreate () Finding Prime Numbers \u00b6 Algorithm: take every number from 2 to n find all multiples of these numbers that are smaller than or equal to n (containing duplicates, but that\u2019s ok) subtract from all numbers these composite numbers We see that all tasks but one finished quickly, while the last one takes a long time. before more deep but less efficient n = 5000 allnumbers = sc . parallelize ( range ( 2 , n ), 8 ) . cache () composite = allnumbers . flatMap ( lambda x : range ( x * 2 , n , x )) prime = allnumbers . subtract ( composite ) print ( composite . take ( 10 )) print ( prime . take ( 10 )) [4, 6, 8, 10, 12, 14, 16, 18, 20, 22] [17, 97, 113, 193, 241, 257, 337, 353, 401, 433] # Find the number of elements in each parttion def partitionsize ( it ): yield len ( list ( it )) print ( allnumbers . mapPartitions ( partitionsize ) . collect ()) print ( composite . mapPartitions ( partitionsize ) . collect ()) print ( prime . mapPartitions ( partitionsize ) . collect ()) print ( prime . glom () . collect ()[ 2 ][ 0 : 4 ]) [624, 625, 625, 625, 624, 625, 625, 625] [4174, 4160, 4170, 4170, 4170, 4164, 4170, 4181] [0, 81, 1, 84, 0, 81, 0, 87, 0, 80, 0, 84, 0, 87, 0, 84] [2] after efficient in time but more in total time allnumbers = sc . parallelize ( range ( 2 , n ), 8 ) . cache () composite = allnumbers . flatMap ( lambda x : range ( x * 2 , n , x )) . repartition ( 8 ) prime = allnumbers . subtract ( composite ) print ( composite . take ( 10 )) print ( prime . take ( 10 )) [44, 46, 48, 50, 52, 54, 56, 58, 60, 62] [17, 97, 113, 193, 241, 257, 337, 353, 401, 433] print ( allnumbers . mapPartitions ( partitionsize ) . collect ()) print ( composite . mapPartitions ( partitionsize ) . collect ()) print ( prime . mapPartitions ( partitionsize ) . collect ()) print ( prime . glom () . collect ()[ 1 ][ 0 : 4 ]) [624, 625, 625, 625, 624, 625, 625, 625] [4174, 4160, 4170, 4170, 4170, 4164, 4170, 4181] [0, 81, 1, 84, 0, 81, 0, 87, 0, 80, 0, 84, 0, 87, 0, 84] [17, 97, 113, 193] Data Partitioning \u00b6 data = [ 8 , 96 , 240 , 400 , 1 , 800 , 4 , 12 ] rdd = sc . parallelize ( zip ( data , data ), 4 ) print ( rdd . partitioner ) print ( rdd . glom () . collect ()) rdd = rdd . reduceByKey ( lambda x , y : x + y ) print ( rdd . glom () . collect ()) print ( rdd . partitioner ) print ( rdd . partitioner . partitionFunc ) rdd1 = rdd . map ( lambda x : ( x [ 0 ], x [ 1 ] + 1 )) print ( rdd1 . glom () . collect ()) print ( rdd1 . partitioner ) rdd2 = rdd . mapValues ( lambda x : x + 1 ) print ( rdd2 . partitioner . partitionFunc ) rdd = rdd . sortByKey () print ( rdd . glom () . collect ()) print ( rdd . partitioner . partitionFunc ) rdd3 = rdd . mapValues ( lambda x : x + 1 ) print ( rdd3 . partitioner . partitionFunc ) None [[(8, 8), (96, 96)], [(240, 240), (400, 400)], [(1, 1), (800, 800)], [(4, 4), (12, 12)]] [[(8, 8), (96, 96), (240, 240), (400, 400), (800, 800), (4, 4), (12, 12)], [(1, 1)], [], []] <pyspark.rdd.Partitioner object at 0x7fd9be009198> <function portable_hash at 0x7fd9cc338488> [[(8, 9), (96, 97), (240, 241), (400, 401), (800, 801), (4, 5), (12, 13)], [(1, 2)], [], []] None <function portable_hash at 0x7fd9cc338488> [[(1, 1), (4, 4), (8, 8)], [(12, 12), (96, 96)], [(240, 240), (400, 400)], [(800, 800)]] <function RDD.sortByKey.<locals>.rangePartitioner at 0x7fd9be0b68c8> <function RDD.sortByKey.<locals>.rangePartitioner at 0x7fd9be0b68c8> def partitionsize ( it ): yield len ( list ( it )) n = 40000 def f ( x ): return x % 9 data1 = list ( range ( 0 , n , 16 )) + list ( range ( 0 , n , 16 )) data2 = range ( 0 , n , 8 ) rdd1 = sc . parallelize ( zip ( data1 , data2 ), 8 ) print ( rdd1 . mapPartitions ( partitionsize ) . collect ()) rdd2 = rdd1 . reduceByKey ( lambda x , y : x + y ) print ( rdd2 . mapPartitions ( partitionsize ) . collect ()) rdd3 = rdd2 . partitionBy ( 8 , f ) print ( rdd3 . mapPartitions ( partitionsize ) . collect ()) rdd4 = rdd1 . reduceByKey ( lambda x , y : x + y , partitionFunc = f ) print ( rdd4 . mapPartitions ( partitionsize ) . collect ()) [625, 625, 625, 625, 625, 625, 625, 625] [2500, 0, 0, 0, 0, 0, 0, 0] [556, 278, 277, 278, 277, 278, 278, 278] [556, 278, 277, 278, 277, 278, 278, 278] a = sc . parallelize ( zip ( range ( 10000 ), range ( 10000 )), 8 ) b = sc . parallelize ( zip ( range ( 10000 ), range ( 10000 )), 8 ) print ( a . partitioner ) a = a . reduceByKey ( lambda x , y : x + y ) print ( a . partitioner . partitionFunc ) b = b . reduceByKey ( lambda x , y : x + y ) print ( b . partitioner . partitionFunc ) c = a . join ( b ) print ( c . getNumPartitions ()) print ( c . partitioner . partitionFunc ) print ( c . glom () . first ()[ 0 : 4 ]) None <function portable_hash at 0x7fd9cc338488> <function portable_hash at 0x7fd9cc338488> 8 <function portable_hash at 0x7fd9cc338488> [(0, (0, 0)), (8, (8, 8)), (16, (16, 16)), (24, (24, 24))] Partitioning in DataFrames \u00b6 data1 = [ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 , 4 ] data2 = [ 2 , 2 , 3 , 4 , 5 , 3 , 1 , 1 , 2 , 3 ] df = spark . createDataFrame ( zip ( data1 , data2 )) print ( df . rdd . getNumPartitions ()) print ( df . rdd . glom () . collect ()) 48 [[], [], [], [], [Row(_1=1, _2=2)], [], [], [], [], [Row(_1=1, _2=2)], [], [], [], [], [Row(_1=1, _2=3)], [], [], [], [], [Row(_1=2, _2=4)], [], [], [], [Row(_1=2, _2=5)], [], [], [], [], [Row(_1=2, _2=3)], [], [], [], [], [Row(_1=3, _2=1)], [], [], [], [], [Row(_1=3, _2=1)], [], [], [], [], [Row(_1=3, _2=2)], [], [], [], [Row(_1=4, _2=3)]] df1 = df . repartition ( 6 , df . _1 ) print ( df1 . rdd . glom () . collect ()) df1 . show () [[], [], [Row(_1=2, _2=4), Row(_1=2, _2=5), Row(_1=2, _2=3), Row(_1=4, _2=3)], [Row(_1=3, _2=1), Row(_1=3, _2=1), Row(_1=3, _2=2)], [], [Row(_1=1, _2=2), Row(_1=1, _2=2), Row(_1=1, _2=3)]] +---+---+ | _1| _2| +---+---+ | 2| 4| | 2| 5| | 2| 3| | 4| 3| | 3| 1| | 3| 1| | 3| 2| | 1| 2| | 1| 2| | 1| 3| +---+---+ # A 'real' example from SF Express # Prepare three relational tables from pyspark.sql.functions import * num_waybills = 1000 num_customers = 100 rdd = sc . parallelize (( i , ) for i in range ( num_waybills )) waybills = spark . createDataFrame ( rdd ) . select ( floor ( rand () * num_waybills ) . alias ( 'waybill' ), floor ( rand () * num_customers ) . alias ( 'customer' )) \\ . repartition ( 'waybill' ) \\ . cache () waybills . show () print ( waybills . count ()) rdd = sc . parallelize (( i , i ) for i in range ( num_customers )) customers = spark . createDataFrame ( rdd , [ 'customer' , 'phone' ]) . cache () customers . show () print ( customers . count ()) rdd = sc . parallelize (( i , ) for i in range ( num_waybills )) waybill_status = spark . createDataFrame ( rdd ) . select ( floor ( rand () * num_waybills ) . alias ( 'waybill' ), floor ( rand () * 10 ) . alias ( 'version' )) \\ . groupBy ( 'waybill' ) . max ( 'version' ) . cache () waybill_status . show () print ( waybill_status . count ()) +-------+--------+ |waybill|customer| +-------+--------+ | 964| 90| | 474| 10| | 26| 73| | 26| 66| | 191| 56| | 191| 89| | 541| 73| | 541| 2| | 938| 12| | 278| 78| | 720| 93| | 705| 11| | 367| 22| | 442| 12| | 442| 91| | 367| 1| | 367| 48| | 296| 62| | 926| 86| | 965| 9| +-------+--------+ only showing top 20 rows 1000 +--------+-----+ |customer|phone| +--------+-----+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| | 5| 5| | 6| 6| | 7| 7| | 8| 8| | 9| 9| | 10| 10| | 11| 11| | 12| 12| | 13| 13| | 14| 14| | 15| 15| | 16| 16| | 17| 17| | 18| 18| | 19| 19| +--------+-----+ only showing top 20 rows 100 +-------+------------+ |waybill|max(version)| +-------+------------+ | 474| 4| | 964| 4| | 29| 8| | 191| 7| | 541| 5| | 293| 3| | 270| 7| | 938| 3| | 730| 8| | 243| 3| | 278| 9| | 367| 5| | 442| 6| | 54| 9| | 19| 1| | 965| 9| | 926| 6| | 296| 0| | 0| 5| | 287| 4| +-------+------------+ only showing top 20 rows 635 # We want to join 3 tables together. # Knowing how each table is partitioned helps optimize the join order. # waybills.join(customers, 'customer').join(waybill_status, 'waybill').show() waybills . join ( waybill_status , 'waybill' ) . join ( customers , 'customer' ) . show () +--------+-------+------------+-----+ |customer|waybill|max(version)|phone| +--------+-------+------------+-----+ | 90| 964| 4| 90| | 10| 474| 4| 10| | 56| 191| 7| 56| | 89| 191| 7| 89| | 73| 541| 5| 73| | 2| 541| 5| 2| | 12| 938| 3| 12| | 78| 278| 9| 78| | 22| 367| 5| 22| | 12| 442| 6| 12| | 91| 442| 6| 91| | 1| 367| 5| 1| | 48| 367| 5| 48| | 62| 296| 0| 62| | 86| 926| 6| 86| | 9| 965| 9| 9| | 22| 19| 1| 22| | 45| 54| 9| 45| | 73| 926| 6| 73| | 10| 926| 6| 10| +--------+-------+------------+-----+ only showing top 20 rows Threading \u00b6 import threading import random partitions = 20 n = 5000000 * partitions # use different seeds in different threads and different partitions # a bit ugly, since mapPartitionsWithIndex takes a function with only index # and it as parameters def f1 ( index , it ): random . seed ( index + 987231 ) for i in it : x = random . random () * 2 - 1 y = random . random () * 2 - 1 yield 1 if x ** 2 + y ** 2 < 1 else 0 def f2 ( index , it ): random . seed ( index + 987232 ) for i in it : x = random . random () * 2 - 1 y = random . random () * 2 - 1 yield 1 if x ** 2 + y ** 2 < 1 else 0 def f3 ( index , it ): random . seed ( index + 987233 ) for i in it : x = random . random () * 2 - 1 y = random . random () * 2 - 1 yield 1 if x ** 2 + y ** 2 < 1 else 0 def f4 ( index , it ): random . seed ( index + 987234 ) for i in it : x = random . random () * 2 - 1 y = random . random () * 2 - 1 yield 1 if x ** 2 + y ** 2 < 1 else 0 def f5 ( index , it ): random . seed ( index + 987245 ) for i in it : x = random . random () * 2 - 1 y = random . random () * 2 - 1 yield 1 if x ** 2 + y ** 2 < 1 else 0 f = [ f1 , f2 , f3 , f4 , f5 ] # the function executed in each thread/job def dojob ( i ): count = sc . parallelize ( range ( 1 , n + 1 ), partitions ) \\ . mapPartitionsWithIndex ( f [ i ]) . reduce ( lambda a , b : a + b ) print ( \"Worker\" , i , \"reports: Pi is roughly\" , 4.0 * count / n ) # create and execute the threads threads = [] for i in range ( 5 ): t = threading . Thread ( target = dojob , args = ( i ,)) threads += [ t ] t . start () # wait for all threads to complete for t in threads : t . join () Worker 1 reports: Pi is roughly 3.14160468 Worker 0 reports: Pi is roughly 3.14166108 Worker 2 reports: Pi is roughly 3.141534 Worker 3 reports: Pi is roughly 3.14153212 Worker 4 reports: Pi is roughly 3.1413932","title":"Internal"},{"location":"MSBD5003/notebooks%20in%20class/internal/#finding-prime-numbers","text":"Algorithm: take every number from 2 to n find all multiples of these numbers that are smaller than or equal to n (containing duplicates, but that\u2019s ok) subtract from all numbers these composite numbers We see that all tasks but one finished quickly, while the last one takes a long time. before more deep but less efficient n = 5000 allnumbers = sc . parallelize ( range ( 2 , n ), 8 ) . cache () composite = allnumbers . flatMap ( lambda x : range ( x * 2 , n , x )) prime = allnumbers . subtract ( composite ) print ( composite . take ( 10 )) print ( prime . take ( 10 )) [4, 6, 8, 10, 12, 14, 16, 18, 20, 22] [17, 97, 113, 193, 241, 257, 337, 353, 401, 433] # Find the number of elements in each parttion def partitionsize ( it ): yield len ( list ( it )) print ( allnumbers . mapPartitions ( partitionsize ) . collect ()) print ( composite . mapPartitions ( partitionsize ) . collect ()) print ( prime . mapPartitions ( partitionsize ) . collect ()) print ( prime . glom () . collect ()[ 2 ][ 0 : 4 ]) [624, 625, 625, 625, 624, 625, 625, 625] [4174, 4160, 4170, 4170, 4170, 4164, 4170, 4181] [0, 81, 1, 84, 0, 81, 0, 87, 0, 80, 0, 84, 0, 87, 0, 84] [2] after efficient in time but more in total time allnumbers = sc . parallelize ( range ( 2 , n ), 8 ) . cache () composite = allnumbers . flatMap ( lambda x : range ( x * 2 , n , x )) . repartition ( 8 ) prime = allnumbers . subtract ( composite ) print ( composite . take ( 10 )) print ( prime . take ( 10 )) [44, 46, 48, 50, 52, 54, 56, 58, 60, 62] [17, 97, 113, 193, 241, 257, 337, 353, 401, 433] print ( allnumbers . mapPartitions ( partitionsize ) . collect ()) print ( composite . mapPartitions ( partitionsize ) . collect ()) print ( prime . mapPartitions ( partitionsize ) . collect ()) print ( prime . glom () . collect ()[ 1 ][ 0 : 4 ]) [624, 625, 625, 625, 624, 625, 625, 625] [4174, 4160, 4170, 4170, 4170, 4164, 4170, 4181] [0, 81, 1, 84, 0, 81, 0, 87, 0, 80, 0, 84, 0, 87, 0, 84] [17, 97, 113, 193]","title":"Finding Prime Numbers"},{"location":"MSBD5003/notebooks%20in%20class/internal/#data-partitioning","text":"data = [ 8 , 96 , 240 , 400 , 1 , 800 , 4 , 12 ] rdd = sc . parallelize ( zip ( data , data ), 4 ) print ( rdd . partitioner ) print ( rdd . glom () . collect ()) rdd = rdd . reduceByKey ( lambda x , y : x + y ) print ( rdd . glom () . collect ()) print ( rdd . partitioner ) print ( rdd . partitioner . partitionFunc ) rdd1 = rdd . map ( lambda x : ( x [ 0 ], x [ 1 ] + 1 )) print ( rdd1 . glom () . collect ()) print ( rdd1 . partitioner ) rdd2 = rdd . mapValues ( lambda x : x + 1 ) print ( rdd2 . partitioner . partitionFunc ) rdd = rdd . sortByKey () print ( rdd . glom () . collect ()) print ( rdd . partitioner . partitionFunc ) rdd3 = rdd . mapValues ( lambda x : x + 1 ) print ( rdd3 . partitioner . partitionFunc ) None [[(8, 8), (96, 96)], [(240, 240), (400, 400)], [(1, 1), (800, 800)], [(4, 4), (12, 12)]] [[(8, 8), (96, 96), (240, 240), (400, 400), (800, 800), (4, 4), (12, 12)], [(1, 1)], [], []] <pyspark.rdd.Partitioner object at 0x7fd9be009198> <function portable_hash at 0x7fd9cc338488> [[(8, 9), (96, 97), (240, 241), (400, 401), (800, 801), (4, 5), (12, 13)], [(1, 2)], [], []] None <function portable_hash at 0x7fd9cc338488> [[(1, 1), (4, 4), (8, 8)], [(12, 12), (96, 96)], [(240, 240), (400, 400)], [(800, 800)]] <function RDD.sortByKey.<locals>.rangePartitioner at 0x7fd9be0b68c8> <function RDD.sortByKey.<locals>.rangePartitioner at 0x7fd9be0b68c8> def partitionsize ( it ): yield len ( list ( it )) n = 40000 def f ( x ): return x % 9 data1 = list ( range ( 0 , n , 16 )) + list ( range ( 0 , n , 16 )) data2 = range ( 0 , n , 8 ) rdd1 = sc . parallelize ( zip ( data1 , data2 ), 8 ) print ( rdd1 . mapPartitions ( partitionsize ) . collect ()) rdd2 = rdd1 . reduceByKey ( lambda x , y : x + y ) print ( rdd2 . mapPartitions ( partitionsize ) . collect ()) rdd3 = rdd2 . partitionBy ( 8 , f ) print ( rdd3 . mapPartitions ( partitionsize ) . collect ()) rdd4 = rdd1 . reduceByKey ( lambda x , y : x + y , partitionFunc = f ) print ( rdd4 . mapPartitions ( partitionsize ) . collect ()) [625, 625, 625, 625, 625, 625, 625, 625] [2500, 0, 0, 0, 0, 0, 0, 0] [556, 278, 277, 278, 277, 278, 278, 278] [556, 278, 277, 278, 277, 278, 278, 278] a = sc . parallelize ( zip ( range ( 10000 ), range ( 10000 )), 8 ) b = sc . parallelize ( zip ( range ( 10000 ), range ( 10000 )), 8 ) print ( a . partitioner ) a = a . reduceByKey ( lambda x , y : x + y ) print ( a . partitioner . partitionFunc ) b = b . reduceByKey ( lambda x , y : x + y ) print ( b . partitioner . partitionFunc ) c = a . join ( b ) print ( c . getNumPartitions ()) print ( c . partitioner . partitionFunc ) print ( c . glom () . first ()[ 0 : 4 ]) None <function portable_hash at 0x7fd9cc338488> <function portable_hash at 0x7fd9cc338488> 8 <function portable_hash at 0x7fd9cc338488> [(0, (0, 0)), (8, (8, 8)), (16, (16, 16)), (24, (24, 24))]","title":"Data Partitioning"},{"location":"MSBD5003/notebooks%20in%20class/internal/#partitioning-in-dataframes","text":"data1 = [ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 , 4 ] data2 = [ 2 , 2 , 3 , 4 , 5 , 3 , 1 , 1 , 2 , 3 ] df = spark . createDataFrame ( zip ( data1 , data2 )) print ( df . rdd . getNumPartitions ()) print ( df . rdd . glom () . collect ()) 48 [[], [], [], [], [Row(_1=1, _2=2)], [], [], [], [], [Row(_1=1, _2=2)], [], [], [], [], [Row(_1=1, _2=3)], [], [], [], [], [Row(_1=2, _2=4)], [], [], [], [Row(_1=2, _2=5)], [], [], [], [], [Row(_1=2, _2=3)], [], [], [], [], [Row(_1=3, _2=1)], [], [], [], [], [Row(_1=3, _2=1)], [], [], [], [], [Row(_1=3, _2=2)], [], [], [], [Row(_1=4, _2=3)]] df1 = df . repartition ( 6 , df . _1 ) print ( df1 . rdd . glom () . collect ()) df1 . show () [[], [], [Row(_1=2, _2=4), Row(_1=2, _2=5), Row(_1=2, _2=3), Row(_1=4, _2=3)], [Row(_1=3, _2=1), Row(_1=3, _2=1), Row(_1=3, _2=2)], [], [Row(_1=1, _2=2), Row(_1=1, _2=2), Row(_1=1, _2=3)]] +---+---+ | _1| _2| +---+---+ | 2| 4| | 2| 5| | 2| 3| | 4| 3| | 3| 1| | 3| 1| | 3| 2| | 1| 2| | 1| 2| | 1| 3| +---+---+ # A 'real' example from SF Express # Prepare three relational tables from pyspark.sql.functions import * num_waybills = 1000 num_customers = 100 rdd = sc . parallelize (( i , ) for i in range ( num_waybills )) waybills = spark . createDataFrame ( rdd ) . select ( floor ( rand () * num_waybills ) . alias ( 'waybill' ), floor ( rand () * num_customers ) . alias ( 'customer' )) \\ . repartition ( 'waybill' ) \\ . cache () waybills . show () print ( waybills . count ()) rdd = sc . parallelize (( i , i ) for i in range ( num_customers )) customers = spark . createDataFrame ( rdd , [ 'customer' , 'phone' ]) . cache () customers . show () print ( customers . count ()) rdd = sc . parallelize (( i , ) for i in range ( num_waybills )) waybill_status = spark . createDataFrame ( rdd ) . select ( floor ( rand () * num_waybills ) . alias ( 'waybill' ), floor ( rand () * 10 ) . alias ( 'version' )) \\ . groupBy ( 'waybill' ) . max ( 'version' ) . cache () waybill_status . show () print ( waybill_status . count ()) +-------+--------+ |waybill|customer| +-------+--------+ | 964| 90| | 474| 10| | 26| 73| | 26| 66| | 191| 56| | 191| 89| | 541| 73| | 541| 2| | 938| 12| | 278| 78| | 720| 93| | 705| 11| | 367| 22| | 442| 12| | 442| 91| | 367| 1| | 367| 48| | 296| 62| | 926| 86| | 965| 9| +-------+--------+ only showing top 20 rows 1000 +--------+-----+ |customer|phone| +--------+-----+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| | 5| 5| | 6| 6| | 7| 7| | 8| 8| | 9| 9| | 10| 10| | 11| 11| | 12| 12| | 13| 13| | 14| 14| | 15| 15| | 16| 16| | 17| 17| | 18| 18| | 19| 19| +--------+-----+ only showing top 20 rows 100 +-------+------------+ |waybill|max(version)| +-------+------------+ | 474| 4| | 964| 4| | 29| 8| | 191| 7| | 541| 5| | 293| 3| | 270| 7| | 938| 3| | 730| 8| | 243| 3| | 278| 9| | 367| 5| | 442| 6| | 54| 9| | 19| 1| | 965| 9| | 926| 6| | 296| 0| | 0| 5| | 287| 4| +-------+------------+ only showing top 20 rows 635 # We want to join 3 tables together. # Knowing how each table is partitioned helps optimize the join order. # waybills.join(customers, 'customer').join(waybill_status, 'waybill').show() waybills . join ( waybill_status , 'waybill' ) . join ( customers , 'customer' ) . show () +--------+-------+------------+-----+ |customer|waybill|max(version)|phone| +--------+-------+------------+-----+ | 90| 964| 4| 90| | 10| 474| 4| 10| | 56| 191| 7| 56| | 89| 191| 7| 89| | 73| 541| 5| 73| | 2| 541| 5| 2| | 12| 938| 3| 12| | 78| 278| 9| 78| | 22| 367| 5| 22| | 12| 442| 6| 12| | 91| 442| 6| 91| | 1| 367| 5| 1| | 48| 367| 5| 48| | 62| 296| 0| 62| | 86| 926| 6| 86| | 9| 965| 9| 9| | 22| 19| 1| 22| | 45| 54| 9| 45| | 73| 926| 6| 73| | 10| 926| 6| 10| +--------+-------+------------+-----+ only showing top 20 rows","title":"Partitioning in DataFrames"},{"location":"MSBD5003/notebooks%20in%20class/internal/#threading","text":"import threading import random partitions = 20 n = 5000000 * partitions # use different seeds in different threads and different partitions # a bit ugly, since mapPartitionsWithIndex takes a function with only index # and it as parameters def f1 ( index , it ): random . seed ( index + 987231 ) for i in it : x = random . random () * 2 - 1 y = random . random () * 2 - 1 yield 1 if x ** 2 + y ** 2 < 1 else 0 def f2 ( index , it ): random . seed ( index + 987232 ) for i in it : x = random . random () * 2 - 1 y = random . random () * 2 - 1 yield 1 if x ** 2 + y ** 2 < 1 else 0 def f3 ( index , it ): random . seed ( index + 987233 ) for i in it : x = random . random () * 2 - 1 y = random . random () * 2 - 1 yield 1 if x ** 2 + y ** 2 < 1 else 0 def f4 ( index , it ): random . seed ( index + 987234 ) for i in it : x = random . random () * 2 - 1 y = random . random () * 2 - 1 yield 1 if x ** 2 + y ** 2 < 1 else 0 def f5 ( index , it ): random . seed ( index + 987245 ) for i in it : x = random . random () * 2 - 1 y = random . random () * 2 - 1 yield 1 if x ** 2 + y ** 2 < 1 else 0 f = [ f1 , f2 , f3 , f4 , f5 ] # the function executed in each thread/job def dojob ( i ): count = sc . parallelize ( range ( 1 , n + 1 ), partitions ) \\ . mapPartitionsWithIndex ( f [ i ]) . reduce ( lambda a , b : a + b ) print ( \"Worker\" , i , \"reports: Pi is roughly\" , 4.0 * count / n ) # create and execute the threads threads = [] for i in range ( 5 ): t = threading . Thread ( target = dojob , args = ( i ,)) threads += [ t ] t . start () # wait for all threads to complete for t in threads : t . join () Worker 1 reports: Pi is roughly 3.14160468 Worker 0 reports: Pi is roughly 3.14166108 Worker 2 reports: Pi is roughly 3.141534 Worker 3 reports: Pi is roughly 3.14153212 Worker 4 reports: Pi is roughly 3.1413932","title":"Threading"},{"location":"MSBD5003/notebooks%20in%20class/mllib/","text":"from pyspark.ml import Pipeline from pyspark.ml.linalg import Vectors from pyspark.ml.classification import LogisticRegression from pyspark.ml.feature import * from pyspark.ml.evaluation import BinaryClassificationEvaluator from pyspark.ml.tuning import CrossValidator , ParamGridBuilder from pyspark.sql import Row from pyspark.sql.functions import * from pyspark.sql.types import * Basic example on Transformer and Estimator \u00b6 # Prepare training data from a list of (label, features) tuples. # Dense Vectors are just NumPy arrays training = spark . createDataFrame ([ ( 1 , Vectors . dense ([ 0.0 , 1.1 , 0.1 ])), ( 0 , Vectors . dense ([ 2.0 , 1.0 , - 1.0 ])), ( 0 , Vectors . dense ([ 2.0 , 1.3 , 1.0 ])), ( 1 , Vectors . dense ([ 0.0 , 1.2 , - 0.5 ]))], [ \"label\" , \"features\" ]) # Create a LogisticRegression instance. This instance is an Estimator. lr = LogisticRegression ( maxIter = 10 , regParam = 0.01 ) # Print out the parameters, documentation, and any default values. print ( lr . explainParams ()) aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2) elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0) family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto) featuresCol: features column name. (default: features) fitIntercept: whether to fit an intercept term. (default: True) labelCol: label column name. (default: label) lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined) lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined) maxIter: max number of iterations (>= 0). (default: 100, current: 10) predictionCol: prediction column name. (default: prediction) probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability) rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction) regParam: regularization parameter (>= 0). (default: 0.0, current: 0.01) standardization: whether to standardize the training features before fitting the model. (default: True) threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5) thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined) tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06) upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined) upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined) weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined) # Learn a LogisticRegression model. This uses the parameters stored in lr. model1 = lr . fit ( training ) print ( model1 ) # model1 is a Model (i.e., a transformer produced by an Estimator) print ( \"Model 1's trained coefficients: \" , model1 . coefficients ) LogisticRegressionModel: uid=LogisticRegression_9b20229274e6, numClasses=2, numFeatures=3 Model 1's trained coefficients: [-3.10093560102053,2.608214738321436,-0.38017912254302655] # We may alternatively specify parameters using a Python dictionary as a paramMap paramMap = { lr . maxIter : 20 } paramMap [ lr . maxIter ] = 30 # Specify 1 Param, overwriting the original maxIter. paramMap . update ({ lr . regParam : 0.1 , lr . threshold : 0.55 }) # Specify multiple Params. # You can combine paramMaps, which are python dictionaries. paramMap [ lr . probabilityCol ] = \"myProbability\" # Change output column name # Now learn a new model using the paramMapCombined parameters. # paramMapCombined overrides all parameters set earlier via lr.set* methods. model2 = lr . fit ( training , paramMap ) print ( \"Model 2's trained coefficients: \" , model2 . coefficients ) Model 2's trained coefficients: [-1.431365881570681,0.4320887101487553,-0.1492041947797477] # Prepare test data test = spark . createDataFrame ([ ( 1 , Vectors . dense ([ - 1.0 , 1.5 , 1.3 ])), ( 2 , Vectors . dense ([ 3.0 , 2.0 , - 0.1 ])), ( 3 , Vectors . dense ([ 0.0 , 2.2 , - 1.5 ]))], [ \"id\" , \"features\" ]) # Make predictions on test data using the Transformer.transform() method. # LogisticRegression.transform will only use the 'features' column. # Note that model2.transform() outputs a \"myProbability\" column instead of the usual # 'probability' column since we renamed the lr.probabilityCol parameter previously. model1 . transform ( test ) . show () model2 . transform ( test ) . show () +---+--------------+--------------------+--------------------+----------+ | id| features| rawPrediction| probability|prediction| +---+--------------+--------------------+--------------------+----------+ | 1|[-1.0,1.5,1.3]|[-6.5872014439355...|[0.00137599470692...| 1.0| | 2|[3.0,2.0,-0.1]|[3.98018281942565...|[0.98166040093741...| 0.0| | 3|[0.0,2.2,-1.5]|[-6.3765177028604...|[0.00169814755783...| 1.0| +---+--------------+--------------------+--------------------+----------+ +---+--------------+--------------------+--------------------+----------+ | id| features| rawPrediction| myProbability|prediction| +---+--------------+--------------------+--------------------+----------+ | 1|[-1.0,1.5,1.3]|[-2.8046569418746...|[0.05707304171034...| 1.0| | 2|[3.0,2.0,-0.1]|[2.49587635664205...|[0.92385223117041...| 0.0| | 3|[0.0,2.2,-1.5]|[-2.0935249027913...|[0.10972776114779...| 1.0| +---+--------------+--------------------+--------------------+----------+ Pipeline example \u00b6 # Prepare training documents from a list of (id, text, label) tuples. training = spark . createDataFrame ([ ( 0 , \"a b c d spark spark\" , 1 ), ( 1 , \"b d\" , 0 ), ( 2 , \"spark f g h\" , 1 ), ( 3 , \"hadoop mapreduce\" , 0 ) ], [ \"id\" , \"text\" , \"label\" ]) # A tokenizer converts the input string to lowercase and then splits it by white spaces. tokenizer = Tokenizer ( inputCol = \"text\" , outputCol = \"words\" ) tokenizer . transform ( training ) . show () +---+-------------------+-----+--------------------+ | id| text|label| words| +---+-------------------+-----+--------------------+ | 0|a b c d spark spark| 1|[a, b, c, d, spar...| | 1| b d| 0| [b, d]| | 2| spark f g h| 1| [spark, f, g, h]| | 3| hadoop mapreduce| 0| [hadoop, mapreduce]| +---+-------------------+-----+--------------------+ # The same can be achieved by DataFrameAPI: # But you will need to wrap it as a transformer to use it in a pipeline. training . select ( '*' , split ( training [ 'text' ], ' ' ) . alias ( 'words' )) . show () +---+-------------------+-----+--------------------+ | id| text|label| words| +---+-------------------+-----+--------------------+ | 0|a b c d spark spark| 1|[a, b, c, d, spar...| | 1| b d| 0| [b, d]| | 2| spark f g h| 1| [spark, f, g, h]| | 3| hadoop mapreduce| 0| [hadoop, mapreduce]| +---+-------------------+-----+--------------------+ # Maps a sequence of terms to their term frequencies using the hashing trick. hashingTF = HashingTF ( inputCol = tokenizer . getOutputCol (), outputCol = \"features\" ) a = hashingTF . transform ( tokenizer . transform ( training )) a . show ( truncate = False ) print ( a . select ( 'features' ) . first ()) +---+-------------------+-----+--------------------------+-----------------------------------------------------------------+ |id |text |label|words |features | +---+-------------------+-----+--------------------------+-----------------------------------------------------------------+ |0 |a b c d spark spark|1 |[a, b, c, d, spark, spark]|(262144,[74920,89530,107107,148981,173558],[1.0,1.0,1.0,1.0,2.0])| |1 |b d |0 |[b, d] |(262144,[89530,148981],[1.0,1.0]) | |2 |spark f g h |1 |[spark, f, g, h] |(262144,[36803,173558,209078,228158],[1.0,1.0,1.0,1.0]) | |3 |hadoop mapreduce |0 |[hadoop, mapreduce] |(262144,[132966,198017],[1.0,1.0]) | +---+-------------------+-----+--------------------------+-----------------------------------------------------------------+ Row(features=SparseVector(262144, {74920: 1.0, 89530: 1.0, 107107: 1.0, 148981: 1.0, 173558: 2.0})) # lr is an estimator lr = LogisticRegression ( maxIter = 10 , regParam = 0.001 ) # Now we are ready to assumble the pipeline pipeline = Pipeline ( stages = [ tokenizer , hashingTF , lr ]) # Fit the pipeline to training documents. model = pipeline . fit ( training ) # Prepare test documents, which are unlabeled (id, text) tuples. test = spark . createDataFrame ([ ( 4 , \"spark i j k\" ), ( 5 , \"l m n\" ), ( 6 , \"spark hadoop spark\" ), ( 7 , \"apache hadoop\" ) ], [ \"id\" , \"text\" ]) # Make predictions on test documents and print columns of interest. model . transform ( test ) . show () +---+------------------+--------------------+--------------------+--------------------+--------------------+----------+ | id| text| words| features| rawPrediction| probability|prediction| +---+------------------+--------------------+--------------------+--------------------+--------------------+----------+ | 4| spark i j k| [spark, i, j, k]|(262144,[19036,68...|[-0.7500987629692...|[0.32079978124886...| 1.0| | 5| l m n| [l, m, n]|(262144,[1303,526...|[1.58033363406988...|[0.82925176350810...| 0.0| | 6|spark hadoop spark|[spark, hadoop, s...|(262144,[173558,1...|[-0.7053638244645...|[0.33062407361580...| 1.0| | 7| apache hadoop| [apache, hadoop]|(262144,[68303,19...|[3.95550096961371...|[0.98121072417264...| 0.0| +---+------------------+--------------------+--------------------+--------------------+--------------------+----------+ Example: Analyzing food inspection data using logistic regression \u00b6 # Data at https://www.cse.ust.hk/msbd5003/data/Food_Inspections1.csv inspections = spark . read . csv ( '../data/Food_Inspections1.csv' , inferSchema = True ) Let's take a look at its schema: inspections . printSchema () root |-- _c0: integer (nullable = true) |-- _c1: string (nullable = true) |-- _c2: string (nullable = true) |-- _c3: integer (nullable = true) |-- _c4: string (nullable = true) |-- _c5: string (nullable = true) |-- _c6: string (nullable = true) |-- _c7: string (nullable = true) |-- _c8: string (nullable = true) |-- _c9: integer (nullable = true) |-- _c10: string (nullable = true) |-- _c11: string (nullable = true) |-- _c12: string (nullable = true) |-- _c13: string (nullable = true) |-- _c14: double (nullable = true) |-- _c15: double (nullable = true) |-- _c16: string (nullable = true) inspections . show () +------+--------------------+--------------------+-------+--------------------+---------------+--------------------+-------+---+-----+----------+--------------------+------------------+--------------------+------------------+------------------+--------------------+ | _c0| _c1| _c2| _c3| _c4| _c5| _c6| _c7|_c8| _c9| _c10| _c11| _c12| _c13| _c14| _c15| _c16| +------+--------------------+--------------------+-------+--------------------+---------------+--------------------+-------+---+-----+----------+--------------------+------------------+--------------------+------------------+------------------+--------------------+ |413707| LUNA PARK INC| LUNA PARK DAY CARE|2049789|Children's Servic...| Risk 1 (High)| 3250 W FOSTER AVE |CHICAGO| IL|60625|09/21/2010| License-Task Force| Fail|24. DISH WASHING ...| 41.97583445690982| -87.7107455232781|(41.9758344569098...| |391234| CAFE SELMARIE| CAFE SELMARIE|1069067| Restaurant| Risk 1 (High)| 4729 N LINCOLN AVE |CHICAGO| IL|60625|09/21/2010| Canvass| Fail|2. FACILITIES TO ...| 41.96740659751604|-87.68761642361608|(41.9674065975160...| |413751| MANCHU WOK|MANCHU WOK (T3-H/...|1909522| Restaurant| Risk 1 (High)| 11601 W TOUHY AVE |CHICAGO| IL|60666|09/21/2010| Canvass| Pass|33. FOOD AND NON-...|42.008536400868735|-87.91442843927047|(42.0085364008687...| |413708|BENCHMARK HOSPITA...|BENCHMARK HOSPITA...|2049411| Restaurant| Risk 1 (High)|325 N LA SALLE ST...|CHICAGO| IL|60654|09/21/2010|Task Force Liquor...| Pass| null| 41.88819879207664|-87.63236298373182|(41.8881987920766...| |413722| JJ BURGER| JJ BURGER|2055016| Restaurant|Risk 2 (Medium)| 749 S CICERO AVE |CHICAGO| IL|60644|09/21/2010| License| Pass| null| 41.87082601444883|-87.74476763884662|(41.8708260144488...| |413752|GOLDEN HOOKS FISH...|GOLDEN HOOKS FISH...|2042435| Restaurant|Risk 2 (Medium)| 3958 W MONROE ST |CHICAGO| IL|60624|09/21/2010|Short Form Complaint| Pass| null| 41.87987261425607|-87.72551692436804|(41.8798726142560...| |413714|THE DOCK AT MONTR...|THE DOCK AT MONTR...|2043260| Restaurant| Risk 1 (High)| 4400 N SIMONDS DR |CHICAGO| IL|60640|09/21/2010| License| Fail| null| 41.96390893734172|-87.63863624840039|(41.9639089373417...| |413753|CLARK FOOD & CIGA...| null|2042203| Grocery Store| Risk 3 (Low)|6761 N CLARK ST B...|CHICAGO| IL|60626|09/21/2010| Canvass| Pass| null| 42.0053117273606|-87.67294053846207|(42.0053117273606...| |120580| SUSHI PINK| SUSHI PINK|1847340| Restaurant| Risk 1 (High)|909 W WASHINGTON ...|CHICAGO| IL|60607|09/21/2010| Canvass| Pass|32. FOOD AND NON-...|41.882987317760424|-87.65014022876997|(41.8829873177604...| |401216| M.H.R.,L.L.C.| M.H.R.,L.L.C.|1621323| Restaurant|Risk 2 (Medium)| 623 S WABASH AVE |CHICAGO| IL|60605|09/21/2010| Canvass| Out of Business| null| 41.87390845559158|-87.62583770570953|(41.8739084555915...| |413715| NABO'S| NABO'S|1931861| Restaurant| Risk 1 (High)| 3351 N BROADWAY |CHICAGO| IL|60657|09/21/2010|Canvass Re-Inspec...| Pass|19. OUTSIDE GARBA...| 41.94334005547684|-87.64466387044959|(41.9433400554768...| |413721|THE NICHOLSON SCHOOL|THE NICHOLSON SCHOOL|2002702|Daycare (2 - 6 Ye...| Risk 1 (High)| 1700 W CORTLAND ST |CHICAGO| IL|60622|09/21/2010| License| Pass| null| 41.91618227133264| -87.6703413842735|(41.9161822713326...| |401215| M.H.R.,L.L.C.| M.H.R.,L.L.C.|1621322| Restaurant| Risk 1 (High)| 600 S MICHIGAN AVE |CHICAGO| IL|60605|09/21/2010| Canvass| Out of Business| null| 41.87437161535891|-87.62437952778167|(41.8743716153589...| |420207| WHOLE FOODS MARKET| WHOLE FOODS MARKET|1933690| Grocery Store| Risk 1 (High)|1550 N KINGSBURY ST |CHICAGO| IL|60642|09/21/2010| Complaint| Pass|32. FOOD AND NON-...| 41.90939878780941|-87.65305069789407|(41.9093987878094...| |154514| LAS FUENTES| LAS FUENTES| 12575| Restaurant| Risk 1 (High)| 2558 N HALSTED ST |CHICAGO| IL|60614|09/21/2010| Canvass| Fail|18. NO EVIDENCE O...| 41.9290354100918|-87.64903392789199|(41.9290354100918...| |413711|CASA CENTRAL COMM...|CASA CENTRAL COMM...| 60766| Restaurant| Risk 1 (High)|1343 N CALIFORNIA...|CHICAGO| IL|60622|09/21/2010| License| Pass|41. PREMISES MAIN...| 41.90598597077873|-87.69680735572291|(41.9059859707787...| |413764|LA BRUQUENA RESTA...|LA BRUQUENA RESTA...|1492868| Restaurant| Risk 1 (High)| 2726 W DIVISION ST |CHICAGO| IL|60622|09/21/2010|Suspected Food Po...|Pass w/ Conditions|4. SOURCE OF CROS...|41.903046386818346| -87.695535129416|(41.9030463868183...| |413732|SODEXHO AT UNITED...|SODEXHO AT UNITED...| 20467| Restaurant| Risk 1 (High)| 11601 W TOUHY AVE |CHICAGO| IL|60666|09/21/2010| Canvass| Out of Business| null|42.008536400868735|-87.91442843927047|(42.0085364008687...| |413757|WHIZ KIDS NURSERY...|WHIZ KIDS NURSERY...|1948277|Daycare Above and...| Risk 1 (High)| 514-522 W 103RD ST |CHICAGO| IL|60628|09/21/2010| Canvass| Pass|35. WALLS, CEILIN...|41.707112812685075|-87.63620425242559|(41.7071128126850...| |363272|FRUTERIA GUAYAUIT...|FRUTERIA GUAYAUIT...|1446823| Grocery Store| Risk 3 (Low)| 3849 S KEDZIE AVE |CHICAGO| IL|60632|09/21/2010| Canvass| Out of Business| null| 41.82290845958193|-87.70426021024545|(41.8229084595819...| +------+--------------------+--------------------+-------+--------------------+---------------+--------------------+-------+---+-----+----------+--------------------+------------------+--------------------+------------------+------------------+--------------------+ only showing top 20 rows We now have the CSV file as a DataFrame. It has some columns we will not use. Dropping them can save memory when caching the DataFrame. Also, we should give these columns meaningful names. # Drop unused columns and rename interesting columns. # Keep interesting columns and rename them to something meaningful # Mapping column index to name. columnNames = { 0 : \"id\" , 1 : \"name\" , 12 : \"results\" , 13 : \"violations\" } # Rename column from '_c{id}' to something meaningful. cols = [ inspections [ i ] . alias ( columnNames [ i ]) for i in columnNames . keys ()] # Drop columns we are not using. df = inspections . select ( cols ) . where ( col ( 'violations' ) . isNotNull ()) df . cache () df . show () df . count () +------+--------------------+------------------+--------------------+ | id| name| results| violations| +------+--------------------+------------------+--------------------+ |413707| LUNA PARK INC| Fail|24. DISH WASHING ...| |391234| CAFE SELMARIE| Fail|2. FACILITIES TO ...| |413751| MANCHU WOK| Pass|33. FOOD AND NON-...| |120580| SUSHI PINK| Pass|32. FOOD AND NON-...| |413715| NABO'S| Pass|19. OUTSIDE GARBA...| |420207| WHOLE FOODS MARKET| Pass|32. FOOD AND NON-...| |154514| LAS FUENTES| Fail|18. NO EVIDENCE O...| |413711|CASA CENTRAL COMM...| Pass|41. PREMISES MAIN...| |413764|LA BRUQUENA RESTA...|Pass w/ Conditions|4. SOURCE OF CROS...| |413757|WHIZ KIDS NURSERY...| Pass|35. WALLS, CEILIN...| |154516|TACO & BURRITO PA...| Pass|30. FOOD IN ORIGI...| |413759| MARISCOS EL VENENO| Pass|18. NO EVIDENCE O...| |114554|THE HANGGE- UPPE,...| Pass|18. NO EVIDENCE O...| |413758| LINDY'S CHILI INC|Pass w/ Conditions|30. FOOD IN ORIGI...| |343362| FUMARE MEATS| Pass|40. REFRIGERATION...| |413754| Subway| Pass|38. VENTILATION: ...| |289222|LITTLE CAESARS PIZZA| Pass|34. FLOORS: CONST...| |413755| BILLY'S GRILL| Pass|33. FOOD AND NON-...| |343364| FRESHII| Fail|18. NO EVIDENCE O...| |289221|NICKY'S GRILL & Y...| Pass|33. FOOD AND NON-...| +------+--------------------+------------------+--------------------+ only showing top 20 rows 10469 df . take ( 1 ) [Row(id=413707, name='LUNA PARK INC', results='Fail', violations='24. DISH WASHING FACILITIES: PROPERLY DESIGNED, CONSTRUCTED, MAINTAINED, INSTALLED, LOCATED AND OPERATED - Comments: All dishwashing machines must be of a type that complies with all requirements of the plumbing section of the Municipal Code of Chicago and Rules and Regulation of the Board of Health. OBSEVERD THE 3 COMPARTMENT SINK BACKING UP INTO THE 1ST AND 2ND COMPARTMENT WITH CLEAR WATER AND SLOWLY DRAINING OUT. INST NEED HAVE IT REPAIR. CITATION ISSUED, SERIOUS VIOLATION 7-38-030 H000062369-10 COURT DATE 10-28-10 TIME 1 P.M. ROOM 107 400 W. SURPERIOR. | 36. LIGHTING: REQUIRED MINIMUM FOOT-CANDLES OF LIGHT PROVIDED, FIXTURES SHIELDED - Comments: Shielding to protect against broken glass falling into food shall be provided for all artificial lighting sources in preparation, service, and display facilities. LIGHT SHIELD ARE MISSING UNDER HOOD OF COOKING EQUIPMENT AND NEED TO REPLACE LIGHT UNDER UNIT. 4 LIGHTS ARE OUT IN THE REAR CHILDREN AREA,IN THE KINDERGARDEN CLASS ROOM. 2 LIGHT ARE OUT EAST REAR, LIGHT FRONT WEST ROOM. NEED TO REPLACE ALL LIGHT THAT ARE NOT WORKING. | 35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: The walls and ceilings shall be in good repair and easily cleaned. MISSING CEILING TILES WITH STAINS IN WEST,EAST, IN FRONT AREA WEST, AND BY THE 15MOS AREA. NEED TO BE REPLACED. | 32. FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: All food and non-food contact equipment and utensils shall be smooth, easily cleanable, and durable, and shall be in good repair. SPLASH GUARDED ARE NEEDED BY THE EXPOSED HAND SINK IN THE KITCHEN AREA | 34. FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED - Comments: The floors shall be constructed per code, be smooth and easily cleaned, and be kept clean and in good repair. INST NEED TO ELEVATE ALL FOOD ITEMS 6INCH OFF THE FLOOR 6 INCH AWAY FORM WALL. ')] The output of the above cell gives us an idea of the schema of the input file; the file includes the name of every establishment, the type of establishment, the address, the data of the inspections, and the location, among other things. Let's start to get a sense of what our dataset contains. For example, what are the different values in the results column? df . select ( 'results' ) . distinct () . show () +------------------+ | results| +------------------+ | Fail| |Pass w/ Conditions| | Pass| +------------------+ df . groupBy ( 'results' ) . count () . show () +------------------+-----+ | results|count| +------------------+-----+ | Fail| 2607| |Pass w/ Conditions| 1028| | Pass| 6834| +------------------+-----+ Let us develop a model that can guess the outcome of a food inspection, given the violations. Since logistic regression is a binary classification method, it makes sense to group our data into two categories: Fail and Pass . A \"Pass w/ Conditions\" is still a Pass, so when we train the model, we will consider the two results equivalent. Let us go ahead and convert our existing dataframe( df ) into a new dataframe where each inspection is represented as a label-violations pair. In our case, a label of 0 represents a failure, a label of 1 represents a success. # The function to clean the data labeledData = df . select ( when ( df . results == 'Fail' , 0 ) . when ( df . results == 'Pass' , 1 ) . otherwise ( 1 ) . alias ( 'label' ), 'violations' ) labeledData . show () +-----+--------------------+ |label| violations| +-----+--------------------+ | 0|24. DISH WASHING ...| | 0|2. FACILITIES TO ...| | 1|33. FOOD AND NON-...| | 1|32. FOOD AND NON-...| | 1|19. OUTSIDE GARBA...| | 1|32. FOOD AND NON-...| | 0|18. NO EVIDENCE O...| | 1|41. PREMISES MAIN...| | 1|4. SOURCE OF CROS...| | 1|35. WALLS, CEILIN...| | 1|30. FOOD IN ORIGI...| | 1|18. NO EVIDENCE O...| | 1|18. NO EVIDENCE O...| | 1|30. FOOD IN ORIGI...| | 1|40. REFRIGERATION...| | 1|38. VENTILATION: ...| | 1|34. FLOORS: CONST...| | 1|33. FOOD AND NON-...| | 0|18. NO EVIDENCE O...| | 1|33. FOOD AND NON-...| +-----+--------------------+ only showing top 20 rows Train a logistic regression model from the input dataframe trainingData , testData = labeledData . randomSplit ([ 0.8 , 0.2 ]) tokenizer = Tokenizer ( inputCol = \"violations\" , outputCol = \"words\" ) hashingTF = HashingTF ( inputCol = tokenizer . getOutputCol (), outputCol = \"features\" ) lr = LogisticRegression ( maxIter = 10 , regParam = 0.01 ) pipeline = Pipeline ( stages = [ tokenizer , hashingTF , lr ]) model = pipeline . fit ( trainingData ) predictionsDf = model . transform ( testData ) predictionsDf . show () +-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+ |label| violations| words| features| rawPrediction| probability|prediction| +-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+ | 0|1. SOURCE SOUND C...|[1., source, soun...|(262144,[699,1411...|[8.21045734370492...|[0.99972827746471...| 0.0| | 0|10. SEWAGE AND WA...|[10., sewage, and...|(262144,[1797,542...|[0.72532689463172...|[0.67377895526181...| 0.0| | 0|10. SEWAGE AND WA...|[10., sewage, and...|(262144,[2786,610...|[4.76342457489383...|[0.99153592609727...| 0.0| | 0|12. HAND WASHING ...|[12., hand, washi...|(262144,[1455,278...|[7.50001649481132...|[0.99944723047594...| 0.0| | 0|13. NO EVIDENCE O...|[13., no, evidenc...|(262144,[284,1797...|[10.5730088915445...|[0.99997440297665...| 0.0| | 0|13. NO EVIDENCE O...|[13., no, evidenc...|(262144,[2786,610...|[2.06744925670706...|[0.88769893037329...| 0.0| | 0|16. FOOD PROTECTE...|[16., food, prote...|(262144,[6106,914...|[-1.4603273358789...|[0.18841726515000...| 1.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[161,1797...|[1.20630674122104...|[0.76964481719305...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[1797,278...|[0.20311794686958...|[0.55060562031582...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[161,2325...|[4.42895943956892...|[0.98821368031221...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[2786,542...|[4.38608040086412...|[0.98770365203887...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[2786,361...|[2.47491920164261...|[0.92236475183085...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[2325,278...|[2.42852129891198...|[0.91897649882414...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[2786,542...|[2.52500010935656...|[0.92587594231444...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[2786,649...|[11.3588170519728...|[0.99998833396253...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[2786,610...|[4.93010908820459...|[0.99282612140406...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[2786,610...|[2.09044930412383...|[0.88997143023289...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[2786,656...|[4.78228732981419...|[0.99169277163220...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[2325,278...|[5.37386912945687...|[0.99538523611218...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[161,877,...|[-1.4626391002146...|[0.18806401354055...| 1.0| +-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+ only showing top 20 rows numSuccesses = predictionsDf . where ( 'label == prediction' ) . count () numInspections = predictionsDf . count () print ( \"There were %d inspections and there were %d successful predictions\" % ( numInspections , numSuccesses )) print ( \"This is a %d%% success rate\" % ( float ( numSuccesses ) / float ( numInspections ) * 100 )) There were 2131 inspections and there were 1859 successful predictions This is a 87% success rate Cross-Validation \u00b6 CrossValidator begins by splitting the dataset into a set of folds which are used as separate training and test datasets. E.g., with k=5 folds, CrossValidator will generate 5 (training, test) dataset pairs, each of which uses 4/5 of the data for training and 1/5 for testing. To evaluate a particular ParamMap, CrossValidator computes the average evaluation metric for the 5 Models produced by fitting the Estimator on the 5 different (training, test) dataset pairs. After identifying the best ParamMap, CrossValidator finally re-fits the Estimator using the best ParamMap and the entire dataset. # We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance. # This will allow us to jointly choose parameters for all Pipeline stages. # A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator. # We use a ParamGridBuilder to construct a grid of parameters to search over. # With 3 values for hashingTF.numFeatures and 2 values for lr.regParam, # this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from. paramGrid = ParamGridBuilder () \\ . addGrid ( hashingTF . numFeatures , [ 10 , 100 , 1000 ]) \\ . addGrid ( lr . regParam , [ 0.1 , 0.01 ]) \\ . build () crossval = CrossValidator ( estimator = pipeline , estimatorParamMaps = paramGrid , evaluator = BinaryClassificationEvaluator (), numFolds = 5 ) # Run cross-validation, and choose the best set of parameters. cvModel = crossval . fit ( trainingData ) predictionsDf = cvModel . transform ( testData ) numSuccesses = predictionsDf . where ( 'label == prediction' ) . count () numInspections = predictionsDf . count () print ( \"There were %d inspections and there were %d successful predictions\" % ( numInspections , numSuccesses )) print ( \"This is a %d%% success rate\" % ( float ( numSuccesses ) / float ( numInspections ) * 100 )) There were 2131 inspections and there were 1909 successful predictions This is a 89% success rate cvModel . explainParams () \"estimator: estimator to be cross-validated (current: Pipeline_c5b1e2db18ab)\\nestimatorParamMaps: estimator param maps (current: [{Param(parent='HashingTF_7bc1a253c00d', name='numFeatures', doc='Number of features. Should be greater than 0.'): 10, Param(parent='LogisticRegression_0a9fda7dc002', name='regParam', doc='regularization parameter (>= 0).'): 0.1}, {Param(parent='HashingTF_7bc1a253c00d', name='numFeatures', doc='Number of features. Should be greater than 0.'): 10, Param(parent='LogisticRegression_0a9fda7dc002', name='regParam', doc='regularization parameter (>= 0).'): 0.01}, {Param(parent='HashingTF_7bc1a253c00d', name='numFeatures', doc='Number of features. Should be greater than 0.'): 100, Param(parent='LogisticRegression_0a9fda7dc002', name='regParam', doc='regularization parameter (>= 0).'): 0.1}, {Param(parent='HashingTF_7bc1a253c00d', name='numFeatures', doc='Number of features. Should be greater than 0.'): 100, Param(parent='LogisticRegression_0a9fda7dc002', name='regParam', doc='regularization parameter (>= 0).'): 0.01}, {Param(parent='HashingTF_7bc1a253c00d', name='numFeatures', doc='Number of features. Should be greater than 0.'): 1000, Param(parent='LogisticRegression_0a9fda7dc002', name='regParam', doc='regularization parameter (>= 0).'): 0.1}, {Param(parent='HashingTF_7bc1a253c00d', name='numFeatures', doc='Number of features. Should be greater than 0.'): 1000, Param(parent='LogisticRegression_0a9fda7dc002', name='regParam', doc='regularization parameter (>= 0).'): 0.01}])\\nevaluator: evaluator used to select hyper-parameters that maximize the validator metric (current: BinaryClassificationEvaluator_17036e9cdcf5)\\nnumFolds: number of folds for cross validation (default: 3, current: 5)\\nseed: random seed. (default: 7809051150349531440)\"","title":"Mllib"},{"location":"MSBD5003/notebooks%20in%20class/mllib/#basic-example-on-transformer-and-estimator","text":"# Prepare training data from a list of (label, features) tuples. # Dense Vectors are just NumPy arrays training = spark . createDataFrame ([ ( 1 , Vectors . dense ([ 0.0 , 1.1 , 0.1 ])), ( 0 , Vectors . dense ([ 2.0 , 1.0 , - 1.0 ])), ( 0 , Vectors . dense ([ 2.0 , 1.3 , 1.0 ])), ( 1 , Vectors . dense ([ 0.0 , 1.2 , - 0.5 ]))], [ \"label\" , \"features\" ]) # Create a LogisticRegression instance. This instance is an Estimator. lr = LogisticRegression ( maxIter = 10 , regParam = 0.01 ) # Print out the parameters, documentation, and any default values. print ( lr . explainParams ()) aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2) elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0) family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto) featuresCol: features column name. (default: features) fitIntercept: whether to fit an intercept term. (default: True) labelCol: label column name. (default: label) lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined) lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined) maxIter: max number of iterations (>= 0). (default: 100, current: 10) predictionCol: prediction column name. (default: prediction) probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability) rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction) regParam: regularization parameter (>= 0). (default: 0.0, current: 0.01) standardization: whether to standardize the training features before fitting the model. (default: True) threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5) thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined) tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06) upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined) upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined) weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined) # Learn a LogisticRegression model. This uses the parameters stored in lr. model1 = lr . fit ( training ) print ( model1 ) # model1 is a Model (i.e., a transformer produced by an Estimator) print ( \"Model 1's trained coefficients: \" , model1 . coefficients ) LogisticRegressionModel: uid=LogisticRegression_9b20229274e6, numClasses=2, numFeatures=3 Model 1's trained coefficients: [-3.10093560102053,2.608214738321436,-0.38017912254302655] # We may alternatively specify parameters using a Python dictionary as a paramMap paramMap = { lr . maxIter : 20 } paramMap [ lr . maxIter ] = 30 # Specify 1 Param, overwriting the original maxIter. paramMap . update ({ lr . regParam : 0.1 , lr . threshold : 0.55 }) # Specify multiple Params. # You can combine paramMaps, which are python dictionaries. paramMap [ lr . probabilityCol ] = \"myProbability\" # Change output column name # Now learn a new model using the paramMapCombined parameters. # paramMapCombined overrides all parameters set earlier via lr.set* methods. model2 = lr . fit ( training , paramMap ) print ( \"Model 2's trained coefficients: \" , model2 . coefficients ) Model 2's trained coefficients: [-1.431365881570681,0.4320887101487553,-0.1492041947797477] # Prepare test data test = spark . createDataFrame ([ ( 1 , Vectors . dense ([ - 1.0 , 1.5 , 1.3 ])), ( 2 , Vectors . dense ([ 3.0 , 2.0 , - 0.1 ])), ( 3 , Vectors . dense ([ 0.0 , 2.2 , - 1.5 ]))], [ \"id\" , \"features\" ]) # Make predictions on test data using the Transformer.transform() method. # LogisticRegression.transform will only use the 'features' column. # Note that model2.transform() outputs a \"myProbability\" column instead of the usual # 'probability' column since we renamed the lr.probabilityCol parameter previously. model1 . transform ( test ) . show () model2 . transform ( test ) . show () +---+--------------+--------------------+--------------------+----------+ | id| features| rawPrediction| probability|prediction| +---+--------------+--------------------+--------------------+----------+ | 1|[-1.0,1.5,1.3]|[-6.5872014439355...|[0.00137599470692...| 1.0| | 2|[3.0,2.0,-0.1]|[3.98018281942565...|[0.98166040093741...| 0.0| | 3|[0.0,2.2,-1.5]|[-6.3765177028604...|[0.00169814755783...| 1.0| +---+--------------+--------------------+--------------------+----------+ +---+--------------+--------------------+--------------------+----------+ | id| features| rawPrediction| myProbability|prediction| +---+--------------+--------------------+--------------------+----------+ | 1|[-1.0,1.5,1.3]|[-2.8046569418746...|[0.05707304171034...| 1.0| | 2|[3.0,2.0,-0.1]|[2.49587635664205...|[0.92385223117041...| 0.0| | 3|[0.0,2.2,-1.5]|[-2.0935249027913...|[0.10972776114779...| 1.0| +---+--------------+--------------------+--------------------+----------+","title":"Basic example on Transformer and Estimator"},{"location":"MSBD5003/notebooks%20in%20class/mllib/#pipeline-example","text":"# Prepare training documents from a list of (id, text, label) tuples. training = spark . createDataFrame ([ ( 0 , \"a b c d spark spark\" , 1 ), ( 1 , \"b d\" , 0 ), ( 2 , \"spark f g h\" , 1 ), ( 3 , \"hadoop mapreduce\" , 0 ) ], [ \"id\" , \"text\" , \"label\" ]) # A tokenizer converts the input string to lowercase and then splits it by white spaces. tokenizer = Tokenizer ( inputCol = \"text\" , outputCol = \"words\" ) tokenizer . transform ( training ) . show () +---+-------------------+-----+--------------------+ | id| text|label| words| +---+-------------------+-----+--------------------+ | 0|a b c d spark spark| 1|[a, b, c, d, spar...| | 1| b d| 0| [b, d]| | 2| spark f g h| 1| [spark, f, g, h]| | 3| hadoop mapreduce| 0| [hadoop, mapreduce]| +---+-------------------+-----+--------------------+ # The same can be achieved by DataFrameAPI: # But you will need to wrap it as a transformer to use it in a pipeline. training . select ( '*' , split ( training [ 'text' ], ' ' ) . alias ( 'words' )) . show () +---+-------------------+-----+--------------------+ | id| text|label| words| +---+-------------------+-----+--------------------+ | 0|a b c d spark spark| 1|[a, b, c, d, spar...| | 1| b d| 0| [b, d]| | 2| spark f g h| 1| [spark, f, g, h]| | 3| hadoop mapreduce| 0| [hadoop, mapreduce]| +---+-------------------+-----+--------------------+ # Maps a sequence of terms to their term frequencies using the hashing trick. hashingTF = HashingTF ( inputCol = tokenizer . getOutputCol (), outputCol = \"features\" ) a = hashingTF . transform ( tokenizer . transform ( training )) a . show ( truncate = False ) print ( a . select ( 'features' ) . first ()) +---+-------------------+-----+--------------------------+-----------------------------------------------------------------+ |id |text |label|words |features | +---+-------------------+-----+--------------------------+-----------------------------------------------------------------+ |0 |a b c d spark spark|1 |[a, b, c, d, spark, spark]|(262144,[74920,89530,107107,148981,173558],[1.0,1.0,1.0,1.0,2.0])| |1 |b d |0 |[b, d] |(262144,[89530,148981],[1.0,1.0]) | |2 |spark f g h |1 |[spark, f, g, h] |(262144,[36803,173558,209078,228158],[1.0,1.0,1.0,1.0]) | |3 |hadoop mapreduce |0 |[hadoop, mapreduce] |(262144,[132966,198017],[1.0,1.0]) | +---+-------------------+-----+--------------------------+-----------------------------------------------------------------+ Row(features=SparseVector(262144, {74920: 1.0, 89530: 1.0, 107107: 1.0, 148981: 1.0, 173558: 2.0})) # lr is an estimator lr = LogisticRegression ( maxIter = 10 , regParam = 0.001 ) # Now we are ready to assumble the pipeline pipeline = Pipeline ( stages = [ tokenizer , hashingTF , lr ]) # Fit the pipeline to training documents. model = pipeline . fit ( training ) # Prepare test documents, which are unlabeled (id, text) tuples. test = spark . createDataFrame ([ ( 4 , \"spark i j k\" ), ( 5 , \"l m n\" ), ( 6 , \"spark hadoop spark\" ), ( 7 , \"apache hadoop\" ) ], [ \"id\" , \"text\" ]) # Make predictions on test documents and print columns of interest. model . transform ( test ) . show () +---+------------------+--------------------+--------------------+--------------------+--------------------+----------+ | id| text| words| features| rawPrediction| probability|prediction| +---+------------------+--------------------+--------------------+--------------------+--------------------+----------+ | 4| spark i j k| [spark, i, j, k]|(262144,[19036,68...|[-0.7500987629692...|[0.32079978124886...| 1.0| | 5| l m n| [l, m, n]|(262144,[1303,526...|[1.58033363406988...|[0.82925176350810...| 0.0| | 6|spark hadoop spark|[spark, hadoop, s...|(262144,[173558,1...|[-0.7053638244645...|[0.33062407361580...| 1.0| | 7| apache hadoop| [apache, hadoop]|(262144,[68303,19...|[3.95550096961371...|[0.98121072417264...| 0.0| +---+------------------+--------------------+--------------------+--------------------+--------------------+----------+","title":"Pipeline example"},{"location":"MSBD5003/notebooks%20in%20class/mllib/#example-analyzing-food-inspection-data-using-logistic-regression","text":"# Data at https://www.cse.ust.hk/msbd5003/data/Food_Inspections1.csv inspections = spark . read . csv ( '../data/Food_Inspections1.csv' , inferSchema = True ) Let's take a look at its schema: inspections . printSchema () root |-- _c0: integer (nullable = true) |-- _c1: string (nullable = true) |-- _c2: string (nullable = true) |-- _c3: integer (nullable = true) |-- _c4: string (nullable = true) |-- _c5: string (nullable = true) |-- _c6: string (nullable = true) |-- _c7: string (nullable = true) |-- _c8: string (nullable = true) |-- _c9: integer (nullable = true) |-- _c10: string (nullable = true) |-- _c11: string (nullable = true) |-- _c12: string (nullable = true) |-- _c13: string (nullable = true) |-- _c14: double (nullable = true) |-- _c15: double (nullable = true) |-- _c16: string (nullable = true) inspections . show () +------+--------------------+--------------------+-------+--------------------+---------------+--------------------+-------+---+-----+----------+--------------------+------------------+--------------------+------------------+------------------+--------------------+ | _c0| _c1| _c2| _c3| _c4| _c5| _c6| _c7|_c8| _c9| _c10| _c11| _c12| _c13| _c14| _c15| _c16| +------+--------------------+--------------------+-------+--------------------+---------------+--------------------+-------+---+-----+----------+--------------------+------------------+--------------------+------------------+------------------+--------------------+ |413707| LUNA PARK INC| LUNA PARK DAY CARE|2049789|Children's Servic...| Risk 1 (High)| 3250 W FOSTER AVE |CHICAGO| IL|60625|09/21/2010| License-Task Force| Fail|24. DISH WASHING ...| 41.97583445690982| -87.7107455232781|(41.9758344569098...| |391234| CAFE SELMARIE| CAFE SELMARIE|1069067| Restaurant| Risk 1 (High)| 4729 N LINCOLN AVE |CHICAGO| IL|60625|09/21/2010| Canvass| Fail|2. FACILITIES TO ...| 41.96740659751604|-87.68761642361608|(41.9674065975160...| |413751| MANCHU WOK|MANCHU WOK (T3-H/...|1909522| Restaurant| Risk 1 (High)| 11601 W TOUHY AVE |CHICAGO| IL|60666|09/21/2010| Canvass| Pass|33. FOOD AND NON-...|42.008536400868735|-87.91442843927047|(42.0085364008687...| |413708|BENCHMARK HOSPITA...|BENCHMARK HOSPITA...|2049411| Restaurant| Risk 1 (High)|325 N LA SALLE ST...|CHICAGO| IL|60654|09/21/2010|Task Force Liquor...| Pass| null| 41.88819879207664|-87.63236298373182|(41.8881987920766...| |413722| JJ BURGER| JJ BURGER|2055016| Restaurant|Risk 2 (Medium)| 749 S CICERO AVE |CHICAGO| IL|60644|09/21/2010| License| Pass| null| 41.87082601444883|-87.74476763884662|(41.8708260144488...| |413752|GOLDEN HOOKS FISH...|GOLDEN HOOKS FISH...|2042435| Restaurant|Risk 2 (Medium)| 3958 W MONROE ST |CHICAGO| IL|60624|09/21/2010|Short Form Complaint| Pass| null| 41.87987261425607|-87.72551692436804|(41.8798726142560...| |413714|THE DOCK AT MONTR...|THE DOCK AT MONTR...|2043260| Restaurant| Risk 1 (High)| 4400 N SIMONDS DR |CHICAGO| IL|60640|09/21/2010| License| Fail| null| 41.96390893734172|-87.63863624840039|(41.9639089373417...| |413753|CLARK FOOD & CIGA...| null|2042203| Grocery Store| Risk 3 (Low)|6761 N CLARK ST B...|CHICAGO| IL|60626|09/21/2010| Canvass| Pass| null| 42.0053117273606|-87.67294053846207|(42.0053117273606...| |120580| SUSHI PINK| SUSHI PINK|1847340| Restaurant| Risk 1 (High)|909 W WASHINGTON ...|CHICAGO| IL|60607|09/21/2010| Canvass| Pass|32. FOOD AND NON-...|41.882987317760424|-87.65014022876997|(41.8829873177604...| |401216| M.H.R.,L.L.C.| M.H.R.,L.L.C.|1621323| Restaurant|Risk 2 (Medium)| 623 S WABASH AVE |CHICAGO| IL|60605|09/21/2010| Canvass| Out of Business| null| 41.87390845559158|-87.62583770570953|(41.8739084555915...| |413715| NABO'S| NABO'S|1931861| Restaurant| Risk 1 (High)| 3351 N BROADWAY |CHICAGO| IL|60657|09/21/2010|Canvass Re-Inspec...| Pass|19. OUTSIDE GARBA...| 41.94334005547684|-87.64466387044959|(41.9433400554768...| |413721|THE NICHOLSON SCHOOL|THE NICHOLSON SCHOOL|2002702|Daycare (2 - 6 Ye...| Risk 1 (High)| 1700 W CORTLAND ST |CHICAGO| IL|60622|09/21/2010| License| Pass| null| 41.91618227133264| -87.6703413842735|(41.9161822713326...| |401215| M.H.R.,L.L.C.| M.H.R.,L.L.C.|1621322| Restaurant| Risk 1 (High)| 600 S MICHIGAN AVE |CHICAGO| IL|60605|09/21/2010| Canvass| Out of Business| null| 41.87437161535891|-87.62437952778167|(41.8743716153589...| |420207| WHOLE FOODS MARKET| WHOLE FOODS MARKET|1933690| Grocery Store| Risk 1 (High)|1550 N KINGSBURY ST |CHICAGO| IL|60642|09/21/2010| Complaint| Pass|32. FOOD AND NON-...| 41.90939878780941|-87.65305069789407|(41.9093987878094...| |154514| LAS FUENTES| LAS FUENTES| 12575| Restaurant| Risk 1 (High)| 2558 N HALSTED ST |CHICAGO| IL|60614|09/21/2010| Canvass| Fail|18. NO EVIDENCE O...| 41.9290354100918|-87.64903392789199|(41.9290354100918...| |413711|CASA CENTRAL COMM...|CASA CENTRAL COMM...| 60766| Restaurant| Risk 1 (High)|1343 N CALIFORNIA...|CHICAGO| IL|60622|09/21/2010| License| Pass|41. PREMISES MAIN...| 41.90598597077873|-87.69680735572291|(41.9059859707787...| |413764|LA BRUQUENA RESTA...|LA BRUQUENA RESTA...|1492868| Restaurant| Risk 1 (High)| 2726 W DIVISION ST |CHICAGO| IL|60622|09/21/2010|Suspected Food Po...|Pass w/ Conditions|4. SOURCE OF CROS...|41.903046386818346| -87.695535129416|(41.9030463868183...| |413732|SODEXHO AT UNITED...|SODEXHO AT UNITED...| 20467| Restaurant| Risk 1 (High)| 11601 W TOUHY AVE |CHICAGO| IL|60666|09/21/2010| Canvass| Out of Business| null|42.008536400868735|-87.91442843927047|(42.0085364008687...| |413757|WHIZ KIDS NURSERY...|WHIZ KIDS NURSERY...|1948277|Daycare Above and...| Risk 1 (High)| 514-522 W 103RD ST |CHICAGO| IL|60628|09/21/2010| Canvass| Pass|35. WALLS, CEILIN...|41.707112812685075|-87.63620425242559|(41.7071128126850...| |363272|FRUTERIA GUAYAUIT...|FRUTERIA GUAYAUIT...|1446823| Grocery Store| Risk 3 (Low)| 3849 S KEDZIE AVE |CHICAGO| IL|60632|09/21/2010| Canvass| Out of Business| null| 41.82290845958193|-87.70426021024545|(41.8229084595819...| +------+--------------------+--------------------+-------+--------------------+---------------+--------------------+-------+---+-----+----------+--------------------+------------------+--------------------+------------------+------------------+--------------------+ only showing top 20 rows We now have the CSV file as a DataFrame. It has some columns we will not use. Dropping them can save memory when caching the DataFrame. Also, we should give these columns meaningful names. # Drop unused columns and rename interesting columns. # Keep interesting columns and rename them to something meaningful # Mapping column index to name. columnNames = { 0 : \"id\" , 1 : \"name\" , 12 : \"results\" , 13 : \"violations\" } # Rename column from '_c{id}' to something meaningful. cols = [ inspections [ i ] . alias ( columnNames [ i ]) for i in columnNames . keys ()] # Drop columns we are not using. df = inspections . select ( cols ) . where ( col ( 'violations' ) . isNotNull ()) df . cache () df . show () df . count () +------+--------------------+------------------+--------------------+ | id| name| results| violations| +------+--------------------+------------------+--------------------+ |413707| LUNA PARK INC| Fail|24. DISH WASHING ...| |391234| CAFE SELMARIE| Fail|2. FACILITIES TO ...| |413751| MANCHU WOK| Pass|33. FOOD AND NON-...| |120580| SUSHI PINK| Pass|32. FOOD AND NON-...| |413715| NABO'S| Pass|19. OUTSIDE GARBA...| |420207| WHOLE FOODS MARKET| Pass|32. FOOD AND NON-...| |154514| LAS FUENTES| Fail|18. NO EVIDENCE O...| |413711|CASA CENTRAL COMM...| Pass|41. PREMISES MAIN...| |413764|LA BRUQUENA RESTA...|Pass w/ Conditions|4. SOURCE OF CROS...| |413757|WHIZ KIDS NURSERY...| Pass|35. WALLS, CEILIN...| |154516|TACO & BURRITO PA...| Pass|30. FOOD IN ORIGI...| |413759| MARISCOS EL VENENO| Pass|18. NO EVIDENCE O...| |114554|THE HANGGE- UPPE,...| Pass|18. NO EVIDENCE O...| |413758| LINDY'S CHILI INC|Pass w/ Conditions|30. FOOD IN ORIGI...| |343362| FUMARE MEATS| Pass|40. REFRIGERATION...| |413754| Subway| Pass|38. VENTILATION: ...| |289222|LITTLE CAESARS PIZZA| Pass|34. FLOORS: CONST...| |413755| BILLY'S GRILL| Pass|33. FOOD AND NON-...| |343364| FRESHII| Fail|18. NO EVIDENCE O...| |289221|NICKY'S GRILL & Y...| Pass|33. FOOD AND NON-...| +------+--------------------+------------------+--------------------+ only showing top 20 rows 10469 df . take ( 1 ) [Row(id=413707, name='LUNA PARK INC', results='Fail', violations='24. DISH WASHING FACILITIES: PROPERLY DESIGNED, CONSTRUCTED, MAINTAINED, INSTALLED, LOCATED AND OPERATED - Comments: All dishwashing machines must be of a type that complies with all requirements of the plumbing section of the Municipal Code of Chicago and Rules and Regulation of the Board of Health. OBSEVERD THE 3 COMPARTMENT SINK BACKING UP INTO THE 1ST AND 2ND COMPARTMENT WITH CLEAR WATER AND SLOWLY DRAINING OUT. INST NEED HAVE IT REPAIR. CITATION ISSUED, SERIOUS VIOLATION 7-38-030 H000062369-10 COURT DATE 10-28-10 TIME 1 P.M. ROOM 107 400 W. SURPERIOR. | 36. LIGHTING: REQUIRED MINIMUM FOOT-CANDLES OF LIGHT PROVIDED, FIXTURES SHIELDED - Comments: Shielding to protect against broken glass falling into food shall be provided for all artificial lighting sources in preparation, service, and display facilities. LIGHT SHIELD ARE MISSING UNDER HOOD OF COOKING EQUIPMENT AND NEED TO REPLACE LIGHT UNDER UNIT. 4 LIGHTS ARE OUT IN THE REAR CHILDREN AREA,IN THE KINDERGARDEN CLASS ROOM. 2 LIGHT ARE OUT EAST REAR, LIGHT FRONT WEST ROOM. NEED TO REPLACE ALL LIGHT THAT ARE NOT WORKING. | 35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: The walls and ceilings shall be in good repair and easily cleaned. MISSING CEILING TILES WITH STAINS IN WEST,EAST, IN FRONT AREA WEST, AND BY THE 15MOS AREA. NEED TO BE REPLACED. | 32. FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: All food and non-food contact equipment and utensils shall be smooth, easily cleanable, and durable, and shall be in good repair. SPLASH GUARDED ARE NEEDED BY THE EXPOSED HAND SINK IN THE KITCHEN AREA | 34. FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED - Comments: The floors shall be constructed per code, be smooth and easily cleaned, and be kept clean and in good repair. INST NEED TO ELEVATE ALL FOOD ITEMS 6INCH OFF THE FLOOR 6 INCH AWAY FORM WALL. ')] The output of the above cell gives us an idea of the schema of the input file; the file includes the name of every establishment, the type of establishment, the address, the data of the inspections, and the location, among other things. Let's start to get a sense of what our dataset contains. For example, what are the different values in the results column? df . select ( 'results' ) . distinct () . show () +------------------+ | results| +------------------+ | Fail| |Pass w/ Conditions| | Pass| +------------------+ df . groupBy ( 'results' ) . count () . show () +------------------+-----+ | results|count| +------------------+-----+ | Fail| 2607| |Pass w/ Conditions| 1028| | Pass| 6834| +------------------+-----+ Let us develop a model that can guess the outcome of a food inspection, given the violations. Since logistic regression is a binary classification method, it makes sense to group our data into two categories: Fail and Pass . A \"Pass w/ Conditions\" is still a Pass, so when we train the model, we will consider the two results equivalent. Let us go ahead and convert our existing dataframe( df ) into a new dataframe where each inspection is represented as a label-violations pair. In our case, a label of 0 represents a failure, a label of 1 represents a success. # The function to clean the data labeledData = df . select ( when ( df . results == 'Fail' , 0 ) . when ( df . results == 'Pass' , 1 ) . otherwise ( 1 ) . alias ( 'label' ), 'violations' ) labeledData . show () +-----+--------------------+ |label| violations| +-----+--------------------+ | 0|24. DISH WASHING ...| | 0|2. FACILITIES TO ...| | 1|33. FOOD AND NON-...| | 1|32. FOOD AND NON-...| | 1|19. OUTSIDE GARBA...| | 1|32. FOOD AND NON-...| | 0|18. NO EVIDENCE O...| | 1|41. PREMISES MAIN...| | 1|4. SOURCE OF CROS...| | 1|35. WALLS, CEILIN...| | 1|30. FOOD IN ORIGI...| | 1|18. NO EVIDENCE O...| | 1|18. NO EVIDENCE O...| | 1|30. FOOD IN ORIGI...| | 1|40. REFRIGERATION...| | 1|38. VENTILATION: ...| | 1|34. FLOORS: CONST...| | 1|33. FOOD AND NON-...| | 0|18. NO EVIDENCE O...| | 1|33. FOOD AND NON-...| +-----+--------------------+ only showing top 20 rows Train a logistic regression model from the input dataframe trainingData , testData = labeledData . randomSplit ([ 0.8 , 0.2 ]) tokenizer = Tokenizer ( inputCol = \"violations\" , outputCol = \"words\" ) hashingTF = HashingTF ( inputCol = tokenizer . getOutputCol (), outputCol = \"features\" ) lr = LogisticRegression ( maxIter = 10 , regParam = 0.01 ) pipeline = Pipeline ( stages = [ tokenizer , hashingTF , lr ]) model = pipeline . fit ( trainingData ) predictionsDf = model . transform ( testData ) predictionsDf . show () +-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+ |label| violations| words| features| rawPrediction| probability|prediction| +-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+ | 0|1. SOURCE SOUND C...|[1., source, soun...|(262144,[699,1411...|[8.21045734370492...|[0.99972827746471...| 0.0| | 0|10. SEWAGE AND WA...|[10., sewage, and...|(262144,[1797,542...|[0.72532689463172...|[0.67377895526181...| 0.0| | 0|10. SEWAGE AND WA...|[10., sewage, and...|(262144,[2786,610...|[4.76342457489383...|[0.99153592609727...| 0.0| | 0|12. HAND WASHING ...|[12., hand, washi...|(262144,[1455,278...|[7.50001649481132...|[0.99944723047594...| 0.0| | 0|13. NO EVIDENCE O...|[13., no, evidenc...|(262144,[284,1797...|[10.5730088915445...|[0.99997440297665...| 0.0| | 0|13. NO EVIDENCE O...|[13., no, evidenc...|(262144,[2786,610...|[2.06744925670706...|[0.88769893037329...| 0.0| | 0|16. FOOD PROTECTE...|[16., food, prote...|(262144,[6106,914...|[-1.4603273358789...|[0.18841726515000...| 1.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[161,1797...|[1.20630674122104...|[0.76964481719305...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[1797,278...|[0.20311794686958...|[0.55060562031582...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[161,2325...|[4.42895943956892...|[0.98821368031221...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[2786,542...|[4.38608040086412...|[0.98770365203887...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[2786,361...|[2.47491920164261...|[0.92236475183085...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[2325,278...|[2.42852129891198...|[0.91897649882414...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[2786,542...|[2.52500010935656...|[0.92587594231444...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[2786,649...|[11.3588170519728...|[0.99998833396253...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[2786,610...|[4.93010908820459...|[0.99282612140406...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[2786,610...|[2.09044930412383...|[0.88997143023289...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[2786,656...|[4.78228732981419...|[0.99169277163220...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[2325,278...|[5.37386912945687...|[0.99538523611218...| 0.0| | 0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[161,877,...|[-1.4626391002146...|[0.18806401354055...| 1.0| +-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+ only showing top 20 rows numSuccesses = predictionsDf . where ( 'label == prediction' ) . count () numInspections = predictionsDf . count () print ( \"There were %d inspections and there were %d successful predictions\" % ( numInspections , numSuccesses )) print ( \"This is a %d%% success rate\" % ( float ( numSuccesses ) / float ( numInspections ) * 100 )) There were 2131 inspections and there were 1859 successful predictions This is a 87% success rate","title":"Example: Analyzing food inspection data using logistic regression"},{"location":"MSBD5003/notebooks%20in%20class/mllib/#cross-validation","text":"CrossValidator begins by splitting the dataset into a set of folds which are used as separate training and test datasets. E.g., with k=5 folds, CrossValidator will generate 5 (training, test) dataset pairs, each of which uses 4/5 of the data for training and 1/5 for testing. To evaluate a particular ParamMap, CrossValidator computes the average evaluation metric for the 5 Models produced by fitting the Estimator on the 5 different (training, test) dataset pairs. After identifying the best ParamMap, CrossValidator finally re-fits the Estimator using the best ParamMap and the entire dataset. # We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance. # This will allow us to jointly choose parameters for all Pipeline stages. # A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator. # We use a ParamGridBuilder to construct a grid of parameters to search over. # With 3 values for hashingTF.numFeatures and 2 values for lr.regParam, # this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from. paramGrid = ParamGridBuilder () \\ . addGrid ( hashingTF . numFeatures , [ 10 , 100 , 1000 ]) \\ . addGrid ( lr . regParam , [ 0.1 , 0.01 ]) \\ . build () crossval = CrossValidator ( estimator = pipeline , estimatorParamMaps = paramGrid , evaluator = BinaryClassificationEvaluator (), numFolds = 5 ) # Run cross-validation, and choose the best set of parameters. cvModel = crossval . fit ( trainingData ) predictionsDf = cvModel . transform ( testData ) numSuccesses = predictionsDf . where ( 'label == prediction' ) . count () numInspections = predictionsDf . count () print ( \"There were %d inspections and there were %d successful predictions\" % ( numInspections , numSuccesses )) print ( \"This is a %d%% success rate\" % ( float ( numSuccesses ) / float ( numInspections ) * 100 )) There were 2131 inspections and there were 1909 successful predictions This is a 89% success rate cvModel . explainParams () \"estimator: estimator to be cross-validated (current: Pipeline_c5b1e2db18ab)\\nestimatorParamMaps: estimator param maps (current: [{Param(parent='HashingTF_7bc1a253c00d', name='numFeatures', doc='Number of features. Should be greater than 0.'): 10, Param(parent='LogisticRegression_0a9fda7dc002', name='regParam', doc='regularization parameter (>= 0).'): 0.1}, {Param(parent='HashingTF_7bc1a253c00d', name='numFeatures', doc='Number of features. Should be greater than 0.'): 10, Param(parent='LogisticRegression_0a9fda7dc002', name='regParam', doc='regularization parameter (>= 0).'): 0.01}, {Param(parent='HashingTF_7bc1a253c00d', name='numFeatures', doc='Number of features. Should be greater than 0.'): 100, Param(parent='LogisticRegression_0a9fda7dc002', name='regParam', doc='regularization parameter (>= 0).'): 0.1}, {Param(parent='HashingTF_7bc1a253c00d', name='numFeatures', doc='Number of features. Should be greater than 0.'): 100, Param(parent='LogisticRegression_0a9fda7dc002', name='regParam', doc='regularization parameter (>= 0).'): 0.01}, {Param(parent='HashingTF_7bc1a253c00d', name='numFeatures', doc='Number of features. Should be greater than 0.'): 1000, Param(parent='LogisticRegression_0a9fda7dc002', name='regParam', doc='regularization parameter (>= 0).'): 0.1}, {Param(parent='HashingTF_7bc1a253c00d', name='numFeatures', doc='Number of features. Should be greater than 0.'): 1000, Param(parent='LogisticRegression_0a9fda7dc002', name='regParam', doc='regularization parameter (>= 0).'): 0.01}])\\nevaluator: evaluator used to select hyper-parameters that maximize the validator metric (current: BinaryClassificationEvaluator_17036e9cdcf5)\\nnumFolds: number of folds for cross validation (default: 3, current: 5)\\nseed: random seed. (default: 7809051150349531440)\"","title":"Cross-Validation"},{"location":"MSBD5003/notebooks%20in%20class/rdd/","text":"! pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 67kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 43.8MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=2e05d41450b321a95e338481ca93d3ac73ca8949d26874ce7b1a793684007856 Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 from google.colab import drive drive . mount ( '/content/drive' ) Mounted at /content/drive from pyspark.context import SparkContext from pyspark.sql.session import SparkSession sc = SparkContext . getOrCreate () spark = SparkSession ( sc ) How do I make an RDD? \u00b6 RDDs can be created from stable storage or by transforming other RDDs. Run the cells below to create RDDs from files on the local drive. All data files can be downloaded from https://www.cse.ust.hk/msbd5003/data/ For example, https://www.cse.ust.hk/msbd5003/data/fruits.txt # Read data from local file system: print ( sc . version ) fruits = sc . textFile ( '../data/fruits.txt' ) yellowThings = sc . textFile ( '../data/yellowthings.txt' ) print ( fruits . collect ()) print ( yellowThings . collect ()) 3.0.0 ['apple', 'banana', 'canary melon', 'grape', 'lemon', 'orange', 'pineapple', 'strawberry'] ['banana', 'bee', 'butter', 'canary melon', 'gold', 'lemon', 'pineapple', 'sunflower'] # Read data from HDFS : fruits = sc . textFile ( 'hdfs://url:9000/pathname/fruits.txt' ) fruits . collect () RDD operations \u00b6 # map fruitsReversed = fruits . map ( lambda fruit : fruit [:: - 1 ]) # fruitsReversed = fruits.map(lambda fruit: fruit[::-1]) fruitsReversed . persist () # try changing the file and re-execute with and without cache print ( fruitsReversed . collect ()) # What happens when you uncomment the first line and run the whole program again with cache()? ['elppab', 'ananab', 'nolem yranac', 'parg', 'nomel', 'egnaro', 'elppaenip', 'yrrebwarts'] # filter shortFruits = fruits . filter ( lambda fruit : len ( fruit ) <= 5 ) print ( shortFruits . collect ()) ['grap', 'lemon'] # flatMap characters = fruits . flatMap ( lambda fruit : list ( fruit )) print ( characters . collect ()) ['b', 'a', 'p', 'p', 'l', 'e', 'b', 'a', 'n', 'a', 'n', 'a', 'c', 'a', 'n', 'a', 'r', 'y', ' ', 'm', 'e', 'l', 'o', 'n', 'g', 'r', 'a', 'p', 'l', 'e', 'm', 'o', 'n', 'o', 'r', 'a', 'n', 'g', 'e', 'p', 'i', 'n', 'e', 'a', 'p', 'p', 'l', 'e', 's', 't', 'r', 'a', 'w', 'b', 'e', 'r', 'r', 'y'] # union fruitsAndYellowThings = fruits . union ( yellowThings ) print ( fruitsAndYellowThings . collect ()) ['bapple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry', 'banana', 'bee', 'butter', 'canary melon', 'gold', 'lemon', 'pineapple', 'sunflower'] # intersection yellowFruits = fruits . intersection ( yellowThings ) print ( yellowFruits . collect ()) ['pineapple', 'canary melon', 'lemon', 'banana'] # distinct distinctFruitsAndYellowThings = fruitsAndYellowThings . distinct () print ( distinctFruitsAndYellowThings . collect ()) ['orange', 'pineapple', 'canary melon', 'lemon', 'bee', 'banana', 'butter', 'gold', 'sunflower', 'apple', 'grap', 'strawberry'] RDD actions \u00b6 Following are examples of some of the common actions available. For a detailed list, see RDD Actions . Run some transformations below to understand this better. Place the cursor in the cell and press SHIFT + ENTER . # collect fruitsArray = fruits . collect () yellowThingsArray = yellowThings . collect () print ( fruitsArray ) ['apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry'] # count numFruits = fruits . count () print ( numFruits ) 8 # take first3Fruits = fruits . take ( 3 ) print ( first3Fruits ) ['apple', 'banana', 'canary melon'] # reduce letterSet = fruits . map ( lambda fruit : set ( fruit )) . reduce ( lambda x , y : x . union ( y )) print ( letterSet ) {'o', 'r', 'a', 'i', 'p', 'g', 'c', ' ', 'l', 'y', 'e', 'w', 'n', 'b', 'm', 't', 's'} letterSet = fruits . flatMap ( lambda fruit : list ( fruit )) . distinct () . collect () print ( letterSet ) ['p', 'l', 'b', 'c', 'r', 'y', 'g', 'i', 's', 'a', 'e', 'n', ' ', 'm', 'o', 't', 'w'] Closure \u00b6 counter = 0 rdd = sc . parallelize ( range ( 10 )) # Wrong: Don't do this!! def increment_counter ( x ): global counter counter += x print ( rdd . collect ()) rdd . foreach ( increment_counter ) print ( counter ) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 0 rdd = sc . parallelize ( range ( 10 )) accum = sc . accumulator ( 0 ) def g ( x ): global accum accum += x a = rdd . foreach ( g ) print ( accum . value ) -45 rdd = sc . parallelize ( range ( 10 )) accum = sc . accumulator ( 0 ) def g ( x ): global accum accum += x return x * x a = rdd . map ( g ) print ( accum . value ) #print(a.reduce(lambda x, y: x+y)) a . cache () tmp = a . count () print ( accum . value ) print ( rdd . reduce ( lambda x , y : x + y )) tmp = a . count () print ( accum . value ) print ( rdd . reduce ( lambda x , y : x + y )) 0 45 45 45 45 Computing Pi using Monte Carlo simulation \u00b6 # From the official spark examples. import random import time partitions = 1000 n = 1000 * partitions def f ( _ ): x = random . random () y = random . random () return 1 if x ** 2 + y ** 2 < 1 else 0 count = sc . parallelize ( range ( 1 , n + 1 ), partitions ) \\ . map ( f ) . sum () print ( \"Pi is roughly\" , 4.0 * count / n ) Pi is roughly 3.140944 # Example: glom import sys import random def f ( _ ): random . seed ( time . time ()) return random . random () a = sc . parallelize ( range ( 0 , 100 ), 10 ) print ( a . collect ()) print ( a . glom () . collect ()) print ( a . map ( f ) . glom () . collect ()) # Weird behavior: Initially, random numbers are synched across all workers, but will get # out-of-sync after a large (e.g, 1000000) number of random numbers have been generated. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [[0.6608713426170987, 0.698767024318554, 0.1874105777790005, 0.7623702078433652, 0.8851287594440702, 0.31740294580255735, 0.19323310102732394, 0.42450071105921683, 0.5933781451859748, 0.7458943680790939], [0.7261502175930336, 0.22659503054053598, 0.9192074261481535, 0.4774662604141523, 0.7974422880272903, 0.2584976474338707, 0.6055611352765481, 0.5244790798752513, 0.6861813792912159, 0.5652815222674437], [0.27860057141024697, 0.27383515025078553, 0.9176819782462265, 0.417689753313761, 0.6135860183360143, 0.8162090147099693, 0.39224804876974406, 0.543173888187219, 0.3098912544023783, 0.633182881742779], [0.0952563896474653, 0.7477071810186972, 0.5004564582092008, 0.2614834043253954, 0.5982982446751687, 0.8544002333592715, 0.26000819037953216, 0.40177311792144454, 0.03851083747397188, 0.05167636277510712], [0.9726302497724043, 0.42432064255976365, 0.9305610323744404, 0.771694551386715, 0.6789841281422876, 0.9487832709253969, 0.4943030306526911, 0.22888583384514705, 0.6165263440265218, 0.8948635092093183], [0.9816006872849989, 0.3233518004555158, 0.6660672115030636, 0.9921564654020117, 0.9574487554669273, 0.00033642413291157247, 0.5729463981674527, 0.63676146970985, 0.1068707761119706, 0.4974835849045728], [0.6877782810075579, 0.11000878013616322, 0.6630366287015564, 0.0320757478156235, 0.5550374523078817, 0.11429763248899893, 0.7746616174182379, 0.6935564378314162, 0.6081187039755812, 0.3594774747771995], [0.3402744125431225, 0.8533066685831103, 0.18605963113570156, 0.9700428171414653, 0.9046533776474858, 0.4199976147427207, 0.01833313615444565, 0.5003118405702941, 0.9167261953361863, 0.6543553598701435], [0.5463089308369264, 0.19187434980340723, 0.5311179490604816, 0.7210872364087648, 0.25848050944241396, 0.9138829006068386, 0.5015098582184656, 0.9245322749204768, 0.4746635193819774, 0.733561516539988], [0.5924804586325896, 0.44157691425623313, 0.06474182310396659, 0.3705313104712945, 0.218280453275444, 0.911250263493956, 0.4908690024649712, 0.031427016100674665, 0.3749922950484815, 0.29534800562581187]] # Example: mapPartition and mapPartitionWithIndex a = sc . parallelize ( range ( 0 , 20 ), 4 ) print ( a . glom () . collect ()) def f ( it ): s = 0 for i in it : s += i yield s print ( a . mapPartitions ( f ) . collect ()) def f ( index , it ): s = index for i in it : s += i yield s print ( a . mapPartitionsWithIndex ( f ) . collect ()) [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]] [0, 1, 3, 6, 10, 5, 11, 18, 26, 35, 10, 21, 33, 46, 60, 15, 31, 48, 66, 85] [0, 1, 3, 6, 10, 6, 12, 19, 27, 36, 12, 23, 35, 48, 62, 18, 34, 51, 69, 88] # Correct version import random import time partitions = 1000 n = 1000 * partitions seed = time . time () def f ( index , it ): random . seed ( index + seed ) for i in it : x = random . random () y = random . random () yield 1 if x ** 2 + y ** 2 < 1 else 0 count = sc . parallelize ( range ( 1 , n + 1 ), partitions ) \\ . mapPartitionsWithIndex ( f ) . sum () print ( \"Pi is roughly\" , 4.0 * count / n ) Pi is roughly 3.141832 Closure and Persistence \u00b6 # RDD variables are references A = sc . parallelize ( range ( 10 )) B = A . map ( lambda x : x * 2 ) A = B . map ( lambda x : x + 1 ) A . take ( 10 ) [1, 3, 5, 7, 9, 11, 13, 15, 17, 19] # Linear-time selection data = [ 34 , 67 , 21 , 56 , 47 , 89 , 12 , 44 , 74 , 43 , 26 ] A = sc . parallelize ( data , 2 ) k = 4 while True : x = A . first () A1 = A . filter ( lambda z : z < x ) A2 = A . filter ( lambda z : z > x ) A1 . cache () A2 . cache () mid = A1 . count () if mid == k : print ( x ) break if k < mid : A = A1 else : A = A2 k = k - mid - 1 43 sorted ( data ) [12, 21, 26, 34, 43, 44, 47, 56, 67, 74, 89] A = sc . parallelize ( range ( 10 )) x = 5 B = A . filter ( lambda z : z < x ) # B.cache() print ( B . count ()) x = 3 print ( B . count ()) 5 5 A = sc . parallelize ( range ( 10 )) x = 5 B = A . filter ( lambda z : z < x ) # B.cache() B . unpersist () # print(B.take(10)) print ( B . collect ()) x = 3 #print(B.take(10)) print ( B . collect ()) # collect() doesn't always re-collect data - bad design! # Always use take() instead of collect() [0, 1, 2, 3, 4] [0, 1, 2, 3, 4] Key-Value Pairs \u00b6 # reduceByKey numFruitsByLength = fruits . map ( lambda fruit : ( len ( fruit ), 1 )) . reduceByKey ( lambda x , y : x + y ) print ( numFruitsByLength . take ( 10 )) from operator import add lines = sc . textFile ( '../data/course.txt' ) counts = lines . flatMap ( lambda x : x . split ()) \\ . map ( lambda x : ( x , 1 )) \\ . reduceByKey ( add ) print ( counts . sortByKey () . take ( 20 )) print ( counts . sortBy ( lambda x : x [ 1 ], False ) . take ( 20 )) # Join simple example products = sc . parallelize ([( 1 , \"Apple\" ), ( 2 , \"Orange\" ), ( 3 , \"TV\" ), ( 5 , \"Computer\" )]) #trans = sc.parallelize([(1, 134, \"OK\"), (3, 34, \"OK\"), (5, 162, \"Error\"), (1, 135, \"OK\"), (2, 53, \"OK\"), (1, 45, \"OK\")]) trans = sc . parallelize ([( 1 , ( 134 , \"OK\" )), ( 3 , ( 34 , \"OK\" )), ( 5 , ( 162 , \"Error\" )), ( 1 , ( 135 , \"OK\" )), ( 2 , ( 53 , \"OK\" )), ( 1 , ( 45 , \"OK\" ))]) print ( products . join ( trans ) . take ( 20 )) K-means clustering \u00b6 import numpy as np def parseVector ( line ): return np . array ([ float ( x ) for x in line . split ()]) def closestPoint ( p , centers ): bestIndex = 0 closest = float ( \"+inf\" ) for i in range ( len ( centers )): tempDist = np . sum (( p - centers [ i ]) ** 2 ) if tempDist < closest : closest = tempDist bestIndex = i return bestIndex # The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_data.txt lines = sc . textFile ( '../data/kmeans_data.txt' , 5 ) # The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_bigdata.txt # lines = sc.textFile('../data/kmeans_bigdata.txt', 5) # lines is an RDD of strings K = 3 convergeDist = 0.01 # terminate algorithm when the total distance from old center to new centers is less than this value data = lines . map ( parseVector ) . cache () # data is an RDD of arrays kCenters = data . takeSample ( False , K , 1 ) # intial centers as a list of arrays tempDist = 1.0 # total distance from old centers to new centers while tempDist > convergeDist : closest = data . map ( lambda p : ( closestPoint ( p , kCenters ), ( p , 1 ))) # for each point in data, find its closest center # closest is an RDD of tuples (index of closest center, (point, 1)) pointStats = closest . reduceByKey ( lambda p1 , p2 : ( p1 [ 0 ] + p2 [ 0 ], p1 [ 1 ] + p2 [ 1 ])) # pointStats is an RDD of tuples (index of center, # (array of sums of coordinates, total number of points assigned)) newCenters = pointStats . map ( lambda st : ( st [ 0 ], st [ 1 ][ 0 ] / st [ 1 ][ 1 ])) . collect () # compute the new centers tempDist = sum ( np . sum (( kCenters [ i ] - p ) ** 2 ) for ( i , p ) in newCenters ) # compute the total disctance from old centers to new centers for ( i , p ) in newCenters : kCenters [ i ] = p print ( \"Final centers: \" , kCenters ) PageRank \u00b6 import re from operator import add def computeContribs ( urls , rank ): # Calculates URL contributions to the rank of other URLs. num_urls = len ( urls ) for url in urls : yield ( url , rank / num_urls ) def parseNeighbors ( urls ): # Parses a urls pair string into urls pair.\"\"\" parts = urls . split ( ' ' ) return parts [ 0 ], parts [ 1 ] # Loads in input file. It should be in format of: # URL neighbor URL # URL neighbor URL # URL neighbor URL # ... # The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/* lines = sc . textFile ( \"/content/drive/My Drive/\u8bfe\u7a0b/HKUST/MSBD5003/homeworks/hw2/pagerank_data.txt\" , 2 ) # lines = sc.textFile(\"../data/dblp.in\", 5) numOfIterations = 10 # Loads all URLs from input file and initialize their neighbors. links = lines . map ( lambda urls : parseNeighbors ( urls )) \\ . groupByKey () # Loads all URLs with other URL(s) link to from input file # and initialize ranks of them to one. ranks = links . mapValues ( lambda neighbors : 1.0 ) print ( 'ranks' , ranks . collect ()) print ( 'links' , links . collect ()) # Calculates and updates URL ranks continuously using PageRank algorithm. for iteration in range ( numOfIterations ): # Calculates URL contributions to the rank of other URLs. contribs = links . join ( ranks ) \\ . flatMap ( lambda url_urls_rank : computeContribs ( url_urls_rank [ 1 ][ 0 ], url_urls_rank [ 1 ][ 1 ])) # After the join, each element in the RDD is of the form # (url, (list of neighbor urls, rank)) # Re-calculates URL ranks based on neighbor contributions. # ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15) ranks = contribs . reduceByKey ( add ) . map ( lambda t : ( t [ 0 ], t [ 1 ] * 0.85 + 0.15 )) print ( ranks . top ( 5 , lambda x : x [ 1 ])) ranks [('1', 1.0), ('4', 1.0), ('2', 1.0), ('3', 1.0)] links [('1', <pyspark.resultiterable.ResultIterable object at 0x7f8b12a20ef0>), ('4', <pyspark.resultiterable.ResultIterable object at 0x7f8b12a20a58>), ('2', <pyspark.resultiterable.ResultIterable object at 0x7f8b12a20780>), ('3', <pyspark.resultiterable.ResultIterable object at 0x7f8b12a207f0>)] [('1', 1.2981882732854677), ('4', 0.9999999999999998), ('3', 0.9999999999999998), ('2', 0.7018117267145316)] Join vs. Broadcast Variables \u00b6 products = sc . parallelize ([( 1 , \"Apple\" ), ( 2 , \"Orange\" ), ( 3 , \"TV\" ), ( 5 , \"Computer\" )]) trans = sc . parallelize ([( 1 , ( 134 , \"OK\" )), ( 3 , ( 34 , \"OK\" )), ( 5 , ( 162 , \"Error\" )), ( 1 , ( 135 , \"OK\" )), ( 2 , ( 53 , \"OK\" )), ( 1 , ( 45 , \"OK\" ))]) print ( trans . join ( products ) . take ( 20 )) products = { 1 : \"Apple\" , 2 : \"Orange\" , 3 : \"TV\" , 5 : \"Computer\" } trans = sc . parallelize ([( 1 , ( 134 , \"OK\" )), ( 3 , ( 34 , \"OK\" )), ( 5 , ( 162 , \"Error\" )), ( 1 , ( 135 , \"OK\" )), ( 2 , ( 53 , \"OK\" )), ( 1 , ( 45 , \"OK\" ))]) broadcasted_products = sc . broadcast ( products ) results = trans . map ( lambda x : ( x [ 0 ], broadcasted_products . value [ x [ 0 ]], x [ 1 ])) # results = trans.map(lambda x: (x[0], products[x[0]], x[1])) print ( results . take ( 20 ))","title":"Rdd"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#how-do-i-make-an-rdd","text":"RDDs can be created from stable storage or by transforming other RDDs. Run the cells below to create RDDs from files on the local drive. All data files can be downloaded from https://www.cse.ust.hk/msbd5003/data/ For example, https://www.cse.ust.hk/msbd5003/data/fruits.txt # Read data from local file system: print ( sc . version ) fruits = sc . textFile ( '../data/fruits.txt' ) yellowThings = sc . textFile ( '../data/yellowthings.txt' ) print ( fruits . collect ()) print ( yellowThings . collect ()) 3.0.0 ['apple', 'banana', 'canary melon', 'grape', 'lemon', 'orange', 'pineapple', 'strawberry'] ['banana', 'bee', 'butter', 'canary melon', 'gold', 'lemon', 'pineapple', 'sunflower'] # Read data from HDFS : fruits = sc . textFile ( 'hdfs://url:9000/pathname/fruits.txt' ) fruits . collect ()","title":"How do I make an RDD?"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#rdd-operations","text":"# map fruitsReversed = fruits . map ( lambda fruit : fruit [:: - 1 ]) # fruitsReversed = fruits.map(lambda fruit: fruit[::-1]) fruitsReversed . persist () # try changing the file and re-execute with and without cache print ( fruitsReversed . collect ()) # What happens when you uncomment the first line and run the whole program again with cache()? ['elppab', 'ananab', 'nolem yranac', 'parg', 'nomel', 'egnaro', 'elppaenip', 'yrrebwarts'] # filter shortFruits = fruits . filter ( lambda fruit : len ( fruit ) <= 5 ) print ( shortFruits . collect ()) ['grap', 'lemon'] # flatMap characters = fruits . flatMap ( lambda fruit : list ( fruit )) print ( characters . collect ()) ['b', 'a', 'p', 'p', 'l', 'e', 'b', 'a', 'n', 'a', 'n', 'a', 'c', 'a', 'n', 'a', 'r', 'y', ' ', 'm', 'e', 'l', 'o', 'n', 'g', 'r', 'a', 'p', 'l', 'e', 'm', 'o', 'n', 'o', 'r', 'a', 'n', 'g', 'e', 'p', 'i', 'n', 'e', 'a', 'p', 'p', 'l', 'e', 's', 't', 'r', 'a', 'w', 'b', 'e', 'r', 'r', 'y'] # union fruitsAndYellowThings = fruits . union ( yellowThings ) print ( fruitsAndYellowThings . collect ()) ['bapple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry', 'banana', 'bee', 'butter', 'canary melon', 'gold', 'lemon', 'pineapple', 'sunflower'] # intersection yellowFruits = fruits . intersection ( yellowThings ) print ( yellowFruits . collect ()) ['pineapple', 'canary melon', 'lemon', 'banana'] # distinct distinctFruitsAndYellowThings = fruitsAndYellowThings . distinct () print ( distinctFruitsAndYellowThings . collect ()) ['orange', 'pineapple', 'canary melon', 'lemon', 'bee', 'banana', 'butter', 'gold', 'sunflower', 'apple', 'grap', 'strawberry']","title":"RDD operations"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#rdd-actions","text":"Following are examples of some of the common actions available. For a detailed list, see RDD Actions . Run some transformations below to understand this better. Place the cursor in the cell and press SHIFT + ENTER . # collect fruitsArray = fruits . collect () yellowThingsArray = yellowThings . collect () print ( fruitsArray ) ['apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry'] # count numFruits = fruits . count () print ( numFruits ) 8 # take first3Fruits = fruits . take ( 3 ) print ( first3Fruits ) ['apple', 'banana', 'canary melon'] # reduce letterSet = fruits . map ( lambda fruit : set ( fruit )) . reduce ( lambda x , y : x . union ( y )) print ( letterSet ) {'o', 'r', 'a', 'i', 'p', 'g', 'c', ' ', 'l', 'y', 'e', 'w', 'n', 'b', 'm', 't', 's'} letterSet = fruits . flatMap ( lambda fruit : list ( fruit )) . distinct () . collect () print ( letterSet ) ['p', 'l', 'b', 'c', 'r', 'y', 'g', 'i', 's', 'a', 'e', 'n', ' ', 'm', 'o', 't', 'w']","title":"RDD actions"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#closure","text":"counter = 0 rdd = sc . parallelize ( range ( 10 )) # Wrong: Don't do this!! def increment_counter ( x ): global counter counter += x print ( rdd . collect ()) rdd . foreach ( increment_counter ) print ( counter ) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 0 rdd = sc . parallelize ( range ( 10 )) accum = sc . accumulator ( 0 ) def g ( x ): global accum accum += x a = rdd . foreach ( g ) print ( accum . value ) -45 rdd = sc . parallelize ( range ( 10 )) accum = sc . accumulator ( 0 ) def g ( x ): global accum accum += x return x * x a = rdd . map ( g ) print ( accum . value ) #print(a.reduce(lambda x, y: x+y)) a . cache () tmp = a . count () print ( accum . value ) print ( rdd . reduce ( lambda x , y : x + y )) tmp = a . count () print ( accum . value ) print ( rdd . reduce ( lambda x , y : x + y )) 0 45 45 45 45","title":"Closure"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#computing-pi-using-monte-carlo-simulation","text":"# From the official spark examples. import random import time partitions = 1000 n = 1000 * partitions def f ( _ ): x = random . random () y = random . random () return 1 if x ** 2 + y ** 2 < 1 else 0 count = sc . parallelize ( range ( 1 , n + 1 ), partitions ) \\ . map ( f ) . sum () print ( \"Pi is roughly\" , 4.0 * count / n ) Pi is roughly 3.140944 # Example: glom import sys import random def f ( _ ): random . seed ( time . time ()) return random . random () a = sc . parallelize ( range ( 0 , 100 ), 10 ) print ( a . collect ()) print ( a . glom () . collect ()) print ( a . map ( f ) . glom () . collect ()) # Weird behavior: Initially, random numbers are synched across all workers, but will get # out-of-sync after a large (e.g, 1000000) number of random numbers have been generated. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [[0.6608713426170987, 0.698767024318554, 0.1874105777790005, 0.7623702078433652, 0.8851287594440702, 0.31740294580255735, 0.19323310102732394, 0.42450071105921683, 0.5933781451859748, 0.7458943680790939], [0.7261502175930336, 0.22659503054053598, 0.9192074261481535, 0.4774662604141523, 0.7974422880272903, 0.2584976474338707, 0.6055611352765481, 0.5244790798752513, 0.6861813792912159, 0.5652815222674437], [0.27860057141024697, 0.27383515025078553, 0.9176819782462265, 0.417689753313761, 0.6135860183360143, 0.8162090147099693, 0.39224804876974406, 0.543173888187219, 0.3098912544023783, 0.633182881742779], [0.0952563896474653, 0.7477071810186972, 0.5004564582092008, 0.2614834043253954, 0.5982982446751687, 0.8544002333592715, 0.26000819037953216, 0.40177311792144454, 0.03851083747397188, 0.05167636277510712], [0.9726302497724043, 0.42432064255976365, 0.9305610323744404, 0.771694551386715, 0.6789841281422876, 0.9487832709253969, 0.4943030306526911, 0.22888583384514705, 0.6165263440265218, 0.8948635092093183], [0.9816006872849989, 0.3233518004555158, 0.6660672115030636, 0.9921564654020117, 0.9574487554669273, 0.00033642413291157247, 0.5729463981674527, 0.63676146970985, 0.1068707761119706, 0.4974835849045728], [0.6877782810075579, 0.11000878013616322, 0.6630366287015564, 0.0320757478156235, 0.5550374523078817, 0.11429763248899893, 0.7746616174182379, 0.6935564378314162, 0.6081187039755812, 0.3594774747771995], [0.3402744125431225, 0.8533066685831103, 0.18605963113570156, 0.9700428171414653, 0.9046533776474858, 0.4199976147427207, 0.01833313615444565, 0.5003118405702941, 0.9167261953361863, 0.6543553598701435], [0.5463089308369264, 0.19187434980340723, 0.5311179490604816, 0.7210872364087648, 0.25848050944241396, 0.9138829006068386, 0.5015098582184656, 0.9245322749204768, 0.4746635193819774, 0.733561516539988], [0.5924804586325896, 0.44157691425623313, 0.06474182310396659, 0.3705313104712945, 0.218280453275444, 0.911250263493956, 0.4908690024649712, 0.031427016100674665, 0.3749922950484815, 0.29534800562581187]] # Example: mapPartition and mapPartitionWithIndex a = sc . parallelize ( range ( 0 , 20 ), 4 ) print ( a . glom () . collect ()) def f ( it ): s = 0 for i in it : s += i yield s print ( a . mapPartitions ( f ) . collect ()) def f ( index , it ): s = index for i in it : s += i yield s print ( a . mapPartitionsWithIndex ( f ) . collect ()) [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]] [0, 1, 3, 6, 10, 5, 11, 18, 26, 35, 10, 21, 33, 46, 60, 15, 31, 48, 66, 85] [0, 1, 3, 6, 10, 6, 12, 19, 27, 36, 12, 23, 35, 48, 62, 18, 34, 51, 69, 88] # Correct version import random import time partitions = 1000 n = 1000 * partitions seed = time . time () def f ( index , it ): random . seed ( index + seed ) for i in it : x = random . random () y = random . random () yield 1 if x ** 2 + y ** 2 < 1 else 0 count = sc . parallelize ( range ( 1 , n + 1 ), partitions ) \\ . mapPartitionsWithIndex ( f ) . sum () print ( \"Pi is roughly\" , 4.0 * count / n ) Pi is roughly 3.141832","title":"Computing Pi using Monte Carlo simulation"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#closure-and-persistence","text":"# RDD variables are references A = sc . parallelize ( range ( 10 )) B = A . map ( lambda x : x * 2 ) A = B . map ( lambda x : x + 1 ) A . take ( 10 ) [1, 3, 5, 7, 9, 11, 13, 15, 17, 19] # Linear-time selection data = [ 34 , 67 , 21 , 56 , 47 , 89 , 12 , 44 , 74 , 43 , 26 ] A = sc . parallelize ( data , 2 ) k = 4 while True : x = A . first () A1 = A . filter ( lambda z : z < x ) A2 = A . filter ( lambda z : z > x ) A1 . cache () A2 . cache () mid = A1 . count () if mid == k : print ( x ) break if k < mid : A = A1 else : A = A2 k = k - mid - 1 43 sorted ( data ) [12, 21, 26, 34, 43, 44, 47, 56, 67, 74, 89] A = sc . parallelize ( range ( 10 )) x = 5 B = A . filter ( lambda z : z < x ) # B.cache() print ( B . count ()) x = 3 print ( B . count ()) 5 5 A = sc . parallelize ( range ( 10 )) x = 5 B = A . filter ( lambda z : z < x ) # B.cache() B . unpersist () # print(B.take(10)) print ( B . collect ()) x = 3 #print(B.take(10)) print ( B . collect ()) # collect() doesn't always re-collect data - bad design! # Always use take() instead of collect() [0, 1, 2, 3, 4] [0, 1, 2, 3, 4]","title":"Closure and Persistence"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#key-value-pairs","text":"# reduceByKey numFruitsByLength = fruits . map ( lambda fruit : ( len ( fruit ), 1 )) . reduceByKey ( lambda x , y : x + y ) print ( numFruitsByLength . take ( 10 )) from operator import add lines = sc . textFile ( '../data/course.txt' ) counts = lines . flatMap ( lambda x : x . split ()) \\ . map ( lambda x : ( x , 1 )) \\ . reduceByKey ( add ) print ( counts . sortByKey () . take ( 20 )) print ( counts . sortBy ( lambda x : x [ 1 ], False ) . take ( 20 )) # Join simple example products = sc . parallelize ([( 1 , \"Apple\" ), ( 2 , \"Orange\" ), ( 3 , \"TV\" ), ( 5 , \"Computer\" )]) #trans = sc.parallelize([(1, 134, \"OK\"), (3, 34, \"OK\"), (5, 162, \"Error\"), (1, 135, \"OK\"), (2, 53, \"OK\"), (1, 45, \"OK\")]) trans = sc . parallelize ([( 1 , ( 134 , \"OK\" )), ( 3 , ( 34 , \"OK\" )), ( 5 , ( 162 , \"Error\" )), ( 1 , ( 135 , \"OK\" )), ( 2 , ( 53 , \"OK\" )), ( 1 , ( 45 , \"OK\" ))]) print ( products . join ( trans ) . take ( 20 ))","title":"Key-Value Pairs"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#k-means-clustering","text":"import numpy as np def parseVector ( line ): return np . array ([ float ( x ) for x in line . split ()]) def closestPoint ( p , centers ): bestIndex = 0 closest = float ( \"+inf\" ) for i in range ( len ( centers )): tempDist = np . sum (( p - centers [ i ]) ** 2 ) if tempDist < closest : closest = tempDist bestIndex = i return bestIndex # The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_data.txt lines = sc . textFile ( '../data/kmeans_data.txt' , 5 ) # The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_bigdata.txt # lines = sc.textFile('../data/kmeans_bigdata.txt', 5) # lines is an RDD of strings K = 3 convergeDist = 0.01 # terminate algorithm when the total distance from old center to new centers is less than this value data = lines . map ( parseVector ) . cache () # data is an RDD of arrays kCenters = data . takeSample ( False , K , 1 ) # intial centers as a list of arrays tempDist = 1.0 # total distance from old centers to new centers while tempDist > convergeDist : closest = data . map ( lambda p : ( closestPoint ( p , kCenters ), ( p , 1 ))) # for each point in data, find its closest center # closest is an RDD of tuples (index of closest center, (point, 1)) pointStats = closest . reduceByKey ( lambda p1 , p2 : ( p1 [ 0 ] + p2 [ 0 ], p1 [ 1 ] + p2 [ 1 ])) # pointStats is an RDD of tuples (index of center, # (array of sums of coordinates, total number of points assigned)) newCenters = pointStats . map ( lambda st : ( st [ 0 ], st [ 1 ][ 0 ] / st [ 1 ][ 1 ])) . collect () # compute the new centers tempDist = sum ( np . sum (( kCenters [ i ] - p ) ** 2 ) for ( i , p ) in newCenters ) # compute the total disctance from old centers to new centers for ( i , p ) in newCenters : kCenters [ i ] = p print ( \"Final centers: \" , kCenters )","title":"K-means clustering"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#pagerank","text":"import re from operator import add def computeContribs ( urls , rank ): # Calculates URL contributions to the rank of other URLs. num_urls = len ( urls ) for url in urls : yield ( url , rank / num_urls ) def parseNeighbors ( urls ): # Parses a urls pair string into urls pair.\"\"\" parts = urls . split ( ' ' ) return parts [ 0 ], parts [ 1 ] # Loads in input file. It should be in format of: # URL neighbor URL # URL neighbor URL # URL neighbor URL # ... # The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/* lines = sc . textFile ( \"/content/drive/My Drive/\u8bfe\u7a0b/HKUST/MSBD5003/homeworks/hw2/pagerank_data.txt\" , 2 ) # lines = sc.textFile(\"../data/dblp.in\", 5) numOfIterations = 10 # Loads all URLs from input file and initialize their neighbors. links = lines . map ( lambda urls : parseNeighbors ( urls )) \\ . groupByKey () # Loads all URLs with other URL(s) link to from input file # and initialize ranks of them to one. ranks = links . mapValues ( lambda neighbors : 1.0 ) print ( 'ranks' , ranks . collect ()) print ( 'links' , links . collect ()) # Calculates and updates URL ranks continuously using PageRank algorithm. for iteration in range ( numOfIterations ): # Calculates URL contributions to the rank of other URLs. contribs = links . join ( ranks ) \\ . flatMap ( lambda url_urls_rank : computeContribs ( url_urls_rank [ 1 ][ 0 ], url_urls_rank [ 1 ][ 1 ])) # After the join, each element in the RDD is of the form # (url, (list of neighbor urls, rank)) # Re-calculates URL ranks based on neighbor contributions. # ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15) ranks = contribs . reduceByKey ( add ) . map ( lambda t : ( t [ 0 ], t [ 1 ] * 0.85 + 0.15 )) print ( ranks . top ( 5 , lambda x : x [ 1 ])) ranks [('1', 1.0), ('4', 1.0), ('2', 1.0), ('3', 1.0)] links [('1', <pyspark.resultiterable.ResultIterable object at 0x7f8b12a20ef0>), ('4', <pyspark.resultiterable.ResultIterable object at 0x7f8b12a20a58>), ('2', <pyspark.resultiterable.ResultIterable object at 0x7f8b12a20780>), ('3', <pyspark.resultiterable.ResultIterable object at 0x7f8b12a207f0>)] [('1', 1.2981882732854677), ('4', 0.9999999999999998), ('3', 0.9999999999999998), ('2', 0.7018117267145316)]","title":"PageRank"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#join-vs-broadcast-variables","text":"products = sc . parallelize ([( 1 , \"Apple\" ), ( 2 , \"Orange\" ), ( 3 , \"TV\" ), ( 5 , \"Computer\" )]) trans = sc . parallelize ([( 1 , ( 134 , \"OK\" )), ( 3 , ( 34 , \"OK\" )), ( 5 , ( 162 , \"Error\" )), ( 1 , ( 135 , \"OK\" )), ( 2 , ( 53 , \"OK\" )), ( 1 , ( 45 , \"OK\" ))]) print ( trans . join ( products ) . take ( 20 )) products = { 1 : \"Apple\" , 2 : \"Orange\" , 3 : \"TV\" , 5 : \"Computer\" } trans = sc . parallelize ([( 1 , ( 134 , \"OK\" )), ( 3 , ( 34 , \"OK\" )), ( 5 , ( 162 , \"Error\" )), ( 1 , ( 135 , \"OK\" )), ( 2 , ( 53 , \"OK\" )), ( 1 , ( 45 , \"OK\" ))]) broadcasted_products = sc . broadcast ( products ) results = trans . map ( lambda x : ( x [ 0 ], broadcasted_products . value [ x [ 0 ]], x [ 1 ])) # results = trans.map(lambda x: (x[0], products[x[0]], x[1])) print ( results . take ( 20 ))","title":"Join vs. Broadcast Variables"},{"location":"MSBD5003/notebooks%20in%20class/sparksql/","text":"! pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 56kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 39.0MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=2f5ff611b6f601d04626ac58c802aeb82b7faa0f99da467b2288b8fc05a7d419 Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 from pyspark.context import SparkContext from pyspark.sql.session import SparkSession sc = SparkContext . getOrCreate () spark = SparkSession ( sc ) Dataframe operations \u00b6 from pyspark.sql import Row row = Row ( name = \"Alice\" , age = 11 ) print ( row ) print ( row [ 'name' ], row [ 'age' ]) print ( row . name , row . age ) row = Row ( name = \"Alice\" , age = 11 , count = 1 ) print ( row . count ) print ( row [ 'count' ]) Row(name='Alice', age=11) Alice 11 Alice 11 <built-in method count of Row object at 0x7f3384ce6e08> 1 ! wget https : // www . cse . ust . hk / msbd5003 / data / building . csv --2020-12-09 02:43:31-- https://www.cse.ust.hk/msbd5003/data/building.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 544 [text/plain] Saving to: \u2018building.csv\u2019 building.csv 100%[===================>] 544 --.-KB/s in 0s 2020-12-09 02:43:33 (25.1 MB/s) - \u2018building.csv\u2019 saved [544/544] # Data file at https://www.cse.ust.hk/msbd5003/data/building.csv df = spark . read . csv ( 'building.csv' , header = True , inferSchema = True ) # show the content of the dataframe df . show () +----------+-----------+-----------+-----------+------------+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country| +----------+-----------+-----------+-----------+------------+ | 1| M1| 25| AC1000| USA| | 2| M2| 27| FN39TG| France| | 3| M3| 28| JDNS77| Brazil| | 4| M4| 17| GG1919| Finland| | 5| M5| 3| ACMAX22| Hong Kong| | 6| M6| 9| AC1000| Singapore| | 7| M7| 13| FN39TG|South Africa| | 8| M8| 25| JDNS77| Australia| | 9| M9| 11| GG1919| Mexico| | 10| M10| 23| ACMAX22| China| | 11| M11| 14| AC1000| Belgium| | 12| M12| 26| FN39TG| Finland| | 13| M13| 25| JDNS77|Saudi Arabia| | 14| M14| 17| GG1919| Germany| | 15| M15| 19| ACMAX22| Israel| | 16| M16| 23| AC1000| Turkey| | 17| M17| 11| FN39TG| Egypt| | 18| M18| 25| JDNS77| Indonesia| | 19| M19| 14| GG1919| Canada| | 20| M20| 19| ACMAX22| Argentina| +----------+-----------+-----------+-----------+------------+ # Print the dataframe schema in a tree format df . printSchema () root |-- BuildingID: integer (nullable = true) |-- BuildingMgr: string (nullable = true) |-- BuildingAge: integer (nullable = true) |-- HVACproduct: string (nullable = true) |-- Country: string (nullable = true) # Create an RDD from the dataframe dfrdd = df . rdd dfrdd . take ( 3 ) [Row(BuildingID=1, BuildingMgr='M1', BuildingAge=25, HVACproduct='AC1000', Country='USA'), Row(BuildingID=2, BuildingMgr='M2', BuildingAge=27, HVACproduct='FN39TG', Country='France'), Row(BuildingID=3, BuildingMgr='M3', BuildingAge=28, HVACproduct='JDNS77', Country='Brazil')] # Retrieve specific columns from the dataframe df . select ( 'BuildingID' , 'Country' ) . show () +----------+------------+ |BuildingID| Country| +----------+------------+ | 1| USA| | 2| France| | 3| Brazil| | 4| Finland| | 5| Hong Kong| | 6| Singapore| | 7|South Africa| | 8| Australia| | 9| Mexico| | 10| China| | 11| Belgium| | 12| Finland| | 13|Saudi Arabia| | 14| Germany| | 15| Israel| | 16| Turkey| | 17| Egypt| | 18| Indonesia| | 19| Canada| | 20| Argentina| +----------+------------+ from pyspark.sql.functions import * df . where ( \"Country<'USA'\" ) . select ( 'BuildingID' , lit ( 'OK' )) . show () +----------+---+ |BuildingID| OK| +----------+---+ | 2| OK| | 3| OK| | 4| OK| | 5| OK| | 6| OK| | 7| OK| | 8| OK| | 9| OK| | 10| OK| | 11| OK| | 12| OK| | 13| OK| | 14| OK| | 15| OK| | 16| OK| | 17| OK| | 18| OK| | 19| OK| | 20| OK| +----------+---+ # Use GroupBy clause with dataframe df . groupBy ( 'HVACProduct' ) . count () . show () +-----------+-----+ |HVACProduct|count| +-----------+-----+ | ACMAX22| 4| | AC1000| 4| | JDNS77| 4| | FN39TG| 4| | GG1919| 4| +-----------+-----+ ! wget https : // www . cse . ust . hk / msbd5003 / data / Customer . csv ! wget https : // www . cse . ust . hk / msbd5003 / data / Product . csv ! wget https : // www . cse . ust . hk / msbd5003 / data / SalesOrderDetail . csv ! wget https : // www . cse . ust . hk / msbd5003 / data / SalesOrderHeader . csv --2020-12-09 03:07:42-- https://www.cse.ust.hk/msbd5003/data/Customer.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 199491 (195K) [text/plain] Saving to: \u2018Customer.csv\u2019 Customer.csv 100%[===================>] 194.82K 242KB/s in 0.8s 2020-12-09 03:07:44 (242 KB/s) - \u2018Customer.csv\u2019 saved [199491/199491] --2020-12-09 03:07:44-- https://www.cse.ust.hk/msbd5003/data/Product.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1355634 (1.3M) [text/plain] Saving to: \u2018Product.csv\u2019 Product.csv 100%[===================>] 1.29M 1.06MB/s in 1.2s 2020-12-09 03:07:46 (1.06 MB/s) - \u2018Product.csv\u2019 saved [1355634/1355634] --2020-12-09 03:07:46-- https://www.cse.ust.hk/msbd5003/data/SalesOrderDetail.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 56766 (55K) [text/plain] Saving to: \u2018SalesOrderDetail.csv\u2019 SalesOrderDetail.cs 100%[===================>] 55.44K 137KB/s in 0.4s 2020-12-09 03:07:47 (137 KB/s) - \u2018SalesOrderDetail.csv\u2019 saved [56766/56766] --2020-12-09 03:07:47-- https://www.cse.ust.hk/msbd5003/data/SalesOrderHeader.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 8680 (8.5K) [text/plain] Saving to: \u2018SalesOrderHeader.csv\u2019 SalesOrderHeader.cs 100%[===================>] 8.48K --.-KB/s in 0s 2020-12-09 03:07:49 (131 MB/s) - \u2018SalesOrderHeader.csv\u2019 saved [8680/8680] Rewriting SQL with DataFrame API \u00b6 # Load data from csv files # Data files at https://www.cse.ust.hk/msbd5003/data dfCustomer = spark . read . csv ( 'Customer.csv' , header = True , inferSchema = True ) dfProduct = spark . read . csv ( 'Product.csv' , header = True , inferSchema = True ) dfDetail = spark . read . csv ( 'SalesOrderDetail.csv' , header = True , inferSchema = True ) dfHeader = spark . read . csv ( 'SalesOrderHeader.csv' , header = True , inferSchema = True ) # SELECT ProductID, Name, ListPrice # FROM Product # WHERE Color = 'black' dfProduct . filter ( \"Color = 'Black'\" ) \\ . select ( 'ProductID' , 'Name' , 'ListPrice' ) \\ . show ( truncate = False ) +---------+-----------------------------+---------+ |ProductID|Name |ListPrice| +---------+-----------------------------+---------+ |680 |HL Road Frame - Black, 58 |1431.5 | |708 |Sport-100 Helmet, Black |34.99 | |722 |LL Road Frame - Black, 58 |337.22 | |723 |LL Road Frame - Black, 60 |337.22 | |724 |LL Road Frame - Black, 62 |337.22 | |736 |LL Road Frame - Black, 44 |337.22 | |737 |LL Road Frame - Black, 48 |337.22 | |738 |LL Road Frame - Black, 52 |337.22 | |743 |HL Mountain Frame - Black, 42|1349.6 | |744 |HL Mountain Frame - Black, 44|1349.6 | |745 |HL Mountain Frame - Black, 48|1349.6 | |746 |HL Mountain Frame - Black, 46|1349.6 | |747 |HL Mountain Frame - Black, 38|1349.6 | |765 |Road-650 Black, 58 |782.99 | |766 |Road-650 Black, 60 |782.99 | |767 |Road-650 Black, 62 |782.99 | |768 |Road-650 Black, 44 |782.99 | |769 |Road-650 Black, 48 |782.99 | |770 |Road-650 Black, 52 |782.99 | |775 |Mountain-100 Black, 38 |3374.99 | +---------+-----------------------------+---------+ only showing top 20 rows dfProduct . where ( dfProduct . Color == 'Black' ) \\ . select ( dfProduct . ProductID , dfProduct [ 'Name' ], ( dfProduct [ 'ListPrice' ] * 2 ) . alias ( 'Double price' )) \\ . show ( truncate = False ) +---------+-----------------------------+------------+ |ProductID|Name |Double price| +---------+-----------------------------+------------+ |680 |HL Road Frame - Black, 58 |2863.0 | |708 |Sport-100 Helmet, Black |69.98 | |722 |LL Road Frame - Black, 58 |674.44 | |723 |LL Road Frame - Black, 60 |674.44 | |724 |LL Road Frame - Black, 62 |674.44 | |736 |LL Road Frame - Black, 44 |674.44 | |737 |LL Road Frame - Black, 48 |674.44 | |738 |LL Road Frame - Black, 52 |674.44 | |743 |HL Mountain Frame - Black, 42|2699.2 | |744 |HL Mountain Frame - Black, 44|2699.2 | |745 |HL Mountain Frame - Black, 48|2699.2 | |746 |HL Mountain Frame - Black, 46|2699.2 | |747 |HL Mountain Frame - Black, 38|2699.2 | |765 |Road-650 Black, 58 |1565.98 | |766 |Road-650 Black, 60 |1565.98 | |767 |Road-650 Black, 62 |1565.98 | |768 |Road-650 Black, 44 |1565.98 | |769 |Road-650 Black, 48 |1565.98 | |770 |Road-650 Black, 52 |1565.98 | |775 |Mountain-100 Black, 38 |6749.98 | +---------+-----------------------------+------------+ only showing top 20 rows dfProduct . where ( dfProduct . ListPrice * 2 > 100 ) \\ . select ( dfProduct . ProductID , dfProduct [ 'Name' ], dfProduct . ListPrice * 2 ) \\ . show ( truncate = False ) +---------+-------------------------+---------------+ |ProductID|Name |(ListPrice * 2)| +---------+-------------------------+---------------+ |680 |HL Road Frame - Black, 58|2863.0 | |706 |HL Road Frame - Red, 58 |2863.0 | |717 |HL Road Frame - Red, 62 |2863.0 | |718 |HL Road Frame - Red, 44 |2863.0 | |719 |HL Road Frame - Red, 48 |2863.0 | |720 |HL Road Frame - Red, 52 |2863.0 | |721 |HL Road Frame - Red, 56 |2863.0 | |722 |LL Road Frame - Black, 58|674.44 | |723 |LL Road Frame - Black, 60|674.44 | |724 |LL Road Frame - Black, 62|674.44 | |725 |LL Road Frame - Red, 44 |674.44 | |726 |LL Road Frame - Red, 48 |674.44 | |727 |LL Road Frame - Red, 52 |674.44 | |728 |LL Road Frame - Red, 58 |674.44 | |729 |LL Road Frame - Red, 60 |674.44 | |730 |LL Road Frame - Red, 62 |674.44 | |731 |ML Road Frame - Red, 44 |1189.66 | |732 |ML Road Frame - Red, 48 |1189.66 | |733 |ML Road Frame - Red, 52 |1189.66 | |734 |ML Road Frame - Red, 58 |1189.66 | +---------+-------------------------+---------------+ only showing top 20 rows # SELECT ProductID, Name, ListPrice # FROM Product # WHERE Color = 'black' # ORDER BY ProductID dfProduct . filter ( \"Color = 'Black'\" ) \\ . select ( 'ProductID' , 'Name' , 'ListPrice' ) \\ . orderBy ( 'ListPrice' ) \\ . show ( truncate = False ) +---------+--------------------------+---------+ |ProductID|Name |ListPrice| +---------+--------------------------+---------+ |860 |Half-Finger Gloves, L |24.49 | |859 |Half-Finger Gloves, M |24.49 | |858 |Half-Finger Gloves, S |24.49 | |708 |Sport-100 Helmet, Black |34.99 | |862 |Full-Finger Gloves, M |37.99 | |861 |Full-Finger Gloves, S |37.99 | |863 |Full-Finger Gloves, L |37.99 | |841 |Men's Sports Shorts, S |59.99 | |849 |Men's Sports Shorts, M |59.99 | |851 |Men's Sports Shorts, XL |59.99 | |850 |Men's Sports Shorts, L |59.99 | |815 |LL Mountain Front Wheel |60.745 | |868 |Women's Mountain Shorts, M|69.99 | |869 |Women's Mountain Shorts, L|69.99 | |867 |Women's Mountain Shorts, S|69.99 | |853 |Women's Tights, M |74.99 | |854 |Women's Tights, L |74.99 | |852 |Women's Tights, S |74.99 | |818 |LL Road Front Wheel |85.565 | |823 |LL Mountain Rear Wheel |87.745 | +---------+--------------------------+---------+ only showing top 20 rows # Find all orders and details on black product, # return the product SalesOrderID, SalesOrderDetailID, Name, UnitPrice, and OrderQty # SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty # FROM SalesLT.SalesOrderDetail, SalesLT.Product # WHERE SalesOrderDetail.ProductID = Product.ProductID AND Color = 'Black' # SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty # FROM SalesLT.SalesOrderDetail # JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID # WHERE Color = 'Black' # Spark SQL supports natural joins dfDetail . join ( dfProduct , 'ProductID' ) \\ . select ( 'SalesOrderID' , 'SalesOrderDetailID' , 'Name' , 'UnitPrice' , 'OrderQty' ) \\ . filter ( \"Color='Black'\" ) \\ . show () # If we move the filter to after select, it still works. Why? +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71797| 111045|LL Road Frame - B...| 202.332| 3| | 71783| 110730|LL Road Frame - B...| 202.332| 6| | 71938| 113297|LL Road Frame - B...| 202.332| 3| | 71915| 113090|LL Road Frame - B...| 202.332| 2| | 71815| 111451|LL Road Frame - B...| 202.332| 1| | 71797| 111044|LL Road Frame - B...| 202.332| 1| | 71783| 110710|LL Road Frame - B...| 202.332| 4| | 71936| 113260|HL Mountain Frame...| 809.76| 4| | 71899| 112937|HL Mountain Frame...| 809.76| 1| | 71845| 112137|HL Mountain Frame...| 809.76| 2| | 71832| 111806|HL Mountain Frame...| 809.76| 4| | 71780| 110622|HL Mountain Frame...| 809.76| 1| | 71936| 113235|HL Mountain Frame...| 809.76| 4| | 71845| 112134|HL Mountain Frame...| 809.76| 3| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows # This also works: d1 = dfDetail . join ( dfProduct , 'ProductID' ) \\ . select ( 'SalesOrderID' , 'SalesOrderDetailID' , 'Name' , 'UnitPrice' , 'OrderQty' ) d1 . show () d2 = d1 . filter ( \"Color = 'Black'\" ) d2 . show () d2 . explain () +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113278|Sport-100 Helmet,...| 20.994| 3| | 71936| 113228|Sport-100 Helmet,...| 20.994| 1| | 71902| 112980|Sport-100 Helmet,...| 20.994| 2| | 71797| 111075|Sport-100 Helmet,...| 20.994| 6| | 71784| 110794|Sport-100 Helmet,...| 20.994| 10| | 71783| 110751|Sport-100 Helmet,...| 20.994| 10| | 71782| 110709|Sport-100 Helmet,...| 20.994| 3| | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71938| 113282|Sport-100 Helmet,...| 20.994| 3| | 71902| 112995|Sport-100 Helmet,...| 20.994| 7| | 71863| 112395|Sport-100 Helmet,...| 20.994| 1| | 71797| 111038|Sport-100 Helmet,...| 20.994| 4| | 71784| 110753|Sport-100 Helmet,...| 20.994| 2| | 71783| 110749|Sport-100 Helmet,...| 19.2445| 15| | 71782| 110708|Sport-100 Helmet,...| 20.994| 6| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71797| 111045|LL Road Frame - B...| 202.332| 3| | 71783| 110730|LL Road Frame - B...| 202.332| 6| | 71938| 113297|LL Road Frame - B...| 202.332| 3| | 71915| 113090|LL Road Frame - B...| 202.332| 2| | 71815| 111451|LL Road Frame - B...| 202.332| 1| | 71797| 111044|LL Road Frame - B...| 202.332| 1| | 71783| 110710|LL Road Frame - B...| 202.332| 4| | 71936| 113260|HL Mountain Frame...| 809.76| 4| | 71899| 112937|HL Mountain Frame...| 809.76| 1| | 71845| 112137|HL Mountain Frame...| 809.76| 2| | 71832| 111806|HL Mountain Frame...| 809.76| 4| | 71780| 110622|HL Mountain Frame...| 809.76| 1| | 71936| 113235|HL Mountain Frame...| 809.76| 4| | 71845| 112134|HL Mountain Frame...| 809.76| 3| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows == Physical Plan == *(2) Project [SalesOrderID#217, SalesOrderDetailID#218, Name#168, UnitPrice#221, OrderQty#219] +- *(2) BroadcastHashJoin [ProductID#220], [ProductID#167], Inner, BuildLeft :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[3, int, true] as bigint))), [id=#380] : +- *(1) Project [SalesOrderID#217, SalesOrderDetailID#218, OrderQty#219, ProductID#220, UnitPrice#221] : +- *(1) Filter isnotnull(ProductID#220) : +- FileScan csv [SalesOrderID#217,SalesOrderDetailID#218,OrderQty#219,ProductID#220,UnitPrice#221] Batched: false, DataFilters: [isnotnull(ProductID#220)], Format: CSV, Location: InMemoryFileIndex[file:/content/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double> +- *(2) Project [ProductID#167, Name#168] +- *(2) Filter ((isnotnull(Color#170) AND (Color#170 = Black)) AND isnotnull(ProductID#167)) +- FileScan csv [ProductID#167,Name#168,Color#170] Batched: false, DataFilters: [isnotnull(Color#170), (Color#170 = Black), isnotnull(ProductID#167)], Format: CSV, Location: InMemoryFileIndex[file:/content/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string> # SparkSQL performs optimization depending on whether intermediate dataframe are cached or not: d1 = dfDetail . join ( dfProduct , 'ProductID' ) \\ . select ( 'SalesOrderID' , 'SalesOrderDetailID' , 'Name' , 'UnitPrice' , 'OrderQty' ) d1 . persist () d1 . show () +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113278|Sport-100 Helmet,...| 20.994| 3| | 71936| 113228|Sport-100 Helmet,...| 20.994| 1| | 71902| 112980|Sport-100 Helmet,...| 20.994| 2| | 71797| 111075|Sport-100 Helmet,...| 20.994| 6| | 71784| 110794|Sport-100 Helmet,...| 20.994| 10| | 71783| 110751|Sport-100 Helmet,...| 20.994| 10| | 71782| 110709|Sport-100 Helmet,...| 20.994| 3| | 71938| 113295|Sport-100 Helmet,...| 20.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71938| 113282|Sport-100 Helmet,...| 20.994| 3| | 71902| 112995|Sport-100 Helmet,...| 20.994| 7| | 71863| 112395|Sport-100 Helmet,...| 20.994| 1| | 71797| 111038|Sport-100 Helmet,...| 20.994| 4| | 71784| 110753|Sport-100 Helmet,...| 20.994| 2| | 71783| 110749|Sport-100 Helmet,...| 19.2445| 15| | 71782| 110708|Sport-100 Helmet,...| 20.994| 6| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows d2 = d1 . filter ( \"Color = 'Black'\" ) #d2 = d1.filter(\"OrderQty >= 10\") d2 . show () d2 . explain () +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71797| 111045|LL Road Frame - B...| 202.332| 3| | 71783| 110730|LL Road Frame - B...| 202.332| 6| | 71938| 113297|LL Road Frame - B...| 202.332| 3| | 71915| 113090|LL Road Frame - B...| 202.332| 2| | 71815| 111451|LL Road Frame - B...| 202.332| 1| | 71797| 111044|LL Road Frame - B...| 202.332| 1| | 71783| 110710|LL Road Frame - B...| 202.332| 4| | 71936| 113260|HL Mountain Frame...| 809.76| 4| | 71899| 112937|HL Mountain Frame...| 809.76| 1| | 71845| 112137|HL Mountain Frame...| 809.76| 2| | 71832| 111806|HL Mountain Frame...| 809.76| 4| | 71780| 110622|HL Mountain Frame...| 809.76| 1| | 71936| 113235|HL Mountain Frame...| 809.76| 4| | 71845| 112134|HL Mountain Frame...| 809.76| 3| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows == Physical Plan == *(2) Project [SalesOrderID#112, SalesOrderDetailID#113, Name#63, UnitPrice#116, OrderQty#114] +- *(2) BroadcastHashJoin [ProductID#115], [ProductID#62], Inner, BuildLeft :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[3, int, true] as bigint))), [id=#280] : +- *(1) Project [SalesOrderID#112, SalesOrderDetailID#113, OrderQty#114, ProductID#115, UnitPrice#116] : +- *(1) Filter isnotnull(ProductID#115) : +- FileScan csv [SalesOrderID#112,SalesOrderDetailID#113,OrderQty#114,ProductID#115,UnitPrice#116] Batched: false, DataFilters: [isnotnull(ProductID#115)], Format: CSV, Location: InMemoryFileIndex[file:/csproject/msbd5003/public_html/data/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double> +- *(2) Project [ProductID#62, Name#63] +- *(2) Filter ((isnotnull(Color#65) AND (Color#65 = Black)) AND isnotnull(ProductID#62)) +- FileScan csv [ProductID#62,Name#63,Color#65] Batched: false, DataFilters: [isnotnull(Color#65), (Color#65 = Black), isnotnull(ProductID#62)], Format: CSV, Location: InMemoryFileIndex[file:/csproject/msbd5003/public_html/data/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string> # This will report an error: d1 = dfDetail . join ( dfProduct , 'ProductID' ) \\ . select ( 'SalesOrderID' , 'SalesOrderDetailID' , 'Name' , 'UnitPrice' , 'OrderQty' ) d1 . write . csv ( 'temp.csv' , mode = 'overwrite' , header = True ) d2 = spark . read . csv ( 'temp.csv' , header = True , inferSchema = True ) d2 . filter ( \"Color = 'Black'\" ) . show () --------------------------------------------------------------------------- AnalysisException Traceback (most recent call last) <ipython-input-18-168595bb0376> in <module> 5 d1.write.csv('temp.csv', mode = 'overwrite', header = True) 6 d2 = spark.read.csv('temp.csv', header = True, inferSchema = True) ----> 7 d2.filter(\"Color = 'Black'\").show() /csproject/msbd5003/python/pyspark/sql/dataframe.py in filter(self, condition) 1457 \"\"\" 1458 if isinstance(condition, basestring): -> 1459 jdf = self._jdf.filter(condition) 1460 elif isinstance(condition, Column): 1461 jdf = self._jdf.filter(condition._jc) /csproject/msbd5003/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args) 1303 answer = self.gateway_client.send_command(command) 1304 return_value = get_return_value( -> 1305 answer, self.gateway_client, self.target_id, self.name) 1306 1307 for temp_arg in temp_args: /csproject/msbd5003/python/pyspark/sql/utils.py in deco(*a, **kw) 135 # Hide where the exception came from that shows a non-Pythonic 136 # JVM exception message. --> 137 raise_from(converted) 138 else: 139 raise /csproject/msbd5003/python/pyspark/sql/utils.py in raise_from(e) AnalysisException: cannot resolve '`Color`' given input columns: [Name, OrderQty, SalesOrderDetailID, SalesOrderID, UnitPrice]; line 1 pos 0; 'Filter ('Color = Black) +- Relation[SalesOrderID#580,SalesOrderDetailID#581,Name#582,UnitPrice#583,OrderQty#584] csv # Find all orders that include at least one black product, # return the product SalesOrderID, Name, UnitPrice, and OrderQty # SELECT DISTINCT SalesOrderID # FROM SalesLT.SalesOrderDetail # JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID # WHERE Color = 'Black' dfDetail . join ( dfProduct . filter ( \"Color='Black'\" ), 'ProductID' ) \\ . select ( 'SalesOrderID' ) \\ . distinct () \\ . show () +------------+ |SalesOrderID| +------------+ | 71902| | 71832| | 71915| | 71831| | 71898| | 71935| | 71938| | 71845| | 71783| | 71815| | 71936| | 71863| | 71780| | 71782| | 71899| | 71784| | 71797| +------------+ # How many colors in the products? # SELECT COUNT(DISTINCT Color) # FROM SalesLT.Product dfProduct . select ( 'Color' ) . distinct () . count () # It's 1 more than standard SQL. In standard SQL, COUNT() does not count NULLs. 10 # Find the total price of each order, # return SalesOrderID and total price (column name should be \u2018totalprice\u2019) # SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice # FROM SalesLT.SalesOrderDetail # GROUP BY SalesOrderID dfDetail . select ( '*' , ( dfDetail . UnitPrice * dfDetail . OrderQty * ( 1 - dfDetail . UnitPriceDiscount )) . alias ( 'netprice' )) \\ . groupBy ( 'SalesOrderID' ) . sum ( 'netprice' ) \\ . withColumnRenamed ( 'sum(netprice)' , 'TotalPrice' ) \\ . show () +------------+------------------+ |SalesOrderID| sum(netprice)| +------------+------------------+ | 71867| 858.9| | 71902|59894.209199999976| | 71832| 28950.678108| | 71915|1732.8899999999999| | 71946| 31.584| | 71895|221.25600000000003| | 71816|2847.4079999999994| | 71831| 1712.946| | 71923| 96.108824| | 71858|11528.844000000001| | 71917| 37.758| | 71897| 10585.05| | 71885| 524.664| | 71856|500.30400000000003| | 71898| 53248.69200000002| | 71774| 713.796| | 71796| 47848.02600000001| | 71935|5533.8689079999995| | 71938| 74160.228| | 71845| 34118.5356| +------------+------------------+ only showing top 20 rows # Find the total price of each order where the total price > 10000 # SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice # FROM SalesLT.SalesOrderDetail # GROUP BY SalesOrderID # HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000 dfDetail . select ( '*' , ( dfDetail . UnitPrice * dfDetail . OrderQty * ( 1 - dfDetail . UnitPriceDiscount )) . alias ( 'netprice' )) \\ . groupBy ( 'SalesOrderID' ) . sum ( 'netprice' ) \\ . withColumnRenamed ( 'sum(netprice)' , 'TotalPrice' ) \\ . where ( 'TotalPrice > 10000' ) \\ . show () +------------+------------------+ |SalesOrderID| TotalPrice| +------------+------------------+ | 71902|59894.209199999976| | 71832| 28950.678108| | 71858|11528.844000000001| | 71897| 10585.05| | 71898| 53248.69200000002| | 71796| 47848.02600000001| | 71938| 74160.228| | 71845| 34118.5356| | 71783| 65683.367986| | 71936| 79589.61602399996| | 71780|29923.007999999998| | 71782| 33319.98600000001| | 71784| 89869.27631400003| | 71797| 65123.46341800001| +------------+------------------+ # Find the total price on the black products of each order where the total price > 10000 # SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice # FROM SalesLT.SalesOrderDetail, SalesLT.Product # WHERE SalesLT.SalesOrderDetail.ProductID = SalesLT.Product.ProductID AND Color = 'Black' # GROUP BY SalesOrderID # HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000 dfDetail . select ( '*' , ( dfDetail . UnitPrice * dfDetail . OrderQty * ( 1 - dfDetail . UnitPriceDiscount )) . alias ( 'netprice' )) \\ . join ( dfProduct , 'ProductID' ) \\ . where ( \"Color = 'Black'\" ) \\ . groupBy ( 'SalesOrderID' ) . sum ( 'netprice' ) \\ . withColumnRenamed ( 'sum(netprice)' , 'TotalPrice' ) \\ . where ( 'TotalPrice > 10000' ) \\ . show () +------------+------------------+ |SalesOrderID| TotalPrice| +------------+------------------+ | 71902|26677.883999999995| | 71832| 16883.748108| | 71938| 33779.448| | 71845| 18109.836| | 71783|15524.117476000003| | 71936| 44490.290424| | 71780| 16964.322| | 71797| 27581.613792| +------------+------------------+ # For each customer, find the total quantity of black products bought. # Report CustomerID, FirstName, LastName, and total quantity # select saleslt.customer.customerid, FirstName, LastName, sum(orderqty) # from saleslt.customer # left outer join # ( # saleslt.salesorderheader # join saleslt.salesorderdetail # on saleslt.salesorderdetail.salesorderid = saleslt.salesorderheader.salesorderid # join saleslt.product # on saleslt.product.productid = saleslt.salesorderdetail.productid and color = 'black' # ) # on saleslt.customer.customerid = saleslt.salesorderheader.customerid # group by saleslt.customer.customerid, FirstName, LastName # order by sum(orderqty) desc d1 = dfDetail . join ( dfProduct , 'ProductID' ) \\ . where ( 'Color = \"Black\"' ) \\ . join ( dfHeader , 'SalesOrderID' ) \\ . groupBy ( 'CustomerID' ) . sum ( 'OrderQty' ) dfCustomer . join ( d1 , 'CustomerID' , 'left_outer' ) \\ . select ( 'CustomerID' , 'FirstName' , 'LastName' , 'sum(OrderQty)' ) \\ . orderBy ( 'sum(OrderQty)' , ascending = False ) \\ . show () +----------+------------+------------+-------------+ |CustomerID| FirstName| LastName|sum(OrderQty)| +----------+------------+------------+-------------+ | 30050| Krishna|Sunkammurali| 89| | 29796| Jon| Grande| 65| | 29957| Kevin| Liu| 62| | 29929| Jeffrey| Kurtz| 46| | 29546| Christopher| Beck| 45| | 29922| Pamala| Kotc| 34| | 30113| Raja| Venugopal| 34| | 29938| Frank| Campbell| 29| | 29736| Terry| Eminhizer| 23| | 29485| Catherine| Abel| 10| | 30019| Matthew| Miller| 9| | 29932| Rebecca| Laszlo| 7| | 29975| Walter| Mays| 5| | 29638| Rosmarie| Carroll| 2| | 29531| Cory| Booth| 1| | 30089|Michael John| Troyer| 1| | 29568| Donald| Blanton| 1| | 29868| Denean| Ison| null| | 29646| Stacey| Cereghino| null| | 29905| Elizabeth| Keyser| null| +----------+------------+------------+-------------+ only showing top 20 rows Embed SQL queries \u00b6 You can also run SQL queries over dataframes once you register them as temporary tables within the SparkSession. # Register the dataframe as a temporary view called HVAC df . createOrReplaceTempView ( 'HVAC' ) spark . sql ( 'SELECT * FROM HVAC WHERE BuildingAge >= 10' ) . show () +----------+-----------+-----------+-----------+------------+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country| +----------+-----------+-----------+-----------+------------+ | 1| M1| 25| AC1000| USA| | 2| M2| 27| FN39TG| France| | 3| M3| 28| JDNS77| Brazil| | 4| M4| 17| GG1919| Finland| | 7| M7| 13| FN39TG|South Africa| | 8| M8| 25| JDNS77| Australia| | 9| M9| 11| GG1919| Mexico| | 10| M10| 23| ACMAX22| China| | 11| M11| 14| AC1000| Belgium| | 12| M12| 26| FN39TG| Finland| | 13| M13| 25| JDNS77|Saudi Arabia| | 14| M14| 17| GG1919| Germany| | 15| M15| 19| ACMAX22| Israel| | 16| M16| 23| AC1000| Turkey| | 17| M17| 11| FN39TG| Egypt| | 18| M18| 25| JDNS77| Indonesia| | 19| M19| 14| GG1919| Canada| | 20| M20| 19| ACMAX22| Argentina| +----------+-----------+-----------+-----------+------------+ # Can even mix DataFrame API with SQL: df . where ( 'BuildingAge >= 10' ) . createOrReplaceTempView ( 'OldBuildings' ) spark . sql ( 'SELECT HVACproduct, COUNT(*) FROM OldBuildings GROUP BY HVACproduct' ) . show () +-----------+--------+ |HVACproduct|count(1)| +-----------+--------+ | ACMAX22| 3| | AC1000| 3| | JDNS77| 4| | FN39TG| 4| | GG1919| 4| +-----------+--------+ d1 = spark . sql ( 'SELECT * FROM HVAC WHERE BuildingAge >= 10' ) d1 . groupBy ( 'HVACproduct' ) . count () . show () +-----------+-----+ |HVACproduct|count| +-----------+-----+ | ACMAX22| 3| | AC1000| 3| | JDNS77| 4| | FN39TG| 4| | GG1919| 4| +-----------+-----+ # UDF from pyspark.sql.functions import udf from pyspark.sql.types import IntegerType slen = udf ( lambda s : len ( s ) + 2 , IntegerType ()) df . select ( '*' , slen ( df [ 'Country' ]) . alias ( 'slen' )) . show () +----------+-----------+-----------+-----------+------------+----+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country|slen| +----------+-----------+-----------+-----------+------------+----+ | 1| M1| 25| AC1000| USA| 5| | 2| M2| 27| FN39TG| France| 8| | 3| M3| 28| JDNS77| Brazil| 8| | 4| M4| 17| GG1919| Finland| 9| | 5| M5| 3| ACMAX22| Hong Kong| 11| | 6| M6| 9| AC1000| Singapore| 11| | 7| M7| 13| FN39TG|South Africa| 14| | 8| M8| 25| JDNS77| Australia| 11| | 9| M9| 11| GG1919| Mexico| 8| | 10| M10| 23| ACMAX22| China| 7| | 11| M11| 14| AC1000| Belgium| 9| | 12| M12| 26| FN39TG| Finland| 9| | 13| M13| 25| JDNS77|Saudi Arabia| 14| | 14| M14| 17| GG1919| Germany| 9| | 15| M15| 19| ACMAX22| Israel| 8| | 16| M16| 23| AC1000| Turkey| 8| | 17| M17| 11| FN39TG| Egypt| 7| | 18| M18| 25| JDNS77| Indonesia| 11| | 19| M19| 14| GG1919| Canada| 8| | 20| M20| 19| ACMAX22| Argentina| 11| +----------+-----------+-----------+-----------+------------+----+ spark . udf . register ( 'slen' , lambda s : len ( s ), IntegerType ()) spark . sql ( 'SELECT *, slen(Country) AS slen FROM HVAC' ) . show () +----------+-----------+-----------+-----------+------------+----+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country|slen| +----------+-----------+-----------+-----------+------------+----+ | 1| M1| 25| AC1000| USA| 3| | 2| M2| 27| FN39TG| France| 6| | 3| M3| 28| JDNS77| Brazil| 6| | 4| M4| 17| GG1919| Finland| 7| | 5| M5| 3| ACMAX22| Hong Kong| 9| | 6| M6| 9| AC1000| Singapore| 9| | 7| M7| 13| FN39TG|South Africa| 12| | 8| M8| 25| JDNS77| Australia| 9| | 9| M9| 11| GG1919| Mexico| 6| | 10| M10| 23| ACMAX22| China| 5| | 11| M11| 14| AC1000| Belgium| 7| | 12| M12| 26| FN39TG| Finland| 7| | 13| M13| 25| JDNS77|Saudi Arabia| 12| | 14| M14| 17| GG1919| Germany| 7| | 15| M15| 19| ACMAX22| Israel| 6| | 16| M16| 23| AC1000| Turkey| 6| | 17| M17| 11| FN39TG| Egypt| 5| | 18| M18| 25| JDNS77| Indonesia| 9| | 19| M19| 14| GG1919| Canada| 6| | 20| M20| 19| ACMAX22| Argentina| 9| +----------+-----------+-----------+-----------+------------+----+ Flexible Data Model \u00b6 Sample data file at https://www.cse.ust.hk/msbd5003/data/products.json df = spark . read . json ( '../data/products.json' ) df . printSchema () root |-- dimensions: struct (nullable = true) | |-- height: double (nullable = true) | |-- length: double (nullable = true) | |-- width: double (nullable = true) |-- id: long (nullable = true) |-- name: string (nullable = true) |-- price: double (nullable = true) |-- tags: array (nullable = true) | |-- element: string (containsNull = true) |-- warehouseLocation: struct (nullable = true) | |-- latitude: double (nullable = true) | |-- longitude: double (nullable = true) df . show () +----------------+---+----------------+-----+-----------+-----------------+ | dimensions| id| name|price| tags|warehouseLocation| +----------------+---+----------------+-----+-----------+-----------------+ |[9.5, 7.0, 12.0]| 2|An ice sculpture| 12.5|[cold, ice]| [-78.75, 20.4]| | [1.0, 3.1, 1.0]| 3| A blue mouse| 25.5| null| [54.4, -32.7]| +----------------+---+----------------+-----+-----------+-----------------+ # Accessing nested fields df . select ( df [ 'dimensions.height' ]) . show () +------+ |height| +------+ | 9.5| | 1.0| +------+ df . select ( 'dimensions.height' ) . show () +------+ |height| +------+ | 9.5| | 1.0| +------+ df . select ( 'dimensions.height' ) \\ . filter ( \"tags[0] = 'cold' AND warehouseLocation.latitude < 0\" ) \\ . show () +------+ |height| +------+ | 9.5| +------+ df . rdd . take ( 3 ) [Row(dimensions=Row(height=9.5, length=7.0, width=12.0), id=2, name='An ice sculpture', price=12.5, tags=['cold', 'ice'], warehouseLocation=Row(latitude=-78.75, longitude=20.4)), Row(dimensions=Row(height=1.0, length=3.1, width=1.0), id=3, name='A blue mouse', price=25.5, tags=None, warehouseLocation=Row(latitude=54.4, longitude=-32.7))] Converting between RDD and DataFrame \u00b6 Sample data file at: https://www.cse.ust.hk/msbd5003/data/people.txt # Load a text file and convert each line to a Row. lines = sc . textFile ( \"../data/people.txt\" ) def parse ( l ): a = l . split ( ',' ) return ( a [ 0 ], int ( a [ 1 ])) rdd = lines . map ( parse ) rdd . collect () [('Michael', 29), ('Andy', 30), ('Justin', 19)] # Create the DataFrame from an RDD of tuples, schema is inferred df = spark . createDataFrame ( rdd ) df . printSchema () df . show () root |-- _1: string (nullable = true) |-- _2: long (nullable = true) +-------+---+ | _1| _2| +-------+---+ |Michael| 29| | Andy| 30| | Justin| 19| +-------+---+ # Create the DataFrame from an RDD of tuples with column names, type is inferred df = spark . createDataFrame ( rdd , [ 'name' , 'age' ]) df . printSchema () df . show () root |-- name: string (nullable = true) |-- age: long (nullable = true) +-------+---+ | name|age| +-------+---+ |Michael| 29| | Andy| 30| | Justin| 19| +-------+---+ # Create the DataFrame from an RDD of Rows, type is given in the Row objects from pyspark.sql import Row rdd_rows = rdd . map ( lambda p : Row ( name = p [ 0 ], age = p [ 1 ])) df = spark . createDataFrame ( rdd_rows ) df . printSchema () df . show () root |-- name: string (nullable = true) |-- age: long (nullable = true) +-------+---+ | name|age| +-------+---+ |Michael| 29| | Andy| 30| | Justin| 19| +-------+---+ # Row fields with types incompatible with that of previous rows will be turned into nulls row1 = Row ( name = \"Alice\" , age = 11 ) row2 = Row ( name = \"Bob\" , age = '12' ) rdd_rows = sc . parallelize ([ row1 , row2 ]) df1 = spark . createDataFrame ( rdd_rows ) df1 . show () +-----+----+ | name| age| +-----+----+ |Alice| 11| | Bob|null| +-----+----+ # rdd returns the content as an RDD of Rows teenagers = df . filter ( 'age >= 13 and age <= 19' ) teenNames = teenagers . rdd . map ( lambda p : \"Name: \" + p . name ) teenNames . collect () ['Name: Justin'] Note: \u00b6 DataFrames are stored using columnar storage with compression RDDs are stored using row storage without compression The RDD view of DataFrame just provides an interface, the Row objects are constructed on the fly and do not necessarily represent the internal storage format of the data Closure in DataFrames \u00b6 data = range ( 10 ) df = spark . createDataFrame ( zip ( data , data )) df . printSchema () df . show () root |-- _1: long (nullable = true) |-- _2: long (nullable = true) +---+---+ | _1| _2| +---+---+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| | 5| 5| | 6| 6| | 7| 7| | 8| 8| | 9| 9| +---+---+ # The 'closure' behaviour in RDD doesn't seem to exist for DataFrames x = 5 df1 = df . filter ( df . _1 < x ) df1 . show () x = 3 df1 . show () +---+---+ | _1| _2| +---+---+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| +---+---+ +---+---+ | _1| _2| +---+---+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| +---+---+ # Because of the Catalyst optimizer ! df1 . explain () == Physical Plan == *(1) Filter (isnotnull(_1#1265L) AND (_1#1265L < 5)) +- *(1) Scan ExistingRDD[_1#1265L,_2#1266L] def f (): return x / 2 x = 5 df1 = df . select ( df . _1 * 2 + f () + 1 + 1 ) df1 . explain () df1 . show () == Physical Plan == *(1) Project [(((cast((_1#1265L * 2) as double) + 2.5) + 1.0) + 1.0) AS ((((_1 * 2) + 2.5) + 1) + 1)#1296] +- *(1) Scan ExistingRDD[_1#1265L,_2#1266L] +----------------------------+ |((((_1 * 2) + 2.5) + 1) + 1)| +----------------------------+ | 4.5| | 6.5| | 8.5| | 10.5| | 12.5| | 14.5| | 16.5| | 18.5| | 20.5| | 22.5| +----------------------------+ rdd = sc . parallelize ( range ( 10 )) x = 5 a = rdd . filter ( lambda z : z < x ) print ( a . take ( 10 )) x = 3 print ( a . take ( 10 )) [0, 1, 2, 3, 4] [0, 1, 2] counter = 0 def increment_counter ( x ): global counter counter += 1 df . foreach ( increment_counter ) print ( counter ) 0","title":"Sparksql"},{"location":"MSBD5003/notebooks%20in%20class/sparksql/#dataframe-operations","text":"from pyspark.sql import Row row = Row ( name = \"Alice\" , age = 11 ) print ( row ) print ( row [ 'name' ], row [ 'age' ]) print ( row . name , row . age ) row = Row ( name = \"Alice\" , age = 11 , count = 1 ) print ( row . count ) print ( row [ 'count' ]) Row(name='Alice', age=11) Alice 11 Alice 11 <built-in method count of Row object at 0x7f3384ce6e08> 1 ! wget https : // www . cse . ust . hk / msbd5003 / data / building . csv --2020-12-09 02:43:31-- https://www.cse.ust.hk/msbd5003/data/building.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 544 [text/plain] Saving to: \u2018building.csv\u2019 building.csv 100%[===================>] 544 --.-KB/s in 0s 2020-12-09 02:43:33 (25.1 MB/s) - \u2018building.csv\u2019 saved [544/544] # Data file at https://www.cse.ust.hk/msbd5003/data/building.csv df = spark . read . csv ( 'building.csv' , header = True , inferSchema = True ) # show the content of the dataframe df . show () +----------+-----------+-----------+-----------+------------+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country| +----------+-----------+-----------+-----------+------------+ | 1| M1| 25| AC1000| USA| | 2| M2| 27| FN39TG| France| | 3| M3| 28| JDNS77| Brazil| | 4| M4| 17| GG1919| Finland| | 5| M5| 3| ACMAX22| Hong Kong| | 6| M6| 9| AC1000| Singapore| | 7| M7| 13| FN39TG|South Africa| | 8| M8| 25| JDNS77| Australia| | 9| M9| 11| GG1919| Mexico| | 10| M10| 23| ACMAX22| China| | 11| M11| 14| AC1000| Belgium| | 12| M12| 26| FN39TG| Finland| | 13| M13| 25| JDNS77|Saudi Arabia| | 14| M14| 17| GG1919| Germany| | 15| M15| 19| ACMAX22| Israel| | 16| M16| 23| AC1000| Turkey| | 17| M17| 11| FN39TG| Egypt| | 18| M18| 25| JDNS77| Indonesia| | 19| M19| 14| GG1919| Canada| | 20| M20| 19| ACMAX22| Argentina| +----------+-----------+-----------+-----------+------------+ # Print the dataframe schema in a tree format df . printSchema () root |-- BuildingID: integer (nullable = true) |-- BuildingMgr: string (nullable = true) |-- BuildingAge: integer (nullable = true) |-- HVACproduct: string (nullable = true) |-- Country: string (nullable = true) # Create an RDD from the dataframe dfrdd = df . rdd dfrdd . take ( 3 ) [Row(BuildingID=1, BuildingMgr='M1', BuildingAge=25, HVACproduct='AC1000', Country='USA'), Row(BuildingID=2, BuildingMgr='M2', BuildingAge=27, HVACproduct='FN39TG', Country='France'), Row(BuildingID=3, BuildingMgr='M3', BuildingAge=28, HVACproduct='JDNS77', Country='Brazil')] # Retrieve specific columns from the dataframe df . select ( 'BuildingID' , 'Country' ) . show () +----------+------------+ |BuildingID| Country| +----------+------------+ | 1| USA| | 2| France| | 3| Brazil| | 4| Finland| | 5| Hong Kong| | 6| Singapore| | 7|South Africa| | 8| Australia| | 9| Mexico| | 10| China| | 11| Belgium| | 12| Finland| | 13|Saudi Arabia| | 14| Germany| | 15| Israel| | 16| Turkey| | 17| Egypt| | 18| Indonesia| | 19| Canada| | 20| Argentina| +----------+------------+ from pyspark.sql.functions import * df . where ( \"Country<'USA'\" ) . select ( 'BuildingID' , lit ( 'OK' )) . show () +----------+---+ |BuildingID| OK| +----------+---+ | 2| OK| | 3| OK| | 4| OK| | 5| OK| | 6| OK| | 7| OK| | 8| OK| | 9| OK| | 10| OK| | 11| OK| | 12| OK| | 13| OK| | 14| OK| | 15| OK| | 16| OK| | 17| OK| | 18| OK| | 19| OK| | 20| OK| +----------+---+ # Use GroupBy clause with dataframe df . groupBy ( 'HVACProduct' ) . count () . show () +-----------+-----+ |HVACProduct|count| +-----------+-----+ | ACMAX22| 4| | AC1000| 4| | JDNS77| 4| | FN39TG| 4| | GG1919| 4| +-----------+-----+ ! wget https : // www . cse . ust . hk / msbd5003 / data / Customer . csv ! wget https : // www . cse . ust . hk / msbd5003 / data / Product . csv ! wget https : // www . cse . ust . hk / msbd5003 / data / SalesOrderDetail . csv ! wget https : // www . cse . ust . hk / msbd5003 / data / SalesOrderHeader . csv --2020-12-09 03:07:42-- https://www.cse.ust.hk/msbd5003/data/Customer.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 199491 (195K) [text/plain] Saving to: \u2018Customer.csv\u2019 Customer.csv 100%[===================>] 194.82K 242KB/s in 0.8s 2020-12-09 03:07:44 (242 KB/s) - \u2018Customer.csv\u2019 saved [199491/199491] --2020-12-09 03:07:44-- https://www.cse.ust.hk/msbd5003/data/Product.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1355634 (1.3M) [text/plain] Saving to: \u2018Product.csv\u2019 Product.csv 100%[===================>] 1.29M 1.06MB/s in 1.2s 2020-12-09 03:07:46 (1.06 MB/s) - \u2018Product.csv\u2019 saved [1355634/1355634] --2020-12-09 03:07:46-- https://www.cse.ust.hk/msbd5003/data/SalesOrderDetail.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 56766 (55K) [text/plain] Saving to: \u2018SalesOrderDetail.csv\u2019 SalesOrderDetail.cs 100%[===================>] 55.44K 137KB/s in 0.4s 2020-12-09 03:07:47 (137 KB/s) - \u2018SalesOrderDetail.csv\u2019 saved [56766/56766] --2020-12-09 03:07:47-- https://www.cse.ust.hk/msbd5003/data/SalesOrderHeader.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 8680 (8.5K) [text/plain] Saving to: \u2018SalesOrderHeader.csv\u2019 SalesOrderHeader.cs 100%[===================>] 8.48K --.-KB/s in 0s 2020-12-09 03:07:49 (131 MB/s) - \u2018SalesOrderHeader.csv\u2019 saved [8680/8680]","title":"Dataframe operations"},{"location":"MSBD5003/notebooks%20in%20class/sparksql/#rewriting-sql-with-dataframe-api","text":"# Load data from csv files # Data files at https://www.cse.ust.hk/msbd5003/data dfCustomer = spark . read . csv ( 'Customer.csv' , header = True , inferSchema = True ) dfProduct = spark . read . csv ( 'Product.csv' , header = True , inferSchema = True ) dfDetail = spark . read . csv ( 'SalesOrderDetail.csv' , header = True , inferSchema = True ) dfHeader = spark . read . csv ( 'SalesOrderHeader.csv' , header = True , inferSchema = True ) # SELECT ProductID, Name, ListPrice # FROM Product # WHERE Color = 'black' dfProduct . filter ( \"Color = 'Black'\" ) \\ . select ( 'ProductID' , 'Name' , 'ListPrice' ) \\ . show ( truncate = False ) +---------+-----------------------------+---------+ |ProductID|Name |ListPrice| +---------+-----------------------------+---------+ |680 |HL Road Frame - Black, 58 |1431.5 | |708 |Sport-100 Helmet, Black |34.99 | |722 |LL Road Frame - Black, 58 |337.22 | |723 |LL Road Frame - Black, 60 |337.22 | |724 |LL Road Frame - Black, 62 |337.22 | |736 |LL Road Frame - Black, 44 |337.22 | |737 |LL Road Frame - Black, 48 |337.22 | |738 |LL Road Frame - Black, 52 |337.22 | |743 |HL Mountain Frame - Black, 42|1349.6 | |744 |HL Mountain Frame - Black, 44|1349.6 | |745 |HL Mountain Frame - Black, 48|1349.6 | |746 |HL Mountain Frame - Black, 46|1349.6 | |747 |HL Mountain Frame - Black, 38|1349.6 | |765 |Road-650 Black, 58 |782.99 | |766 |Road-650 Black, 60 |782.99 | |767 |Road-650 Black, 62 |782.99 | |768 |Road-650 Black, 44 |782.99 | |769 |Road-650 Black, 48 |782.99 | |770 |Road-650 Black, 52 |782.99 | |775 |Mountain-100 Black, 38 |3374.99 | +---------+-----------------------------+---------+ only showing top 20 rows dfProduct . where ( dfProduct . Color == 'Black' ) \\ . select ( dfProduct . ProductID , dfProduct [ 'Name' ], ( dfProduct [ 'ListPrice' ] * 2 ) . alias ( 'Double price' )) \\ . show ( truncate = False ) +---------+-----------------------------+------------+ |ProductID|Name |Double price| +---------+-----------------------------+------------+ |680 |HL Road Frame - Black, 58 |2863.0 | |708 |Sport-100 Helmet, Black |69.98 | |722 |LL Road Frame - Black, 58 |674.44 | |723 |LL Road Frame - Black, 60 |674.44 | |724 |LL Road Frame - Black, 62 |674.44 | |736 |LL Road Frame - Black, 44 |674.44 | |737 |LL Road Frame - Black, 48 |674.44 | |738 |LL Road Frame - Black, 52 |674.44 | |743 |HL Mountain Frame - Black, 42|2699.2 | |744 |HL Mountain Frame - Black, 44|2699.2 | |745 |HL Mountain Frame - Black, 48|2699.2 | |746 |HL Mountain Frame - Black, 46|2699.2 | |747 |HL Mountain Frame - Black, 38|2699.2 | |765 |Road-650 Black, 58 |1565.98 | |766 |Road-650 Black, 60 |1565.98 | |767 |Road-650 Black, 62 |1565.98 | |768 |Road-650 Black, 44 |1565.98 | |769 |Road-650 Black, 48 |1565.98 | |770 |Road-650 Black, 52 |1565.98 | |775 |Mountain-100 Black, 38 |6749.98 | +---------+-----------------------------+------------+ only showing top 20 rows dfProduct . where ( dfProduct . ListPrice * 2 > 100 ) \\ . select ( dfProduct . ProductID , dfProduct [ 'Name' ], dfProduct . ListPrice * 2 ) \\ . show ( truncate = False ) +---------+-------------------------+---------------+ |ProductID|Name |(ListPrice * 2)| +---------+-------------------------+---------------+ |680 |HL Road Frame - Black, 58|2863.0 | |706 |HL Road Frame - Red, 58 |2863.0 | |717 |HL Road Frame - Red, 62 |2863.0 | |718 |HL Road Frame - Red, 44 |2863.0 | |719 |HL Road Frame - Red, 48 |2863.0 | |720 |HL Road Frame - Red, 52 |2863.0 | |721 |HL Road Frame - Red, 56 |2863.0 | |722 |LL Road Frame - Black, 58|674.44 | |723 |LL Road Frame - Black, 60|674.44 | |724 |LL Road Frame - Black, 62|674.44 | |725 |LL Road Frame - Red, 44 |674.44 | |726 |LL Road Frame - Red, 48 |674.44 | |727 |LL Road Frame - Red, 52 |674.44 | |728 |LL Road Frame - Red, 58 |674.44 | |729 |LL Road Frame - Red, 60 |674.44 | |730 |LL Road Frame - Red, 62 |674.44 | |731 |ML Road Frame - Red, 44 |1189.66 | |732 |ML Road Frame - Red, 48 |1189.66 | |733 |ML Road Frame - Red, 52 |1189.66 | |734 |ML Road Frame - Red, 58 |1189.66 | +---------+-------------------------+---------------+ only showing top 20 rows # SELECT ProductID, Name, ListPrice # FROM Product # WHERE Color = 'black' # ORDER BY ProductID dfProduct . filter ( \"Color = 'Black'\" ) \\ . select ( 'ProductID' , 'Name' , 'ListPrice' ) \\ . orderBy ( 'ListPrice' ) \\ . show ( truncate = False ) +---------+--------------------------+---------+ |ProductID|Name |ListPrice| +---------+--------------------------+---------+ |860 |Half-Finger Gloves, L |24.49 | |859 |Half-Finger Gloves, M |24.49 | |858 |Half-Finger Gloves, S |24.49 | |708 |Sport-100 Helmet, Black |34.99 | |862 |Full-Finger Gloves, M |37.99 | |861 |Full-Finger Gloves, S |37.99 | |863 |Full-Finger Gloves, L |37.99 | |841 |Men's Sports Shorts, S |59.99 | |849 |Men's Sports Shorts, M |59.99 | |851 |Men's Sports Shorts, XL |59.99 | |850 |Men's Sports Shorts, L |59.99 | |815 |LL Mountain Front Wheel |60.745 | |868 |Women's Mountain Shorts, M|69.99 | |869 |Women's Mountain Shorts, L|69.99 | |867 |Women's Mountain Shorts, S|69.99 | |853 |Women's Tights, M |74.99 | |854 |Women's Tights, L |74.99 | |852 |Women's Tights, S |74.99 | |818 |LL Road Front Wheel |85.565 | |823 |LL Mountain Rear Wheel |87.745 | +---------+--------------------------+---------+ only showing top 20 rows # Find all orders and details on black product, # return the product SalesOrderID, SalesOrderDetailID, Name, UnitPrice, and OrderQty # SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty # FROM SalesLT.SalesOrderDetail, SalesLT.Product # WHERE SalesOrderDetail.ProductID = Product.ProductID AND Color = 'Black' # SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty # FROM SalesLT.SalesOrderDetail # JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID # WHERE Color = 'Black' # Spark SQL supports natural joins dfDetail . join ( dfProduct , 'ProductID' ) \\ . select ( 'SalesOrderID' , 'SalesOrderDetailID' , 'Name' , 'UnitPrice' , 'OrderQty' ) \\ . filter ( \"Color='Black'\" ) \\ . show () # If we move the filter to after select, it still works. Why? +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71797| 111045|LL Road Frame - B...| 202.332| 3| | 71783| 110730|LL Road Frame - B...| 202.332| 6| | 71938| 113297|LL Road Frame - B...| 202.332| 3| | 71915| 113090|LL Road Frame - B...| 202.332| 2| | 71815| 111451|LL Road Frame - B...| 202.332| 1| | 71797| 111044|LL Road Frame - B...| 202.332| 1| | 71783| 110710|LL Road Frame - B...| 202.332| 4| | 71936| 113260|HL Mountain Frame...| 809.76| 4| | 71899| 112937|HL Mountain Frame...| 809.76| 1| | 71845| 112137|HL Mountain Frame...| 809.76| 2| | 71832| 111806|HL Mountain Frame...| 809.76| 4| | 71780| 110622|HL Mountain Frame...| 809.76| 1| | 71936| 113235|HL Mountain Frame...| 809.76| 4| | 71845| 112134|HL Mountain Frame...| 809.76| 3| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows # This also works: d1 = dfDetail . join ( dfProduct , 'ProductID' ) \\ . select ( 'SalesOrderID' , 'SalesOrderDetailID' , 'Name' , 'UnitPrice' , 'OrderQty' ) d1 . show () d2 = d1 . filter ( \"Color = 'Black'\" ) d2 . show () d2 . explain () +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113278|Sport-100 Helmet,...| 20.994| 3| | 71936| 113228|Sport-100 Helmet,...| 20.994| 1| | 71902| 112980|Sport-100 Helmet,...| 20.994| 2| | 71797| 111075|Sport-100 Helmet,...| 20.994| 6| | 71784| 110794|Sport-100 Helmet,...| 20.994| 10| | 71783| 110751|Sport-100 Helmet,...| 20.994| 10| | 71782| 110709|Sport-100 Helmet,...| 20.994| 3| | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71938| 113282|Sport-100 Helmet,...| 20.994| 3| | 71902| 112995|Sport-100 Helmet,...| 20.994| 7| | 71863| 112395|Sport-100 Helmet,...| 20.994| 1| | 71797| 111038|Sport-100 Helmet,...| 20.994| 4| | 71784| 110753|Sport-100 Helmet,...| 20.994| 2| | 71783| 110749|Sport-100 Helmet,...| 19.2445| 15| | 71782| 110708|Sport-100 Helmet,...| 20.994| 6| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71797| 111045|LL Road Frame - B...| 202.332| 3| | 71783| 110730|LL Road Frame - B...| 202.332| 6| | 71938| 113297|LL Road Frame - B...| 202.332| 3| | 71915| 113090|LL Road Frame - B...| 202.332| 2| | 71815| 111451|LL Road Frame - B...| 202.332| 1| | 71797| 111044|LL Road Frame - B...| 202.332| 1| | 71783| 110710|LL Road Frame - B...| 202.332| 4| | 71936| 113260|HL Mountain Frame...| 809.76| 4| | 71899| 112937|HL Mountain Frame...| 809.76| 1| | 71845| 112137|HL Mountain Frame...| 809.76| 2| | 71832| 111806|HL Mountain Frame...| 809.76| 4| | 71780| 110622|HL Mountain Frame...| 809.76| 1| | 71936| 113235|HL Mountain Frame...| 809.76| 4| | 71845| 112134|HL Mountain Frame...| 809.76| 3| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows == Physical Plan == *(2) Project [SalesOrderID#217, SalesOrderDetailID#218, Name#168, UnitPrice#221, OrderQty#219] +- *(2) BroadcastHashJoin [ProductID#220], [ProductID#167], Inner, BuildLeft :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[3, int, true] as bigint))), [id=#380] : +- *(1) Project [SalesOrderID#217, SalesOrderDetailID#218, OrderQty#219, ProductID#220, UnitPrice#221] : +- *(1) Filter isnotnull(ProductID#220) : +- FileScan csv [SalesOrderID#217,SalesOrderDetailID#218,OrderQty#219,ProductID#220,UnitPrice#221] Batched: false, DataFilters: [isnotnull(ProductID#220)], Format: CSV, Location: InMemoryFileIndex[file:/content/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double> +- *(2) Project [ProductID#167, Name#168] +- *(2) Filter ((isnotnull(Color#170) AND (Color#170 = Black)) AND isnotnull(ProductID#167)) +- FileScan csv [ProductID#167,Name#168,Color#170] Batched: false, DataFilters: [isnotnull(Color#170), (Color#170 = Black), isnotnull(ProductID#167)], Format: CSV, Location: InMemoryFileIndex[file:/content/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string> # SparkSQL performs optimization depending on whether intermediate dataframe are cached or not: d1 = dfDetail . join ( dfProduct , 'ProductID' ) \\ . select ( 'SalesOrderID' , 'SalesOrderDetailID' , 'Name' , 'UnitPrice' , 'OrderQty' ) d1 . persist () d1 . show () +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113278|Sport-100 Helmet,...| 20.994| 3| | 71936| 113228|Sport-100 Helmet,...| 20.994| 1| | 71902| 112980|Sport-100 Helmet,...| 20.994| 2| | 71797| 111075|Sport-100 Helmet,...| 20.994| 6| | 71784| 110794|Sport-100 Helmet,...| 20.994| 10| | 71783| 110751|Sport-100 Helmet,...| 20.994| 10| | 71782| 110709|Sport-100 Helmet,...| 20.994| 3| | 71938| 113295|Sport-100 Helmet,...| 20.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71938| 113282|Sport-100 Helmet,...| 20.994| 3| | 71902| 112995|Sport-100 Helmet,...| 20.994| 7| | 71863| 112395|Sport-100 Helmet,...| 20.994| 1| | 71797| 111038|Sport-100 Helmet,...| 20.994| 4| | 71784| 110753|Sport-100 Helmet,...| 20.994| 2| | 71783| 110749|Sport-100 Helmet,...| 19.2445| 15| | 71782| 110708|Sport-100 Helmet,...| 20.994| 6| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows d2 = d1 . filter ( \"Color = 'Black'\" ) #d2 = d1.filter(\"OrderQty >= 10\") d2 . show () d2 . explain () +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71797| 111045|LL Road Frame - B...| 202.332| 3| | 71783| 110730|LL Road Frame - B...| 202.332| 6| | 71938| 113297|LL Road Frame - B...| 202.332| 3| | 71915| 113090|LL Road Frame - B...| 202.332| 2| | 71815| 111451|LL Road Frame - B...| 202.332| 1| | 71797| 111044|LL Road Frame - B...| 202.332| 1| | 71783| 110710|LL Road Frame - B...| 202.332| 4| | 71936| 113260|HL Mountain Frame...| 809.76| 4| | 71899| 112937|HL Mountain Frame...| 809.76| 1| | 71845| 112137|HL Mountain Frame...| 809.76| 2| | 71832| 111806|HL Mountain Frame...| 809.76| 4| | 71780| 110622|HL Mountain Frame...| 809.76| 1| | 71936| 113235|HL Mountain Frame...| 809.76| 4| | 71845| 112134|HL Mountain Frame...| 809.76| 3| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows == Physical Plan == *(2) Project [SalesOrderID#112, SalesOrderDetailID#113, Name#63, UnitPrice#116, OrderQty#114] +- *(2) BroadcastHashJoin [ProductID#115], [ProductID#62], Inner, BuildLeft :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[3, int, true] as bigint))), [id=#280] : +- *(1) Project [SalesOrderID#112, SalesOrderDetailID#113, OrderQty#114, ProductID#115, UnitPrice#116] : +- *(1) Filter isnotnull(ProductID#115) : +- FileScan csv [SalesOrderID#112,SalesOrderDetailID#113,OrderQty#114,ProductID#115,UnitPrice#116] Batched: false, DataFilters: [isnotnull(ProductID#115)], Format: CSV, Location: InMemoryFileIndex[file:/csproject/msbd5003/public_html/data/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double> +- *(2) Project [ProductID#62, Name#63] +- *(2) Filter ((isnotnull(Color#65) AND (Color#65 = Black)) AND isnotnull(ProductID#62)) +- FileScan csv [ProductID#62,Name#63,Color#65] Batched: false, DataFilters: [isnotnull(Color#65), (Color#65 = Black), isnotnull(ProductID#62)], Format: CSV, Location: InMemoryFileIndex[file:/csproject/msbd5003/public_html/data/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string> # This will report an error: d1 = dfDetail . join ( dfProduct , 'ProductID' ) \\ . select ( 'SalesOrderID' , 'SalesOrderDetailID' , 'Name' , 'UnitPrice' , 'OrderQty' ) d1 . write . csv ( 'temp.csv' , mode = 'overwrite' , header = True ) d2 = spark . read . csv ( 'temp.csv' , header = True , inferSchema = True ) d2 . filter ( \"Color = 'Black'\" ) . show () --------------------------------------------------------------------------- AnalysisException Traceback (most recent call last) <ipython-input-18-168595bb0376> in <module> 5 d1.write.csv('temp.csv', mode = 'overwrite', header = True) 6 d2 = spark.read.csv('temp.csv', header = True, inferSchema = True) ----> 7 d2.filter(\"Color = 'Black'\").show() /csproject/msbd5003/python/pyspark/sql/dataframe.py in filter(self, condition) 1457 \"\"\" 1458 if isinstance(condition, basestring): -> 1459 jdf = self._jdf.filter(condition) 1460 elif isinstance(condition, Column): 1461 jdf = self._jdf.filter(condition._jc) /csproject/msbd5003/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args) 1303 answer = self.gateway_client.send_command(command) 1304 return_value = get_return_value( -> 1305 answer, self.gateway_client, self.target_id, self.name) 1306 1307 for temp_arg in temp_args: /csproject/msbd5003/python/pyspark/sql/utils.py in deco(*a, **kw) 135 # Hide where the exception came from that shows a non-Pythonic 136 # JVM exception message. --> 137 raise_from(converted) 138 else: 139 raise /csproject/msbd5003/python/pyspark/sql/utils.py in raise_from(e) AnalysisException: cannot resolve '`Color`' given input columns: [Name, OrderQty, SalesOrderDetailID, SalesOrderID, UnitPrice]; line 1 pos 0; 'Filter ('Color = Black) +- Relation[SalesOrderID#580,SalesOrderDetailID#581,Name#582,UnitPrice#583,OrderQty#584] csv # Find all orders that include at least one black product, # return the product SalesOrderID, Name, UnitPrice, and OrderQty # SELECT DISTINCT SalesOrderID # FROM SalesLT.SalesOrderDetail # JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID # WHERE Color = 'Black' dfDetail . join ( dfProduct . filter ( \"Color='Black'\" ), 'ProductID' ) \\ . select ( 'SalesOrderID' ) \\ . distinct () \\ . show () +------------+ |SalesOrderID| +------------+ | 71902| | 71832| | 71915| | 71831| | 71898| | 71935| | 71938| | 71845| | 71783| | 71815| | 71936| | 71863| | 71780| | 71782| | 71899| | 71784| | 71797| +------------+ # How many colors in the products? # SELECT COUNT(DISTINCT Color) # FROM SalesLT.Product dfProduct . select ( 'Color' ) . distinct () . count () # It's 1 more than standard SQL. In standard SQL, COUNT() does not count NULLs. 10 # Find the total price of each order, # return SalesOrderID and total price (column name should be \u2018totalprice\u2019) # SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice # FROM SalesLT.SalesOrderDetail # GROUP BY SalesOrderID dfDetail . select ( '*' , ( dfDetail . UnitPrice * dfDetail . OrderQty * ( 1 - dfDetail . UnitPriceDiscount )) . alias ( 'netprice' )) \\ . groupBy ( 'SalesOrderID' ) . sum ( 'netprice' ) \\ . withColumnRenamed ( 'sum(netprice)' , 'TotalPrice' ) \\ . show () +------------+------------------+ |SalesOrderID| sum(netprice)| +------------+------------------+ | 71867| 858.9| | 71902|59894.209199999976| | 71832| 28950.678108| | 71915|1732.8899999999999| | 71946| 31.584| | 71895|221.25600000000003| | 71816|2847.4079999999994| | 71831| 1712.946| | 71923| 96.108824| | 71858|11528.844000000001| | 71917| 37.758| | 71897| 10585.05| | 71885| 524.664| | 71856|500.30400000000003| | 71898| 53248.69200000002| | 71774| 713.796| | 71796| 47848.02600000001| | 71935|5533.8689079999995| | 71938| 74160.228| | 71845| 34118.5356| +------------+------------------+ only showing top 20 rows # Find the total price of each order where the total price > 10000 # SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice # FROM SalesLT.SalesOrderDetail # GROUP BY SalesOrderID # HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000 dfDetail . select ( '*' , ( dfDetail . UnitPrice * dfDetail . OrderQty * ( 1 - dfDetail . UnitPriceDiscount )) . alias ( 'netprice' )) \\ . groupBy ( 'SalesOrderID' ) . sum ( 'netprice' ) \\ . withColumnRenamed ( 'sum(netprice)' , 'TotalPrice' ) \\ . where ( 'TotalPrice > 10000' ) \\ . show () +------------+------------------+ |SalesOrderID| TotalPrice| +------------+------------------+ | 71902|59894.209199999976| | 71832| 28950.678108| | 71858|11528.844000000001| | 71897| 10585.05| | 71898| 53248.69200000002| | 71796| 47848.02600000001| | 71938| 74160.228| | 71845| 34118.5356| | 71783| 65683.367986| | 71936| 79589.61602399996| | 71780|29923.007999999998| | 71782| 33319.98600000001| | 71784| 89869.27631400003| | 71797| 65123.46341800001| +------------+------------------+ # Find the total price on the black products of each order where the total price > 10000 # SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice # FROM SalesLT.SalesOrderDetail, SalesLT.Product # WHERE SalesLT.SalesOrderDetail.ProductID = SalesLT.Product.ProductID AND Color = 'Black' # GROUP BY SalesOrderID # HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000 dfDetail . select ( '*' , ( dfDetail . UnitPrice * dfDetail . OrderQty * ( 1 - dfDetail . UnitPriceDiscount )) . alias ( 'netprice' )) \\ . join ( dfProduct , 'ProductID' ) \\ . where ( \"Color = 'Black'\" ) \\ . groupBy ( 'SalesOrderID' ) . sum ( 'netprice' ) \\ . withColumnRenamed ( 'sum(netprice)' , 'TotalPrice' ) \\ . where ( 'TotalPrice > 10000' ) \\ . show () +------------+------------------+ |SalesOrderID| TotalPrice| +------------+------------------+ | 71902|26677.883999999995| | 71832| 16883.748108| | 71938| 33779.448| | 71845| 18109.836| | 71783|15524.117476000003| | 71936| 44490.290424| | 71780| 16964.322| | 71797| 27581.613792| +------------+------------------+ # For each customer, find the total quantity of black products bought. # Report CustomerID, FirstName, LastName, and total quantity # select saleslt.customer.customerid, FirstName, LastName, sum(orderqty) # from saleslt.customer # left outer join # ( # saleslt.salesorderheader # join saleslt.salesorderdetail # on saleslt.salesorderdetail.salesorderid = saleslt.salesorderheader.salesorderid # join saleslt.product # on saleslt.product.productid = saleslt.salesorderdetail.productid and color = 'black' # ) # on saleslt.customer.customerid = saleslt.salesorderheader.customerid # group by saleslt.customer.customerid, FirstName, LastName # order by sum(orderqty) desc d1 = dfDetail . join ( dfProduct , 'ProductID' ) \\ . where ( 'Color = \"Black\"' ) \\ . join ( dfHeader , 'SalesOrderID' ) \\ . groupBy ( 'CustomerID' ) . sum ( 'OrderQty' ) dfCustomer . join ( d1 , 'CustomerID' , 'left_outer' ) \\ . select ( 'CustomerID' , 'FirstName' , 'LastName' , 'sum(OrderQty)' ) \\ . orderBy ( 'sum(OrderQty)' , ascending = False ) \\ . show () +----------+------------+------------+-------------+ |CustomerID| FirstName| LastName|sum(OrderQty)| +----------+------------+------------+-------------+ | 30050| Krishna|Sunkammurali| 89| | 29796| Jon| Grande| 65| | 29957| Kevin| Liu| 62| | 29929| Jeffrey| Kurtz| 46| | 29546| Christopher| Beck| 45| | 29922| Pamala| Kotc| 34| | 30113| Raja| Venugopal| 34| | 29938| Frank| Campbell| 29| | 29736| Terry| Eminhizer| 23| | 29485| Catherine| Abel| 10| | 30019| Matthew| Miller| 9| | 29932| Rebecca| Laszlo| 7| | 29975| Walter| Mays| 5| | 29638| Rosmarie| Carroll| 2| | 29531| Cory| Booth| 1| | 30089|Michael John| Troyer| 1| | 29568| Donald| Blanton| 1| | 29868| Denean| Ison| null| | 29646| Stacey| Cereghino| null| | 29905| Elizabeth| Keyser| null| +----------+------------+------------+-------------+ only showing top 20 rows","title":"Rewriting SQL with DataFrame API"},{"location":"MSBD5003/notebooks%20in%20class/sparksql/#embed-sql-queries","text":"You can also run SQL queries over dataframes once you register them as temporary tables within the SparkSession. # Register the dataframe as a temporary view called HVAC df . createOrReplaceTempView ( 'HVAC' ) spark . sql ( 'SELECT * FROM HVAC WHERE BuildingAge >= 10' ) . show () +----------+-----------+-----------+-----------+------------+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country| +----------+-----------+-----------+-----------+------------+ | 1| M1| 25| AC1000| USA| | 2| M2| 27| FN39TG| France| | 3| M3| 28| JDNS77| Brazil| | 4| M4| 17| GG1919| Finland| | 7| M7| 13| FN39TG|South Africa| | 8| M8| 25| JDNS77| Australia| | 9| M9| 11| GG1919| Mexico| | 10| M10| 23| ACMAX22| China| | 11| M11| 14| AC1000| Belgium| | 12| M12| 26| FN39TG| Finland| | 13| M13| 25| JDNS77|Saudi Arabia| | 14| M14| 17| GG1919| Germany| | 15| M15| 19| ACMAX22| Israel| | 16| M16| 23| AC1000| Turkey| | 17| M17| 11| FN39TG| Egypt| | 18| M18| 25| JDNS77| Indonesia| | 19| M19| 14| GG1919| Canada| | 20| M20| 19| ACMAX22| Argentina| +----------+-----------+-----------+-----------+------------+ # Can even mix DataFrame API with SQL: df . where ( 'BuildingAge >= 10' ) . createOrReplaceTempView ( 'OldBuildings' ) spark . sql ( 'SELECT HVACproduct, COUNT(*) FROM OldBuildings GROUP BY HVACproduct' ) . show () +-----------+--------+ |HVACproduct|count(1)| +-----------+--------+ | ACMAX22| 3| | AC1000| 3| | JDNS77| 4| | FN39TG| 4| | GG1919| 4| +-----------+--------+ d1 = spark . sql ( 'SELECT * FROM HVAC WHERE BuildingAge >= 10' ) d1 . groupBy ( 'HVACproduct' ) . count () . show () +-----------+-----+ |HVACproduct|count| +-----------+-----+ | ACMAX22| 3| | AC1000| 3| | JDNS77| 4| | FN39TG| 4| | GG1919| 4| +-----------+-----+ # UDF from pyspark.sql.functions import udf from pyspark.sql.types import IntegerType slen = udf ( lambda s : len ( s ) + 2 , IntegerType ()) df . select ( '*' , slen ( df [ 'Country' ]) . alias ( 'slen' )) . show () +----------+-----------+-----------+-----------+------------+----+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country|slen| +----------+-----------+-----------+-----------+------------+----+ | 1| M1| 25| AC1000| USA| 5| | 2| M2| 27| FN39TG| France| 8| | 3| M3| 28| JDNS77| Brazil| 8| | 4| M4| 17| GG1919| Finland| 9| | 5| M5| 3| ACMAX22| Hong Kong| 11| | 6| M6| 9| AC1000| Singapore| 11| | 7| M7| 13| FN39TG|South Africa| 14| | 8| M8| 25| JDNS77| Australia| 11| | 9| M9| 11| GG1919| Mexico| 8| | 10| M10| 23| ACMAX22| China| 7| | 11| M11| 14| AC1000| Belgium| 9| | 12| M12| 26| FN39TG| Finland| 9| | 13| M13| 25| JDNS77|Saudi Arabia| 14| | 14| M14| 17| GG1919| Germany| 9| | 15| M15| 19| ACMAX22| Israel| 8| | 16| M16| 23| AC1000| Turkey| 8| | 17| M17| 11| FN39TG| Egypt| 7| | 18| M18| 25| JDNS77| Indonesia| 11| | 19| M19| 14| GG1919| Canada| 8| | 20| M20| 19| ACMAX22| Argentina| 11| +----------+-----------+-----------+-----------+------------+----+ spark . udf . register ( 'slen' , lambda s : len ( s ), IntegerType ()) spark . sql ( 'SELECT *, slen(Country) AS slen FROM HVAC' ) . show () +----------+-----------+-----------+-----------+------------+----+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country|slen| +----------+-----------+-----------+-----------+------------+----+ | 1| M1| 25| AC1000| USA| 3| | 2| M2| 27| FN39TG| France| 6| | 3| M3| 28| JDNS77| Brazil| 6| | 4| M4| 17| GG1919| Finland| 7| | 5| M5| 3| ACMAX22| Hong Kong| 9| | 6| M6| 9| AC1000| Singapore| 9| | 7| M7| 13| FN39TG|South Africa| 12| | 8| M8| 25| JDNS77| Australia| 9| | 9| M9| 11| GG1919| Mexico| 6| | 10| M10| 23| ACMAX22| China| 5| | 11| M11| 14| AC1000| Belgium| 7| | 12| M12| 26| FN39TG| Finland| 7| | 13| M13| 25| JDNS77|Saudi Arabia| 12| | 14| M14| 17| GG1919| Germany| 7| | 15| M15| 19| ACMAX22| Israel| 6| | 16| M16| 23| AC1000| Turkey| 6| | 17| M17| 11| FN39TG| Egypt| 5| | 18| M18| 25| JDNS77| Indonesia| 9| | 19| M19| 14| GG1919| Canada| 6| | 20| M20| 19| ACMAX22| Argentina| 9| +----------+-----------+-----------+-----------+------------+----+","title":"Embed SQL queries"},{"location":"MSBD5003/notebooks%20in%20class/sparksql/#flexible-data-model","text":"Sample data file at https://www.cse.ust.hk/msbd5003/data/products.json df = spark . read . json ( '../data/products.json' ) df . printSchema () root |-- dimensions: struct (nullable = true) | |-- height: double (nullable = true) | |-- length: double (nullable = true) | |-- width: double (nullable = true) |-- id: long (nullable = true) |-- name: string (nullable = true) |-- price: double (nullable = true) |-- tags: array (nullable = true) | |-- element: string (containsNull = true) |-- warehouseLocation: struct (nullable = true) | |-- latitude: double (nullable = true) | |-- longitude: double (nullable = true) df . show () +----------------+---+----------------+-----+-----------+-----------------+ | dimensions| id| name|price| tags|warehouseLocation| +----------------+---+----------------+-----+-----------+-----------------+ |[9.5, 7.0, 12.0]| 2|An ice sculpture| 12.5|[cold, ice]| [-78.75, 20.4]| | [1.0, 3.1, 1.0]| 3| A blue mouse| 25.5| null| [54.4, -32.7]| +----------------+---+----------------+-----+-----------+-----------------+ # Accessing nested fields df . select ( df [ 'dimensions.height' ]) . show () +------+ |height| +------+ | 9.5| | 1.0| +------+ df . select ( 'dimensions.height' ) . show () +------+ |height| +------+ | 9.5| | 1.0| +------+ df . select ( 'dimensions.height' ) \\ . filter ( \"tags[0] = 'cold' AND warehouseLocation.latitude < 0\" ) \\ . show () +------+ |height| +------+ | 9.5| +------+ df . rdd . take ( 3 ) [Row(dimensions=Row(height=9.5, length=7.0, width=12.0), id=2, name='An ice sculpture', price=12.5, tags=['cold', 'ice'], warehouseLocation=Row(latitude=-78.75, longitude=20.4)), Row(dimensions=Row(height=1.0, length=3.1, width=1.0), id=3, name='A blue mouse', price=25.5, tags=None, warehouseLocation=Row(latitude=54.4, longitude=-32.7))]","title":"Flexible Data Model"},{"location":"MSBD5003/notebooks%20in%20class/sparksql/#converting-between-rdd-and-dataframe","text":"Sample data file at: https://www.cse.ust.hk/msbd5003/data/people.txt # Load a text file and convert each line to a Row. lines = sc . textFile ( \"../data/people.txt\" ) def parse ( l ): a = l . split ( ',' ) return ( a [ 0 ], int ( a [ 1 ])) rdd = lines . map ( parse ) rdd . collect () [('Michael', 29), ('Andy', 30), ('Justin', 19)] # Create the DataFrame from an RDD of tuples, schema is inferred df = spark . createDataFrame ( rdd ) df . printSchema () df . show () root |-- _1: string (nullable = true) |-- _2: long (nullable = true) +-------+---+ | _1| _2| +-------+---+ |Michael| 29| | Andy| 30| | Justin| 19| +-------+---+ # Create the DataFrame from an RDD of tuples with column names, type is inferred df = spark . createDataFrame ( rdd , [ 'name' , 'age' ]) df . printSchema () df . show () root |-- name: string (nullable = true) |-- age: long (nullable = true) +-------+---+ | name|age| +-------+---+ |Michael| 29| | Andy| 30| | Justin| 19| +-------+---+ # Create the DataFrame from an RDD of Rows, type is given in the Row objects from pyspark.sql import Row rdd_rows = rdd . map ( lambda p : Row ( name = p [ 0 ], age = p [ 1 ])) df = spark . createDataFrame ( rdd_rows ) df . printSchema () df . show () root |-- name: string (nullable = true) |-- age: long (nullable = true) +-------+---+ | name|age| +-------+---+ |Michael| 29| | Andy| 30| | Justin| 19| +-------+---+ # Row fields with types incompatible with that of previous rows will be turned into nulls row1 = Row ( name = \"Alice\" , age = 11 ) row2 = Row ( name = \"Bob\" , age = '12' ) rdd_rows = sc . parallelize ([ row1 , row2 ]) df1 = spark . createDataFrame ( rdd_rows ) df1 . show () +-----+----+ | name| age| +-----+----+ |Alice| 11| | Bob|null| +-----+----+ # rdd returns the content as an RDD of Rows teenagers = df . filter ( 'age >= 13 and age <= 19' ) teenNames = teenagers . rdd . map ( lambda p : \"Name: \" + p . name ) teenNames . collect () ['Name: Justin']","title":"Converting between RDD and DataFrame"},{"location":"MSBD5003/notebooks%20in%20class/sparksql/#note","text":"DataFrames are stored using columnar storage with compression RDDs are stored using row storage without compression The RDD view of DataFrame just provides an interface, the Row objects are constructed on the fly and do not necessarily represent the internal storage format of the data","title":"Note:"},{"location":"MSBD5003/notebooks%20in%20class/sparksql/#closure-in-dataframes","text":"data = range ( 10 ) df = spark . createDataFrame ( zip ( data , data )) df . printSchema () df . show () root |-- _1: long (nullable = true) |-- _2: long (nullable = true) +---+---+ | _1| _2| +---+---+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| | 5| 5| | 6| 6| | 7| 7| | 8| 8| | 9| 9| +---+---+ # The 'closure' behaviour in RDD doesn't seem to exist for DataFrames x = 5 df1 = df . filter ( df . _1 < x ) df1 . show () x = 3 df1 . show () +---+---+ | _1| _2| +---+---+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| +---+---+ +---+---+ | _1| _2| +---+---+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| +---+---+ # Because of the Catalyst optimizer ! df1 . explain () == Physical Plan == *(1) Filter (isnotnull(_1#1265L) AND (_1#1265L < 5)) +- *(1) Scan ExistingRDD[_1#1265L,_2#1266L] def f (): return x / 2 x = 5 df1 = df . select ( df . _1 * 2 + f () + 1 + 1 ) df1 . explain () df1 . show () == Physical Plan == *(1) Project [(((cast((_1#1265L * 2) as double) + 2.5) + 1.0) + 1.0) AS ((((_1 * 2) + 2.5) + 1) + 1)#1296] +- *(1) Scan ExistingRDD[_1#1265L,_2#1266L] +----------------------------+ |((((_1 * 2) + 2.5) + 1) + 1)| +----------------------------+ | 4.5| | 6.5| | 8.5| | 10.5| | 12.5| | 14.5| | 16.5| | 18.5| | 20.5| | 22.5| +----------------------------+ rdd = sc . parallelize ( range ( 10 )) x = 5 a = rdd . filter ( lambda z : z < x ) print ( a . take ( 10 )) x = 3 print ( a . take ( 10 )) [0, 1, 2, 3, 4] [0, 1, 2] counter = 0 def increment_counter ( x ): global counter counter += 1 df . foreach ( increment_counter ) print ( counter ) 0","title":"Closure in DataFrames"},{"location":"MSBD5003/projects/Project/","text":"Install dependencies \u00b6 !pip install googletrans Requirement already satisfied: googletrans in /usr/local/lib/python3.6/dist-packages (3.0.0) Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.6/dist-packages (from googletrans) (0.13.3) Requirement already satisfied: idna==2.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2.10) Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2020.11.8) Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (3.0.4) Requirement already satisfied: hstspreload in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2020.11.21) Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (1.4.0) Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (0.9.1) Requirement already satisfied: sniffio in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (1.2.0) Requirement already satisfied: h2==3.* in /usr/local/lib/python3.6/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (3.2.0) Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.6/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (0.9.0) Requirement already satisfied: contextvars>=2.1; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from sniffio->httpx==0.13.3->googletrans) (2.4) Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.6/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (3.0.0) Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (5.2.0) Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars>=2.1; python_version < \"3.7\"->sniffio->httpx==0.13.3->googletrans) (0.14) !pip install jieba Requirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (0.42.1) !pip install snownlp Requirement already satisfied: snownlp in /usr/local/lib/python3.6/dist-packages (0.12.3) !pip install pyspark Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (3.0.1) Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.9) !git clone https://github.com/goto456/stopwords.git fatal: destination path 'stopwords' already exists and is not an empty directory. Read stop words \u00b6 stop_words = [] with open ( '/content/stopwords/hit_stopwords.txt' ) as f : lines = f . readlines () for line in lines : stop_words . append ( line . replace ( '\\n' , '' )) print ( stop_words [: 10 ]) ['\u2014\u2014\u2014', '\u300b\uff09\uff0c', '\uff09\u00f7\uff08\uff11\uff0d', '\u201d\uff0c', '\uff09\u3001', '\uff1d\uff08', ':', '\u2192', '\u2103 ', '&'] from pyspark.context import SparkContext from pyspark.sql.session import SparkSession from pyspark.sql.types import Row # NLP Module from snownlp import SnowNLP import jieba # translation from googletrans import Translator import matplotlib.pyplot as plt import pandas as pd sc = SparkContext . getOrCreate () spark = SparkSession ( sc ) import urllib.request file_url = \"https://github.com/sirily11/hot-keywords/releases/download/master/Sina_keywords--12.03.2020.csv\" file_name = \"Sina_keyword.csv\" urllib . request . urlretrieve ( file_url , file_name ) Read csv file \u00b6 from pyspark.sql.types import StructType , StructField , IntegerType , StringType , TimestampType schema = StructType ([ StructField ( 'keyword' , StringType (), False ), StructField ( \"content\" , StringType (), False ), StructField ( \"time\" , TimestampType (), False ), StructField ( \"rank\" , IntegerType (), False ), StructField ( \"numbers\" , IntegerType (), False ), ]) df = spark.read.csv(file_name, schema=schema, lineSep=\";\") df.show() rdd = df.rdd rdd.take(10) from time import sleep Translation \u00b6 data = pd.read_csv(\"Sina_keyword.csv\", error_bad_lines=False, names=['Keyword', 'Content', 'Time', 'Rank', 'Number']) data = data[:100] from time import sleep from tqdm.auto import tqdm tqdm . pandas () translator = Translator () translator.translate(\"\u4f60\u597d\").text def translate ( index : str , content : str ): has_translated = False while not has_translated : try : return translator . translate ( content ) except Exception : sleep ( 1 ) continue data [ 'Content_Translation' ] = data . progress_apply ( lambda row : translate ( row . name , row [ 'Content' ]), axis = 1 ) data.head(10) preprocess data \u00b6 def preprocess ( row ): global stop_words d = row . asDict () if d [ 'content' ]: d [ 'content' ] = d [ 'content' ]. replace ( \"\u5c55\u5f00\u5168\u6587c\" , \"\" ) d [ 'content' ] = d [ 'content' ]. replace ( \"\u6536\u8d77\u5168\u6587d\" , \"\" ) d [ 'content' ] = d [ 'content' ]. replace ( f \"{d['keyword']}\" , \"\" ) d [ 'content' ] = d [ 'content' ]. replace ( \"#\" , \"\" ) d [ 'content' ] = d [ 'content' ]. replace ( \"\\n\" , \"\" ) d [ 'content' ] = d [ 'content' ]. replace ( 'O\u7f51\u9875\u94fe\u63a5' , '' ) for word in stop_words : d [ 'content' ] = d [ 'content' ]. replace ( word , '' ) new_row = Row ( ** d ) return new_row new_data = rdd . map ( preprocess ) print ( new_data . take ( 10 )) new_data . cache () jieba.initialize() tokenizer = jieba.Tokenizer() def sentiment ( row ): try : s = SnowNLP ( row . content ) return ( row . keyword , ( s . sentiments , row . content )) except Exception : return ( \"error\" , \"nil\" ) def keyword ( row ): return jieba . lcut ( row . keyword , cut_all = False ), row sentiments = new_data . map ( sentiment ). filter ( lambda x : x [ 0 ] != \"error\" and x [ 1 ] != \"nil\" ) sentiments . cache (). take ( 10 ) --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-29-7e85580c7ffe> in <module>() 13 14 sentiments = new_data.map(sentiment).filter(lambda x: x[0] != \"error\" and x[1] != \"nil\") ---> 15 sentiments.cache().take(10) /usr/local/lib/python3.6/dist-packages/pyspark/rdd.py in take(self, num) 1444 1445 p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts)) -> 1446 res = self.context.runJob(self, takeUpToNumLeft, p) 1447 1448 items += res /usr/local/lib/python3.6/dist-packages/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal) 1116 # SparkContext#runJob. 1117 mappedRDD = rdd.mapPartitions(partitionFunc) -> 1118 sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions) 1119 return list(_load_from_socket(sock_info, mappedRDD._jrdd_deserializer)) 1120 /usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py in __call__(self, *args) 1301 proto.END_COMMAND_PART 1302 -> 1303 answer = self.gateway_client.send_command(command) 1304 return_value = get_return_value( 1305 answer, self.gateway_client, self.target_id, self.name) /usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py in send_command(self, command, retry, binary) 1031 connection = self._get_connection() 1032 try: -> 1033 response = connection.send_command(command) 1034 if binary: 1035 return response, self._create_connection_guard(connection) /usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py in send_command(self, command) 1198 1199 try: -> 1200 answer = smart_decode(self.stream.readline()[:-1]) 1201 logger.debug(\"Answer received: {0}\".format(answer)) 1202 if answer.startswith(proto.RETURN_MESSAGE): /usr/lib/python3.6/socket.py in readinto(self, b) 584 while True: 585 try: --> 586 return self._sock.recv_into(b) 587 except timeout: 588 self._timeout_occurred = True KeyboardInterrupt: write sentiments to local sentiments. def groupValues ( values ): total = 0 i = 0 for v in values : sentiments , content = v total += sentiments i += 1 return total / i grouped = sentiments . groupByKey (). mapValues ( groupValues ) print ( grouped . take ( 10 )) pos = sentiments.filter(lambda x: x[1][0] > 0.75).count() neg = sentiments.filter(lambda x: x[1][0] <= 0.35).count() net = sentiments.filter(lambda x: x[1][0] > 0.35 and x[1][0] <= 0.75).count() plt.figure(figsize=(10,10)) plt.bar(\"Positive\", pos) plt.bar(\"Neutral\", net) plt.bar(\"Negative\", neg) plt.xlabel(\"Sentiment\") plt.ylabel(\"Number of posts\") plt.title(\"Sina Weibo Sentiment\") plt.savefig(\"sina.png\")","title":"Project"},{"location":"MSBD5003/projects/Project/#install-dependencies","text":"!pip install googletrans Requirement already satisfied: googletrans in /usr/local/lib/python3.6/dist-packages (3.0.0) Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.6/dist-packages (from googletrans) (0.13.3) Requirement already satisfied: idna==2.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2.10) Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2020.11.8) Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (3.0.4) Requirement already satisfied: hstspreload in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2020.11.21) Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (1.4.0) Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (0.9.1) Requirement already satisfied: sniffio in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (1.2.0) Requirement already satisfied: h2==3.* in /usr/local/lib/python3.6/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (3.2.0) Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.6/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (0.9.0) Requirement already satisfied: contextvars>=2.1; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from sniffio->httpx==0.13.3->googletrans) (2.4) Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.6/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (3.0.0) Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (5.2.0) Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars>=2.1; python_version < \"3.7\"->sniffio->httpx==0.13.3->googletrans) (0.14) !pip install jieba Requirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (0.42.1) !pip install snownlp Requirement already satisfied: snownlp in /usr/local/lib/python3.6/dist-packages (0.12.3) !pip install pyspark Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (3.0.1) Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.9) !git clone https://github.com/goto456/stopwords.git fatal: destination path 'stopwords' already exists and is not an empty directory.","title":"Install dependencies"},{"location":"MSBD5003/projects/Project/#read-stop-words","text":"stop_words = [] with open ( '/content/stopwords/hit_stopwords.txt' ) as f : lines = f . readlines () for line in lines : stop_words . append ( line . replace ( '\\n' , '' )) print ( stop_words [: 10 ]) ['\u2014\u2014\u2014', '\u300b\uff09\uff0c', '\uff09\u00f7\uff08\uff11\uff0d', '\u201d\uff0c', '\uff09\u3001', '\uff1d\uff08', ':', '\u2192', '\u2103 ', '&'] from pyspark.context import SparkContext from pyspark.sql.session import SparkSession from pyspark.sql.types import Row # NLP Module from snownlp import SnowNLP import jieba # translation from googletrans import Translator import matplotlib.pyplot as plt import pandas as pd sc = SparkContext . getOrCreate () spark = SparkSession ( sc ) import urllib.request file_url = \"https://github.com/sirily11/hot-keywords/releases/download/master/Sina_keywords--12.03.2020.csv\" file_name = \"Sina_keyword.csv\" urllib . request . urlretrieve ( file_url , file_name )","title":"Read stop words"},{"location":"MSBD5003/projects/Project/#read-csv-file","text":"from pyspark.sql.types import StructType , StructField , IntegerType , StringType , TimestampType schema = StructType ([ StructField ( 'keyword' , StringType (), False ), StructField ( \"content\" , StringType (), False ), StructField ( \"time\" , TimestampType (), False ), StructField ( \"rank\" , IntegerType (), False ), StructField ( \"numbers\" , IntegerType (), False ), ]) df = spark.read.csv(file_name, schema=schema, lineSep=\";\") df.show() rdd = df.rdd rdd.take(10) from time import sleep","title":"Read csv file"},{"location":"MSBD5003/projects/Project/#translation","text":"data = pd.read_csv(\"Sina_keyword.csv\", error_bad_lines=False, names=['Keyword', 'Content', 'Time', 'Rank', 'Number']) data = data[:100] from time import sleep from tqdm.auto import tqdm tqdm . pandas () translator = Translator () translator.translate(\"\u4f60\u597d\").text def translate ( index : str , content : str ): has_translated = False while not has_translated : try : return translator . translate ( content ) except Exception : sleep ( 1 ) continue data [ 'Content_Translation' ] = data . progress_apply ( lambda row : translate ( row . name , row [ 'Content' ]), axis = 1 ) data.head(10)","title":"Translation"},{"location":"MSBD5003/projects/Project/#preprocess-data","text":"def preprocess ( row ): global stop_words d = row . asDict () if d [ 'content' ]: d [ 'content' ] = d [ 'content' ]. replace ( \"\u5c55\u5f00\u5168\u6587c\" , \"\" ) d [ 'content' ] = d [ 'content' ]. replace ( \"\u6536\u8d77\u5168\u6587d\" , \"\" ) d [ 'content' ] = d [ 'content' ]. replace ( f \"{d['keyword']}\" , \"\" ) d [ 'content' ] = d [ 'content' ]. replace ( \"#\" , \"\" ) d [ 'content' ] = d [ 'content' ]. replace ( \"\\n\" , \"\" ) d [ 'content' ] = d [ 'content' ]. replace ( 'O\u7f51\u9875\u94fe\u63a5' , '' ) for word in stop_words : d [ 'content' ] = d [ 'content' ]. replace ( word , '' ) new_row = Row ( ** d ) return new_row new_data = rdd . map ( preprocess ) print ( new_data . take ( 10 )) new_data . cache () jieba.initialize() tokenizer = jieba.Tokenizer() def sentiment ( row ): try : s = SnowNLP ( row . content ) return ( row . keyword , ( s . sentiments , row . content )) except Exception : return ( \"error\" , \"nil\" ) def keyword ( row ): return jieba . lcut ( row . keyword , cut_all = False ), row sentiments = new_data . map ( sentiment ). filter ( lambda x : x [ 0 ] != \"error\" and x [ 1 ] != \"nil\" ) sentiments . cache (). take ( 10 ) --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-29-7e85580c7ffe> in <module>() 13 14 sentiments = new_data.map(sentiment).filter(lambda x: x[0] != \"error\" and x[1] != \"nil\") ---> 15 sentiments.cache().take(10) /usr/local/lib/python3.6/dist-packages/pyspark/rdd.py in take(self, num) 1444 1445 p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts)) -> 1446 res = self.context.runJob(self, takeUpToNumLeft, p) 1447 1448 items += res /usr/local/lib/python3.6/dist-packages/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal) 1116 # SparkContext#runJob. 1117 mappedRDD = rdd.mapPartitions(partitionFunc) -> 1118 sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions) 1119 return list(_load_from_socket(sock_info, mappedRDD._jrdd_deserializer)) 1120 /usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py in __call__(self, *args) 1301 proto.END_COMMAND_PART 1302 -> 1303 answer = self.gateway_client.send_command(command) 1304 return_value = get_return_value( 1305 answer, self.gateway_client, self.target_id, self.name) /usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py in send_command(self, command, retry, binary) 1031 connection = self._get_connection() 1032 try: -> 1033 response = connection.send_command(command) 1034 if binary: 1035 return response, self._create_connection_guard(connection) /usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py in send_command(self, command) 1198 1199 try: -> 1200 answer = smart_decode(self.stream.readline()[:-1]) 1201 logger.debug(\"Answer received: {0}\".format(answer)) 1202 if answer.startswith(proto.RETURN_MESSAGE): /usr/lib/python3.6/socket.py in readinto(self, b) 584 while True: 585 try: --> 586 return self._sock.recv_into(b) 587 except timeout: 588 self._timeout_occurred = True KeyboardInterrupt: write sentiments to local sentiments. def groupValues ( values ): total = 0 i = 0 for v in values : sentiments , content = v total += sentiments i += 1 return total / i grouped = sentiments . groupByKey (). mapValues ( groupValues ) print ( grouped . take ( 10 )) pos = sentiments.filter(lambda x: x[1][0] > 0.75).count() neg = sentiments.filter(lambda x: x[1][0] <= 0.35).count() net = sentiments.filter(lambda x: x[1][0] > 0.35 and x[1][0] <= 0.75).count() plt.figure(figsize=(10,10)) plt.bar(\"Positive\", pos) plt.bar(\"Neutral\", net) plt.bar(\"Negative\", neg) plt.xlabel(\"Sentiment\") plt.ylabel(\"Number of posts\") plt.title(\"Sina Weibo Sentiment\") plt.savefig(\"sina.png\")","title":"preprocess data"},{"location":"MSBD5006/Lecture%201/","text":"Asset returns \u00b6 Let \\(p_t\\) be the price of an asset at time t, and assume no dividend. One-period simple return or simple net return. \\[R_t = \\frac{P_t}{P_t-1} - 1 = \\frac{P_t - P_{t-1}}{P_{t-1}} \\] Gross return \\[ 1 + P_t = \\frac{P_t}{P_{t - 1}}\\ or P_t = P_{t-1}(1+P_t) \\] Multi-period simple return or the k-period simple net return \\[R_t(k) = \\frac{P_t}{P_{t-k}}-1\\] Gross return \\[1 + R_t(k) = \\sum^{k -1 }_{j=0}(1+R_{t-j}) \\] Continues Compounding \u00b6 from math import exp def pay_interest ( base , interest , payments ): return base * ( base + interest / payments ) ** payments def pay_interest_continue ( base , interest , number_of_years ): return base * exp ( interest * number_of_years ) pay_interest_continue(1, 0.1, 1) 1.1051709180756477 \\[ R_t = log \\] Log return \u00b6 \\[r_{pt} = \\sum_{i = 1}{n}w_ir_{it}\\] Excess return \u00b6 \\[Z_t = R_t - R_{0t}, z_t = r_t - r_{0t}\\] where \\(r_{0t}\\) denotes the log return of a reference asset (e.g. risk-free interest rate) such as shortterm U.S. Treasury bill return, etc..","title":"Lecture 1"},{"location":"MSBD5006/Lecture%201/#asset-returns","text":"Let \\(p_t\\) be the price of an asset at time t, and assume no dividend. One-period simple return or simple net return. \\[R_t = \\frac{P_t}{P_t-1} - 1 = \\frac{P_t - P_{t-1}}{P_{t-1}} \\] Gross return \\[ 1 + P_t = \\frac{P_t}{P_{t - 1}}\\ or P_t = P_{t-1}(1+P_t) \\] Multi-period simple return or the k-period simple net return \\[R_t(k) = \\frac{P_t}{P_{t-k}}-1\\] Gross return \\[1 + R_t(k) = \\sum^{k -1 }_{j=0}(1+R_{t-j}) \\]","title":"Asset returns"},{"location":"MSBD5006/Lecture%201/#continues-compounding","text":"from math import exp def pay_interest ( base , interest , payments ): return base * ( base + interest / payments ) ** payments def pay_interest_continue ( base , interest , number_of_years ): return base * exp ( interest * number_of_years ) pay_interest_continue(1, 0.1, 1) 1.1051709180756477 \\[ R_t = log \\]","title":"Continues Compounding"},{"location":"MSBD5006/Lecture%201/#log-return","text":"\\[r_{pt} = \\sum_{i = 1}{n}w_ir_{it}\\]","title":"Log return"},{"location":"MSBD5006/Lecture%201/#excess-return","text":"\\[Z_t = R_t - R_{0t}, z_t = r_t - r_{0t}\\] where \\(r_{0t}\\) denotes the log return of a reference asset (e.g. risk-free interest rate) such as shortterm U.S. Treasury bill return, etc..","title":"Excess return"},{"location":"MSBD5006/Lecture%202/","text":"Mean and variance: \u03bcx = E(X) and \u03c3x2 = Var(X) = E(X \u2212 \u03bcx)2 Skewness (symmetry) and kurtosis (fat-tails) Kurtosis: How high is the p High kurtosis implies heavy (or long) tails in dis- tribution. Symmetry has important implications in holding short or long financial positions and in risk man- agement. (X \u2212 \u03bcx)3 (X \u2212 \u03bcx)4 S ( x ) = E \u03c3 x3 , K ( x ) = E \u03c3 x4 . Normal distribution \u00b6 E(X) = \u03bc Var(X) = \u03c32 S(X) = 0 K(X) = 3 ml = 0, for l is odd. T-distribution \u00b6 Symmetry at 0 \\[E(x) > 0, v >1\\] Chi-squared distribution \u00b6 \\[E(X) = k$$ $$Var(X) = 2k\\] Joint Distribution \u00b6 \\[F_{X,Y}(x, y) = P(X\\leq x, Y\\leq y)\\] Marginal Distribution \u00b6 The marginal distribution of X is obtained by integrating out Y . A similar definition applies to the marginal distribution of Y .","title":"Lecture 2"},{"location":"MSBD5006/Lecture%202/#normal-distribution","text":"E(X) = \u03bc Var(X) = \u03c32 S(X) = 0 K(X) = 3 ml = 0, for l is odd.","title":"Normal distribution"},{"location":"MSBD5006/Lecture%202/#t-distribution","text":"Symmetry at 0 \\[E(x) > 0, v >1\\]","title":"T-distribution"},{"location":"MSBD5006/Lecture%202/#chi-squared-distribution","text":"\\[E(X) = k$$ $$Var(X) = 2k\\]","title":"Chi-squared distribution"},{"location":"MSBD5006/Lecture%202/#joint-distribution","text":"\\[F_{X,Y}(x, y) = P(X\\leq x, Y\\leq y)\\]","title":"Joint Distribution"},{"location":"MSBD5006/Lecture%202/#marginal-distribution","text":"The marginal distribution of X is obtained by integrating out Y . A similar definition applies to the marginal distribution of Y .","title":"Marginal Distribution"},{"location":"MSBD5008/Lecture2/","text":"Undirected vs Directed Networks \u00b6 Undirected graph Links: undirected Example Collaborations Friendship on Facebook Directed Links: Directed Example Phone calls (If you want to represent the relationship between caller and callee, then you can use undirected graph) Following on Twitter Connectivity of graphs \u00b6 Undirected graph \u00b6 Connected: Any two vertices can be joined by a path A disconnected graph is made up of two or more connected components. Bridge edge: If we erase it, the graph becomes disconnected. Articulation point: If we erase it, the graph becomes disconnected Isolated node: Node without any connected neighbors. Directed graph \u00b6 Strongly connected directed graph: Has a path from each node to every other node and vice versa Weakly connected directed graph: Is connected if we disregard the edge directions Directed Acyclic Graph: Has no cyclesL if u can reach v, the v cannot reach u. Strongly connected components: is a set of nodes S so that: Every pair of nodes in S can reach each other There is no larger set containing S with this property Node degree: The number of edges adjacent to node i Avg. degree: \\(\\hat{k} = \\frac{1}{N}\\) , \\(K_i = \\frac{2E}{N}\\) In directed networks we define an in-degree and out-degree . The total degree of a node is the sum of in-and out-degrees. Complete Graph \u00b6 Maximum number edges in undirected graph on N nodes. \\[ E_{max} = \\frac{N(N-1)}{2} \\] A graph with the number of edges $E = E_{max} $ is a complete graph and its average degree is N - 1 Unweighted graph \u00b6 Graph without weight. For example, Friendship and sex. Weighted graph \u00b6 Graph with weight. For example collaboration with internet roads. Self-edges (Self-loops) \u00b6 Examples: Proteins, Hyperlinks (A web page link which points to itself) Multigraph \u00b6 Examples: Communication, collaboration Examples \u00b6 Example Type www Directed Facebook friendships Undirected, unweighted Citation networks Unweighted, directed, acyclic Collaboration networks Undirected multigraph or weighted graph Mobile phone calls directed Protein interactions Undirected, unweighted with self-interactions Bipartite Graph \u00b6 A graph whose nodes can be divided into two disjoint sets U and V such that every link connects a node in U to one in V; that is U and V are independent sets. Examples \u00b6 Authors-to-papers Actors-to-movies Degree distribution P(k): \u00b6 Probability that a randomly chosen node has degree k. Normalized histogram: \u00b6 \\(P(k) = N_k / N\\) Distance \u00b6 Between a pair of nodes is defined as the number of edges along the shortest path connecting the nodes In directed graphs paths need to follow the direction of the arrows. So that distance is not symmetric \\(h_{a,c} \\neq h_{c,a}\\) Diameter \u00b6 The maximum distance (shortest graph) between any pair of nodes in a graph. Clustering coefficient \u00b6 \\( \\(C_i = \\frac{2e_i}{k_i(k_i - 1)}\\) \\) where \\(e_i\\) is the number of edges between the neighbors of node i. Web as a graph \u00b6 Nodes = Web pages Edges = hyperlinks Side issues \u00b6 Dynamic pages created on the fly Dark matter - inaccessible database generated pages","title":"Lecture2"},{"location":"MSBD5008/Lecture2/#undirected-vs-directed-networks","text":"Undirected graph Links: undirected Example Collaborations Friendship on Facebook Directed Links: Directed Example Phone calls (If you want to represent the relationship between caller and callee, then you can use undirected graph) Following on Twitter","title":"Undirected vs Directed Networks"},{"location":"MSBD5008/Lecture2/#connectivity-of-graphs","text":"","title":"Connectivity of graphs"},{"location":"MSBD5008/Lecture2/#undirected-graph","text":"Connected: Any two vertices can be joined by a path A disconnected graph is made up of two or more connected components. Bridge edge: If we erase it, the graph becomes disconnected. Articulation point: If we erase it, the graph becomes disconnected Isolated node: Node without any connected neighbors.","title":"Undirected graph"},{"location":"MSBD5008/Lecture2/#directed-graph","text":"Strongly connected directed graph: Has a path from each node to every other node and vice versa Weakly connected directed graph: Is connected if we disregard the edge directions Directed Acyclic Graph: Has no cyclesL if u can reach v, the v cannot reach u. Strongly connected components: is a set of nodes S so that: Every pair of nodes in S can reach each other There is no larger set containing S with this property Node degree: The number of edges adjacent to node i Avg. degree: \\(\\hat{k} = \\frac{1}{N}\\) , \\(K_i = \\frac{2E}{N}\\) In directed networks we define an in-degree and out-degree . The total degree of a node is the sum of in-and out-degrees.","title":"Directed graph"},{"location":"MSBD5008/Lecture2/#complete-graph","text":"Maximum number edges in undirected graph on N nodes. \\[ E_{max} = \\frac{N(N-1)}{2} \\] A graph with the number of edges $E = E_{max} $ is a complete graph and its average degree is N - 1","title":"Complete Graph"},{"location":"MSBD5008/Lecture2/#unweighted-graph","text":"Graph without weight. For example, Friendship and sex.","title":"Unweighted graph"},{"location":"MSBD5008/Lecture2/#weighted-graph","text":"Graph with weight. For example collaboration with internet roads.","title":"Weighted graph"},{"location":"MSBD5008/Lecture2/#self-edges-self-loops","text":"Examples: Proteins, Hyperlinks (A web page link which points to itself)","title":"Self-edges (Self-loops)"},{"location":"MSBD5008/Lecture2/#multigraph","text":"Examples: Communication, collaboration","title":"Multigraph"},{"location":"MSBD5008/Lecture2/#examples","text":"Example Type www Directed Facebook friendships Undirected, unweighted Citation networks Unweighted, directed, acyclic Collaboration networks Undirected multigraph or weighted graph Mobile phone calls directed Protein interactions Undirected, unweighted with self-interactions","title":"Examples"},{"location":"MSBD5008/Lecture2/#bipartite-graph","text":"A graph whose nodes can be divided into two disjoint sets U and V such that every link connects a node in U to one in V; that is U and V are independent sets.","title":"Bipartite Graph"},{"location":"MSBD5008/Lecture2/#examples_1","text":"Authors-to-papers Actors-to-movies","title":"Examples"},{"location":"MSBD5008/Lecture2/#degree-distribution-pk","text":"Probability that a randomly chosen node has degree k.","title":"Degree distribution P(k):"},{"location":"MSBD5008/Lecture2/#normalized-histogram","text":"\\(P(k) = N_k / N\\)","title":"Normalized histogram:"},{"location":"MSBD5008/Lecture2/#distance","text":"Between a pair of nodes is defined as the number of edges along the shortest path connecting the nodes In directed graphs paths need to follow the direction of the arrows. So that distance is not symmetric \\(h_{a,c} \\neq h_{c,a}\\)","title":"Distance"},{"location":"MSBD5008/Lecture2/#diameter","text":"The maximum distance (shortest graph) between any pair of nodes in a graph.","title":"Diameter"},{"location":"MSBD5008/Lecture2/#clustering-coefficient","text":"\\( \\(C_i = \\frac{2e_i}{k_i(k_i - 1)}\\) \\) where \\(e_i\\) is the number of edges between the neighbors of node i.","title":"Clustering coefficient"},{"location":"MSBD5008/Lecture2/#web-as-a-graph","text":"Nodes = Web pages Edges = hyperlinks","title":"Web as a graph"},{"location":"MSBD5008/Lecture2/#side-issues","text":"Dynamic pages created on the fly Dark matter - inaccessible database generated pages","title":"Side issues"},{"location":"MSBD5008/Lecture3/","text":"Representation Learning on networks \u00b6 Why Is It Hard? \u00b6 Modern deep learning toolbox is designed for simple sequences or grids. But networks are far more complex! Complex topographical structure (i.e., NO Spatial locality like grids) Node embeddings \u00b6 Goal is to encode nodes so that similarity in the embedding space (e.g., dot product) approximates similarity in the original network. Setup \u00b6 Assume we have a graph G: V is the vertex set. A is the adjacency matrix (assume binary). No node features or extra information is used! \u201cShallow\u201d Encoding \u00b6 Simplest encoding approach: each node is assigned a unique embedding vector. We will focus on shallow encoding in this section... After the break we will discuss more encoders based on deep neural networks. How to Define Node Similarity? \u00b6 Key distinction between \u201cshallow\u201d methods is how they define node similarity. E.g., should two nodes have similar embeddings if they - are connected? - share neighbors - have similar \u201cstructural roles\u201d? Adjacency-based Similarity \u00b6 Similarity function is just the edge weight between u and v in the original network. Intuition: Dot products between node embeddings approximate edge existence. Drawbacks \u00b6 \\(O(V^2)\\) runtime \\(O(V)\\) paramters Only sonsiders direct, local connections Multihop Similarity \u00b6 Idea: Consider k-hop node neighbors Issues \u00b6 Expensive: Generally \\(O(|V^2|)\\) , since we need to iterate over all pairs of nodes. Brittle: Must hand-design deterministic node similarity measures. Massive parameter space: \\(O(|V|)\\) parameters Randome walk Method \u00b6 Estimate probability of visiting node \\(v\\) on a random walk starting from node \\(u\\) using some random walk strategy \\(R\\) . Optimize embeddings to encode these random walk statistics. Why Random Walks? \u00b6 Expressivity: Flexible stochastic definition of node similarity that incorporates both local and higher- order neighborhood information. Efficiency: Do not need to consider all node pairs when training; only need to consider pairs that co-occur on random walks. Random Walk Optimization \u00b6 Run short random walks starting from each node on the graph using some strategy For each node wu collect \\(N_R(U)\\) , the multiset\u201d of nodes visited on random walks starting rom \\(uw\\) . Optimize embeddings to according to \\[L = \\sum_{u\\in V}\\sum_{v \\in NR(U)} - log(P(v |Z_u))\\] (1) Sum over all nodes u (2) sum over nodes v seen on random walks starting from u (3) Predicted probability if u and v co-occuring on random walk Optimizing random walk embeddings = Finding embeddings \\(Z_u\\) that minimize L Graph neural networks \u00b6 From Shallow to Deep \u00b6 Limitations of shallow encoding: O(|V|) parameters are needed: there no parameter sharing and every node has its own unique embedding vector. Inherently \u201ctransductive\u2019: It is impossible to generate embeddings for nodes that were not seen during training. Do not incorporate node features: Many graphs have features that we can and should leverage. Setup \u00b6 Assume we have a graph G: V is the vertex set. A is the adjacency matrix (assume binary). \\(X\\in R^{m \\times |v|}\\) is a matrix of node features Categorical attributes, text, image data \u2014 E.g., profile information in a social network. Node degrees, clustering coefficients, etc. \" Indicator vectors (i.e., one-hot encoding of each node) Neighborhood aggregation \u00b6 Generate node embeddings based on local neighborhoods NetworkX \u00b6 !pip install networkx Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (2.5) Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx) (4.4.2)","title":"Lecture3"},{"location":"MSBD5008/Lecture3/#representation-learning-on-networks","text":"","title":"Representation Learning on networks"},{"location":"MSBD5008/Lecture3/#why-is-it-hard","text":"Modern deep learning toolbox is designed for simple sequences or grids. But networks are far more complex! Complex topographical structure (i.e., NO Spatial locality like grids)","title":"Why Is It Hard?"},{"location":"MSBD5008/Lecture3/#node-embeddings","text":"Goal is to encode nodes so that similarity in the embedding space (e.g., dot product) approximates similarity in the original network.","title":"Node embeddings"},{"location":"MSBD5008/Lecture3/#setup","text":"Assume we have a graph G: V is the vertex set. A is the adjacency matrix (assume binary). No node features or extra information is used!","title":"Setup"},{"location":"MSBD5008/Lecture3/#shallow-encoding","text":"Simplest encoding approach: each node is assigned a unique embedding vector. We will focus on shallow encoding in this section... After the break we will discuss more encoders based on deep neural networks.","title":"\u201cShallow\u201d Encoding"},{"location":"MSBD5008/Lecture3/#how-to-define-node-similarity","text":"Key distinction between \u201cshallow\u201d methods is how they define node similarity. E.g., should two nodes have similar embeddings if they - are connected? - share neighbors - have similar \u201cstructural roles\u201d?","title":"How to Define Node Similarity?"},{"location":"MSBD5008/Lecture3/#adjacency-based-similarity","text":"Similarity function is just the edge weight between u and v in the original network. Intuition: Dot products between node embeddings approximate edge existence.","title":"Adjacency-based Similarity"},{"location":"MSBD5008/Lecture3/#drawbacks","text":"\\(O(V^2)\\) runtime \\(O(V)\\) paramters Only sonsiders direct, local connections","title":"Drawbacks"},{"location":"MSBD5008/Lecture3/#multihop-similarity","text":"Idea: Consider k-hop node neighbors","title":"Multihop Similarity"},{"location":"MSBD5008/Lecture3/#issues","text":"Expensive: Generally \\(O(|V^2|)\\) , since we need to iterate over all pairs of nodes. Brittle: Must hand-design deterministic node similarity measures. Massive parameter space: \\(O(|V|)\\) parameters","title":"Issues"},{"location":"MSBD5008/Lecture3/#randome-walk-method","text":"Estimate probability of visiting node \\(v\\) on a random walk starting from node \\(u\\) using some random walk strategy \\(R\\) . Optimize embeddings to encode these random walk statistics.","title":"Randome walk Method"},{"location":"MSBD5008/Lecture3/#why-random-walks","text":"Expressivity: Flexible stochastic definition of node similarity that incorporates both local and higher- order neighborhood information. Efficiency: Do not need to consider all node pairs when training; only need to consider pairs that co-occur on random walks.","title":"Why Random Walks?"},{"location":"MSBD5008/Lecture3/#random-walk-optimization","text":"Run short random walks starting from each node on the graph using some strategy For each node wu collect \\(N_R(U)\\) , the multiset\u201d of nodes visited on random walks starting rom \\(uw\\) . Optimize embeddings to according to \\[L = \\sum_{u\\in V}\\sum_{v \\in NR(U)} - log(P(v |Z_u))\\] (1) Sum over all nodes u (2) sum over nodes v seen on random walks starting from u (3) Predicted probability if u and v co-occuring on random walk Optimizing random walk embeddings = Finding embeddings \\(Z_u\\) that minimize L","title":"Random Walk Optimization"},{"location":"MSBD5008/Lecture3/#graph-neural-networks","text":"","title":"Graph neural networks"},{"location":"MSBD5008/Lecture3/#from-shallow-to-deep","text":"Limitations of shallow encoding: O(|V|) parameters are needed: there no parameter sharing and every node has its own unique embedding vector. Inherently \u201ctransductive\u2019: It is impossible to generate embeddings for nodes that were not seen during training. Do not incorporate node features: Many graphs have features that we can and should leverage.","title":"From Shallow to Deep"},{"location":"MSBD5008/Lecture3/#setup_1","text":"Assume we have a graph G: V is the vertex set. A is the adjacency matrix (assume binary). \\(X\\in R^{m \\times |v|}\\) is a matrix of node features Categorical attributes, text, image data \u2014 E.g., profile information in a social network. Node degrees, clustering coefficients, etc. \" Indicator vectors (i.e., one-hot encoding of each node)","title":"Setup"},{"location":"MSBD5008/Lecture3/#neighborhood-aggregation","text":"Generate node embeddings based on local neighborhoods","title":"Neighborhood aggregation"},{"location":"MSBD5008/Lecture3/#networkx","text":"!pip install networkx Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (2.5) Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx) (4.4.2)","title":"NetworkX"},{"location":"MSBD5008/Lecture4/","text":"Jaccard similarity: \u00b6 measures similarity between sample sets - \\[J(a, b) = \\frac{A \\cap B}{A \\cup B}\\] Problem with previous similarity functions \u00b6 You need to re-run the algorithm again when new node has been added. Graph Neural Network \u00b6 learn the mapping between node and vector. Train the model \u00b6 Directly train the model for a supervised task (e.g., node classification) After K-layers of neighborhood aggregation, we get output embeddings for each node. We can feed these embeddings into any loss function and run stochastic gradient descent to train the aggregation parameters. Granovetter's explaination \u00b6 Define Bridge edge If removed, it disconnects the graph Extremely rare in social networks Define: Local bridge Endpoints have no friends in common a Edge of Span > 2 (Span of an edge is the distance of the edge endpoints if the edge is deleted. Local bridges with long span are like real bridges) Define: Two types of edges: Strong (friend), Weak (acquaintance) Define: Strong triadic closure: Two strong ties imply a third edge","title":"Lecture4"},{"location":"MSBD5008/Lecture4/#jaccard-similarity","text":"measures similarity between sample sets - \\[J(a, b) = \\frac{A \\cap B}{A \\cup B}\\]","title":"Jaccard similarity:"},{"location":"MSBD5008/Lecture4/#problem-with-previous-similarity-functions","text":"You need to re-run the algorithm again when new node has been added.","title":"Problem with previous similarity functions"},{"location":"MSBD5008/Lecture4/#graph-neural-network","text":"learn the mapping between node and vector.","title":"Graph Neural Network"},{"location":"MSBD5008/Lecture4/#train-the-model","text":"Directly train the model for a supervised task (e.g., node classification) After K-layers of neighborhood aggregation, we get output embeddings for each node. We can feed these embeddings into any loss function and run stochastic gradient descent to train the aggregation parameters.","title":"Train the model"},{"location":"MSBD5008/Lecture4/#granovetters-explaination","text":"Define Bridge edge If removed, it disconnects the graph Extremely rare in social networks Define: Local bridge Endpoints have no friends in common a Edge of Span > 2 (Span of an edge is the distance of the edge endpoints if the edge is deleted. Local bridges with long span are like real bridges) Define: Two types of edges: Strong (friend), Weak (acquaintance) Define: Strong triadic closure: Two strong ties imply a third edge","title":"Granovetter's explaination"},{"location":"MSBD5008/Lecture5/","text":"Giravan Newman \u00b6 Divisive hierarchical clustering based on the notion of edge betweenness: - Number of shortest paths passing through the edge Girvan-Newman Algorithm: - Undirected unweighted networks - Repeat until no edges are left: - Calculate betweenness of edges - Remove the edge with the highest betweenness (if two or more edges tie for highest score, remove all of them) Connected components are communities Gives a hierarchical decomposition of the network Betweeness \u00b6 BFS Starting from the node you want to start from typing import List class Node : def __init__ ( self , value ): self . connections : List [ Node ] = [] self . value = value self . distance = 0 3","title":"Lecture5"},{"location":"MSBD5008/Lecture5/#giravan-newman","text":"Divisive hierarchical clustering based on the notion of edge betweenness: - Number of shortest paths passing through the edge Girvan-Newman Algorithm: - Undirected unweighted networks - Repeat until no edges are left: - Calculate betweenness of edges - Remove the edge with the highest betweenness (if two or more edges tie for highest score, remove all of them) Connected components are communities Gives a hierarchical decomposition of the network","title":"Giravan Newman"},{"location":"MSBD5008/Lecture5/#betweeness","text":"BFS Starting from the node you want to start from typing import List class Node : def __init__ ( self , value ): self . connections : List [ Node ] = [] self . value = value self . distance = 0 3","title":"Betweeness"},{"location":"MSBD5008/Lectyre6/","text":"The SIR model \u00b6 S: The number of susceptible individuals. When a susceptible and an infectious individual come into \"infectious contact\", the susceptible individual contracts the disease and transitions to the infectious compartment. I: The number of infectious individuals. These are individuals who have been infected and are capable of infecting susceptible individuals. R for the number of removed (and immune) or deceased individuals. These are individuals who have been infected and have either recovered from the disease and entered the removed compartment, or died. It is assumed that the number of deaths is negligible with respect to the total population. This compartment may also be called \"recovered\" or \"resistant\". The SIS model \u00b6 Some infections, for example, those from the common cold and influenza, do not confer any long-lasting immunity. Such infections do not give immunity upon recovery from infection, and individuals become susceptible again. Decision Based Models \u00b6 Payoffs: Utility of making a particular choice Signals Public information Private Information Scenario: Graph where everyone starts witn b. Small set S of early adopters of A Hard-wire S \u2014 they keep using A no matter what payoffs tell them to do Stopping cascade \u00b6 1 Two facts: - 1) If G\\S contains a cluster of density >= (1-q) then S cannot cause a cascade 2) If S fails to create a cascade, then \u00b0 there is a cluster of density >= (1-q) in G\\S","title":"Lectyre6"},{"location":"MSBD5008/Lectyre6/#the-sir-model","text":"S: The number of susceptible individuals. When a susceptible and an infectious individual come into \"infectious contact\", the susceptible individual contracts the disease and transitions to the infectious compartment. I: The number of infectious individuals. These are individuals who have been infected and are capable of infecting susceptible individuals. R for the number of removed (and immune) or deceased individuals. These are individuals who have been infected and have either recovered from the disease and entered the removed compartment, or died. It is assumed that the number of deaths is negligible with respect to the total population. This compartment may also be called \"recovered\" or \"resistant\".","title":"The SIR model"},{"location":"MSBD5008/Lectyre6/#the-sis-model","text":"Some infections, for example, those from the common cold and influenza, do not confer any long-lasting immunity. Such infections do not give immunity upon recovery from infection, and individuals become susceptible again.","title":"The SIS model"},{"location":"MSBD5008/Lectyre6/#decision-based-models","text":"Payoffs: Utility of making a particular choice Signals Public information Private Information Scenario: Graph where everyone starts witn b. Small set S of early adopters of A Hard-wire S \u2014 they keep using A no matter what payoffs tell them to do","title":"Decision Based Models"},{"location":"MSBD5008/Lectyre6/#stopping-cascade","text":"1 Two facts: - 1) If G\\S contains a cluster of density >= (1-q) then S cannot cause a cascade 2) If S fails to create a cascade, then \u00b0 there is a cluster of density >= (1-q) in G\\S","title":"Stopping cascade"},{"location":"MSBD5008/homework/homework1/","text":"!pip install networkx Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (2.5) Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx) (4.4.2)","title":"Homework1"},{"location":"MSBD5012/Regression/","text":"Linear regression \u00b6 see extra readings/ Linear Regression.pdf Mean Squared Error: \\[MSE(\\beta) = \\frac{1}{n}\\sum_{i=1}^{n}e_i^2(\\beta) \\] Which can be represented in matrix form $$\\frac{1}{n}e^Te $$ Since $$MSE(\\beta) = \\begin{bmatrix}e_1 e_2... e_n \\end{bmatrix} \\begin{bmatrix}e_1\\ e_2\\ .\\.\\.\\ e_n \\end{bmatrix} $$ Minimizing the MSE \\[\\hat{\\beta}=(x^Tx)^{-1}x^Ty \\] Create data import numpy as np import matplotlib.pyplot as plt x = 2 * np.random.rand(100,1) y = 4 +3 * x+np.random.randn(100,1) plt.scatter(x, y) <matplotlib.collections.PathCollection at 0x7f98ca8f5d68> build regression class def linear_regression ( x , y ) : result = np . linalg . inv ( x . T . dot ( x )). dot ( x . T ). dot ( y ) return result result = linear_regression ( x , y ) y_hat = x . dot ( result ) print ( result ) [[6.07728867]] plot result plt.scatter(x, y, label='Original') plt.plot(x, y_hat, label='Predicted line') plt.legend() <matplotlib.legend.Legend at 0x7f98ca873668> Gradient Descent \u00b6 \\[ \\theta = \\theta - \\alpha(1/m \\sum_{i=1}^m (h(\\theta^i - y^i)X_j) \\] where \\(\\alpha\\) is Learning Rate def cal_cost ( theta , X , y ) : ''' Calculates the cost for given X and Y. The following shows and example of a single dimensional X theta = Vector of thetas X = Row of X' s np . zeros (( 2 , j )) y = Actual y 's np.zeros((2,1)) where: j is the no of features ''' m = len ( y ) predictions = X . dot ( theta ) cost = ( 1 / 2 * m ) * np . sum ( np . square ( predictions - y )) return cost def gradient_descent ( X , y , theta , learning_rate = 0.01 , iterations = 100 ) : ''' X = Matrix of X y = Vector of Y theta=Vector of thetas np.random.randn(j,1) learning_rate iterations = no of iterations Returns the final theta vector and array of cost history over no of iterations ''' m = len ( y ) cost_history = np . zeros ( iterations ) theta_history = np . zeros (( iterations , 2 )) for it in range ( iterations ) : prediction = np . dot ( X , theta ) theta = theta - ( 1 / m ) * learning_rate * ( X . T . dot (( prediction - y ))) theta_history [ it,: ] = theta . T cost_history [ it ] = cal_cost ( theta , X , y ) return theta , cost_history , theta_history lr = 0 . 01 n_iter = 1000 # theta0 and theta 1 theta = np . random . randn ( 2 , 1 ) print ( theta ) x_b = np . c_ [ np . ones (( len ( x ), 1 )), x ] theta , cost_history , theta_history = gradient_descent ( x_b , y , theta , lr , n_iter ) plt . plot ( cost_history ) [[-2.65809484] [-0.51257682]] [<matplotlib.lines.Line2D at 0x7f98bfaf9978>] y_hat = x_b.dot(theta) plt.plot(x, y_hat, label='Predictions') plt.scatter(x, y, label=\"Actural\") plt.legend() <matplotlib.legend.Legend at 0x7f98c7a8c9e8> def stocashtic_gradient_descent ( X , y , theta , learning_rate = 0.01 , iterations = 10 ) : ''' X = Matrix of X with added bias units y = Vector of Y theta=Vector of thetas np.random.randn(j,1) learning_rate iterations = no of iterations Returns the final theta vector and array of cost history over no of iterations ''' m = len ( y ) cost_history = np . zeros ( iterations ) for it in range ( iterations ) : cost = 0.0 for i in range ( m ) : rand_ind = np . random . randint ( 0 , m ) X_i = X [ rand_ind,: ] . reshape ( 1 , X . shape [ 1 ] ) y_i = y [ rand_ind ] . reshape ( 1 , 1 ) prediction = np . dot ( X_i , theta ) theta = theta - ( 1 / m ) * learning_rate * ( X_i . T . dot (( prediction - y_i ))) cost += cal_cost ( theta , X_i , y_i ) cost_history [ it ] = cost return theta , cost_history def minibatch_gradient_descent ( X , y , theta , learning_rate = 0.01 , iterations = 10 , batch_size = 20 ) : ''' X = Matrix of X without added bias units y = Vector of Y theta=Vector of thetas np.random.randn(j,1) learning_rate iterations = no of iterations Returns the final theta vector and array of cost history over no of iterations ''' m = len ( y ) cost_history = np . zeros ( iterations ) n_batches = int ( m / batch_size ) for it in range ( iterations ) : cost = 0.0 indices = np . random . permutation ( m ) X = X [ indices ] y = y [ indices ] for i in range ( 0 , m , batch_size ) : X_i = X [ i:i+batch_size ] y_i = y [ i:i+batch_size ] X_i = np . c_ [ np.ones(len(X_i)),X_i ] prediction = np . dot ( X_i , theta ) theta = theta - ( 1 / m ) * learning_rate * ( X_i . T . dot (( prediction - y_i ))) cost += cal_cost ( theta , X_i , y_i ) cost_history [ it ] = cost return theta , cost_history Logistic Regression \u00b6 https://towardsdatascience.com/understanding-logistic-regression-81779525d5c6 Logistic Regression is a Linear Classifier We have sigmoid function defined as follow \\[S(x) = \\frac{1}{1+e^{-x}} \\] And we can replace it as \\[S(xw) =\\hat{y} $$ $$\\to \\frac{1}{1+e^{-xw}}=\\hat{y} $$ $$\\to 1+e^{-xw} = \\frac{1}{\\hat{y}} $$ $$ \\to e^{-xw} = \\frac{1}{\\hat{y}} -1 $$ $$\\to xw = -ln(\\frac{1}{\\hat{y}} - 1)\\] where \\(\\hat{y}\\) is the prediction value Then we can replace the linear regression expression with by using the equation above $$\\hat{\\beta}=(x^Tx)^{-1}x^Ty $$ 1. \\( \\(\\to \\hat{\\beta} = -(x^Tx)^{-1}x^Tln(\\frac{1}{\\hat{y}} - 1))\\) \\) \\[\\nabla f = 2x^T(xw + ln(\\frac{1}{y} - 1)) \\] x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).reshape((-1, 1)) y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1]) plt.scatter(x, y) <matplotlib.collections.PathCollection at 0x7f3082f6f668> class LogisticRegression: EPS = 1e-5 def __init__ ( self , x , y ): self . x = x self . y = y def sigmoid ( self , x ): return 1 / ( 1 + np . exp (- x )) def logistic ( self ): x = self . x y = self . y y = np . maximum ( self . EPS , np . minimum ( y , 1 - self . EPS ) ) w = - np . linalg . inv ( x . T . dot ( x )). dot ( x . T ). dot ( np . log ( np . divide ( 1 , y ) - 1 )) return w def gradient ( self ): w = self . logistic () x = self . x y = self . y y = np . maximum ( self . EPS , np . minimum ( y , 1 - self . EPS ) ) result = 2 * x . T . dot ( x . dot ( w ) + np . log ( 1 / y - 1 )) return result def predict ( self , x ): w = self . logistic () return self . sigmoid ( x . dot ( w )) model = LogisticRegression(x, y) print(model.logistic()) print(model.gradient()) [1.00990487] [1.42108547e-14] y_hat = model.predict(x) plt.plot(x, y_hat) plt.scatter(x, y) <matplotlib.collections.PathCollection at 0x7f3082c9d0b8> Softmax \u00b6 cross entropy \\[- \\sum T * log(O) \\] from sklearn.preprocessing import OneHotEncoder y = np . array ([ 0 , 1 , 2 , 2 ]) y = OneHotEncoder ( sparse = False ) . fit_transform ( y . reshape ( - 1 , 1 )) x = np . array ([[ 0.1 , 0.5 ], [ 1.1 , 2.3 ], [ - 1.1 , - 2.3 ], [ - 1.5 , - 2.5 ]]) w = np . array ([[ 0.1 , 0.2 , 0.3 ], [ 0.1 , 0.2 , 0.3 ]]) print ( f \"x: {x}\" ) print ( f \"y: {y}\" ) print ( f \"w: {w}\" ) x: [[ 0.1 0.5] [ 1.1 2.3] [-1.1 -2.3] [-1.5 -2.5]] y: [[1. 0. 0.] [0. 1. 0.] [0. 0. 1.] [0. 0. 1.]] w: [[0.1 0.2 0.3] [0.1 0.2 0.3]] def softmax ( z ): return ( np . exp ( z . T ) / np . sum ( np . exp ( z ), axis = 1 )). T def net ( x , w ): z = x . dot ( w ) return softmax ( z ) def cross_entropy ( predicted , actual ): return - np . sum ( np . log ( predicted ). dot ( actual ), axis = 1 ) prediction = net(x, w) print(prediction) [[0.31354514 0.33293369 0.35352116] [0.22837175 0.32085034 0.45077791] [0.45077791 0.32085034 0.22837175] [0.47177622 0.31624106 0.21198272]] cross_entropy(prediction, y) array([1.15981193, 1.1367805 , 1.4767805 , 1.55125051])","title":"Linear regression"},{"location":"MSBD5012/Regression/#linear-regression","text":"see extra readings/ Linear Regression.pdf Mean Squared Error: \\[MSE(\\beta) = \\frac{1}{n}\\sum_{i=1}^{n}e_i^2(\\beta) \\] Which can be represented in matrix form $$\\frac{1}{n}e^Te $$ Since $$MSE(\\beta) = \\begin{bmatrix}e_1 e_2... e_n \\end{bmatrix} \\begin{bmatrix}e_1\\ e_2\\ .\\.\\.\\ e_n \\end{bmatrix} $$ Minimizing the MSE \\[\\hat{\\beta}=(x^Tx)^{-1}x^Ty \\] Create data import numpy as np import matplotlib.pyplot as plt x = 2 * np.random.rand(100,1) y = 4 +3 * x+np.random.randn(100,1) plt.scatter(x, y) <matplotlib.collections.PathCollection at 0x7f98ca8f5d68> build regression class def linear_regression ( x , y ) : result = np . linalg . inv ( x . T . dot ( x )). dot ( x . T ). dot ( y ) return result result = linear_regression ( x , y ) y_hat = x . dot ( result ) print ( result ) [[6.07728867]] plot result plt.scatter(x, y, label='Original') plt.plot(x, y_hat, label='Predicted line') plt.legend() <matplotlib.legend.Legend at 0x7f98ca873668>","title":"Linear regression"},{"location":"MSBD5012/Regression/#gradient-descent","text":"\\[ \\theta = \\theta - \\alpha(1/m \\sum_{i=1}^m (h(\\theta^i - y^i)X_j) \\] where \\(\\alpha\\) is Learning Rate def cal_cost ( theta , X , y ) : ''' Calculates the cost for given X and Y. The following shows and example of a single dimensional X theta = Vector of thetas X = Row of X' s np . zeros (( 2 , j )) y = Actual y 's np.zeros((2,1)) where: j is the no of features ''' m = len ( y ) predictions = X . dot ( theta ) cost = ( 1 / 2 * m ) * np . sum ( np . square ( predictions - y )) return cost def gradient_descent ( X , y , theta , learning_rate = 0.01 , iterations = 100 ) : ''' X = Matrix of X y = Vector of Y theta=Vector of thetas np.random.randn(j,1) learning_rate iterations = no of iterations Returns the final theta vector and array of cost history over no of iterations ''' m = len ( y ) cost_history = np . zeros ( iterations ) theta_history = np . zeros (( iterations , 2 )) for it in range ( iterations ) : prediction = np . dot ( X , theta ) theta = theta - ( 1 / m ) * learning_rate * ( X . T . dot (( prediction - y ))) theta_history [ it,: ] = theta . T cost_history [ it ] = cal_cost ( theta , X , y ) return theta , cost_history , theta_history lr = 0 . 01 n_iter = 1000 # theta0 and theta 1 theta = np . random . randn ( 2 , 1 ) print ( theta ) x_b = np . c_ [ np . ones (( len ( x ), 1 )), x ] theta , cost_history , theta_history = gradient_descent ( x_b , y , theta , lr , n_iter ) plt . plot ( cost_history ) [[-2.65809484] [-0.51257682]] [<matplotlib.lines.Line2D at 0x7f98bfaf9978>] y_hat = x_b.dot(theta) plt.plot(x, y_hat, label='Predictions') plt.scatter(x, y, label=\"Actural\") plt.legend() <matplotlib.legend.Legend at 0x7f98c7a8c9e8> def stocashtic_gradient_descent ( X , y , theta , learning_rate = 0.01 , iterations = 10 ) : ''' X = Matrix of X with added bias units y = Vector of Y theta=Vector of thetas np.random.randn(j,1) learning_rate iterations = no of iterations Returns the final theta vector and array of cost history over no of iterations ''' m = len ( y ) cost_history = np . zeros ( iterations ) for it in range ( iterations ) : cost = 0.0 for i in range ( m ) : rand_ind = np . random . randint ( 0 , m ) X_i = X [ rand_ind,: ] . reshape ( 1 , X . shape [ 1 ] ) y_i = y [ rand_ind ] . reshape ( 1 , 1 ) prediction = np . dot ( X_i , theta ) theta = theta - ( 1 / m ) * learning_rate * ( X_i . T . dot (( prediction - y_i ))) cost += cal_cost ( theta , X_i , y_i ) cost_history [ it ] = cost return theta , cost_history def minibatch_gradient_descent ( X , y , theta , learning_rate = 0.01 , iterations = 10 , batch_size = 20 ) : ''' X = Matrix of X without added bias units y = Vector of Y theta=Vector of thetas np.random.randn(j,1) learning_rate iterations = no of iterations Returns the final theta vector and array of cost history over no of iterations ''' m = len ( y ) cost_history = np . zeros ( iterations ) n_batches = int ( m / batch_size ) for it in range ( iterations ) : cost = 0.0 indices = np . random . permutation ( m ) X = X [ indices ] y = y [ indices ] for i in range ( 0 , m , batch_size ) : X_i = X [ i:i+batch_size ] y_i = y [ i:i+batch_size ] X_i = np . c_ [ np.ones(len(X_i)),X_i ] prediction = np . dot ( X_i , theta ) theta = theta - ( 1 / m ) * learning_rate * ( X_i . T . dot (( prediction - y_i ))) cost += cal_cost ( theta , X_i , y_i ) cost_history [ it ] = cost return theta , cost_history","title":"Gradient Descent"},{"location":"MSBD5012/Regression/#logistic-regression","text":"https://towardsdatascience.com/understanding-logistic-regression-81779525d5c6 Logistic Regression is a Linear Classifier We have sigmoid function defined as follow \\[S(x) = \\frac{1}{1+e^{-x}} \\] And we can replace it as \\[S(xw) =\\hat{y} $$ $$\\to \\frac{1}{1+e^{-xw}}=\\hat{y} $$ $$\\to 1+e^{-xw} = \\frac{1}{\\hat{y}} $$ $$ \\to e^{-xw} = \\frac{1}{\\hat{y}} -1 $$ $$\\to xw = -ln(\\frac{1}{\\hat{y}} - 1)\\] where \\(\\hat{y}\\) is the prediction value Then we can replace the linear regression expression with by using the equation above $$\\hat{\\beta}=(x^Tx)^{-1}x^Ty $$ 1. \\( \\(\\to \\hat{\\beta} = -(x^Tx)^{-1}x^Tln(\\frac{1}{\\hat{y}} - 1))\\) \\) \\[\\nabla f = 2x^T(xw + ln(\\frac{1}{y} - 1)) \\] x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).reshape((-1, 1)) y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1]) plt.scatter(x, y) <matplotlib.collections.PathCollection at 0x7f3082f6f668> class LogisticRegression: EPS = 1e-5 def __init__ ( self , x , y ): self . x = x self . y = y def sigmoid ( self , x ): return 1 / ( 1 + np . exp (- x )) def logistic ( self ): x = self . x y = self . y y = np . maximum ( self . EPS , np . minimum ( y , 1 - self . EPS ) ) w = - np . linalg . inv ( x . T . dot ( x )). dot ( x . T ). dot ( np . log ( np . divide ( 1 , y ) - 1 )) return w def gradient ( self ): w = self . logistic () x = self . x y = self . y y = np . maximum ( self . EPS , np . minimum ( y , 1 - self . EPS ) ) result = 2 * x . T . dot ( x . dot ( w ) + np . log ( 1 / y - 1 )) return result def predict ( self , x ): w = self . logistic () return self . sigmoid ( x . dot ( w )) model = LogisticRegression(x, y) print(model.logistic()) print(model.gradient()) [1.00990487] [1.42108547e-14] y_hat = model.predict(x) plt.plot(x, y_hat) plt.scatter(x, y) <matplotlib.collections.PathCollection at 0x7f3082c9d0b8>","title":"Logistic Regression"},{"location":"MSBD5012/Regression/#softmax","text":"cross entropy \\[- \\sum T * log(O) \\] from sklearn.preprocessing import OneHotEncoder y = np . array ([ 0 , 1 , 2 , 2 ]) y = OneHotEncoder ( sparse = False ) . fit_transform ( y . reshape ( - 1 , 1 )) x = np . array ([[ 0.1 , 0.5 ], [ 1.1 , 2.3 ], [ - 1.1 , - 2.3 ], [ - 1.5 , - 2.5 ]]) w = np . array ([[ 0.1 , 0.2 , 0.3 ], [ 0.1 , 0.2 , 0.3 ]]) print ( f \"x: {x}\" ) print ( f \"y: {y}\" ) print ( f \"w: {w}\" ) x: [[ 0.1 0.5] [ 1.1 2.3] [-1.1 -2.3] [-1.5 -2.5]] y: [[1. 0. 0.] [0. 1. 0.] [0. 0. 1.] [0. 0. 1.]] w: [[0.1 0.2 0.3] [0.1 0.2 0.3]] def softmax ( z ): return ( np . exp ( z . T ) / np . sum ( np . exp ( z ), axis = 1 )). T def net ( x , w ): z = x . dot ( w ) return softmax ( z ) def cross_entropy ( predicted , actual ): return - np . sum ( np . log ( predicted ). dot ( actual ), axis = 1 ) prediction = net(x, w) print(prediction) [[0.31354514 0.33293369 0.35352116] [0.22837175 0.32085034 0.45077791] [0.45077791 0.32085034 0.22837175] [0.47177622 0.31624106 0.21198272]] cross_entropy(prediction, y) array([1.15981193, 1.1367805 , 1.4767805 , 1.55125051])","title":"Softmax"},{"location":"MSBD5012/homework2/","text":"Problem 1 \u00b6 \\[ P(B|A) = \\frac{p(A\\ and\\ B)}{P(A)} $$ $$p(A|B) = \\frac{P(A) \\times P(B|A) }{P(B)} \\] from sklearn.naive_bayes import GaussianNB import numpy as np x1 = [0, 0, 0, 0, 1, 1, 1, 1] x2 = [0, 0, 1, 1, 0, 0, 1, 1] y = [1, 1, 1, 1, 0, 0, 1, 0] X = np.column_stack((x1, x2)) X array([[0, 0], [0, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 1], [1, 1]]) gnb = GaussianNB() gnb.fit(X, y) GaussianNB(priors=None, var_smoothing=1e-09) gnb.get_params() {'priors': None, 'var_smoothing': 1e-09} (4/5) * (2/5) 0.32000000000000006 Problem 2 \u00b6 w1 = np.array([[1, -1], [-1, 1]]) w2 = np.array([[-1, -1], [1, 1]]) w3 = np.array([[1], [1]]) x = np.array([1, 2]) import math from numpy import tanh def tanh_derivative ( x ): t = ( np . exp ( x ) - np . exp ( - x )) / ( np . exp ( x ) + np . exp ( - x )) dt = 1 - t ** 2 return dt def sigmoid ( X ): return 1 / ( 1 + np . exp ( - X )) def sigmoid_derivative ( x ): outcome = sigmoid ( x ) return outcome * ( 1 - outcome ) layer1 = x.dot(w1) layer1 = tanh(layer1) layer1 array([-0.76159416, 0.76159416]) layer2 = layer1.dot(w2) print(layer2) layer2 = tanh(layer2) layer2 [1.52318831 1.52318831] array([0.90925167, 0.90925167]) output = layer2.dot(w3) output *2 array([3.6370067]) e = sigmoid(output) e array([0.86038644]) backprop \\[error_{hidden} = w^T * error_{output}\\] b_layer2 = e * tanh_derivative(layer2) b_layer2 array([0.41340402, 0.41340402]) b_layer1_err = b_layer2.dot(w2.T) b_layer1 = b_layer1_err * tanh_derivative(layer1) b_layer1 array([-0.48601162, 0.48601162]) x[1] * b_layer1[0] 0.0 0.76 * b_layer2[1] 0.31418705778577294 w = np.array([[0.1, 0.5], [-0.3, 0.8]]) x = np.array([[0.2], [0.4]]) w.dot(x).dot(2) array([[0.44], [0.52]]) Feedforward Neural Network \u00b6 def relu(n): if n<0: return 0 else: return n def layer(m1, m2, w1, w2): z = m1.dot(w1) + m2.dot(w2) + b array = np.array([[1,1,1], [2, 2, 2]]) array[[0, 1], [1, 1]] - 1 array([0, 1]) def softmax_grad(softmax): s = softmax.reshape(-1,1) return np.diagflat(s) - np.dot(s, s.T) softmax_grad(np.array([[0.5, 0.5]])) array([[ 0.25, -0.25], [-0.25, 0.25]]) (array > 1) * np.array([[1, 1, 1], [1, 1, 1]]) array([[0, 0, 0], [1, 1, 1]]) array - 1 array([[0, 0, 0], [1, 1, 1]])","title":"Problem 1"},{"location":"MSBD5012/homework2/#problem-1","text":"\\[ P(B|A) = \\frac{p(A\\ and\\ B)}{P(A)} $$ $$p(A|B) = \\frac{P(A) \\times P(B|A) }{P(B)} \\] from sklearn.naive_bayes import GaussianNB import numpy as np x1 = [0, 0, 0, 0, 1, 1, 1, 1] x2 = [0, 0, 1, 1, 0, 0, 1, 1] y = [1, 1, 1, 1, 0, 0, 1, 0] X = np.column_stack((x1, x2)) X array([[0, 0], [0, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 1], [1, 1]]) gnb = GaussianNB() gnb.fit(X, y) GaussianNB(priors=None, var_smoothing=1e-09) gnb.get_params() {'priors': None, 'var_smoothing': 1e-09} (4/5) * (2/5) 0.32000000000000006","title":"Problem 1"},{"location":"MSBD5012/homework2/#problem-2","text":"w1 = np.array([[1, -1], [-1, 1]]) w2 = np.array([[-1, -1], [1, 1]]) w3 = np.array([[1], [1]]) x = np.array([1, 2]) import math from numpy import tanh def tanh_derivative ( x ): t = ( np . exp ( x ) - np . exp ( - x )) / ( np . exp ( x ) + np . exp ( - x )) dt = 1 - t ** 2 return dt def sigmoid ( X ): return 1 / ( 1 + np . exp ( - X )) def sigmoid_derivative ( x ): outcome = sigmoid ( x ) return outcome * ( 1 - outcome ) layer1 = x.dot(w1) layer1 = tanh(layer1) layer1 array([-0.76159416, 0.76159416]) layer2 = layer1.dot(w2) print(layer2) layer2 = tanh(layer2) layer2 [1.52318831 1.52318831] array([0.90925167, 0.90925167]) output = layer2.dot(w3) output *2 array([3.6370067]) e = sigmoid(output) e array([0.86038644]) backprop \\[error_{hidden} = w^T * error_{output}\\] b_layer2 = e * tanh_derivative(layer2) b_layer2 array([0.41340402, 0.41340402]) b_layer1_err = b_layer2.dot(w2.T) b_layer1 = b_layer1_err * tanh_derivative(layer1) b_layer1 array([-0.48601162, 0.48601162]) x[1] * b_layer1[0] 0.0 0.76 * b_layer2[1] 0.31418705778577294 w = np.array([[0.1, 0.5], [-0.3, 0.8]]) x = np.array([[0.2], [0.4]]) w.dot(x).dot(2) array([[0.44], [0.52]])","title":"Problem 2"},{"location":"MSBD5012/homework2/#feedforward-neural-network","text":"def relu(n): if n<0: return 0 else: return n def layer(m1, m2, w1, w2): z = m1.dot(w1) + m2.dot(w2) + b array = np.array([[1,1,1], [2, 2, 2]]) array[[0, 1], [1, 1]] - 1 array([0, 1]) def softmax_grad(softmax): s = softmax.reshape(-1,1) return np.diagflat(s) - np.dot(s, s.T) softmax_grad(np.array([[0.5, 0.5]])) array([[ 0.25, -0.25], [-0.25, 0.25]]) (array > 1) * np.array([[1, 1, 1], [1, 1, 1]]) array([[0, 0, 0], [1, 1, 1]]) array - 1 array([[0, 0, 0], [1, 1, 1]])","title":"Feedforward Neural Network"},{"location":"MSBD5012/homeworks/hw3/","text":"def output_cnn(w1, f, p, s): \"\"\" w1: input size f: filter size p: number of zero padding s: the stride k: number of filters \"\"\" return (w1 - f + 2 * p) / s + 1 def number_of_parameter(f, d, k): return (f * f *d + 1) * k def number_of_flops(f, d, w2, h2, d2): return (f * f * d + 1) * w2 * h2 * d2 number_of_flops(3, 256, 25, 25, 384) 553200000","title":"Hw3"},{"location":"MSBD5012/homeworks/pa3/pa3%20keras/","text":"","title":"Pa3 keras"},{"location":"MSBD5012/homeworks/pa3/pa3/","text":"!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.6-cp36-cp36m-linux_x86_64.whl Requirement already satisfied: cloud-tpu-client==0.10 in /usr/local/lib/python3.6/dist-packages (0.10) Requirement already satisfied: torch-xla==1.6 from https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.6-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (1.6) Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client==0.10) (4.1.3) Requirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client==0.10) (1.8.0) Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.2.8) Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.17.4) Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (1.15.0) Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.4.8) Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (4.6) Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1) Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4) Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.16.0) Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.17.2) Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0) Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9) Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.12.4) Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.52.0) Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (50.3.2) Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.1.1) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3) Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4) Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10) Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2020.6.20) import torch import torchvision import torchvision.transforms as transforms # import torch_xla # import torch_xla.core.xla_model as xm # device = xm.xla_device() device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") print(device) cuda:0 Train \u00b6 transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) trainset = torchvision . datasets . CIFAR10 ( root = './data' , train = True , download = True , transform = transform ) trainloader = torch . utils . data . DataLoader ( trainset , batch_size = 4 , shuffle = True , num_workers = 2 ) testset = torchvision . datasets . CIFAR10 ( root = './data' , train = False , download = True , transform = transform ) testloader = torch . utils . data . DataLoader ( testset , batch_size = 4 , shuffle = False , num_workers = 2 ) classes = ( 'plane' , 'car' , 'bird' , 'cat' , 'deer' , 'dog' , 'frog' , 'horse' , 'ship' , 'truck' ) Files already downloaded and verified Files already downloaded and verified #import matplotlib.pyplot as plt import numpy as np # get some random training images dataiter = iter ( trainloader ) images , labels = dataiter . next () images [ 0 ] . shape torch.Size([3, 32, 32]) import torch.nn as nn import torch.nn.functional as F class Inception ( nn . Module ): def __init__ ( self , in_planes , n1x1 , n3x3red , n3x3 , n5x5red , n5x5 , pool_planes ): super ( Inception , self ) . __init__ () # 1x1 conv branch self . b1 = nn . Sequential ( nn . Conv2d ( in_planes , n1x1 , kernel_size = 1 ), nn . BatchNorm2d ( n1x1 ), nn . ReLU ( True ), ) # 1x1 conv -> 3x3 conv branch self . b2 = nn . Sequential ( nn . Conv2d ( in_planes , n3x3red , kernel_size = 1 ), nn . BatchNorm2d ( n3x3red ), nn . ReLU ( True ), nn . Conv2d ( n3x3red , n3x3 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( n3x3 ), nn . ReLU ( True ), ) # 1x1 conv -> 5x5 conv branch self . b3 = nn . Sequential ( nn . Conv2d ( in_planes , n5x5red , kernel_size = 1 ), nn . BatchNorm2d ( n5x5red ), nn . ReLU ( True ), nn . Conv2d ( n5x5red , n5x5 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( n5x5 ), nn . ReLU ( True ), nn . Conv2d ( n5x5 , n5x5 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( n5x5 ), nn . ReLU ( True ), ) # 3x3 pool -> 1x1 conv branch self . b4 = nn . Sequential ( nn . MaxPool2d ( 3 , stride = 1 , padding = 1 ), nn . Conv2d ( in_planes , pool_planes , kernel_size = 1 ), nn . BatchNorm2d ( pool_planes ), nn . ReLU ( True ), ) def forward ( self , x ): y1 = self . b1 ( x ) y2 = self . b2 ( x ) y3 = self . b3 ( x ) y4 = self . b4 ( x ) return torch . cat ([ y1 , y2 , y3 , y4 ], 1 ) class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . pre_layers = nn . Sequential ( nn . Conv2d ( 3 , 192 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( 192 ), nn . ReLU ( True ), ) self . a3 = Inception ( 192 , 64 , 96 , 128 , 16 , 32 , 32 ) self . b3 = Inception ( 256 , 128 , 128 , 192 , 32 , 96 , 64 ) self . maxpool = nn . MaxPool2d ( 3 , stride = 2 , padding = 1 ) self . a4 = Inception ( 480 , 192 , 96 , 208 , 16 , 48 , 64 ) self . b4 = Inception ( 512 , 160 , 112 , 224 , 24 , 64 , 64 ) self . c4 = Inception ( 512 , 128 , 128 , 256 , 24 , 64 , 64 ) self . d4 = Inception ( 512 , 112 , 144 , 288 , 32 , 64 , 64 ) self . e4 = Inception ( 528 , 256 , 160 , 320 , 32 , 128 , 128 ) self . a5 = Inception ( 832 , 256 , 160 , 320 , 32 , 128 , 128 ) self . b5 = Inception ( 832 , 384 , 192 , 384 , 48 , 128 , 128 ) self . avgpool = nn . AvgPool2d ( 8 , stride = 1 ) self . linear = nn . Linear ( 1024 , 10 ) def forward ( self , x ): out = self . pre_layers ( x ) out = self . a3 ( out ) out = self . b3 ( out ) out = self . maxpool ( out ) out = self . a4 ( out ) out = self . b4 ( out ) out = self . c4 ( out ) out = self . d4 ( out ) out = self . e4 ( out ) out = self . maxpool ( out ) out = self . a5 ( out ) out = self . b5 ( out ) out = self . avgpool ( out ) out = out . view ( out . size ( 0 ), - 1 ) out = self . linear ( out ) return out net = Net () net . to ( device ) Net( (pre_layers): Sequential( (0): Conv2d(3, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (a3): Inception( (b1): Sequential( (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (b2): Sequential( (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (b3): Sequential( (0): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) ) (b4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False) (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1)) (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU(inplace=True) ) ) (b3): Inception( (b1): Sequential( (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (b2): Sequential( (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (b3): Sequential( (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) ) (b4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False) (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1)) (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU(inplace=True) ) ) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (a4): Inception( (b1): Sequential( (0): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (b2): Sequential( (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (b3): Sequential( (0): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) ) (b4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False) (1): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1)) (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU(inplace=True) ) ) (b4): Inception( (b1): Sequential( (0): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (b2): Sequential( (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (b3): Sequential( (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) ) (b4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False) (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1)) (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU(inplace=True) ) ) (c4): Inception( (b1): Sequential( (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (b2): Sequential( (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (b3): Sequential( (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) ) (b4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False) (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1)) (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU(inplace=True) ) ) (d4): Inception( (b1): Sequential( (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (b2): Sequential( (0): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (b3): Sequential( (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) ) (b4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False) (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1)) (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU(inplace=True) ) ) (e4): Inception( (b1): Sequential( (0): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (b2): Sequential( (0): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (b3): Sequential( (0): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) ) (b4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False) (1): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1)) (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU(inplace=True) ) ) (a5): Inception( (b1): Sequential( (0): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (b2): Sequential( (0): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (b3): Sequential( (0): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) ) (b4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False) (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1)) (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU(inplace=True) ) ) (b5): Inception( (b1): Sequential( (0): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (b2): Sequential( (0): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (b3): Sequential( (0): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) ) (b4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False) (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1)) (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU(inplace=True) ) ) (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0) (linear): Linear(in_features=1024, out_features=10, bias=True) ) import torch.optim as optim criterion = nn . CrossEntropyLoss () optimizer = optim . RMSprop ( net . parameters (), lr = 0.001 ) from time import time start = time () for epoch in range ( 4 ): # loop over the dataset multiple times running_loss = 0.0 for i , data in enumerate ( trainloader , 0 ): # get the inputs; data is a list of [inputs, labels] inputs , labels = data inputs = inputs . to ( device ) labels = labels . to ( device ) # zero the parameter gradients optimizer . zero_grad () # forward + backward + optimize outputs = net ( inputs ) loss = criterion ( outputs , labels ) loss . backward () optimizer . step () # print statistics running_loss += loss . item () if i % 2000 == 1999 : # print every 2000 mini-batches print ( '[ %d , %5d ] loss: %.3f ' % ( epoch + 1 , i + 1 , running_loss / 2000 )) running_loss = 0.0 print ( f 'Finished Training. Time: {time() - start}' ) [1, 2000] loss: 2.149 [1, 4000] loss: 1.826 [1, 6000] loss: 1.675 [1, 8000] loss: 1.525 [1, 10000] loss: 1.423 [1, 12000] loss: 1.335 [2, 2000] loss: 1.225 [2, 4000] loss: 1.172 [2, 6000] loss: 1.131 [2, 8000] loss: 1.101 [2, 10000] loss: 1.041 [2, 12000] loss: 1.020 [3, 2000] loss: 0.932 [3, 4000] loss: 0.912 [3, 6000] loss: 0.917 [3, 8000] loss: 0.857 [3, 10000] loss: 0.843 [3, 12000] loss: 0.824 [4, 2000] loss: 0.758 [4, 4000] loss: 0.746 [4, 6000] loss: 0.745 [4, 8000] loss: 0.750 [4, 10000] loss: 0.712 [4, 12000] loss: 0.709 PATH = './cifar_net.pth' torch.save(net.state_dict(), PATH) dataiter = iter ( testloader ) images , labels = dataiter . next () print ( 'GroundTruth: ' , ' ' . join ( '%5s' % classes [ labels[j ] ] for j in range ( 4 ))) GroundTruth: cat ship ship plane Test \u00b6 net = Net () net . load_state_dict ( torch . load ( PATH )) outputs = net ( images ) correct = 0 total = 0 with torch . no_grad (): for data in testloader : images , labels = data outputs = net ( images ) _ , predicted = torch . max ( outputs . data , 1 ) total += labels . size ( 0 ) correct += ( predicted == labels ) . sum () . item () print ( 'Accuracy of the network on the 10000 test images: %d %% ' % ( 100 * correct / total )) Accuracy of the network on the 10000 test images: 28 %","title":"Pa3"},{"location":"MSBD5012/homeworks/pa3/pa3/#train","text":"transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) trainset = torchvision . datasets . CIFAR10 ( root = './data' , train = True , download = True , transform = transform ) trainloader = torch . utils . data . DataLoader ( trainset , batch_size = 4 , shuffle = True , num_workers = 2 ) testset = torchvision . datasets . CIFAR10 ( root = './data' , train = False , download = True , transform = transform ) testloader = torch . utils . data . DataLoader ( testset , batch_size = 4 , shuffle = False , num_workers = 2 ) classes = ( 'plane' , 'car' , 'bird' , 'cat' , 'deer' , 'dog' , 'frog' , 'horse' , 'ship' , 'truck' ) Files already downloaded and verified Files already downloaded and verified #import matplotlib.pyplot as plt import numpy as np # get some random training images dataiter = iter ( trainloader ) images , labels = dataiter . next () images [ 0 ] . shape torch.Size([3, 32, 32]) import torch.nn as nn import torch.nn.functional as F class Inception ( nn . Module ): def __init__ ( self , in_planes , n1x1 , n3x3red , n3x3 , n5x5red , n5x5 , pool_planes ): super ( Inception , self ) . __init__ () # 1x1 conv branch self . b1 = nn . Sequential ( nn . Conv2d ( in_planes , n1x1 , kernel_size = 1 ), nn . BatchNorm2d ( n1x1 ), nn . ReLU ( True ), ) # 1x1 conv -> 3x3 conv branch self . b2 = nn . Sequential ( nn . Conv2d ( in_planes , n3x3red , kernel_size = 1 ), nn . BatchNorm2d ( n3x3red ), nn . ReLU ( True ), nn . Conv2d ( n3x3red , n3x3 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( n3x3 ), nn . ReLU ( True ), ) # 1x1 conv -> 5x5 conv branch self . b3 = nn . Sequential ( nn . Conv2d ( in_planes , n5x5red , kernel_size = 1 ), nn . BatchNorm2d ( n5x5red ), nn . ReLU ( True ), nn . Conv2d ( n5x5red , n5x5 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( n5x5 ), nn . ReLU ( True ), nn . Conv2d ( n5x5 , n5x5 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( n5x5 ), nn . ReLU ( True ), ) # 3x3 pool -> 1x1 conv branch self . b4 = nn . Sequential ( nn . MaxPool2d ( 3 , stride = 1 , padding = 1 ), nn . Conv2d ( in_planes , pool_planes , kernel_size = 1 ), nn . BatchNorm2d ( pool_planes ), nn . ReLU ( True ), ) def forward ( self , x ): y1 = self . b1 ( x ) y2 = self . b2 ( x ) y3 = self . b3 ( x ) y4 = self . b4 ( x ) return torch . cat ([ y1 , y2 , y3 , y4 ], 1 ) class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . pre_layers = nn . Sequential ( nn . Conv2d ( 3 , 192 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( 192 ), nn . ReLU ( True ), ) self . a3 = Inception ( 192 , 64 , 96 , 128 , 16 , 32 , 32 ) self . b3 = Inception ( 256 , 128 , 128 , 192 , 32 , 96 , 64 ) self . maxpool = nn . MaxPool2d ( 3 , stride = 2 , padding = 1 ) self . a4 = Inception ( 480 , 192 , 96 , 208 , 16 , 48 , 64 ) self . b4 = Inception ( 512 , 160 , 112 , 224 , 24 , 64 , 64 ) self . c4 = Inception ( 512 , 128 , 128 , 256 , 24 , 64 , 64 ) self . d4 = Inception ( 512 , 112 , 144 , 288 , 32 , 64 , 64 ) self . e4 = Inception ( 528 , 256 , 160 , 320 , 32 , 128 , 128 ) self . a5 = Inception ( 832 , 256 , 160 , 320 , 32 , 128 , 128 ) self . b5 = Inception ( 832 , 384 , 192 , 384 , 48 , 128 , 128 ) self . avgpool = nn . AvgPool2d ( 8 , stride = 1 ) self . linear = nn . Linear ( 1024 , 10 ) def forward ( self , x ): out = self . pre_layers ( x ) out = self . a3 ( out ) out = self . b3 ( out ) out = self . maxpool ( out ) out = self . a4 ( out ) out = self . b4 ( out ) out = self . c4 ( out ) out = self . d4 ( out ) out = self . e4 ( out ) out = self . maxpool ( out ) out = self . a5 ( out ) out = self . b5 ( out ) out = self . avgpool ( out ) out = out . view ( out . size ( 0 ), - 1 ) out = self . linear ( out ) return out net = Net () net . to ( device ) Net( (pre_layers): Sequential( (0): Conv2d(3, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (a3): Inception( (b1): Sequential( (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (b2): Sequential( (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (b3): Sequential( (0): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) ) (b4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False) (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1)) (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU(inplace=True) ) ) (b3): Inception( (b1): Sequential( (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (b2): Sequential( (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (b3): Sequential( (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) ) (b4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False) (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1)) (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU(inplace=True) ) ) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (a4): Inception( (b1): Sequential( (0): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (b2): Sequential( (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (b3): Sequential( (0): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) ) (b4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False) (1): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1)) (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU(inplace=True) ) ) (b4): Inception( (b1): Sequential( (0): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (b2): Sequential( (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (b3): Sequential( (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) ) (b4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False) (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1)) (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU(inplace=True) ) ) (c4): Inception( (b1): Sequential( (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (b2): Sequential( (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (b3): Sequential( (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) ) (b4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False) (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1)) (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU(inplace=True) ) ) (d4): Inception( (b1): Sequential( (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (b2): Sequential( (0): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (b3): Sequential( (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) ) (b4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False) (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1)) (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU(inplace=True) ) ) (e4): Inception( (b1): Sequential( (0): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (b2): Sequential( (0): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (b3): Sequential( (0): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) ) (b4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False) (1): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1)) (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU(inplace=True) ) ) (a5): Inception( (b1): Sequential( (0): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (b2): Sequential( (0): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (b3): Sequential( (0): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) ) (b4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False) (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1)) (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU(inplace=True) ) ) (b5): Inception( (b1): Sequential( (0): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (b2): Sequential( (0): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (b3): Sequential( (0): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) ) (b4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False) (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1)) (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU(inplace=True) ) ) (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0) (linear): Linear(in_features=1024, out_features=10, bias=True) ) import torch.optim as optim criterion = nn . CrossEntropyLoss () optimizer = optim . RMSprop ( net . parameters (), lr = 0.001 ) from time import time start = time () for epoch in range ( 4 ): # loop over the dataset multiple times running_loss = 0.0 for i , data in enumerate ( trainloader , 0 ): # get the inputs; data is a list of [inputs, labels] inputs , labels = data inputs = inputs . to ( device ) labels = labels . to ( device ) # zero the parameter gradients optimizer . zero_grad () # forward + backward + optimize outputs = net ( inputs ) loss = criterion ( outputs , labels ) loss . backward () optimizer . step () # print statistics running_loss += loss . item () if i % 2000 == 1999 : # print every 2000 mini-batches print ( '[ %d , %5d ] loss: %.3f ' % ( epoch + 1 , i + 1 , running_loss / 2000 )) running_loss = 0.0 print ( f 'Finished Training. Time: {time() - start}' ) [1, 2000] loss: 2.149 [1, 4000] loss: 1.826 [1, 6000] loss: 1.675 [1, 8000] loss: 1.525 [1, 10000] loss: 1.423 [1, 12000] loss: 1.335 [2, 2000] loss: 1.225 [2, 4000] loss: 1.172 [2, 6000] loss: 1.131 [2, 8000] loss: 1.101 [2, 10000] loss: 1.041 [2, 12000] loss: 1.020 [3, 2000] loss: 0.932 [3, 4000] loss: 0.912 [3, 6000] loss: 0.917 [3, 8000] loss: 0.857 [3, 10000] loss: 0.843 [3, 12000] loss: 0.824 [4, 2000] loss: 0.758 [4, 4000] loss: 0.746 [4, 6000] loss: 0.745 [4, 8000] loss: 0.750 [4, 10000] loss: 0.712 [4, 12000] loss: 0.709 PATH = './cifar_net.pth' torch.save(net.state_dict(), PATH) dataiter = iter ( testloader ) images , labels = dataiter . next () print ( 'GroundTruth: ' , ' ' . join ( '%5s' % classes [ labels[j ] ] for j in range ( 4 ))) GroundTruth: cat ship ship plane","title":"Train"},{"location":"MSBD5012/homeworks/pa3/pa3/#test","text":"net = Net () net . load_state_dict ( torch . load ( PATH )) outputs = net ( images ) correct = 0 total = 0 with torch . no_grad (): for data in testloader : images , labels = data outputs = net ( images ) _ , predicted = torch . max ( outputs . data , 1 ) total += labels . size ( 0 ) correct += ( predicted == labels ) . sum () . item () print ( 'Accuracy of the network on the 10000 test images: %d %% ' % ( 100 * correct / total )) Accuracy of the network on the 10000 test images: 28 %","title":"Test"},{"location":"MSBD5012/homeworks/pa4/assignment4/","text":"import tensorflow as tf from os import listdir from os.path import isfile , join import matplotlib.pyplot as plt Use Pre-trained model \u00b6 # We will use google drive as dataset's folder IMAGE_PATH = \"/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images\" image_files = [join(IMAGE_PATH, f) for f in listdir(IMAGE_PATH) if isfile(join(IMAGE_PATH, f))] image_files ['/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/9.png', '/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/8.png', '/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/6.png', '/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/4.png', '/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/10.png', '/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/3.png', '/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/2.png', '/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/7.png', '/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/5.png', '/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/1.png'] # Helper function to preprocess the image so that it can be inputted in MobileNetV2 def preprocess ( image ): image = tf . cast ( image , tf . float32 ) image = image / 255 image = image / tf . math . reduce_max ( image ) image = tf . image . resize ( image , ( 224 , 224 )) image = image [ None , ...] return image # Helper function to extract labels from probability vector def get_imagenet_label ( probs ): return decode_predictions ( probs , top = 1 )[ 0 ][ 0 ] images = [] for i in image_files: image_raw = tf.io.read_file(i) image = tf.image.decode_image(image_raw,3) image = preprocess(image) images.append(image) plt.imshow(images[0][0]) <matplotlib.image.AxesImage at 0x7f36e1fe5390> Use pre-trained model \u00b6 pretrained_model = tf . keras . applications . MobileNetV2 ( include_top = True , weights = 'imagenet' ) # pretrained_model = tf . keras . applications . resnet50 . ResNet50 ( include_top = True , # weights = 'imagenet' ) # pretrained_model = tf . keras . applications . inception_v3 . InceptionV3 ( include_top = True , # weights = 'imagenet' ) # pretrained_model = tf . keras . applications . nasnet . NASNetMobile ( include_top = True , # weights = 'imagenet' ) pretrained_model . trainable = False # ImageNet labels # Decoder is the same for all pretrained imagenet models . decode_predictions = tf . keras . applications . mobilenet_v2 . decode_predictions results = [] confidences = [] probs = [] for i in images: image_probs = pretrained_model.predict(i) plt.figure() plt.imshow(i[0]) _, image_class, class_confidence = get_imagenet_label(image_probs) results.append(image_class) confidences.append(class_confidence * 100) probs.append(image_probs) plt.title('{} : {:.2f}% Confidence'.format(image_class, class_confidence*100)) plt.show() print(results) ['fireboat', 'pillow', 'zebra', 'hourglass', 'carousel', 'yurt', 'peacock', 'school_bus', 'water_tower', 'rapeseed'] plt.figure(num=None, figsize=(15, 8), dpi=80, facecolor='w', edgecolor='k') plt.bar(results, confidences) <BarContainer object of 10 artists> Adversarial Example \u00b6 loss_object = tf . keras . losses . CategoricalCrossentropy () def create_adversarial_pattern ( input_image , input_label ): with tf . GradientTape () as tape : tape . watch ( input_image ) prediction = pretrained_model ( input_image ) loss = loss_object ( input_label , prediction ) # Get the gradients of the loss w . r . t to the input image . gradient = tape . gradient ( loss , input_image ) # Get the sign of the gradients to create the perturbation signed_grad = tf . sign ( gradient ) return signed_grad , gradient # Get the input label of the image . labrador_retriever_index = 208 label = tf . one_hot ( labrador_retriever_index , image_probs . shape [ - 1 ]) label = tf . reshape ( label , ( 1 , image_probs . shape [ - 1 ])) perturbations , enhance = create_adversarial_pattern ( image , label ) plt . imshow ( perturbations [ 0 ]) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). <matplotlib.image.AxesImage at 0x7f36d12e2668> def display_images ( image , description , index ): _ , label , confidence = get_imagenet_label ( pretrained_model . predict ( image )) plt . figure () plt . imshow ( image [ 0 ]) plt . title ( '{} \\n {} : {:.2f}% Confidence' . format ( description , label , confidence * 100 )) # plt . savefig ( f \"{index}-{description}.jpg\" ) plt . show () return label , confidence Display images \u00b6 epsilons = [ 0.1, 0.5 ] new_labels = [] new_confidences = [] for index , prob in enumerate ( probs ) : label = tf . one_hot ( labrador_retriever_index , prob . shape [ -1 ] ) label = tf . reshape ( label , ( 1 , prob . shape [ -1 ] )) perturbations , enhance = create_adversarial_pattern ( images [ index ] , label ) descriptions = [ ('Epsilon = {:0.3f}'.format(eps) if eps else 'Input') for eps in epsilons ] temp_confidence = [] temp_label = [] for i , eps in enumerate ( epsilons ) : adv_x = eps * perturbations + images [ index ] adv_x = tf . clip_by_value ( adv_x , 0 , 1 ) label , confidence = display_images ( adv_x , descriptions [ i ] , index ) temp_label . append ( label ) temp_confidence . append ( confidence * 100 ) new_labels . append ( temp_label ) new_confidences . append ( temp_confidence ) for i in range ( len ( results )) : print ( f \"Original label: {results[i]} {confidences[i]}\" ) print ( f \"New result 0.1: {new_labels[i][0]} {new_confidences[i][0]}\" ) print ( f \"New result 0.5: {new_labels[i][1]} {new_confidences[i][1]}\" ) Original label: fireboat 75.41137337684631 New result 0.1: paddlewheel 9.37194898724556 New result 0.5: dishrag 12.636317312717438 Original label: pillow 99.77816939353943 New result 0.1: pillow 98.71640205383301 New result 0.5: stole 20.831426978111267 Original label: zebra 98.61569404602051 New result 0.1: zebra 17.946863174438477 New result 0.5: prayer_rug 60.016703605651855 Original label: hourglass 97.2355604171753 New result 0.1: hourglass 95.40168046951294 New result 0.5: prayer_rug 42.5665944814682 Original label: carousel 97.31283783912659 New result 0.1: carousel 9.904991090297699 New result 0.5: prayer_rug 21.937750279903412 Original label: yurt 92.97380447387695 New result 0.1: yurt 77.24021673202515 New result 0.5: prayer_rug 40.29488265514374 Original label: peacock 99.12651181221008 New result 0.1: brain_coral 23.743192851543427 New result 0.5: wool 20.01730054616928 Original label: school_bus 98.6170768737793 New result 0.1: school_bus 37.114837765693665 New result 0.5: stole 21.422338485717773 Original label: water_tower 95.00780701637268 New result 0.1: water_tower 26.783213019371033 New result 0.5: prayer_rug 37.235623598098755 Original label: rapeseed 36.22219264507294 New result 0.1: bath_towel 18.45916509628296 New result 0.5: wool 18.765641748905182 plt.bar(range(10), confidences) <BarContainer object of 10 artists> !sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-generic-recommended !jupyter nbconvert --to pdf assignment4.ipynb [NbConvertApp] Converting notebook assignment4.ipynb to pdf [NbConvertApp] Support files will be in assignment4_files/ [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Writing 42903 bytes to ./notebook.tex [NbConvertApp] Building PDF [NbConvertApp] Running xelatex 3 times: [u'xelatex', u'./notebook.tex', '-quiet'] Traceback (most recent call last): File \"/usr/local/bin/jupyter-nbconvert\", line 8, in <module> sys.exit(main()) File \"/usr/local/lib/python2.7/dist-packages/jupyter_core/application.py\", line 267, in launch_instance return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)","title":"Assignment4"},{"location":"MSBD5012/homeworks/pa4/assignment4/#use-pre-trained-model","text":"# We will use google drive as dataset's folder IMAGE_PATH = \"/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images\" image_files = [join(IMAGE_PATH, f) for f in listdir(IMAGE_PATH) if isfile(join(IMAGE_PATH, f))] image_files ['/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/9.png', '/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/8.png', '/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/6.png', '/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/4.png', '/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/10.png', '/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/3.png', '/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/2.png', '/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/7.png', '/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/5.png', '/content/drive/My Drive/courses/HKUST/MSBD5012/homeworks/pa4/images/1.png'] # Helper function to preprocess the image so that it can be inputted in MobileNetV2 def preprocess ( image ): image = tf . cast ( image , tf . float32 ) image = image / 255 image = image / tf . math . reduce_max ( image ) image = tf . image . resize ( image , ( 224 , 224 )) image = image [ None , ...] return image # Helper function to extract labels from probability vector def get_imagenet_label ( probs ): return decode_predictions ( probs , top = 1 )[ 0 ][ 0 ] images = [] for i in image_files: image_raw = tf.io.read_file(i) image = tf.image.decode_image(image_raw,3) image = preprocess(image) images.append(image) plt.imshow(images[0][0]) <matplotlib.image.AxesImage at 0x7f36e1fe5390>","title":"Use Pre-trained model"},{"location":"MSBD5012/homeworks/pa4/assignment4/#use-pre-trained-model_1","text":"pretrained_model = tf . keras . applications . MobileNetV2 ( include_top = True , weights = 'imagenet' ) # pretrained_model = tf . keras . applications . resnet50 . ResNet50 ( include_top = True , # weights = 'imagenet' ) # pretrained_model = tf . keras . applications . inception_v3 . InceptionV3 ( include_top = True , # weights = 'imagenet' ) # pretrained_model = tf . keras . applications . nasnet . NASNetMobile ( include_top = True , # weights = 'imagenet' ) pretrained_model . trainable = False # ImageNet labels # Decoder is the same for all pretrained imagenet models . decode_predictions = tf . keras . applications . mobilenet_v2 . decode_predictions results = [] confidences = [] probs = [] for i in images: image_probs = pretrained_model.predict(i) plt.figure() plt.imshow(i[0]) _, image_class, class_confidence = get_imagenet_label(image_probs) results.append(image_class) confidences.append(class_confidence * 100) probs.append(image_probs) plt.title('{} : {:.2f}% Confidence'.format(image_class, class_confidence*100)) plt.show() print(results) ['fireboat', 'pillow', 'zebra', 'hourglass', 'carousel', 'yurt', 'peacock', 'school_bus', 'water_tower', 'rapeseed'] plt.figure(num=None, figsize=(15, 8), dpi=80, facecolor='w', edgecolor='k') plt.bar(results, confidences) <BarContainer object of 10 artists>","title":"Use pre-trained model"},{"location":"MSBD5012/homeworks/pa4/assignment4/#adversarial-example","text":"loss_object = tf . keras . losses . CategoricalCrossentropy () def create_adversarial_pattern ( input_image , input_label ): with tf . GradientTape () as tape : tape . watch ( input_image ) prediction = pretrained_model ( input_image ) loss = loss_object ( input_label , prediction ) # Get the gradients of the loss w . r . t to the input image . gradient = tape . gradient ( loss , input_image ) # Get the sign of the gradients to create the perturbation signed_grad = tf . sign ( gradient ) return signed_grad , gradient # Get the input label of the image . labrador_retriever_index = 208 label = tf . one_hot ( labrador_retriever_index , image_probs . shape [ - 1 ]) label = tf . reshape ( label , ( 1 , image_probs . shape [ - 1 ])) perturbations , enhance = create_adversarial_pattern ( image , label ) plt . imshow ( perturbations [ 0 ]) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). <matplotlib.image.AxesImage at 0x7f36d12e2668> def display_images ( image , description , index ): _ , label , confidence = get_imagenet_label ( pretrained_model . predict ( image )) plt . figure () plt . imshow ( image [ 0 ]) plt . title ( '{} \\n {} : {:.2f}% Confidence' . format ( description , label , confidence * 100 )) # plt . savefig ( f \"{index}-{description}.jpg\" ) plt . show () return label , confidence","title":"Adversarial Example"},{"location":"MSBD5012/homeworks/pa4/assignment4/#display-images","text":"epsilons = [ 0.1, 0.5 ] new_labels = [] new_confidences = [] for index , prob in enumerate ( probs ) : label = tf . one_hot ( labrador_retriever_index , prob . shape [ -1 ] ) label = tf . reshape ( label , ( 1 , prob . shape [ -1 ] )) perturbations , enhance = create_adversarial_pattern ( images [ index ] , label ) descriptions = [ ('Epsilon = {:0.3f}'.format(eps) if eps else 'Input') for eps in epsilons ] temp_confidence = [] temp_label = [] for i , eps in enumerate ( epsilons ) : adv_x = eps * perturbations + images [ index ] adv_x = tf . clip_by_value ( adv_x , 0 , 1 ) label , confidence = display_images ( adv_x , descriptions [ i ] , index ) temp_label . append ( label ) temp_confidence . append ( confidence * 100 ) new_labels . append ( temp_label ) new_confidences . append ( temp_confidence ) for i in range ( len ( results )) : print ( f \"Original label: {results[i]} {confidences[i]}\" ) print ( f \"New result 0.1: {new_labels[i][0]} {new_confidences[i][0]}\" ) print ( f \"New result 0.5: {new_labels[i][1]} {new_confidences[i][1]}\" ) Original label: fireboat 75.41137337684631 New result 0.1: paddlewheel 9.37194898724556 New result 0.5: dishrag 12.636317312717438 Original label: pillow 99.77816939353943 New result 0.1: pillow 98.71640205383301 New result 0.5: stole 20.831426978111267 Original label: zebra 98.61569404602051 New result 0.1: zebra 17.946863174438477 New result 0.5: prayer_rug 60.016703605651855 Original label: hourglass 97.2355604171753 New result 0.1: hourglass 95.40168046951294 New result 0.5: prayer_rug 42.5665944814682 Original label: carousel 97.31283783912659 New result 0.1: carousel 9.904991090297699 New result 0.5: prayer_rug 21.937750279903412 Original label: yurt 92.97380447387695 New result 0.1: yurt 77.24021673202515 New result 0.5: prayer_rug 40.29488265514374 Original label: peacock 99.12651181221008 New result 0.1: brain_coral 23.743192851543427 New result 0.5: wool 20.01730054616928 Original label: school_bus 98.6170768737793 New result 0.1: school_bus 37.114837765693665 New result 0.5: stole 21.422338485717773 Original label: water_tower 95.00780701637268 New result 0.1: water_tower 26.783213019371033 New result 0.5: prayer_rug 37.235623598098755 Original label: rapeseed 36.22219264507294 New result 0.1: bath_towel 18.45916509628296 New result 0.5: wool 18.765641748905182 plt.bar(range(10), confidences) <BarContainer object of 10 artists> !sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-generic-recommended !jupyter nbconvert --to pdf assignment4.ipynb [NbConvertApp] Converting notebook assignment4.ipynb to pdf [NbConvertApp] Support files will be in assignment4_files/ [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Making directory ./assignment4_files [NbConvertApp] Writing 42903 bytes to ./notebook.tex [NbConvertApp] Building PDF [NbConvertApp] Running xelatex 3 times: [u'xelatex', u'./notebook.tex', '-quiet'] Traceback (most recent call last): File \"/usr/local/bin/jupyter-nbconvert\", line 8, in <module> sys.exit(main()) File \"/usr/local/lib/python2.7/dist-packages/jupyter_core/application.py\", line 267, in launch_instance return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)","title":"Display images"},{"location":"MSBD5012/lectures/Lecture%202/","text":"Divergence \u00b6 ML Setup \\(P(x)\\) -> generate -> Data -> learn \\(Q(x)\\) where Q should as close to P as possible. Entropy, cross entropy, and KL divergence \u00b6 Entropy \u00b6 \\[H(p) = -\\sum_{i}p_{i}log(p_i)\\] Cross Entropy \u00b6 p = true distribution q = predicted distribution \\[H(p, q) = -\\sum_ip_ilog(q_i)\\] Relative entropy or Kullback-Leibler divergence \u00b6 Meassure how much a distribution Q(X) differs from a \"True\" probability distribution P(X) K-L Divergence if Q from P is defined as follows: \\[ KL(P||Q) = \\sum_x{P(X)log(\\frac{P(X)}{Q(X)})} = -log\\sum_x{Q(X)}\\] Relationship between entropy, cross-entropy, and kl divergence $$cross-entropy = entropy + kl divergence $$ \\[or\\] \\[D_{kl}(p||q) = H(p, q) - H(p)\\] Minimize cross entropy = Maximizing log likelyhood Suppose we have likelihood of the training set is \\[\\sum_{i}(probability\\ of\\ i)^{number\\ of\\ occurrences\\ of\\ i} = \\sum_{i}q_i^{Np_i}\\] where N is number of conditionally independent samples in training set So the log-likelihood divided by N is \\[\\frac{1}{N}log\\sum_iq_i^{Np_i} = \\sum_ip_ilog(q_i) = -H(p, q)\\] Supervised learning \u00b6 Unsupervised learning \u00b6 Multual information \u00b6 H(x): Initial uncertainty about x H(X | Y): Expected uncertainty about x if y is tested Linear Regression \u00b6 \\[y = w_0 + w_1x_1\\] Least Square Regression \u00b6 import matplotlib.pyplot as plt import numpy as np x1 = [ 0 , 0 , 1 , 1 ] x2 = [ 0 , 1 , 0 , 1 ] y = [ 1 , 0 , 0 , 1 ] fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( x1 , x2 , y ) <mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f2ec3a9ae80> using sklearn from sklearn import linear_model X = np . column_stack (( x1 , x2 )) lm = linear_model . LinearRegression () model = lm . fit ( X , y ) print ( model . coef_ ) print ( model . intercept_ ) print ( model . score ( X , y )) [0.00000000e+00 2.22044605e-16] 0.4999999999999999 0.0 using numpy \\( \\(A = (X^TX)^{-1}X^TY\\) \\) ones = [1 for i in range(len(x1))] X = np.column_stack((ones, x1, x2)) X_T = X.transpose() print(X) print(X_T) [[1 0 1] [1 0 1] [1 1 1] [1 1 0] [1 1 0]] [[1 1 1 1 1] [0 0 1 1 1] [1 1 1 0 0]] dot = np.dot(X_T, X) inverse = np.linalg.inv(dot) print(dot) print(inverse) [[5 3 3] [3 3 1] [3 1 3]] [[ 2. -1.5 -1.5] [-1.5 1.5 1. ] [-1.5 1. 1.5]] dot2 = np.dot(inverse, X_T).dot(y) dot2 array([ 2. , 1.5, -1.5]) Mean Square Error \u00b6 import numpy as np # Given values Y_true = [ 1 , 1 , 2 , 2 , 4 ] # Y_true = Y (original values) # Calculated values Y_pred = [ 0.6 , 1.29 , 1.99 , 2.69 , 3.4 ] # Y_pred = Y' # Mean Squared Error MSE = np . square ( np . subtract ( Y_true , Y_pred )) . mean () MSE 0.21606 Hypothesis space \u00b6 Is the set of functioins that it is allowed to select as being the solution. Thhe size of the hypothesis space is called the capacity of the model. For polynomial regression, the larger the d, the higher the model capacity. Higher model capacity implies better fit to training data. \\(S_1 = \\{y = w_0 + w_1x_1 | w_0, w_1 \\in R\\}\\) \\(S_2 = \\{y=w_0 + w_1x_1 + w_2x_1^2 + w_3x_1^3 | w_0, w_1, w_2, w_3 \\in R\\}\\) Generalization Error \u00b6 Model select: Validation Split training data into two parts. One part for training and second part for validation. This has to be randomly split. Regularization Regilarization \u00b6 ridge regression \u00b6 The larger the regularization constant \\(\\lambda\\) , the smaller the weights","title":"Lecture 2"},{"location":"MSBD5012/lectures/Lecture%202/#divergence","text":"ML Setup \\(P(x)\\) -> generate -> Data -> learn \\(Q(x)\\) where Q should as close to P as possible.","title":"Divergence"},{"location":"MSBD5012/lectures/Lecture%202/#entropy-cross-entropy-and-kl-divergence","text":"","title":"Entropy, cross entropy, and KL divergence"},{"location":"MSBD5012/lectures/Lecture%202/#entropy","text":"\\[H(p) = -\\sum_{i}p_{i}log(p_i)\\]","title":"Entropy"},{"location":"MSBD5012/lectures/Lecture%202/#cross-entropy","text":"p = true distribution q = predicted distribution \\[H(p, q) = -\\sum_ip_ilog(q_i)\\]","title":"Cross Entropy"},{"location":"MSBD5012/lectures/Lecture%202/#relative-entropy-or-kullback-leibler-divergence","text":"Meassure how much a distribution Q(X) differs from a \"True\" probability distribution P(X) K-L Divergence if Q from P is defined as follows: \\[ KL(P||Q) = \\sum_x{P(X)log(\\frac{P(X)}{Q(X)})} = -log\\sum_x{Q(X)}\\] Relationship between entropy, cross-entropy, and kl divergence $$cross-entropy = entropy + kl divergence $$ \\[or\\] \\[D_{kl}(p||q) = H(p, q) - H(p)\\] Minimize cross entropy = Maximizing log likelyhood Suppose we have likelihood of the training set is \\[\\sum_{i}(probability\\ of\\ i)^{number\\ of\\ occurrences\\ of\\ i} = \\sum_{i}q_i^{Np_i}\\] where N is number of conditionally independent samples in training set So the log-likelihood divided by N is \\[\\frac{1}{N}log\\sum_iq_i^{Np_i} = \\sum_ip_ilog(q_i) = -H(p, q)\\]","title":"Relative entropy or Kullback-Leibler divergence"},{"location":"MSBD5012/lectures/Lecture%202/#supervised-learning","text":"","title":"Supervised learning"},{"location":"MSBD5012/lectures/Lecture%202/#unsupervised-learning","text":"","title":"Unsupervised learning"},{"location":"MSBD5012/lectures/Lecture%202/#multual-information","text":"H(x): Initial uncertainty about x H(X | Y): Expected uncertainty about x if y is tested","title":"Multual information"},{"location":"MSBD5012/lectures/Lecture%202/#linear-regression","text":"\\[y = w_0 + w_1x_1\\]","title":"Linear Regression"},{"location":"MSBD5012/lectures/Lecture%202/#least-square-regression","text":"import matplotlib.pyplot as plt import numpy as np x1 = [ 0 , 0 , 1 , 1 ] x2 = [ 0 , 1 , 0 , 1 ] y = [ 1 , 0 , 0 , 1 ] fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( x1 , x2 , y ) <mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f2ec3a9ae80> using sklearn from sklearn import linear_model X = np . column_stack (( x1 , x2 )) lm = linear_model . LinearRegression () model = lm . fit ( X , y ) print ( model . coef_ ) print ( model . intercept_ ) print ( model . score ( X , y )) [0.00000000e+00 2.22044605e-16] 0.4999999999999999 0.0 using numpy \\( \\(A = (X^TX)^{-1}X^TY\\) \\) ones = [1 for i in range(len(x1))] X = np.column_stack((ones, x1, x2)) X_T = X.transpose() print(X) print(X_T) [[1 0 1] [1 0 1] [1 1 1] [1 1 0] [1 1 0]] [[1 1 1 1 1] [0 0 1 1 1] [1 1 1 0 0]] dot = np.dot(X_T, X) inverse = np.linalg.inv(dot) print(dot) print(inverse) [[5 3 3] [3 3 1] [3 1 3]] [[ 2. -1.5 -1.5] [-1.5 1.5 1. ] [-1.5 1. 1.5]] dot2 = np.dot(inverse, X_T).dot(y) dot2 array([ 2. , 1.5, -1.5])","title":"Least Square Regression"},{"location":"MSBD5012/lectures/Lecture%202/#mean-square-error","text":"import numpy as np # Given values Y_true = [ 1 , 1 , 2 , 2 , 4 ] # Y_true = Y (original values) # Calculated values Y_pred = [ 0.6 , 1.29 , 1.99 , 2.69 , 3.4 ] # Y_pred = Y' # Mean Squared Error MSE = np . square ( np . subtract ( Y_true , Y_pred )) . mean () MSE 0.21606","title":"Mean Square Error"},{"location":"MSBD5012/lectures/Lecture%202/#hypothesis-space","text":"Is the set of functioins that it is allowed to select as being the solution. Thhe size of the hypothesis space is called the capacity of the model. For polynomial regression, the larger the d, the higher the model capacity. Higher model capacity implies better fit to training data. \\(S_1 = \\{y = w_0 + w_1x_1 | w_0, w_1 \\in R\\}\\) \\(S_2 = \\{y=w_0 + w_1x_1 + w_2x_1^2 + w_3x_1^3 | w_0, w_1, w_2, w_3 \\in R\\}\\)","title":"Hypothesis space"},{"location":"MSBD5012/lectures/Lecture%202/#generalization-error","text":"Model select: Validation Split training data into two parts. One part for training and second part for validation. This has to be randomly split. Regularization","title":"Generalization Error"},{"location":"MSBD5012/lectures/Lecture%202/#regilarization","text":"","title":"Regilarization"},{"location":"MSBD5012/lectures/Lecture%202/#ridge-regression","text":"The larger the regularization constant \\(\\lambda\\) , the smaller the weights","title":"ridge regression"},{"location":"MSBD5012/lectures/Lecture1/","text":"Likehood possibility \u00b6 \\[L(H|E) = P(E|H) \\] import matplotlib.pyplot as plt import numpy as np import scipy.stats mu = 3.0 sigma = 0.5 data = np . random . randn ( 100000 ) * sigma + mu hx , hy , _ = plt . hist ( data , bins = 50 , color = \"lightblue\" ) plt . ylim ( 0 . 0 , max ( hx ) + 0 . 05 ) plt . title ( r 'Normal distribution $\\mu_0 = 3$ and $\\sigma_0 = 0.5$' ) plt . grid () Calculate the log-likelihood \u00b6 scipy . stats . norm . pdf ( 6 , 2 . 0 , 1 . 0 ) print ( np . log ( scipy . stats . norm . pdf ( data , 2 . 0 , 1 . 0 )). sum () ) x = np . linspace ( - 10 , 10 , 1000 , endpoint = True ) y = [] for i in x : y . append ( np . log ( scipy . stats . norm . pdf ( data , i , 0 . 5 )). sum ()) plt . plot ( x , y ) plt . title ( r 'Log-Likelihood' ) plt . xlabel ( r '$\\mu$' ) plt . grid () -154314.14596206427 print('mean ---> ', np.mean(data)) print('std deviation ---> ', np.std(data)) mean ---> 2.999606326069087 std deviation ---> 0.4991923934863224 y_min = y . index ( max ( y )) print ( 'mean (from max log likelohood) ---> ' , x [ y_min ] ) mean (from max log likelohood) ---> 2.9929929929929937","title":"Lecture1"},{"location":"MSBD5012/lectures/Lecture1/#likehood-possibility","text":"\\[L(H|E) = P(E|H) \\] import matplotlib.pyplot as plt import numpy as np import scipy.stats mu = 3.0 sigma = 0.5 data = np . random . randn ( 100000 ) * sigma + mu hx , hy , _ = plt . hist ( data , bins = 50 , color = \"lightblue\" ) plt . ylim ( 0 . 0 , max ( hx ) + 0 . 05 ) plt . title ( r 'Normal distribution $\\mu_0 = 3$ and $\\sigma_0 = 0.5$' ) plt . grid ()","title":"Likehood possibility"},{"location":"MSBD5012/lectures/Lecture1/#calculate-the-log-likelihood","text":"scipy . stats . norm . pdf ( 6 , 2 . 0 , 1 . 0 ) print ( np . log ( scipy . stats . norm . pdf ( data , 2 . 0 , 1 . 0 )). sum () ) x = np . linspace ( - 10 , 10 , 1000 , endpoint = True ) y = [] for i in x : y . append ( np . log ( scipy . stats . norm . pdf ( data , i , 0 . 5 )). sum ()) plt . plot ( x , y ) plt . title ( r 'Log-Likelihood' ) plt . xlabel ( r '$\\mu$' ) plt . grid () -154314.14596206427 print('mean ---> ', np.mean(data)) print('std deviation ---> ', np.std(data)) mean ---> 2.999606326069087 std deviation ---> 0.4991923934863224 y_min = y . index ( max ( y )) print ( 'mean (from max log likelohood) ---> ' , x [ y_min ] ) mean (from max log likelohood) ---> 2.9929929929929937","title":"Calculate the log-likelihood"},{"location":"MSBD5012/lectures/Lecture3/","text":"Homework \u00b6 http://home.cse.ust.hk/~lzhang/teach/msbd5012/ Programming assignment (2020.10.10) Writting assignment (2020.10.03) Logestic regression \u00b6 Gradient Descent \u00b6 https://en.wikipedia.org/wiki/Gradient_descent#:~:text=Gradient%20descent%20is%20a%20first,function%20at%20the%20current%20point. \\[J'(w) = \\frac{dJ(w)}{dw} = lim_{e \\to 0}\\frac{J(w+e) - l(w)}{E}$$ $$J(w + e) = J(w) + eJ'(w)\\] from sklearn.linear_model import SGDRegressor import matplotlib.pyplot as plt import numpy as np x0 = np . array ([ 1 , 1 , 1 , 1 ]) x1 = np . array ([ 0 , 0 , 1 , 1 ]) x2 = np . array ([ 0 , 1 , 0 , 1 ]) x3 = x1 * x2 y = [ 1 , 0 , 0 , 1 ] colors = [ 'red' if i == 0 else 'blue' for i in y ] X = np . column_stack (( x1 , x2 )) # X = np.column_stack((x0, x1, x2, x3)) plt . scatter ( x1 , x2 , color = colors ) <matplotlib.collections.PathCollection at 0x7faa3604e198> import statsmodels.api as sm print ( X ) logit_model = sm . Logit ( y , X ) result = logit_model . fit () print ( result . summary2 ()) [[0 0] [0 1] [1 0] [1 1]] Optimization terminated successfully. Current function value: 0.693147 Iterations 1 Results: Logit ============================================================== Model: Logit Pseudo R-squared: 0.000 Dependent Variable: y AIC: 9.5452 Date: 2020-10-03 07:41 BIC: 8.3178 No. Observations: 4 Log-Likelihood: -2.7726 Df Model: 1 LL-Null: -2.7726 Df Residuals: 2 LLR p-value: 1.0000 Converged: 1.0000 Scale: 1.0000 No. Iterations: 1.0000 ----------------------------------------------------------------- Coef. Std.Err. z P>|z| [0.025 0.975] ----------------------------------------------------------------- x1 0.0000 1.6330 0.0000 1.0000 -3.2006 3.2006 x2 0.0000 1.6330 0.0000 1.0000 -3.2006 3.2006 ============================================================== predictions = model.predict(X) predictions array([0, 0, 0, 0]) Batch gradient descent \u00b6 https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/#:~:text=Batch%20gradient%20descent%20is%20a,is%20called%20a%20training%20epoch. \\[w_j \\leftarrow w_j + \\alpha \\frac{1}{N}\\sum^{N}_{i=1}[y_i - \\sigma (w^Tx_i)]x_{i,j} \\] \\(y_i\\) : Observed output \\(\\sigma (w^Tx_i)\\) Predicted output \\(x_{i, j}\\) input import math import numpy as np def sigmoid ( x ): return 1 / ( 1 + math . exp ( - x )) a = 0.1 x_1 = [ 1, 1, 1, 1 ] x_2 = [ 0, 0, 1, 1 ] x_3 = [ 0, 1, 0 ,1 ] w_1 = - 2 w_2 = 1 w_3 = 1 y = [ 1, 0, 0, 1 ] def compute ( x , w_i ) : sum_i = 0 n = len ( x ) for i in range ( n ) : predicted_output = w_1 + w_2 * x_2 [ i ] + w_3 * x_3 [ i ] temp = ( y [ i ] - sigmoid ( predicted_output )) * x [ i ] sum_i += temp result = w_i + a * sum_i / n return result print ( compute ( x_1 , w_1 )) print ( compute ( x_2 , w_2 )) print ( compute ( x_3 , w_3 )) -1.9789271441190528 1.00577646446575 1.00577646446575 for i in range ( 100 ): w_1 = compute ( x_1 , w_1 ) w_2 = compute ( x_2 , w_2 ) w_3 = compute ( x_3 , w_3 ) err1 = w_1 + w_2 * x_2 [ 0 ] + w_3 * x_3 [ 0 ] err2 = w_1 + w_2 * x_2 [ 1 ] + w_3 * x_3 [ 1 ] err3 = w_1 + w_2 * x_2 [ 2 ] + w_3 * x_3 [ 2 ] err4 = w_1 + w_2 * x_2 [ 3 ] + w_3 * x_3 [ 3 ] ( err1 + err2 + err3 + err4 ) / 4 -0.07952691518172225 Stochastic Gradient descent \u00b6 https://en.wikipedia.org/wiki/Stochastic_gradient_descent Newton's method \u00b6 Adavantages \u00b6 If we only do the 1st order, then we will get overestimate, but with second order estimate, we will correct the overestimate. Softmax Regression \u00b6","title":"Lecture3"},{"location":"MSBD5012/lectures/Lecture3/#homework","text":"http://home.cse.ust.hk/~lzhang/teach/msbd5012/ Programming assignment (2020.10.10) Writting assignment (2020.10.03)","title":"Homework"},{"location":"MSBD5012/lectures/Lecture3/#logestic-regression","text":"","title":"Logestic regression"},{"location":"MSBD5012/lectures/Lecture3/#gradient-descent","text":"https://en.wikipedia.org/wiki/Gradient_descent#:~:text=Gradient%20descent%20is%20a%20first,function%20at%20the%20current%20point. \\[J'(w) = \\frac{dJ(w)}{dw} = lim_{e \\to 0}\\frac{J(w+e) - l(w)}{E}$$ $$J(w + e) = J(w) + eJ'(w)\\] from sklearn.linear_model import SGDRegressor import matplotlib.pyplot as plt import numpy as np x0 = np . array ([ 1 , 1 , 1 , 1 ]) x1 = np . array ([ 0 , 0 , 1 , 1 ]) x2 = np . array ([ 0 , 1 , 0 , 1 ]) x3 = x1 * x2 y = [ 1 , 0 , 0 , 1 ] colors = [ 'red' if i == 0 else 'blue' for i in y ] X = np . column_stack (( x1 , x2 )) # X = np.column_stack((x0, x1, x2, x3)) plt . scatter ( x1 , x2 , color = colors ) <matplotlib.collections.PathCollection at 0x7faa3604e198> import statsmodels.api as sm print ( X ) logit_model = sm . Logit ( y , X ) result = logit_model . fit () print ( result . summary2 ()) [[0 0] [0 1] [1 0] [1 1]] Optimization terminated successfully. Current function value: 0.693147 Iterations 1 Results: Logit ============================================================== Model: Logit Pseudo R-squared: 0.000 Dependent Variable: y AIC: 9.5452 Date: 2020-10-03 07:41 BIC: 8.3178 No. Observations: 4 Log-Likelihood: -2.7726 Df Model: 1 LL-Null: -2.7726 Df Residuals: 2 LLR p-value: 1.0000 Converged: 1.0000 Scale: 1.0000 No. Iterations: 1.0000 ----------------------------------------------------------------- Coef. Std.Err. z P>|z| [0.025 0.975] ----------------------------------------------------------------- x1 0.0000 1.6330 0.0000 1.0000 -3.2006 3.2006 x2 0.0000 1.6330 0.0000 1.0000 -3.2006 3.2006 ============================================================== predictions = model.predict(X) predictions array([0, 0, 0, 0])","title":"Gradient Descent"},{"location":"MSBD5012/lectures/Lecture3/#batch-gradient-descent","text":"https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/#:~:text=Batch%20gradient%20descent%20is%20a,is%20called%20a%20training%20epoch. \\[w_j \\leftarrow w_j + \\alpha \\frac{1}{N}\\sum^{N}_{i=1}[y_i - \\sigma (w^Tx_i)]x_{i,j} \\] \\(y_i\\) : Observed output \\(\\sigma (w^Tx_i)\\) Predicted output \\(x_{i, j}\\) input import math import numpy as np def sigmoid ( x ): return 1 / ( 1 + math . exp ( - x )) a = 0.1 x_1 = [ 1, 1, 1, 1 ] x_2 = [ 0, 0, 1, 1 ] x_3 = [ 0, 1, 0 ,1 ] w_1 = - 2 w_2 = 1 w_3 = 1 y = [ 1, 0, 0, 1 ] def compute ( x , w_i ) : sum_i = 0 n = len ( x ) for i in range ( n ) : predicted_output = w_1 + w_2 * x_2 [ i ] + w_3 * x_3 [ i ] temp = ( y [ i ] - sigmoid ( predicted_output )) * x [ i ] sum_i += temp result = w_i + a * sum_i / n return result print ( compute ( x_1 , w_1 )) print ( compute ( x_2 , w_2 )) print ( compute ( x_3 , w_3 )) -1.9789271441190528 1.00577646446575 1.00577646446575 for i in range ( 100 ): w_1 = compute ( x_1 , w_1 ) w_2 = compute ( x_2 , w_2 ) w_3 = compute ( x_3 , w_3 ) err1 = w_1 + w_2 * x_2 [ 0 ] + w_3 * x_3 [ 0 ] err2 = w_1 + w_2 * x_2 [ 1 ] + w_3 * x_3 [ 1 ] err3 = w_1 + w_2 * x_2 [ 2 ] + w_3 * x_3 [ 2 ] err4 = w_1 + w_2 * x_2 [ 3 ] + w_3 * x_3 [ 3 ] ( err1 + err2 + err3 + err4 ) / 4 -0.07952691518172225","title":"Batch gradient descent"},{"location":"MSBD5012/lectures/Lecture3/#stochastic-gradient-descent","text":"https://en.wikipedia.org/wiki/Stochastic_gradient_descent","title":"Stochastic Gradient descent"},{"location":"MSBD5012/lectures/Lecture3/#newtons-method","text":"","title":"Newton's method"},{"location":"MSBD5012/lectures/Lecture3/#adavantages","text":"If we only do the 1st order, then we will get overestimate, but with second order estimate, we will correct the overestimate.","title":"Adavantages"},{"location":"MSBD5012/lectures/Lecture3/#softmax-regression","text":"","title":"Softmax Regression"},{"location":"MSBD5012/lectures/Lecture4/","text":"Empirecal Error \u00b6 Generalization Error \u00b6","title":"Lecture4"},{"location":"MSBD5012/lectures/Lecture4/#empirecal-error","text":"","title":"Empirecal Error"},{"location":"MSBD5012/lectures/Lecture4/#generalization-error","text":"","title":"Generalization Error"},{"location":"MSBD5012/project/LSTM/","text":"import requests import tensorflow as tf import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import mean_absolute_error , mean_squared_error import numpy as np from tensorflow import keras import os import pandas as pd from sklearn.preprocessing import MinMaxScaler , RobustScaler import requests from datetime import datetime import tensorflow as tf import seaborn as sns import matplotlib.pyplot as plt from sklearn.pipeline import Pipeline from sklearn.preprocessing import LabelBinarizer , OneHotEncoder , StandardScaler from sklearn.compose import ColumnTransformer from sklearn.model_selection import train_test_split from sklearn.model_selection import TimeSeriesSplit from sklearn.model_selection import GridSearchCV from sklearn import svm from xgboost import XGBRegressor from sklearn.ensemble import RandomForestRegressor import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.metrics import MeanSquaredError from tensorflow.keras import regularizers import numpy as np from sklearn.metrics import mean_squared_error gpu_info = !nvidia-smi gpu_info = '\\n'.join(gpu_info) if gpu_info.find('failed') >= 0: print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ') print('and then re-execute this cell.') else: print(gpu_info) Fri Dec 4 11:37:50 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 455.38 Driver Version: 418.67 CUDA Version: 10.1 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla V100-SXM2... Off | 00000000:00:04.0 Off | 0 | | N/A 43C P0 26W / 300W | 0MiB / 16130MiB | 0% Default | | | | ERR! | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ set up tpu Load data \u00b6 url = \"https://query1.finance.yahoo.com/v7/finance/download/000001.SS?period1=867801600&period2=1606953600&interval=1d&events=history&includeAdjustedClose=true\" # url = 'https://query1.finance.yahoo.com/v7/finance/download/688981.SS?period1=1594857600&period2=1607040000&interval=1d&events=history&includeAdjustedClose=true' btc = pd.read_csv(url) btc['Date'] = pd.to_datetime(btc['Date']) btc.set_index('Date', inplace=True) btc.tail(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Open High Low Close Adj Close Volume Date 2020-12-01 3388.989990 3457.639893 3386.909912 3451.939941 3451.939941 316200.0 2020-12-02 3453.518066 3465.729004 3435.871094 3449.381104 3449.381104 312800.0 plt.figure(figsize=(15, 5)) plt.plot(btc.Close) plt.xlabel ('Date_time') plt.ylabel ('Price') Text(0, 0.5, 'Price') Preprocess data \u00b6 scaler = MinMaxScaler () # min - max normalization and scale the features in the 0 - 1 range . close_price = btc [ 'Close' ]. values . reshape ( - 1 , 1 ) # The scaler expects the data to be shaped as ( x , y ) scaled_close = scaler . fit_transform ( close_price ) # removing NaNs ( if any ) scaled_close = scaled_close [ ~ np . isnan ( scaled_close )] # reshaping data after removing NaNs scaled_close = scaled_close . reshape ( - 1 , 1 ) print ( scaled_close . shape ) (5842, 1) We will use past 150 days datato predict future 3 days price We will implement a window generator to generate a series of data shown in this picture. SEQ_LEN = 10 OFFSET = 3 class WindowGenerator () : def __init__ ( self , input_width , offset , data , train_split ) : self . data = data self . input_width = input_width self . offset = offset self . train_split = train_split def to_sequences ( self ) : \"\"\" Return both data and label \"\"\" data_len = len ( self . data ) ret = [] ret_label = [] for i in range ( data_len - self . offset - self . input_width + 1 ) : tmp = self . data [ i : i + self . input_width ] tmp_label = self . data [ i + self . input_width + self . offset - 1 ] ret . append ( tmp ) ret_label . append ( tmp_label ) return np . array ( ret ), np . array ( ret_label ) def split ( self ) : x , y = self . to_sequences () num_train = int (( 1 - self . train_split ) * x . shape [ 0 ]) X_train = x [ : num_train ] y_train = y [ : num_train ] X_test = x [ num_train :] y_test = y [ num_train :] return X_train , y_train , X_test , y_test wg = WindowGenerator ( data = scaled_close , input_width = SEQ_LEN , offset = OFFSET , train_split = 0.1 ) X_train , y_train , X_test , y_test = wg . split () print ( X_train . shape , X_test . shape ) print ( y_train . shape , y_test . shape ) (5247, 10, 1) (583, 10, 1) (5247, 1) (583, 1) Training \u00b6 Build model \u00b6 from tensorflow.keras.layers import Bidirectional , Dropout , LSTM , Dense , Activation DROPOUT = 0 . 2 # 20 % Dropout is used to control over - fitting during training WINDOW_SIZE = SEQ_LEN model = keras . Sequential () model . add ( Bidirectional ( LSTM ( WINDOW_SIZE , return_sequences = True ), input_shape = ( WINDOW_SIZE , X_train . shape [ - 1 ]))) \"\"\"Bidirectional RNNs allows to train on the sequence data in forward and backward direction.\"\"\" model . add ( Dropout ( rate = DROPOUT )) model . add ( Bidirectional ( LSTM (( WINDOW_SIZE * 2 ), return_sequences = True ))) model . add ( Dropout ( rate = DROPOUT )) model . add ( Bidirectional ( LSTM (( WINDOW_SIZE * 2 ), return_sequences = True ))) model . add ( Dropout ( rate = DROPOUT )) model . add ( Bidirectional ( LSTM ( WINDOW_SIZE , return_sequences = False ))) # output layer model . add ( Dense ( units = 1 )) model . add ( Activation ( 'linear' )) \"\"\"Output layer has a single neuron (predicted Bitcoin price). We use Linear activation function which activation is proportional to the input.\"\"\" BATCH_SIZE = 64 model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' , metrics = [ 'mae' , 'mse' ]) model . summary () Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= bidirectional (Bidirectional (None, 10, 20) 960 _________________________________________________________________ dropout (Dropout) (None, 10, 20) 0 _________________________________________________________________ bidirectional_1 (Bidirection (None, 10, 40) 6560 _________________________________________________________________ dropout_1 (Dropout) (None, 10, 40) 0 _________________________________________________________________ bidirectional_2 (Bidirection (None, 10, 40) 9760 _________________________________________________________________ dropout_2 (Dropout) (None, 10, 40) 0 _________________________________________________________________ bidirectional_3 (Bidirection (None, 20) 4080 _________________________________________________________________ dense (Dense) (None, 1) 21 _________________________________________________________________ activation (Activation) (None, 1) 0 ================================================================= Total params: 21,381 Trainable params: 21,381 Non-trainable params: 0 _________________________________________________________________ Train model \u00b6 def plot_history ( history ) : hist = pd . DataFrame ( history . history ) hist [ 'epoch' ] = history . epoch plt . figure () plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Mean Abs Error [MPG]' ) plt . plot ( hist [ 'epoch' ] , hist [ 'mae' ] , label = 'Train Error' ) plt . plot ( hist [ 'epoch' ] , hist [ 'val_mae' ] , label = 'Val Error' ) plt . legend () plt . figure () plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Mean Square Error [$MPG^2$]' ) plt . plot ( hist [ 'epoch' ] , hist [ 'mse' ] , label = 'Train Error' ) plt . plot ( hist [ 'epoch' ] , hist [ 'val_mse' ] , label = 'Val Error' ) plt . legend () plt . show () early_stop = keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 20 ) history = model . fit ( X_train , y_train , epochs = 1000 , batch_size = BATCH_SIZE , shuffle = False , validation_split = 0.1 , callbacks =[ early_stop ] ) plot_history ( history ) Epoch 1/1000 74/74 [==============================] - 4s 50ms/step - loss: 0.0217 - mae: 0.0902 - mse: 0.0217 - val_loss: 0.0069 - val_mae: 0.0810 - val_mse: 0.0069 Epoch 2/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0048 - mae: 0.0508 - mse: 0.0048 - val_loss: 8.0070e-04 - val_mae: 0.0254 - val_mse: 8.0070e-04 Epoch 3/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0015 - mae: 0.0279 - mse: 0.0015 - val_loss: 1.8564e-04 - val_mae: 0.0097 - val_mse: 1.8564e-04 Epoch 4/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0010 - mae: 0.0213 - mse: 0.0010 - val_loss: 3.7053e-04 - val_mae: 0.0154 - val_mse: 3.7053e-04 Epoch 5/1000 74/74 [==============================] - 1s 18ms/step - loss: 9.8986e-04 - mae: 0.0198 - mse: 9.8986e-04 - val_loss: 3.3315e-04 - val_mae: 0.0143 - val_mse: 3.3315e-04 Epoch 6/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0013 - mae: 0.0237 - mse: 0.0013 - val_loss: 1.5304e-04 - val_mae: 0.0093 - val_mse: 1.5304e-04 Epoch 7/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0024 - mae: 0.0344 - mse: 0.0024 - val_loss: 1.6594e-04 - val_mae: 0.0102 - val_mse: 1.6594e-04 Epoch 8/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0033 - mae: 0.0421 - mse: 0.0033 - val_loss: 2.6209e-04 - val_mae: 0.0121 - val_mse: 2.6209e-04 Epoch 9/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0028 - mae: 0.0396 - mse: 0.0028 - val_loss: 2.6527e-04 - val_mae: 0.0122 - val_mse: 2.6527e-04 Epoch 10/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0024 - mae: 0.0360 - mse: 0.0024 - val_loss: 2.3552e-04 - val_mae: 0.0112 - val_mse: 2.3552e-04 Epoch 11/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0025 - mae: 0.0365 - mse: 0.0025 - val_loss: 5.8297e-04 - val_mae: 0.0209 - val_mse: 5.8297e-04 Epoch 12/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0018 - mae: 0.0309 - mse: 0.0018 - val_loss: 2.9134e-04 - val_mae: 0.0131 - val_mse: 2.9134e-04 Epoch 13/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0016 - mae: 0.0282 - mse: 0.0016 - val_loss: 2.2296e-04 - val_mae: 0.0109 - val_mse: 2.2296e-04 Epoch 14/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0016 - mae: 0.0288 - mse: 0.0016 - val_loss: 2.5500e-04 - val_mae: 0.0119 - val_mse: 2.5500e-04 Epoch 15/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0019 - mae: 0.0312 - mse: 0.0019 - val_loss: 2.2766e-04 - val_mae: 0.0110 - val_mse: 2.2766e-04 Epoch 16/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0018 - mae: 0.0310 - mse: 0.0018 - val_loss: 2.1390e-04 - val_mae: 0.0106 - val_mse: 2.1390e-04 Epoch 17/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0019 - mae: 0.0319 - mse: 0.0019 - val_loss: 2.7293e-04 - val_mae: 0.0125 - val_mse: 2.7293e-04 Epoch 18/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0016 - mae: 0.0298 - mse: 0.0016 - val_loss: 2.6761e-04 - val_mae: 0.0123 - val_mse: 2.6761e-04 Epoch 19/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0014 - mae: 0.0272 - mse: 0.0014 - val_loss: 2.4100e-04 - val_mae: 0.0115 - val_mse: 2.4100e-04 Epoch 20/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0013 - mae: 0.0264 - mse: 0.0013 - val_loss: 2.9087e-04 - val_mae: 0.0131 - val_mse: 2.9087e-04 Epoch 21/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0011 - mae: 0.0229 - mse: 0.0011 - val_loss: 2.8196e-04 - val_mae: 0.0128 - val_mse: 2.8196e-04 Epoch 22/1000 74/74 [==============================] - 1s 18ms/step - loss: 8.4853e-04 - mae: 0.0198 - mse: 8.4853e-04 - val_loss: 3.0520e-04 - val_mae: 0.0136 - val_mse: 3.0520e-04 Epoch 23/1000 74/74 [==============================] - 1s 18ms/step - loss: 8.2679e-04 - mae: 0.0191 - mse: 8.2679e-04 - val_loss: 2.3351e-04 - val_mae: 0.0113 - val_mse: 2.3351e-04 Epoch 24/1000 74/74 [==============================] - 1s 18ms/step - loss: 8.5238e-04 - mae: 0.0192 - mse: 8.5238e-04 - val_loss: 2.4551e-04 - val_mae: 0.0117 - val_mse: 2.4551e-04 Epoch 25/1000 74/74 [==============================] - 1s 18ms/step - loss: 9.0385e-04 - mae: 0.0202 - mse: 9.0385e-04 - val_loss: 2.1145e-04 - val_mae: 0.0106 - val_mse: 2.1145e-04 Epoch 26/1000 74/74 [==============================] - 1s 18ms/step - loss: 9.6579e-04 - mae: 0.0210 - mse: 9.6579e-04 - val_loss: 1.9588e-04 - val_mae: 0.0101 - val_mse: 1.9588e-04 Testing \u00b6 # prediction on test data y_pred = model.predict(X_test) # invert the test to original values y_test_inverse = pd.DataFrame(scaler.inverse_transform(y_test)) # assigning datetime y_test_inverse.index = btc.index[-len(y_test):] print('Test data:',) print(y_test_inverse.tail(3)); print(); # invert the prediction to understandable values y_pred_inverse = pd.DataFrame(scaler.inverse_transform(y_pred)) # assigning datetime y_pred_inverse.index = y_test_inverse.index print('Prediction data:',) print(y_pred_inverse.tail(3)) # print(y_train.shape) Test data: 0 Date 2020-11-30 3391.760010 2020-12-01 3451.939941 2020-12-02 3449.381104 Prediction data: 0 Date 2020-11-30 3392.729492 2020-12-01 3397.149170 2020-12-02 3404.069824 print(f'MAE {mean_absolute_error(y_test, y_pred)}') print(f'MSE {mean_squared_error(y_test, y_pred)}') print(f'RMSE {np.sqrt(mean_squared_error(y_test, y_pred))}') MAE 0.013780549753230727 MSE 0.0003602061344071959 RMSE 0.018979097302221616 plt.figure(figsize = (30,5)) plt.plot(y_test_inverse) plt.plot(y_pred_inverse) plt.title('Actual vs Prediction plot (Price prediction model)') plt.ylabel('price') plt.xlabel('date') plt.legend(['actual', 'prediction'], loc='upper left') plt.show() today_price = scaled_close[-SEQ_LEN:] today_price = np.expand_dims(today_price, axis=0) print(today_price.shape, X_test.shape) tmr_prediction = model.predict(today_price) scaler.inverse_transform(tmr_prediction)[0][0] (1, 10, 1) (583, 10, 1) 3425.282 LSTM with sentiment score \u00b6 By only using previous stock data to predict future price may cause in accurate future predictions, so we want to use the power of the social media. In this section, we will try to analyze the future stock data in two directions: Use lstm to predict future sentiment score to predict stock price. We will modify the model above to add one Analyze sentiment \u00b6 from datetime import datetime sentiment_files_path = '/content/drive/MyDrive/courses/HKUST/MSBD5012/project/sentiments.csv' def parse_time ( time ): try : return datetime . strptime ( time , '%Y-%m-%d %H:%M:%S.%f+00' ) except Exception : return None sentiments = pd . read_csv ( sentiment_files_path ) sentiments . info () sentiments [ 'Time' ] = sentiments [ 'Time' ]. apply ( parse_time ) sentiments = sentiments . dropna () <class 'pandas.core.frame.DataFrame'> RangeIndex: 614836 entries, 0 to 614835 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Unnamed: 0 614836 non-null int64 1 Keyword 614836 non-null object 2 content 614453 non-null object 3 Time 614832 non-null object 4 Rank 614832 non-null float64 5 Number 614828 non-null float64 6 sentiments 612742 non-null float64 dtypes: float64(3), int64(1), object(3) memory usage: 32.8+ MB sentiments['Date'] = sentiments['Time'].apply(lambda date: date.date()) grouped = sentiments[['Date', 'sentiments']].groupby('Date').mean() grouped = grouped.reset_index() grouped .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date sentiments 0 2020-11-14 0.776240 1 2020-11-15 0.752614 2 2020-11-16 0.740375 3 2020-11-17 0.739457 4 2020-11-18 0.693168 5 2020-11-19 0.719513 6 2020-11-20 0.735112 7 2020-11-21 0.738899 8 2020-11-22 0.676612 9 2020-11-23 0.696438 10 2020-11-24 0.690481 11 2020-11-25 0.729346 12 2020-11-26 0.747991 13 2020-11-27 0.751125 14 2020-11-28 0.749364 15 2020-11-29 0.729915 16 2020-11-30 0.697641 17 2020-12-01 0.736729 18 2020-12-02 0.749541 19 2020-12-03 0.733653 20 2020-12-04 0.710298 Daily sentiment score with stock score \u00b6 data = btc . Close . loc [ '2020-11-14' :] data = data . reset_index () data [ 'Date' ] = data [ 'Date' ]. apply ( lambda x : x . date ()) data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Close 0 2020-11-16 3346.968994 1 2020-11-17 3339.899902 2 2020-11-18 3347.302979 3 2020-11-19 3363.087891 4 2020-11-20 3377.729980 5 2020-11-23 3414.489990 6 2020-11-24 3402.822998 7 2020-11-25 3362.326904 8 2020-11-26 3369.732910 9 2020-11-27 3408.306885 10 2020-11-30 3391.760010 11 2020-12-01 3451.939941 12 2020-12-02 3449.381104 plt.figure(figsize=(20, 10)) plt.plot(data['Date'], data['Close']) plt.title('Stock price') plt.xlabel('Date') plt.ylabel('price') plt.legend() plt.savefig('Stock price.png') No handles with labels found to put in legend. plt.figure(figsize=(20, 10)) plt.plot(grouped['Date'], grouped['sentiments']) plt.title('Sentiment score') plt.xlabel('Date') plt.ylabel('Sentiments') plt.legend() plt.savefig('Sentiment-score.png') No handles with labels found to put in legend. Merge data with sentiment \u00b6 merged = grouped.merge(data, on='Date', how='inner') merged # def generate_training_data ( data : pd . DataFrame , prediction_label , cat_vars =[ 'id', 'IsWeekend','IsHoliday','Hour modify', 'Weather' ] , # num_vars =[ 'Temperature', 'Pressure', 'Humidity', 'Cloud', 'Wind degree' ] , # should_reshape = True , should_split = True ) : # x = data . copy () # y = x [ prediction_label ] . to_list () # y = np . array ( y ) # numeric_transformer = Pipeline ( steps =[ # ('scaler', RobustScaler()) ] ) # categorical_transformer = Pipeline ( steps =[ # ('oneHot',OneHotEncoder(sparse=False)) ] ) # preprocessor = ColumnTransformer ( transformers =[ # ('num',numeric_transformer,num_vars), # ('cat',categorical_transformer,cat_vars) ] ) # data_transformed = preprocessor . fit_transform ( x ) # if should_split : # if should_reshape : # y = y . reshape ( - 1 , 1 ) # scaler = MinMaxScaler () # scaled_y = scaler . fit_transform ( y ) # return train_test_split ( data_transformed , scaled_y , test_size = 0.02 , random_state = 42 ), scaler # else : # return train_test_split ( data_transformed , y , test_size = 0.02 , random_state = 42 ) # else : # return data_transformed , y scaler = MinMaxScaler () scaler2 = MinMaxScaler () x_data = merged [ ['Close', 'sentiments' ] ] . to_numpy () y_data = merged [ ['Close' ] ] . to_numpy () scaled_x = scaler . fit_transform ( x_data ) scaled_y = scaler2 . fit_transform ( y_data ) print ( scaled_x ) print ( scaled_y ) SEQ_LEN = 3 OFFSET = 0 wg = WindowGenerator(data=scaled_x, input_width=SEQ_LEN, offset=OFFSET, train_split=0.1) wg2 = WindowGenerator(data=scaled_y, input_width=SEQ_LEN, offset=OFFSET, train_split=0.1) X_train, _, X_test, _ = wg.split() _, y_train, _, y_test = wg2.split() print(X_train.shape, X_test.shape) print(y_train.shape, y_test.shape) (9, 3, 2) (2, 3, 2) (9, 1) (2, 1) Stock price with sentiment \u00b6 print(X_train[0], y_train[0]) [[0.06309434 0.82273844] [0. 0.80759869] [0.06607528 0.04430082]] [0.06607528] DROPOUT = 0 . 2 # 20 % Dropout is used to control over - fitting during training WINDOW_SIZE = SEQ_LEN model = keras . Sequential () model . add ( Bidirectional ( LSTM ( WINDOW_SIZE , return_sequences = True ), input_shape = ( WINDOW_SIZE , X_train . shape [ - 1 ]))) \"\"\"Bidirectional RNNs allows to train on the sequence data in forward and backward direction.\"\"\" model . add ( Dropout ( rate = DROPOUT )) model . add ( Bidirectional ( LSTM (( WINDOW_SIZE * 2 ), return_sequences = True ))) model . add ( Dropout ( rate = DROPOUT )) model . add ( Bidirectional ( LSTM (( WINDOW_SIZE * 2 ), return_sequences = True ))) model . add ( Dropout ( rate = DROPOUT )) model . add ( Bidirectional ( LSTM ( WINDOW_SIZE , return_sequences = False ))) # output layer model . add ( Dense ( units = 1 )) model . add ( Activation ( 'linear' )) \"\"\"Output layer has a single neuron (predicted Bitcoin price). We use Linear activation function which activation is proportional to the input.\"\"\" BATCH_SIZE = 64 model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' , metrics = [ 'mae' , 'mse' ]) model . summary () Model: \"sequential_3\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= bidirectional_12 (Bidirectio (None, 3, 6) 144 _________________________________________________________________ dropout_9 (Dropout) (None, 3, 6) 0 _________________________________________________________________ bidirectional_13 (Bidirectio (None, 3, 12) 624 _________________________________________________________________ dropout_10 (Dropout) (None, 3, 12) 0 _________________________________________________________________ bidirectional_14 (Bidirectio (None, 3, 12) 912 _________________________________________________________________ dropout_11 (Dropout) (None, 3, 12) 0 _________________________________________________________________ bidirectional_15 (Bidirectio (None, 6) 384 _________________________________________________________________ dense_3 (Dense) (None, 1) 7 _________________________________________________________________ activation_3 (Activation) (None, 1) 0 ================================================================= Total params: 2,071 Trainable params: 2,071 Non-trainable params: 0 _________________________________________________________________ history = model . fit ( X_train , y_train , epochs = 1000 , callbacks =[ early_stop ] , validation_split = 0.1 , shuffle = False ) plot_history ( history ) Epoch 1/1000 1/1 [==============================] - 2s 2s/step - loss: 0.1822 - mae: 0.3731 - mse: 0.1822 - val_loss: 0.2178 - val_mae: 0.4667 - val_mse: 0.2178 Epoch 2/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.1754 - mae: 0.3638 - mse: 0.1754 - val_loss: 0.2137 - val_mae: 0.4622 - val_mse: 0.2137 Epoch 3/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.1736 - mae: 0.3615 - mse: 0.1736 - val_loss: 0.2096 - val_mae: 0.4578 - val_mse: 0.2096 Epoch 4/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.1712 - mae: 0.3580 - mse: 0.1712 - val_loss: 0.2056 - val_mae: 0.4534 - val_mse: 0.2056 Epoch 5/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.1703 - mae: 0.3556 - mse: 0.1703 - val_loss: 0.2016 - val_mae: 0.4491 - val_mse: 0.2016 Epoch 6/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.1660 - mae: 0.3508 - mse: 0.1660 - val_loss: 0.1978 - val_mae: 0.4447 - val_mse: 0.1978 Epoch 7/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.1626 - mae: 0.3451 - mse: 0.1626 - val_loss: 0.1939 - val_mae: 0.4404 - val_mse: 0.1939 Epoch 8/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.1603 - mae: 0.3422 - mse: 0.1603 - val_loss: 0.1901 - val_mae: 0.4360 - val_mse: 0.1901 Epoch 9/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.1560 - mae: 0.3359 - mse: 0.1560 - val_loss: 0.1864 - val_mae: 0.4317 - val_mse: 0.1864 Epoch 10/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.1525 - mae: 0.3312 - mse: 0.1525 - val_loss: 0.1827 - val_mae: 0.4274 - val_mse: 0.1827 Epoch 11/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.1509 - mae: 0.3283 - mse: 0.1509 - val_loss: 0.1790 - val_mae: 0.4231 - val_mse: 0.1790 Epoch 12/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.1471 - mae: 0.3230 - mse: 0.1471 - val_loss: 0.1753 - val_mae: 0.4187 - val_mse: 0.1753 Epoch 13/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.1452 - mae: 0.3193 - mse: 0.1452 - val_loss: 0.1717 - val_mae: 0.4144 - val_mse: 0.1717 Epoch 14/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.1434 - mae: 0.3162 - mse: 0.1434 - val_loss: 0.1681 - val_mae: 0.4100 - val_mse: 0.1681 Epoch 15/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.1403 - mae: 0.3122 - mse: 0.1403 - val_loss: 0.1645 - val_mae: 0.4055 - val_mse: 0.1645 Epoch 16/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.1385 - mae: 0.3083 - mse: 0.1385 - val_loss: 0.1608 - val_mae: 0.4011 - val_mse: 0.1608 Epoch 17/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.1347 - mae: 0.3035 - mse: 0.1347 - val_loss: 0.1573 - val_mae: 0.3966 - val_mse: 0.1573 Epoch 18/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.1316 - mae: 0.2992 - mse: 0.1316 - val_loss: 0.1537 - val_mae: 0.3920 - val_mse: 0.1537 Epoch 19/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.1292 - mae: 0.2956 - mse: 0.1292 - val_loss: 0.1501 - val_mae: 0.3874 - val_mse: 0.1501 Epoch 20/1000 1/1 [==============================] - 0s 23ms/step - loss: 0.1276 - mae: 0.2937 - mse: 0.1276 - val_loss: 0.1465 - val_mae: 0.3827 - val_mse: 0.1465 Epoch 21/1000 1/1 [==============================] - 0s 22ms/step - loss: 0.1239 - mae: 0.2895 - mse: 0.1239 - val_loss: 0.1429 - val_mae: 0.3780 - val_mse: 0.1429 Epoch 22/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.1197 - mae: 0.2835 - mse: 0.1197 - val_loss: 0.1393 - val_mae: 0.3732 - val_mse: 0.1393 Epoch 23/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.1193 - mae: 0.2826 - mse: 0.1193 - val_loss: 0.1357 - val_mae: 0.3683 - val_mse: 0.1357 Epoch 24/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.1162 - mae: 0.2800 - mse: 0.1162 - val_loss: 0.1320 - val_mae: 0.3634 - val_mse: 0.1320 Epoch 25/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.1147 - mae: 0.2756 - mse: 0.1147 - val_loss: 0.1284 - val_mae: 0.3584 - val_mse: 0.1284 Epoch 26/1000 1/1 [==============================] - 0s 22ms/step - loss: 0.1107 - mae: 0.2706 - mse: 0.1107 - val_loss: 0.1248 - val_mae: 0.3532 - val_mse: 0.1248 Epoch 27/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.1097 - mae: 0.2680 - mse: 0.1097 - val_loss: 0.1211 - val_mae: 0.3480 - val_mse: 0.1211 Epoch 28/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.1064 - mae: 0.2637 - mse: 0.1064 - val_loss: 0.1175 - val_mae: 0.3427 - val_mse: 0.1175 Epoch 29/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.1040 - mae: 0.2609 - mse: 0.1040 - val_loss: 0.1138 - val_mae: 0.3373 - val_mse: 0.1138 Epoch 30/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.1013 - mae: 0.2577 - mse: 0.1013 - val_loss: 0.1101 - val_mae: 0.3318 - val_mse: 0.1101 Epoch 31/1000 1/1 [==============================] - 0s 22ms/step - loss: 0.0987 - mae: 0.2512 - mse: 0.0987 - val_loss: 0.1064 - val_mae: 0.3262 - val_mse: 0.1064 Epoch 32/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0955 - mae: 0.2472 - mse: 0.0955 - val_loss: 0.1027 - val_mae: 0.3204 - val_mse: 0.1027 Epoch 33/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0935 - mae: 0.2458 - mse: 0.0935 - val_loss: 0.0990 - val_mae: 0.3146 - val_mse: 0.0990 Epoch 34/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0905 - mae: 0.2401 - mse: 0.0905 - val_loss: 0.0952 - val_mae: 0.3086 - val_mse: 0.0952 Epoch 35/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.0880 - mae: 0.2344 - mse: 0.0880 - val_loss: 0.0915 - val_mae: 0.3024 - val_mse: 0.0915 Epoch 36/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0832 - mae: 0.2298 - mse: 0.0832 - val_loss: 0.0877 - val_mae: 0.2961 - val_mse: 0.0877 Epoch 37/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0815 - mae: 0.2255 - mse: 0.0815 - val_loss: 0.0839 - val_mae: 0.2896 - val_mse: 0.0839 Epoch 38/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.0814 - mae: 0.2191 - mse: 0.0814 - val_loss: 0.0801 - val_mae: 0.2830 - val_mse: 0.0801 Epoch 39/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0789 - mae: 0.2204 - mse: 0.0789 - val_loss: 0.0763 - val_mae: 0.2762 - val_mse: 0.0763 Epoch 40/1000 1/1 [==============================] - 0s 22ms/step - loss: 0.0765 - mae: 0.2151 - mse: 0.0765 - val_loss: 0.0725 - val_mae: 0.2693 - val_mse: 0.0725 Epoch 41/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.0752 - mae: 0.2110 - mse: 0.0752 - val_loss: 0.0687 - val_mae: 0.2622 - val_mse: 0.0687 Epoch 42/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0695 - mae: 0.2038 - mse: 0.0695 - val_loss: 0.0650 - val_mae: 0.2549 - val_mse: 0.0650 Epoch 43/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.0680 - mae: 0.1957 - mse: 0.0680 - val_loss: 0.0612 - val_mae: 0.2474 - val_mse: 0.0612 Epoch 44/1000 1/1 [==============================] - 0s 23ms/step - loss: 0.0663 - mae: 0.1903 - mse: 0.0663 - val_loss: 0.0575 - val_mae: 0.2397 - val_mse: 0.0575 Epoch 45/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0633 - mae: 0.1913 - mse: 0.0633 - val_loss: 0.0538 - val_mae: 0.2319 - val_mse: 0.0538 Epoch 46/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.0592 - mae: 0.1857 - mse: 0.0592 - val_loss: 0.0501 - val_mae: 0.2238 - val_mse: 0.0501 Epoch 47/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0593 - mae: 0.1924 - mse: 0.0593 - val_loss: 0.0464 - val_mae: 0.2155 - val_mse: 0.0464 Epoch 48/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0591 - mae: 0.1830 - mse: 0.0591 - val_loss: 0.0429 - val_mae: 0.2071 - val_mse: 0.0429 Epoch 49/1000 1/1 [==============================] - 0s 22ms/step - loss: 0.0550 - mae: 0.1830 - mse: 0.0550 - val_loss: 0.0394 - val_mae: 0.1984 - val_mse: 0.0394 Epoch 50/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0585 - mae: 0.1908 - mse: 0.0585 - val_loss: 0.0360 - val_mae: 0.1897 - val_mse: 0.0360 Epoch 51/1000 1/1 [==============================] - 0s 22ms/step - loss: 0.0468 - mae: 0.1731 - mse: 0.0468 - val_loss: 0.0327 - val_mae: 0.1807 - val_mse: 0.0327 Epoch 52/1000 1/1 [==============================] - 0s 23ms/step - loss: 0.0503 - mae: 0.1774 - mse: 0.0503 - val_loss: 0.0294 - val_mae: 0.1716 - val_mse: 0.0294 Epoch 53/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0486 - mae: 0.1786 - mse: 0.0486 - val_loss: 0.0264 - val_mae: 0.1624 - val_mse: 0.0264 Epoch 54/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0489 - mae: 0.1787 - mse: 0.0489 - val_loss: 0.0234 - val_mae: 0.1531 - val_mse: 0.0234 Epoch 55/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0418 - mae: 0.1661 - mse: 0.0418 - val_loss: 0.0206 - val_mae: 0.1436 - val_mse: 0.0206 Epoch 56/1000 1/1 [==============================] - 0s 22ms/step - loss: 0.0433 - mae: 0.1756 - mse: 0.0433 - val_loss: 0.0180 - val_mae: 0.1340 - val_mse: 0.0180 Epoch 57/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.0428 - mae: 0.1741 - mse: 0.0428 - val_loss: 0.0155 - val_mae: 0.1245 - val_mse: 0.0155 Epoch 58/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0496 - mae: 0.1857 - mse: 0.0496 - val_loss: 0.0132 - val_mae: 0.1151 - val_mse: 0.0132 Epoch 59/1000 1/1 [==============================] - 0s 23ms/step - loss: 0.0456 - mae: 0.1820 - mse: 0.0456 - val_loss: 0.0112 - val_mae: 0.1060 - val_mse: 0.0112 Epoch 60/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.0479 - mae: 0.1908 - mse: 0.0479 - val_loss: 0.0094 - val_mae: 0.0971 - val_mse: 0.0094 Epoch 61/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0367 - mae: 0.1757 - mse: 0.0367 - val_loss: 0.0078 - val_mae: 0.0886 - val_mse: 0.0078 Epoch 62/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0376 - mae: 0.1741 - mse: 0.0376 - val_loss: 0.0065 - val_mae: 0.0804 - val_mse: 0.0065 Epoch 63/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.0399 - mae: 0.1775 - mse: 0.0399 - val_loss: 0.0052 - val_mae: 0.0724 - val_mse: 0.0052 Epoch 64/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0372 - mae: 0.1706 - mse: 0.0372 - val_loss: 0.0042 - val_mae: 0.0646 - val_mse: 0.0042 Epoch 65/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.0389 - mae: 0.1749 - mse: 0.0389 - val_loss: 0.0033 - val_mae: 0.0573 - val_mse: 0.0033 Epoch 66/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0456 - mae: 0.1948 - mse: 0.0456 - val_loss: 0.0026 - val_mae: 0.0512 - val_mse: 0.0026 Epoch 67/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.0409 - mae: 0.1847 - mse: 0.0409 - val_loss: 0.0021 - val_mae: 0.0461 - val_mse: 0.0021 Epoch 68/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0513 - mae: 0.2031 - mse: 0.0513 - val_loss: 0.0018 - val_mae: 0.0424 - val_mse: 0.0018 Epoch 69/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0383 - mae: 0.1842 - mse: 0.0383 - val_loss: 0.0016 - val_mae: 0.0394 - val_mse: 0.0016 Epoch 70/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0407 - mae: 0.1861 - mse: 0.0407 - val_loss: 0.0014 - val_mae: 0.0375 - val_mse: 0.0014 Epoch 71/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.0385 - mae: 0.1808 - mse: 0.0385 - val_loss: 0.0013 - val_mae: 0.0359 - val_mse: 0.0013 Epoch 72/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0351 - mae: 0.1763 - mse: 0.0351 - val_loss: 0.0012 - val_mae: 0.0344 - val_mse: 0.0012 Epoch 73/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0474 - mae: 0.2025 - mse: 0.0474 - val_loss: 0.0012 - val_mae: 0.0340 - val_mse: 0.0012 Epoch 74/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0458 - mae: 0.2015 - mse: 0.0458 - val_loss: 0.0012 - val_mae: 0.0343 - val_mse: 0.0012 Epoch 75/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0348 - mae: 0.1715 - mse: 0.0348 - val_loss: 0.0012 - val_mae: 0.0350 - val_mse: 0.0012 Epoch 76/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.0457 - mae: 0.2017 - mse: 0.0457 - val_loss: 0.0013 - val_mae: 0.0365 - val_mse: 0.0013 Epoch 77/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0430 - mae: 0.1870 - mse: 0.0430 - val_loss: 0.0015 - val_mae: 0.0386 - val_mse: 0.0015 Epoch 78/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0448 - mae: 0.1896 - mse: 0.0448 - val_loss: 0.0017 - val_mae: 0.0413 - val_mse: 0.0017 Epoch 79/1000 1/1 [==============================] - 0s 23ms/step - loss: 0.0377 - mae: 0.1783 - mse: 0.0377 - val_loss: 0.0019 - val_mae: 0.0438 - val_mse: 0.0019 Epoch 80/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0526 - mae: 0.2150 - mse: 0.0526 - val_loss: 0.0022 - val_mae: 0.0470 - val_mse: 0.0022 Epoch 81/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.0331 - mae: 0.1608 - mse: 0.0331 - val_loss: 0.0025 - val_mae: 0.0496 - val_mse: 0.0025 Epoch 82/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0335 - mae: 0.1673 - mse: 0.0335 - val_loss: 0.0027 - val_mae: 0.0516 - val_mse: 0.0027 Epoch 83/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0365 - mae: 0.1739 - mse: 0.0365 - val_loss: 0.0029 - val_mae: 0.0534 - val_mse: 0.0029 Epoch 84/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0388 - mae: 0.1824 - mse: 0.0388 - val_loss: 0.0030 - val_mae: 0.0551 - val_mse: 0.0030 Epoch 85/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0411 - mae: 0.1787 - mse: 0.0411 - val_loss: 0.0033 - val_mae: 0.0571 - val_mse: 0.0033 Epoch 86/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.0360 - mae: 0.1747 - mse: 0.0360 - val_loss: 0.0035 - val_mae: 0.0589 - val_mse: 0.0035 Epoch 87/1000 1/1 [==============================] - 0s 22ms/step - loss: 0.0378 - mae: 0.1776 - mse: 0.0378 - val_loss: 0.0037 - val_mae: 0.0605 - val_mse: 0.0037 Epoch 88/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0452 - mae: 0.1952 - mse: 0.0452 - val_loss: 0.0039 - val_mae: 0.0622 - val_mse: 0.0039 Epoch 89/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0400 - mae: 0.1828 - mse: 0.0400 - val_loss: 0.0041 - val_mae: 0.0641 - val_mse: 0.0041 Epoch 90/1000 1/1 [==============================] - 0s 17ms/step - loss: 0.0378 - mae: 0.1725 - mse: 0.0378 - val_loss: 0.0043 - val_mae: 0.0658 - val_mse: 0.0043 Epoch 91/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0350 - mae: 0.1635 - mse: 0.0350 - val_loss: 0.0045 - val_mae: 0.0669 - val_mse: 0.0045 Epoch 92/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.0353 - mae: 0.1715 - mse: 0.0353 - val_loss: 0.0046 - val_mae: 0.0678 - val_mse: 0.0046 Epoch 93/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0410 - mae: 0.1849 - mse: 0.0410 - val_loss: 0.0047 - val_mae: 0.0686 - val_mse: 0.0047 pred_data = model.predict(X_test) py = scaler2.inverse_transform(pred_data) ty = scaler2.inverse_transform(y_test) plt.plot(py, label='Predict') plt.plot(ty, label='True') plt.legend() <matplotlib.legend.Legend at 0x7ff2d4a6ecf8> Conclusion \u00b6 Due to the current volume of data, we may not be able to use sentiment data to train the model. However, according to the observation, we found that social media's attitude will impact the stock data which proved the original paper's idea.","title":"LSTM"},{"location":"MSBD5012/project/LSTM/#load-data","text":"url = \"https://query1.finance.yahoo.com/v7/finance/download/000001.SS?period1=867801600&period2=1606953600&interval=1d&events=history&includeAdjustedClose=true\" # url = 'https://query1.finance.yahoo.com/v7/finance/download/688981.SS?period1=1594857600&period2=1607040000&interval=1d&events=history&includeAdjustedClose=true' btc = pd.read_csv(url) btc['Date'] = pd.to_datetime(btc['Date']) btc.set_index('Date', inplace=True) btc.tail(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Open High Low Close Adj Close Volume Date 2020-12-01 3388.989990 3457.639893 3386.909912 3451.939941 3451.939941 316200.0 2020-12-02 3453.518066 3465.729004 3435.871094 3449.381104 3449.381104 312800.0 plt.figure(figsize=(15, 5)) plt.plot(btc.Close) plt.xlabel ('Date_time') plt.ylabel ('Price') Text(0, 0.5, 'Price')","title":"Load data"},{"location":"MSBD5012/project/LSTM/#preprocess-data","text":"scaler = MinMaxScaler () # min - max normalization and scale the features in the 0 - 1 range . close_price = btc [ 'Close' ]. values . reshape ( - 1 , 1 ) # The scaler expects the data to be shaped as ( x , y ) scaled_close = scaler . fit_transform ( close_price ) # removing NaNs ( if any ) scaled_close = scaled_close [ ~ np . isnan ( scaled_close )] # reshaping data after removing NaNs scaled_close = scaled_close . reshape ( - 1 , 1 ) print ( scaled_close . shape ) (5842, 1) We will use past 150 days datato predict future 3 days price We will implement a window generator to generate a series of data shown in this picture. SEQ_LEN = 10 OFFSET = 3 class WindowGenerator () : def __init__ ( self , input_width , offset , data , train_split ) : self . data = data self . input_width = input_width self . offset = offset self . train_split = train_split def to_sequences ( self ) : \"\"\" Return both data and label \"\"\" data_len = len ( self . data ) ret = [] ret_label = [] for i in range ( data_len - self . offset - self . input_width + 1 ) : tmp = self . data [ i : i + self . input_width ] tmp_label = self . data [ i + self . input_width + self . offset - 1 ] ret . append ( tmp ) ret_label . append ( tmp_label ) return np . array ( ret ), np . array ( ret_label ) def split ( self ) : x , y = self . to_sequences () num_train = int (( 1 - self . train_split ) * x . shape [ 0 ]) X_train = x [ : num_train ] y_train = y [ : num_train ] X_test = x [ num_train :] y_test = y [ num_train :] return X_train , y_train , X_test , y_test wg = WindowGenerator ( data = scaled_close , input_width = SEQ_LEN , offset = OFFSET , train_split = 0.1 ) X_train , y_train , X_test , y_test = wg . split () print ( X_train . shape , X_test . shape ) print ( y_train . shape , y_test . shape ) (5247, 10, 1) (583, 10, 1) (5247, 1) (583, 1)","title":"Preprocess data"},{"location":"MSBD5012/project/LSTM/#training","text":"","title":"Training"},{"location":"MSBD5012/project/LSTM/#build-model","text":"from tensorflow.keras.layers import Bidirectional , Dropout , LSTM , Dense , Activation DROPOUT = 0 . 2 # 20 % Dropout is used to control over - fitting during training WINDOW_SIZE = SEQ_LEN model = keras . Sequential () model . add ( Bidirectional ( LSTM ( WINDOW_SIZE , return_sequences = True ), input_shape = ( WINDOW_SIZE , X_train . shape [ - 1 ]))) \"\"\"Bidirectional RNNs allows to train on the sequence data in forward and backward direction.\"\"\" model . add ( Dropout ( rate = DROPOUT )) model . add ( Bidirectional ( LSTM (( WINDOW_SIZE * 2 ), return_sequences = True ))) model . add ( Dropout ( rate = DROPOUT )) model . add ( Bidirectional ( LSTM (( WINDOW_SIZE * 2 ), return_sequences = True ))) model . add ( Dropout ( rate = DROPOUT )) model . add ( Bidirectional ( LSTM ( WINDOW_SIZE , return_sequences = False ))) # output layer model . add ( Dense ( units = 1 )) model . add ( Activation ( 'linear' )) \"\"\"Output layer has a single neuron (predicted Bitcoin price). We use Linear activation function which activation is proportional to the input.\"\"\" BATCH_SIZE = 64 model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' , metrics = [ 'mae' , 'mse' ]) model . summary () Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= bidirectional (Bidirectional (None, 10, 20) 960 _________________________________________________________________ dropout (Dropout) (None, 10, 20) 0 _________________________________________________________________ bidirectional_1 (Bidirection (None, 10, 40) 6560 _________________________________________________________________ dropout_1 (Dropout) (None, 10, 40) 0 _________________________________________________________________ bidirectional_2 (Bidirection (None, 10, 40) 9760 _________________________________________________________________ dropout_2 (Dropout) (None, 10, 40) 0 _________________________________________________________________ bidirectional_3 (Bidirection (None, 20) 4080 _________________________________________________________________ dense (Dense) (None, 1) 21 _________________________________________________________________ activation (Activation) (None, 1) 0 ================================================================= Total params: 21,381 Trainable params: 21,381 Non-trainable params: 0 _________________________________________________________________","title":"Build model"},{"location":"MSBD5012/project/LSTM/#train-model","text":"def plot_history ( history ) : hist = pd . DataFrame ( history . history ) hist [ 'epoch' ] = history . epoch plt . figure () plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Mean Abs Error [MPG]' ) plt . plot ( hist [ 'epoch' ] , hist [ 'mae' ] , label = 'Train Error' ) plt . plot ( hist [ 'epoch' ] , hist [ 'val_mae' ] , label = 'Val Error' ) plt . legend () plt . figure () plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Mean Square Error [$MPG^2$]' ) plt . plot ( hist [ 'epoch' ] , hist [ 'mse' ] , label = 'Train Error' ) plt . plot ( hist [ 'epoch' ] , hist [ 'val_mse' ] , label = 'Val Error' ) plt . legend () plt . show () early_stop = keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 20 ) history = model . fit ( X_train , y_train , epochs = 1000 , batch_size = BATCH_SIZE , shuffle = False , validation_split = 0.1 , callbacks =[ early_stop ] ) plot_history ( history ) Epoch 1/1000 74/74 [==============================] - 4s 50ms/step - loss: 0.0217 - mae: 0.0902 - mse: 0.0217 - val_loss: 0.0069 - val_mae: 0.0810 - val_mse: 0.0069 Epoch 2/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0048 - mae: 0.0508 - mse: 0.0048 - val_loss: 8.0070e-04 - val_mae: 0.0254 - val_mse: 8.0070e-04 Epoch 3/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0015 - mae: 0.0279 - mse: 0.0015 - val_loss: 1.8564e-04 - val_mae: 0.0097 - val_mse: 1.8564e-04 Epoch 4/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0010 - mae: 0.0213 - mse: 0.0010 - val_loss: 3.7053e-04 - val_mae: 0.0154 - val_mse: 3.7053e-04 Epoch 5/1000 74/74 [==============================] - 1s 18ms/step - loss: 9.8986e-04 - mae: 0.0198 - mse: 9.8986e-04 - val_loss: 3.3315e-04 - val_mae: 0.0143 - val_mse: 3.3315e-04 Epoch 6/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0013 - mae: 0.0237 - mse: 0.0013 - val_loss: 1.5304e-04 - val_mae: 0.0093 - val_mse: 1.5304e-04 Epoch 7/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0024 - mae: 0.0344 - mse: 0.0024 - val_loss: 1.6594e-04 - val_mae: 0.0102 - val_mse: 1.6594e-04 Epoch 8/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0033 - mae: 0.0421 - mse: 0.0033 - val_loss: 2.6209e-04 - val_mae: 0.0121 - val_mse: 2.6209e-04 Epoch 9/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0028 - mae: 0.0396 - mse: 0.0028 - val_loss: 2.6527e-04 - val_mae: 0.0122 - val_mse: 2.6527e-04 Epoch 10/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0024 - mae: 0.0360 - mse: 0.0024 - val_loss: 2.3552e-04 - val_mae: 0.0112 - val_mse: 2.3552e-04 Epoch 11/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0025 - mae: 0.0365 - mse: 0.0025 - val_loss: 5.8297e-04 - val_mae: 0.0209 - val_mse: 5.8297e-04 Epoch 12/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0018 - mae: 0.0309 - mse: 0.0018 - val_loss: 2.9134e-04 - val_mae: 0.0131 - val_mse: 2.9134e-04 Epoch 13/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0016 - mae: 0.0282 - mse: 0.0016 - val_loss: 2.2296e-04 - val_mae: 0.0109 - val_mse: 2.2296e-04 Epoch 14/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0016 - mae: 0.0288 - mse: 0.0016 - val_loss: 2.5500e-04 - val_mae: 0.0119 - val_mse: 2.5500e-04 Epoch 15/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0019 - mae: 0.0312 - mse: 0.0019 - val_loss: 2.2766e-04 - val_mae: 0.0110 - val_mse: 2.2766e-04 Epoch 16/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0018 - mae: 0.0310 - mse: 0.0018 - val_loss: 2.1390e-04 - val_mae: 0.0106 - val_mse: 2.1390e-04 Epoch 17/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0019 - mae: 0.0319 - mse: 0.0019 - val_loss: 2.7293e-04 - val_mae: 0.0125 - val_mse: 2.7293e-04 Epoch 18/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0016 - mae: 0.0298 - mse: 0.0016 - val_loss: 2.6761e-04 - val_mae: 0.0123 - val_mse: 2.6761e-04 Epoch 19/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0014 - mae: 0.0272 - mse: 0.0014 - val_loss: 2.4100e-04 - val_mae: 0.0115 - val_mse: 2.4100e-04 Epoch 20/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0013 - mae: 0.0264 - mse: 0.0013 - val_loss: 2.9087e-04 - val_mae: 0.0131 - val_mse: 2.9087e-04 Epoch 21/1000 74/74 [==============================] - 1s 18ms/step - loss: 0.0011 - mae: 0.0229 - mse: 0.0011 - val_loss: 2.8196e-04 - val_mae: 0.0128 - val_mse: 2.8196e-04 Epoch 22/1000 74/74 [==============================] - 1s 18ms/step - loss: 8.4853e-04 - mae: 0.0198 - mse: 8.4853e-04 - val_loss: 3.0520e-04 - val_mae: 0.0136 - val_mse: 3.0520e-04 Epoch 23/1000 74/74 [==============================] - 1s 18ms/step - loss: 8.2679e-04 - mae: 0.0191 - mse: 8.2679e-04 - val_loss: 2.3351e-04 - val_mae: 0.0113 - val_mse: 2.3351e-04 Epoch 24/1000 74/74 [==============================] - 1s 18ms/step - loss: 8.5238e-04 - mae: 0.0192 - mse: 8.5238e-04 - val_loss: 2.4551e-04 - val_mae: 0.0117 - val_mse: 2.4551e-04 Epoch 25/1000 74/74 [==============================] - 1s 18ms/step - loss: 9.0385e-04 - mae: 0.0202 - mse: 9.0385e-04 - val_loss: 2.1145e-04 - val_mae: 0.0106 - val_mse: 2.1145e-04 Epoch 26/1000 74/74 [==============================] - 1s 18ms/step - loss: 9.6579e-04 - mae: 0.0210 - mse: 9.6579e-04 - val_loss: 1.9588e-04 - val_mae: 0.0101 - val_mse: 1.9588e-04","title":"Train model"},{"location":"MSBD5012/project/LSTM/#testing","text":"# prediction on test data y_pred = model.predict(X_test) # invert the test to original values y_test_inverse = pd.DataFrame(scaler.inverse_transform(y_test)) # assigning datetime y_test_inverse.index = btc.index[-len(y_test):] print('Test data:',) print(y_test_inverse.tail(3)); print(); # invert the prediction to understandable values y_pred_inverse = pd.DataFrame(scaler.inverse_transform(y_pred)) # assigning datetime y_pred_inverse.index = y_test_inverse.index print('Prediction data:',) print(y_pred_inverse.tail(3)) # print(y_train.shape) Test data: 0 Date 2020-11-30 3391.760010 2020-12-01 3451.939941 2020-12-02 3449.381104 Prediction data: 0 Date 2020-11-30 3392.729492 2020-12-01 3397.149170 2020-12-02 3404.069824 print(f'MAE {mean_absolute_error(y_test, y_pred)}') print(f'MSE {mean_squared_error(y_test, y_pred)}') print(f'RMSE {np.sqrt(mean_squared_error(y_test, y_pred))}') MAE 0.013780549753230727 MSE 0.0003602061344071959 RMSE 0.018979097302221616 plt.figure(figsize = (30,5)) plt.plot(y_test_inverse) plt.plot(y_pred_inverse) plt.title('Actual vs Prediction plot (Price prediction model)') plt.ylabel('price') plt.xlabel('date') plt.legend(['actual', 'prediction'], loc='upper left') plt.show() today_price = scaled_close[-SEQ_LEN:] today_price = np.expand_dims(today_price, axis=0) print(today_price.shape, X_test.shape) tmr_prediction = model.predict(today_price) scaler.inverse_transform(tmr_prediction)[0][0] (1, 10, 1) (583, 10, 1) 3425.282","title":"Testing"},{"location":"MSBD5012/project/LSTM/#lstm-with-sentiment-score","text":"By only using previous stock data to predict future price may cause in accurate future predictions, so we want to use the power of the social media. In this section, we will try to analyze the future stock data in two directions: Use lstm to predict future sentiment score to predict stock price. We will modify the model above to add one","title":"LSTM with sentiment score"},{"location":"MSBD5012/project/LSTM/#analyze-sentiment","text":"from datetime import datetime sentiment_files_path = '/content/drive/MyDrive/courses/HKUST/MSBD5012/project/sentiments.csv' def parse_time ( time ): try : return datetime . strptime ( time , '%Y-%m-%d %H:%M:%S.%f+00' ) except Exception : return None sentiments = pd . read_csv ( sentiment_files_path ) sentiments . info () sentiments [ 'Time' ] = sentiments [ 'Time' ]. apply ( parse_time ) sentiments = sentiments . dropna () <class 'pandas.core.frame.DataFrame'> RangeIndex: 614836 entries, 0 to 614835 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Unnamed: 0 614836 non-null int64 1 Keyword 614836 non-null object 2 content 614453 non-null object 3 Time 614832 non-null object 4 Rank 614832 non-null float64 5 Number 614828 non-null float64 6 sentiments 612742 non-null float64 dtypes: float64(3), int64(1), object(3) memory usage: 32.8+ MB sentiments['Date'] = sentiments['Time'].apply(lambda date: date.date()) grouped = sentiments[['Date', 'sentiments']].groupby('Date').mean() grouped = grouped.reset_index() grouped .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date sentiments 0 2020-11-14 0.776240 1 2020-11-15 0.752614 2 2020-11-16 0.740375 3 2020-11-17 0.739457 4 2020-11-18 0.693168 5 2020-11-19 0.719513 6 2020-11-20 0.735112 7 2020-11-21 0.738899 8 2020-11-22 0.676612 9 2020-11-23 0.696438 10 2020-11-24 0.690481 11 2020-11-25 0.729346 12 2020-11-26 0.747991 13 2020-11-27 0.751125 14 2020-11-28 0.749364 15 2020-11-29 0.729915 16 2020-11-30 0.697641 17 2020-12-01 0.736729 18 2020-12-02 0.749541 19 2020-12-03 0.733653 20 2020-12-04 0.710298","title":"Analyze sentiment"},{"location":"MSBD5012/project/LSTM/#daily-sentiment-score-with-stock-score","text":"data = btc . Close . loc [ '2020-11-14' :] data = data . reset_index () data [ 'Date' ] = data [ 'Date' ]. apply ( lambda x : x . date ()) data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Close 0 2020-11-16 3346.968994 1 2020-11-17 3339.899902 2 2020-11-18 3347.302979 3 2020-11-19 3363.087891 4 2020-11-20 3377.729980 5 2020-11-23 3414.489990 6 2020-11-24 3402.822998 7 2020-11-25 3362.326904 8 2020-11-26 3369.732910 9 2020-11-27 3408.306885 10 2020-11-30 3391.760010 11 2020-12-01 3451.939941 12 2020-12-02 3449.381104 plt.figure(figsize=(20, 10)) plt.plot(data['Date'], data['Close']) plt.title('Stock price') plt.xlabel('Date') plt.ylabel('price') plt.legend() plt.savefig('Stock price.png') No handles with labels found to put in legend. plt.figure(figsize=(20, 10)) plt.plot(grouped['Date'], grouped['sentiments']) plt.title('Sentiment score') plt.xlabel('Date') plt.ylabel('Sentiments') plt.legend() plt.savefig('Sentiment-score.png') No handles with labels found to put in legend.","title":"Daily sentiment score with stock score"},{"location":"MSBD5012/project/LSTM/#merge-data-with-sentiment","text":"merged = grouped.merge(data, on='Date', how='inner') merged # def generate_training_data ( data : pd . DataFrame , prediction_label , cat_vars =[ 'id', 'IsWeekend','IsHoliday','Hour modify', 'Weather' ] , # num_vars =[ 'Temperature', 'Pressure', 'Humidity', 'Cloud', 'Wind degree' ] , # should_reshape = True , should_split = True ) : # x = data . copy () # y = x [ prediction_label ] . to_list () # y = np . array ( y ) # numeric_transformer = Pipeline ( steps =[ # ('scaler', RobustScaler()) ] ) # categorical_transformer = Pipeline ( steps =[ # ('oneHot',OneHotEncoder(sparse=False)) ] ) # preprocessor = ColumnTransformer ( transformers =[ # ('num',numeric_transformer,num_vars), # ('cat',categorical_transformer,cat_vars) ] ) # data_transformed = preprocessor . fit_transform ( x ) # if should_split : # if should_reshape : # y = y . reshape ( - 1 , 1 ) # scaler = MinMaxScaler () # scaled_y = scaler . fit_transform ( y ) # return train_test_split ( data_transformed , scaled_y , test_size = 0.02 , random_state = 42 ), scaler # else : # return train_test_split ( data_transformed , y , test_size = 0.02 , random_state = 42 ) # else : # return data_transformed , y scaler = MinMaxScaler () scaler2 = MinMaxScaler () x_data = merged [ ['Close', 'sentiments' ] ] . to_numpy () y_data = merged [ ['Close' ] ] . to_numpy () scaled_x = scaler . fit_transform ( x_data ) scaled_y = scaler2 . fit_transform ( y_data ) print ( scaled_x ) print ( scaled_y ) SEQ_LEN = 3 OFFSET = 0 wg = WindowGenerator(data=scaled_x, input_width=SEQ_LEN, offset=OFFSET, train_split=0.1) wg2 = WindowGenerator(data=scaled_y, input_width=SEQ_LEN, offset=OFFSET, train_split=0.1) X_train, _, X_test, _ = wg.split() _, y_train, _, y_test = wg2.split() print(X_train.shape, X_test.shape) print(y_train.shape, y_test.shape) (9, 3, 2) (2, 3, 2) (9, 1) (2, 1)","title":"Merge data with sentiment"},{"location":"MSBD5012/project/LSTM/#stock-price-with-sentiment","text":"print(X_train[0], y_train[0]) [[0.06309434 0.82273844] [0. 0.80759869] [0.06607528 0.04430082]] [0.06607528] DROPOUT = 0 . 2 # 20 % Dropout is used to control over - fitting during training WINDOW_SIZE = SEQ_LEN model = keras . Sequential () model . add ( Bidirectional ( LSTM ( WINDOW_SIZE , return_sequences = True ), input_shape = ( WINDOW_SIZE , X_train . shape [ - 1 ]))) \"\"\"Bidirectional RNNs allows to train on the sequence data in forward and backward direction.\"\"\" model . add ( Dropout ( rate = DROPOUT )) model . add ( Bidirectional ( LSTM (( WINDOW_SIZE * 2 ), return_sequences = True ))) model . add ( Dropout ( rate = DROPOUT )) model . add ( Bidirectional ( LSTM (( WINDOW_SIZE * 2 ), return_sequences = True ))) model . add ( Dropout ( rate = DROPOUT )) model . add ( Bidirectional ( LSTM ( WINDOW_SIZE , return_sequences = False ))) # output layer model . add ( Dense ( units = 1 )) model . add ( Activation ( 'linear' )) \"\"\"Output layer has a single neuron (predicted Bitcoin price). We use Linear activation function which activation is proportional to the input.\"\"\" BATCH_SIZE = 64 model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' , metrics = [ 'mae' , 'mse' ]) model . summary () Model: \"sequential_3\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= bidirectional_12 (Bidirectio (None, 3, 6) 144 _________________________________________________________________ dropout_9 (Dropout) (None, 3, 6) 0 _________________________________________________________________ bidirectional_13 (Bidirectio (None, 3, 12) 624 _________________________________________________________________ dropout_10 (Dropout) (None, 3, 12) 0 _________________________________________________________________ bidirectional_14 (Bidirectio (None, 3, 12) 912 _________________________________________________________________ dropout_11 (Dropout) (None, 3, 12) 0 _________________________________________________________________ bidirectional_15 (Bidirectio (None, 6) 384 _________________________________________________________________ dense_3 (Dense) (None, 1) 7 _________________________________________________________________ activation_3 (Activation) (None, 1) 0 ================================================================= Total params: 2,071 Trainable params: 2,071 Non-trainable params: 0 _________________________________________________________________ history = model . fit ( X_train , y_train , epochs = 1000 , callbacks =[ early_stop ] , validation_split = 0.1 , shuffle = False ) plot_history ( history ) Epoch 1/1000 1/1 [==============================] - 2s 2s/step - loss: 0.1822 - mae: 0.3731 - mse: 0.1822 - val_loss: 0.2178 - val_mae: 0.4667 - val_mse: 0.2178 Epoch 2/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.1754 - mae: 0.3638 - mse: 0.1754 - val_loss: 0.2137 - val_mae: 0.4622 - val_mse: 0.2137 Epoch 3/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.1736 - mae: 0.3615 - mse: 0.1736 - val_loss: 0.2096 - val_mae: 0.4578 - val_mse: 0.2096 Epoch 4/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.1712 - mae: 0.3580 - mse: 0.1712 - val_loss: 0.2056 - val_mae: 0.4534 - val_mse: 0.2056 Epoch 5/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.1703 - mae: 0.3556 - mse: 0.1703 - val_loss: 0.2016 - val_mae: 0.4491 - val_mse: 0.2016 Epoch 6/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.1660 - mae: 0.3508 - mse: 0.1660 - val_loss: 0.1978 - val_mae: 0.4447 - val_mse: 0.1978 Epoch 7/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.1626 - mae: 0.3451 - mse: 0.1626 - val_loss: 0.1939 - val_mae: 0.4404 - val_mse: 0.1939 Epoch 8/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.1603 - mae: 0.3422 - mse: 0.1603 - val_loss: 0.1901 - val_mae: 0.4360 - val_mse: 0.1901 Epoch 9/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.1560 - mae: 0.3359 - mse: 0.1560 - val_loss: 0.1864 - val_mae: 0.4317 - val_mse: 0.1864 Epoch 10/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.1525 - mae: 0.3312 - mse: 0.1525 - val_loss: 0.1827 - val_mae: 0.4274 - val_mse: 0.1827 Epoch 11/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.1509 - mae: 0.3283 - mse: 0.1509 - val_loss: 0.1790 - val_mae: 0.4231 - val_mse: 0.1790 Epoch 12/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.1471 - mae: 0.3230 - mse: 0.1471 - val_loss: 0.1753 - val_mae: 0.4187 - val_mse: 0.1753 Epoch 13/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.1452 - mae: 0.3193 - mse: 0.1452 - val_loss: 0.1717 - val_mae: 0.4144 - val_mse: 0.1717 Epoch 14/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.1434 - mae: 0.3162 - mse: 0.1434 - val_loss: 0.1681 - val_mae: 0.4100 - val_mse: 0.1681 Epoch 15/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.1403 - mae: 0.3122 - mse: 0.1403 - val_loss: 0.1645 - val_mae: 0.4055 - val_mse: 0.1645 Epoch 16/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.1385 - mae: 0.3083 - mse: 0.1385 - val_loss: 0.1608 - val_mae: 0.4011 - val_mse: 0.1608 Epoch 17/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.1347 - mae: 0.3035 - mse: 0.1347 - val_loss: 0.1573 - val_mae: 0.3966 - val_mse: 0.1573 Epoch 18/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.1316 - mae: 0.2992 - mse: 0.1316 - val_loss: 0.1537 - val_mae: 0.3920 - val_mse: 0.1537 Epoch 19/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.1292 - mae: 0.2956 - mse: 0.1292 - val_loss: 0.1501 - val_mae: 0.3874 - val_mse: 0.1501 Epoch 20/1000 1/1 [==============================] - 0s 23ms/step - loss: 0.1276 - mae: 0.2937 - mse: 0.1276 - val_loss: 0.1465 - val_mae: 0.3827 - val_mse: 0.1465 Epoch 21/1000 1/1 [==============================] - 0s 22ms/step - loss: 0.1239 - mae: 0.2895 - mse: 0.1239 - val_loss: 0.1429 - val_mae: 0.3780 - val_mse: 0.1429 Epoch 22/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.1197 - mae: 0.2835 - mse: 0.1197 - val_loss: 0.1393 - val_mae: 0.3732 - val_mse: 0.1393 Epoch 23/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.1193 - mae: 0.2826 - mse: 0.1193 - val_loss: 0.1357 - val_mae: 0.3683 - val_mse: 0.1357 Epoch 24/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.1162 - mae: 0.2800 - mse: 0.1162 - val_loss: 0.1320 - val_mae: 0.3634 - val_mse: 0.1320 Epoch 25/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.1147 - mae: 0.2756 - mse: 0.1147 - val_loss: 0.1284 - val_mae: 0.3584 - val_mse: 0.1284 Epoch 26/1000 1/1 [==============================] - 0s 22ms/step - loss: 0.1107 - mae: 0.2706 - mse: 0.1107 - val_loss: 0.1248 - val_mae: 0.3532 - val_mse: 0.1248 Epoch 27/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.1097 - mae: 0.2680 - mse: 0.1097 - val_loss: 0.1211 - val_mae: 0.3480 - val_mse: 0.1211 Epoch 28/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.1064 - mae: 0.2637 - mse: 0.1064 - val_loss: 0.1175 - val_mae: 0.3427 - val_mse: 0.1175 Epoch 29/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.1040 - mae: 0.2609 - mse: 0.1040 - val_loss: 0.1138 - val_mae: 0.3373 - val_mse: 0.1138 Epoch 30/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.1013 - mae: 0.2577 - mse: 0.1013 - val_loss: 0.1101 - val_mae: 0.3318 - val_mse: 0.1101 Epoch 31/1000 1/1 [==============================] - 0s 22ms/step - loss: 0.0987 - mae: 0.2512 - mse: 0.0987 - val_loss: 0.1064 - val_mae: 0.3262 - val_mse: 0.1064 Epoch 32/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0955 - mae: 0.2472 - mse: 0.0955 - val_loss: 0.1027 - val_mae: 0.3204 - val_mse: 0.1027 Epoch 33/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0935 - mae: 0.2458 - mse: 0.0935 - val_loss: 0.0990 - val_mae: 0.3146 - val_mse: 0.0990 Epoch 34/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0905 - mae: 0.2401 - mse: 0.0905 - val_loss: 0.0952 - val_mae: 0.3086 - val_mse: 0.0952 Epoch 35/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.0880 - mae: 0.2344 - mse: 0.0880 - val_loss: 0.0915 - val_mae: 0.3024 - val_mse: 0.0915 Epoch 36/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0832 - mae: 0.2298 - mse: 0.0832 - val_loss: 0.0877 - val_mae: 0.2961 - val_mse: 0.0877 Epoch 37/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0815 - mae: 0.2255 - mse: 0.0815 - val_loss: 0.0839 - val_mae: 0.2896 - val_mse: 0.0839 Epoch 38/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.0814 - mae: 0.2191 - mse: 0.0814 - val_loss: 0.0801 - val_mae: 0.2830 - val_mse: 0.0801 Epoch 39/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0789 - mae: 0.2204 - mse: 0.0789 - val_loss: 0.0763 - val_mae: 0.2762 - val_mse: 0.0763 Epoch 40/1000 1/1 [==============================] - 0s 22ms/step - loss: 0.0765 - mae: 0.2151 - mse: 0.0765 - val_loss: 0.0725 - val_mae: 0.2693 - val_mse: 0.0725 Epoch 41/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.0752 - mae: 0.2110 - mse: 0.0752 - val_loss: 0.0687 - val_mae: 0.2622 - val_mse: 0.0687 Epoch 42/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0695 - mae: 0.2038 - mse: 0.0695 - val_loss: 0.0650 - val_mae: 0.2549 - val_mse: 0.0650 Epoch 43/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.0680 - mae: 0.1957 - mse: 0.0680 - val_loss: 0.0612 - val_mae: 0.2474 - val_mse: 0.0612 Epoch 44/1000 1/1 [==============================] - 0s 23ms/step - loss: 0.0663 - mae: 0.1903 - mse: 0.0663 - val_loss: 0.0575 - val_mae: 0.2397 - val_mse: 0.0575 Epoch 45/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0633 - mae: 0.1913 - mse: 0.0633 - val_loss: 0.0538 - val_mae: 0.2319 - val_mse: 0.0538 Epoch 46/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.0592 - mae: 0.1857 - mse: 0.0592 - val_loss: 0.0501 - val_mae: 0.2238 - val_mse: 0.0501 Epoch 47/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0593 - mae: 0.1924 - mse: 0.0593 - val_loss: 0.0464 - val_mae: 0.2155 - val_mse: 0.0464 Epoch 48/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0591 - mae: 0.1830 - mse: 0.0591 - val_loss: 0.0429 - val_mae: 0.2071 - val_mse: 0.0429 Epoch 49/1000 1/1 [==============================] - 0s 22ms/step - loss: 0.0550 - mae: 0.1830 - mse: 0.0550 - val_loss: 0.0394 - val_mae: 0.1984 - val_mse: 0.0394 Epoch 50/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0585 - mae: 0.1908 - mse: 0.0585 - val_loss: 0.0360 - val_mae: 0.1897 - val_mse: 0.0360 Epoch 51/1000 1/1 [==============================] - 0s 22ms/step - loss: 0.0468 - mae: 0.1731 - mse: 0.0468 - val_loss: 0.0327 - val_mae: 0.1807 - val_mse: 0.0327 Epoch 52/1000 1/1 [==============================] - 0s 23ms/step - loss: 0.0503 - mae: 0.1774 - mse: 0.0503 - val_loss: 0.0294 - val_mae: 0.1716 - val_mse: 0.0294 Epoch 53/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0486 - mae: 0.1786 - mse: 0.0486 - val_loss: 0.0264 - val_mae: 0.1624 - val_mse: 0.0264 Epoch 54/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0489 - mae: 0.1787 - mse: 0.0489 - val_loss: 0.0234 - val_mae: 0.1531 - val_mse: 0.0234 Epoch 55/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0418 - mae: 0.1661 - mse: 0.0418 - val_loss: 0.0206 - val_mae: 0.1436 - val_mse: 0.0206 Epoch 56/1000 1/1 [==============================] - 0s 22ms/step - loss: 0.0433 - mae: 0.1756 - mse: 0.0433 - val_loss: 0.0180 - val_mae: 0.1340 - val_mse: 0.0180 Epoch 57/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.0428 - mae: 0.1741 - mse: 0.0428 - val_loss: 0.0155 - val_mae: 0.1245 - val_mse: 0.0155 Epoch 58/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0496 - mae: 0.1857 - mse: 0.0496 - val_loss: 0.0132 - val_mae: 0.1151 - val_mse: 0.0132 Epoch 59/1000 1/1 [==============================] - 0s 23ms/step - loss: 0.0456 - mae: 0.1820 - mse: 0.0456 - val_loss: 0.0112 - val_mae: 0.1060 - val_mse: 0.0112 Epoch 60/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.0479 - mae: 0.1908 - mse: 0.0479 - val_loss: 0.0094 - val_mae: 0.0971 - val_mse: 0.0094 Epoch 61/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0367 - mae: 0.1757 - mse: 0.0367 - val_loss: 0.0078 - val_mae: 0.0886 - val_mse: 0.0078 Epoch 62/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0376 - mae: 0.1741 - mse: 0.0376 - val_loss: 0.0065 - val_mae: 0.0804 - val_mse: 0.0065 Epoch 63/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.0399 - mae: 0.1775 - mse: 0.0399 - val_loss: 0.0052 - val_mae: 0.0724 - val_mse: 0.0052 Epoch 64/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0372 - mae: 0.1706 - mse: 0.0372 - val_loss: 0.0042 - val_mae: 0.0646 - val_mse: 0.0042 Epoch 65/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.0389 - mae: 0.1749 - mse: 0.0389 - val_loss: 0.0033 - val_mae: 0.0573 - val_mse: 0.0033 Epoch 66/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0456 - mae: 0.1948 - mse: 0.0456 - val_loss: 0.0026 - val_mae: 0.0512 - val_mse: 0.0026 Epoch 67/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.0409 - mae: 0.1847 - mse: 0.0409 - val_loss: 0.0021 - val_mae: 0.0461 - val_mse: 0.0021 Epoch 68/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0513 - mae: 0.2031 - mse: 0.0513 - val_loss: 0.0018 - val_mae: 0.0424 - val_mse: 0.0018 Epoch 69/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0383 - mae: 0.1842 - mse: 0.0383 - val_loss: 0.0016 - val_mae: 0.0394 - val_mse: 0.0016 Epoch 70/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0407 - mae: 0.1861 - mse: 0.0407 - val_loss: 0.0014 - val_mae: 0.0375 - val_mse: 0.0014 Epoch 71/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.0385 - mae: 0.1808 - mse: 0.0385 - val_loss: 0.0013 - val_mae: 0.0359 - val_mse: 0.0013 Epoch 72/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0351 - mae: 0.1763 - mse: 0.0351 - val_loss: 0.0012 - val_mae: 0.0344 - val_mse: 0.0012 Epoch 73/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0474 - mae: 0.2025 - mse: 0.0474 - val_loss: 0.0012 - val_mae: 0.0340 - val_mse: 0.0012 Epoch 74/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0458 - mae: 0.2015 - mse: 0.0458 - val_loss: 0.0012 - val_mae: 0.0343 - val_mse: 0.0012 Epoch 75/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0348 - mae: 0.1715 - mse: 0.0348 - val_loss: 0.0012 - val_mae: 0.0350 - val_mse: 0.0012 Epoch 76/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.0457 - mae: 0.2017 - mse: 0.0457 - val_loss: 0.0013 - val_mae: 0.0365 - val_mse: 0.0013 Epoch 77/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0430 - mae: 0.1870 - mse: 0.0430 - val_loss: 0.0015 - val_mae: 0.0386 - val_mse: 0.0015 Epoch 78/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0448 - mae: 0.1896 - mse: 0.0448 - val_loss: 0.0017 - val_mae: 0.0413 - val_mse: 0.0017 Epoch 79/1000 1/1 [==============================] - 0s 23ms/step - loss: 0.0377 - mae: 0.1783 - mse: 0.0377 - val_loss: 0.0019 - val_mae: 0.0438 - val_mse: 0.0019 Epoch 80/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0526 - mae: 0.2150 - mse: 0.0526 - val_loss: 0.0022 - val_mae: 0.0470 - val_mse: 0.0022 Epoch 81/1000 1/1 [==============================] - 0s 18ms/step - loss: 0.0331 - mae: 0.1608 - mse: 0.0331 - val_loss: 0.0025 - val_mae: 0.0496 - val_mse: 0.0025 Epoch 82/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0335 - mae: 0.1673 - mse: 0.0335 - val_loss: 0.0027 - val_mae: 0.0516 - val_mse: 0.0027 Epoch 83/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0365 - mae: 0.1739 - mse: 0.0365 - val_loss: 0.0029 - val_mae: 0.0534 - val_mse: 0.0029 Epoch 84/1000 1/1 [==============================] - 0s 19ms/step - loss: 0.0388 - mae: 0.1824 - mse: 0.0388 - val_loss: 0.0030 - val_mae: 0.0551 - val_mse: 0.0030 Epoch 85/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0411 - mae: 0.1787 - mse: 0.0411 - val_loss: 0.0033 - val_mae: 0.0571 - val_mse: 0.0033 Epoch 86/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.0360 - mae: 0.1747 - mse: 0.0360 - val_loss: 0.0035 - val_mae: 0.0589 - val_mse: 0.0035 Epoch 87/1000 1/1 [==============================] - 0s 22ms/step - loss: 0.0378 - mae: 0.1776 - mse: 0.0378 - val_loss: 0.0037 - val_mae: 0.0605 - val_mse: 0.0037 Epoch 88/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0452 - mae: 0.1952 - mse: 0.0452 - val_loss: 0.0039 - val_mae: 0.0622 - val_mse: 0.0039 Epoch 89/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0400 - mae: 0.1828 - mse: 0.0400 - val_loss: 0.0041 - val_mae: 0.0641 - val_mse: 0.0041 Epoch 90/1000 1/1 [==============================] - 0s 17ms/step - loss: 0.0378 - mae: 0.1725 - mse: 0.0378 - val_loss: 0.0043 - val_mae: 0.0658 - val_mse: 0.0043 Epoch 91/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0350 - mae: 0.1635 - mse: 0.0350 - val_loss: 0.0045 - val_mae: 0.0669 - val_mse: 0.0045 Epoch 92/1000 1/1 [==============================] - 0s 21ms/step - loss: 0.0353 - mae: 0.1715 - mse: 0.0353 - val_loss: 0.0046 - val_mae: 0.0678 - val_mse: 0.0046 Epoch 93/1000 1/1 [==============================] - 0s 20ms/step - loss: 0.0410 - mae: 0.1849 - mse: 0.0410 - val_loss: 0.0047 - val_mae: 0.0686 - val_mse: 0.0047 pred_data = model.predict(X_test) py = scaler2.inverse_transform(pred_data) ty = scaler2.inverse_transform(y_test) plt.plot(py, label='Predict') plt.plot(ty, label='True') plt.legend() <matplotlib.legend.Legend at 0x7ff2d4a6ecf8>","title":"Stock price with sentiment"},{"location":"MSBD5012/project/LSTM/#conclusion","text":"Due to the current volume of data, we may not be able to use sentiment data to train the model. However, according to the observation, we found that social media's attitude will impact the stock data which proved the original paper's idea.","title":"Conclusion"}]}