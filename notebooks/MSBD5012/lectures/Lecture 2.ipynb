{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lecture 2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyN3O776IkqqMXSv5+MmT+f9"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xHkp7etQD_JW","colab_type":"text"},"source":["## Divergence"]},{"cell_type":"markdown","metadata":{"id":"qBcpAMGREB2M","colab_type":"text"},"source":["**ML Setup**\n","\n","$P(x)$ -> generate -> Data -> learn $Q(x)$ where Q should as close to P as possible."]},{"cell_type":"markdown","metadata":{"id":"3CLJ151VEB9b","colab_type":"text"},"source":["### Relative entropy or Kullback-Leibler divergence\n","\n","- Meassure ow much a distribution Q(X) differs from a \"True\" probability distribution P(X)\n","\n","- K-L Divergence if Q from P is defined as follows:\n","\n","$$ KL(P||Q) = \\sum_x{P(X)log(\\frac{P(X)}{Q(X)})} = -log\\sum_x{Q(X)}$$"]},{"cell_type":"markdown","metadata":{"id":"SP8g8FUaECFE","colab_type":"text"},"source":["Minimize cross entropy = Maximizing log likelyhood"]},{"cell_type":"markdown","metadata":{"id":"IHwsJEbHKVeE","colab_type":"text"},"source":["## Supervised learning\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iyNsCuuXLJXV","colab_type":"text"},"source":["## Unsupervised learning"]},{"cell_type":"markdown","metadata":{"id":"6Pt4QYb3LMQo","colab_type":"text"},"source":["## Multual information\n","\n","H(x): Initial uncertainty about x\n","\n","H(X | Y): Expected uncertainty about x if y is tested"]},{"cell_type":"markdown","metadata":{"id":"VZOggM43LMXT","colab_type":"text"},"source":["# Linear Regression\n","\n","$$y = w_0 + w_1x_1$$"]},{"cell_type":"markdown","metadata":{"id":"7oob2MIpSEFo","colab_type":"text"},"source":["## Mean Square Error"]},{"cell_type":"code","metadata":{"id":"fBNmdD8cLIjK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600501144933,"user_tz":-480,"elapsed":2931,"user":{"displayName":"Qiwei Li","photoUrl":"","userId":"09519962397524608991"}},"outputId":"399b7469-357a-4ed5-b131-1c3662e15f7f"},"source":["import numpy as np \n","  \n","# Given values \n","Y_true = [1,1,2,2,4]  # Y_true = Y (original values) \n","  \n","# Calculated values \n","Y_pred = [0.6,1.29,1.99,2.69,3.4]  # Y_pred = Y' \n","  \n","# Mean Squared Error \n","MSE = np.square(np.subtract(Y_true,Y_pred)).mean() \n","MSE"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.21606"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"eza6rGe2VWyx","colab_type":"text"},"source":["## Hypothesis space\n","\n","Is the set of functioins that it is allowed to select as being the solution. Thhe size of the hypothesis space is called the capacity of the model.\n","\n","For polynomial regression, the larger the d, the higher the model capacity.\n","\n","Higher model capacity implies better fit to training data.\n","\n","1. $S_1 = \\{y = w_0 + w_1x_1 | w_0, w_1 \\in R\\}$\n","\n","2. $S_2 = \\{y=w_0 + w_1x_1 + w_2x_1^2 + w_3x_1^3 | w_0, w_1, w_2, w_3 \\in R\\}$\n"]},{"cell_type":"markdown","metadata":{"id":"TBpj1eRDbgQs","colab_type":"text"},"source":["## Generalization Error"]},{"cell_type":"markdown","metadata":{"id":"B-okNju6fiTv","colab_type":"text"},"source":["Model select:\n","\n","- Validation\n","    - Split training data into two parts. One part for training and second part for validation. This has to be randomly split.\n","    \n","- Regularization"]},{"cell_type":"code","metadata":{"id":"nen7nEHsTK-B","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}