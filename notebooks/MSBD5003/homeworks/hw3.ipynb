{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw3.ipynb","provenance":[],"toc_visible":true,"mount_file_id":"1iqah0eh2hJ_oN6hlZBXVqYHHX0xKWKqs","authorship_tag":"ABX9TyOFlvMQJArmS/l6aiohvvrQ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Dlp07uHhzyJj","executionInfo":{"status":"ok","timestamp":1604888552689,"user_tz":-480,"elapsed":38929,"user":{"displayName":"Qiwei Li","photoUrl":"","userId":"09519962397524608991"}},"outputId":"0fe0cbbb-ffbc-4631-c1f0-79816ce9f07b","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install pyspark"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting pyspark\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n","\u001b[K     |████████████████████████████████| 204.2MB 65kB/s \n","\u001b[?25hCollecting py4j==0.10.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n","\u001b[K     |████████████████████████████████| 204kB 37.0MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=edc89a8f53022da82822d1b1bfb5ff85b57d72c5c4f9530d935fa927d268c34f\n","  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9 pyspark-3.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"29gn_LAXz-4n","executionInfo":{"status":"ok","timestamp":1604888563790,"user_tz":-480,"elapsed":7180,"user":{"displayName":"Qiwei Li","photoUrl":"","userId":"09519962397524608991"}}},"source":["from pyspark.context import SparkContext\n","from pyspark.sql.session import SparkSession\n","sc = SparkContext.getOrCreate()\n","spark = SparkSession(sc)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q9Q-0wxGzYJU"},"source":["# Question 1\n","\n","Load it into spark and use divide-and-conquer to find the first (adj, noun) pair in which the noun is 'unification'. Print the corresponding adjective.  The skeleton code is provided below.  One solution is to use filter() to find all pairs where the noun is 'unification', and then report the first one.  This is inefficient.  The better idea is to find, in parallel, the first such pair in each partition (if one exists), and then find the first partition that returns such a pair.\n","\n","```python\n","\n","numPartitions = 10\n","\n","lines = sc.textFile(path_to_file, numPartitions)\n","pairs = lines.map(lambda l: tuple(l.split())).filter(lambda p: len(p)==2)\n","pairs.cache()\n","\n","# FILL IN YOUR CODE HERE\n","```"]},{"cell_type":"code","metadata":{"id":"kzYR65i2zA_P","executionInfo":{"status":"ok","timestamp":1604889730005,"user_tz":-480,"elapsed":11700,"user":{"displayName":"Qiwei Li","photoUrl":"","userId":"09519962397524608991"}},"outputId":"aa8d3113-0fd1-4e45-9dda-7470320150e3","colab":{"base_uri":"https://localhost:8080/"}},"source":["numPartitions = 10\n","\n","def find_word(iterator):\n","    for adj, noun in iterator:\n","        if noun == \"unification\":\n","            yield (adj, noun)\n","            break\n","\n","path_to_file = \"/content/drive/My Drive/courses/HKUST/MSBD5003/data/adj_noun_pairs.txt\"\n","lines = sc.textFile(path_to_file, numPartitions)\n","pairs = lines.map(lambda l: tuple(l.split())).filter(lambda p: len(p)==2)\n","pairs.cache()\n","\n","\n","words = pairs.mapPartitions(find_word).collect()\n","print(words[0][0])"],"execution_count":10,"outputs":[{"output_type":"stream","text":["several\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lTJYptt94oUF"},"source":["## Answer\n","\n","```python\n","numPartitions = 10\n","\n","def find_word(iterator):\n","    for adj, noun in iterator:\n","        if noun == \"unification\":\n","            yield (adj, noun)\n","            break\n","\n","path_to_file = \"/content/drive/My Drive/courses/HKUST/MSBD5003/data/adj_noun_pairs.txt\"\n","lines = sc.textFile(path_to_file, numPartitions)\n","pairs = lines.map(lambda l: tuple(l.split())).filter(lambda p: len(p)==2)\n","pairs.cache()\n","\n","\n","words = pairs.mapPartitions(find_word).collect()\n","print(words[0][0])\n","```"]},{"cell_type":"markdown","metadata":{"id":"ctJKBE-hzhCM"},"source":["# Question 2\n","\n","Design a parallel divide-and-conquer algorithm for the following problem: Given two strings of equal length, compare them lexicographically. Output '<', '=', or '>', depending on the comparison result. The skeleton code is provided below.  Your code should run on all partitions of the rdd in parallel.\n","\n","```python\n","x = 'abcccbcbcacaccacaabb'\n","y = 'abcccbcccacaccacaabb'\n","\n","numPartitions = 4\n","rdd = sc.parallelize(zip(x,y), numPartitions)\n","\n","# FILL IN YOUR CODE HERE\n","```"]},{"cell_type":"code","metadata":{"id":"FVjiQAQ85cZL","executionInfo":{"status":"ok","timestamp":1604890644569,"user_tz":-480,"elapsed":1084,"user":{"displayName":"Qiwei Li","photoUrl":"","userId":"09519962397524608991"}},"outputId":"55aec3dd-d020-4c68-c647-60590cba87fd","colab":{"base_uri":"https://localhost:8080/"}},"source":["ord(\"A\")"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["65"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"57NMESGNzobB","executionInfo":{"status":"ok","timestamp":1604891041963,"user_tz":-480,"elapsed":1078,"user":{"displayName":"Qiwei Li","photoUrl":"","userId":"09519962397524608991"}},"outputId":"2437110d-ba33-4384-e955-ea6c009aa08a","colab":{"base_uri":"https://localhost:8080/"}},"source":["x = 'abcccbcbcacaccacaabb'\n","y = 'abcccbcccacaccacaabb'\n","\n","def get_sum_by_partitions(iterator):\n","    sum_1 = 0\n","    sum_2 = 0\n","    for a, b in iterator:\n","        sum_1 += ord(a)\n","        sum_2 += ord(b)\n","    yield (sum_1, sum_2)\n"," \n","numPartitions = 4\n","rdd = sc.parallelize(zip(x,y), numPartitions)\n","sums = rdd.mapPartitions(get_sum_by_partitions).collect()\n","\n","sum_1 = 0\n","sum_2 = 0\n","\n","for s_1, s_2 in sums:\n","    sum_1 += s_1\n","    sum_2 += s_2\n","\n","if sum_1 < sum_2:\n","    print(\"<\")\n","\n","elif sum_1 == sum_2:\n","    print(\"=\")\n","\n","else:\n","    print(\">\")\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":[">\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G1uk3NRU9g05"},"source":["## Answer\n","\n","```python\n","x = 'abcccbcbcacaccacaabb'\n","y = 'abcccbcccacaccacaabb'\n","\n","def get_sum_by_partitions(iterator):\n","    sum_1 = 0\n","    sum_2 = 0\n","    for a, b in iterator:\n","        sum_1 += ord(a)\n","        sum_2 += ord(b)\n","    yield (sum_1, sum_2)\n"," \n","numPartitions = 4\n","rdd = sc.parallelize(zip(x,y), numPartitions)\n","sums = rdd.mapPartitions(get_sum_by_partitions).collect()\n","\n","sum_1 = 0\n","sum_2 = 0\n","\n","for s_1, s_2 in sums:\n","    sum_1 += s_1\n","    sum_2 += s_2\n","\n","if sum_1 < sum_2:\n","    print(\"<\")\n","\n","elif sum_1 == sum_2:\n","    print(\"=\")\n","\n","else:\n","    print(\">\")\n","\n","```"]},{"cell_type":"code","metadata":{"id":"IdLbM9sX5PAG"},"source":[""],"execution_count":null,"outputs":[]}]}