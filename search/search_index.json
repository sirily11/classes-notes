{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"List of notes \u00b6","title":"List of notes"},{"location":"#list-of-notes","text":"","title":"List of notes"},{"location":"MSBD5001/Lecture%202/","text":"Supervised learning \u00b6 Supervised larning Unsupervised learning Reinforcement learning Whats is machine learning \u00b6 A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks. Linear Regression \u00b6 A Part of machine learning Given training set x, y Find a good approximation to f: \\(x \\to y\\) Examples: Spam detection ( Classification) Digit recognition ( Classification) House price prediction (Refression) Terminology \u00b6 Given a data point (x, y), x is called featyre vector, y is called label The dataset given for learning is training data The dataset to be tested is called testing data Machine learning 3 steps \u00b6 Collect data, extract features Determine a model Train the model with the data Loss \u00b6 Loss on traning set We measure the error using a loss function \\(L(y, \\hat{y})\\) For regression, squared error is often used \\( \\(L(y_1, f(x_i)) = (y_i - f(x_i))^2\\) \\) Loss on testing set Empirical loss is measuring the loss on the training set We assume both training set and testing set are i.i.d from the same distribution D - Minimizing loss on training set will make loss on testing set small Minimizing loss functions \u00b6 The minimizers of some loss functions have analytical solutions: an exact solution you can explicitly derive by analyzing the formula. However, most poular supervised learning models use loss functions with no analytical solution We use gradient descent to approximate the minimal value of function. Gradients: A vector, points to the direction where changing value is the fastest. Method \u00b6 For function G, randomly guess an initial value \\(x_0\\) Repeat \\(xi+1 = x_i - r \\times \\nabla G(x)\\) where \\(\\nabla\\) denotes the gradients, r denotes learning rate Until convergence from sympy import symbols , diff r = 0.1 f_i = ( 1 , 1 , 1 ) x , y , z = symbols ( 'x y z' , real = True ) f = ( y + 2 * x ) ** 2 + y + 2 * x g = ( diff ( f , x ), diff ( f , y ), diff ( f , z )) G (8*x + 4*y + 2, 4*x + 2*y + 1, 0) import numpy as np result = np . array ([ 8 , 6 , 3 ]) * r + np . array ([ 1 , 1 , 1 ]) result array([1.8, 1.6, 1.3]) Linear Classification \u00b6 Use a line to separate data points Use \\(x = (x_1, x_2)\\) , \\(w = (w_1, w_2)\\) , i.e., x, w are vectors in 2D space Doesn't work well with classification problem Label y as either 1 or -1 Find f_w(x) = w^Tx that minimizes the loss function \\( \\(L(f_w(x)) = \\frac{1}{n}\\sum_{i=1}^n(w^Tx_i-y_1)^2\\) \\) Find a line that minimizes the distance between red and blue If there is a outlier in the graph, the seperation line will miss classification some points - If the value get very large, the \\(w^TX_i\\) is correct \\(\\to\\) large loss value even if predict value is positive. \u00b6 Solution: We use sigmoid function to minimize the value between 0 and 1 \\( \\(\\sigma(a) = \\frac{1}{1+exp(-a)}\\) \\) Similar to step functions Continuous and easy to compute Some properties of sigmoid function \u00b6 \\(\\sigma(a) = \\frac{1}{1+exp(-a)} \\in (0, 1)\\) symetric Easy to compute gradients Logistic Regression \u00b6 Better approach ( cross-entropy loss function) find w that minimizes loss function If misclassfication happens on i-th data with label 1, \\(log(\\sigma(w^Tx_i))\\) is very large No analytical solution, needs gradient descent SVM \u00b6 A svm performs classification by finding the hyperplane that maximizes the margin between the two classes K-Nearest neighbor methods \u00b6 Learning algorithm: just store training examples Prediction algorithm: Regression: take the average value of k nearest neighbors Classification: assign to the most frequent class of k nearest neighbors Easy to train with high storage requirement, but high-computation cost at prediction --- Linear knn Advantages Easy to fit Strong assumtions on linear relationship Disadvantages Hard to classify the data Takes a lot of computation power Decision Tree \u00b6 Entropy is used to measure how informative is a probability distribution. The more entropy, the more uncertainty. More info Wrap up \u00b6 Collect data, extract features Determine a model Select a good model for your data","title":"Lecture 2"},{"location":"MSBD5001/Lecture%202/#supervised-learning","text":"Supervised larning Unsupervised learning Reinforcement learning","title":"Supervised learning"},{"location":"MSBD5001/Lecture%202/#whats-is-machine-learning","text":"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks.","title":"Whats is machine learning"},{"location":"MSBD5001/Lecture%202/#linear-regression","text":"A Part of machine learning Given training set x, y Find a good approximation to f: \\(x \\to y\\) Examples: Spam detection ( Classification) Digit recognition ( Classification) House price prediction (Refression)","title":"Linear Regression"},{"location":"MSBD5001/Lecture%202/#terminology","text":"Given a data point (x, y), x is called featyre vector, y is called label The dataset given for learning is training data The dataset to be tested is called testing data","title":"Terminology"},{"location":"MSBD5001/Lecture%202/#machine-learning-3-steps","text":"Collect data, extract features Determine a model Train the model with the data","title":"Machine learning 3 steps"},{"location":"MSBD5001/Lecture%202/#loss","text":"Loss on traning set We measure the error using a loss function \\(L(y, \\hat{y})\\) For regression, squared error is often used \\( \\(L(y_1, f(x_i)) = (y_i - f(x_i))^2\\) \\) Loss on testing set Empirical loss is measuring the loss on the training set We assume both training set and testing set are i.i.d from the same distribution D - Minimizing loss on training set will make loss on testing set small","title":"Loss"},{"location":"MSBD5001/Lecture%202/#minimizing-loss-functions","text":"The minimizers of some loss functions have analytical solutions: an exact solution you can explicitly derive by analyzing the formula. However, most poular supervised learning models use loss functions with no analytical solution We use gradient descent to approximate the minimal value of function. Gradients: A vector, points to the direction where changing value is the fastest.","title":"Minimizing loss functions"},{"location":"MSBD5001/Lecture%202/#method","text":"For function G, randomly guess an initial value \\(x_0\\) Repeat \\(xi+1 = x_i - r \\times \\nabla G(x)\\) where \\(\\nabla\\) denotes the gradients, r denotes learning rate Until convergence from sympy import symbols , diff r = 0.1 f_i = ( 1 , 1 , 1 ) x , y , z = symbols ( 'x y z' , real = True ) f = ( y + 2 * x ) ** 2 + y + 2 * x g = ( diff ( f , x ), diff ( f , y ), diff ( f , z )) G (8*x + 4*y + 2, 4*x + 2*y + 1, 0) import numpy as np result = np . array ([ 8 , 6 , 3 ]) * r + np . array ([ 1 , 1 , 1 ]) result array([1.8, 1.6, 1.3])","title":"Method"},{"location":"MSBD5001/Lecture%202/#linear-classification","text":"Use a line to separate data points Use \\(x = (x_1, x_2)\\) , \\(w = (w_1, w_2)\\) , i.e., x, w are vectors in 2D space Doesn't work well with classification problem Label y as either 1 or -1 Find f_w(x) = w^Tx that minimizes the loss function \\( \\(L(f_w(x)) = \\frac{1}{n}\\sum_{i=1}^n(w^Tx_i-y_1)^2\\) \\) Find a line that minimizes the distance between red and blue If there is a outlier in the graph, the seperation line will miss classification some points","title":"Linear Classification"},{"location":"MSBD5001/Lecture%202/#-if-the-value-get-very-large-the-wtx_i-is-correct-to-large-loss-value-even-if-predict-value-is-positive","text":"Solution: We use sigmoid function to minimize the value between 0 and 1 \\( \\(\\sigma(a) = \\frac{1}{1+exp(-a)}\\) \\) Similar to step functions Continuous and easy to compute","title":"- If the value get very large, the \\(w^TX_i\\) is correct \\(\\to\\) large loss value even if predict value is positive."},{"location":"MSBD5001/Lecture%202/#some-properties-of-sigmoid-function","text":"\\(\\sigma(a) = \\frac{1}{1+exp(-a)} \\in (0, 1)\\) symetric Easy to compute gradients","title":"Some properties of sigmoid function"},{"location":"MSBD5001/Lecture%202/#logistic-regression","text":"Better approach ( cross-entropy loss function) find w that minimizes loss function If misclassfication happens on i-th data with label 1, \\(log(\\sigma(w^Tx_i))\\) is very large No analytical solution, needs gradient descent","title":"Logistic Regression"},{"location":"MSBD5001/Lecture%202/#svm","text":"A svm performs classification by finding the hyperplane that maximizes the margin between the two classes","title":"SVM"},{"location":"MSBD5001/Lecture%202/#k-nearest-neighbor-methods","text":"Learning algorithm: just store training examples Prediction algorithm: Regression: take the average value of k nearest neighbors Classification: assign to the most frequent class of k nearest neighbors Easy to train with high storage requirement, but high-computation cost at prediction --- Linear knn Advantages Easy to fit Strong assumtions on linear relationship Disadvantages Hard to classify the data Takes a lot of computation power","title":"K-Nearest neighbor methods"},{"location":"MSBD5001/Lecture%202/#decision-tree","text":"Entropy is used to measure how informative is a probability distribution. The more entropy, the more uncertainty. More info","title":"Decision Tree"},{"location":"MSBD5001/Lecture%202/#wrap-up","text":"Collect data, extract features Determine a model Select a good model for your data","title":"Wrap up"},{"location":"MSBD5001/Lecture%203/","text":"Overfitting and underfitting \u00b6 Overfitting \u00b6 Even when training data and testing data are i.i.d, generalization may also fail. Is a modeling error which occurs when a function is too closely fit to a limited set of data points. Why is overfitting a problem \u00b6 Overfitting leads to low training error yet high testing error. Out goal is to make the testing error small, Not the training error. Plotting a polynomial \u00b6 Using a polynomial of degree N to fit \\(y==\\sum^N_{i=1}w_ix_i\\) Higher degree has more complex curve to fit the data. https://github.com/MSBD-5001/Lecture-Materials/blob/master/l3_simulation_lecture.ipynb Underfitting \u00b6 Occurs when the model or algorithm doesn't fit the the data well enough. Example \u00b6 import numpy as np import matplotlib.pyplot as plt import pandas as pd import statsmodels.api as sm from statsmodels import regression from scipy i0mport poly1d /usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm x = np . arange ( 10 ) y = 2 * np . random . randn ( 10 ) + x ** 2 xs = np . linspace ( - 0 . 25 , 9 . 25 , 200 ) lin = np . polyfit ( x , y , 1 ) quad = np . polyfit ( x , y , 2 ) many = np . polyfit ( x , y , 9 ) plt . scatter ( x , y ) plt . plot ( xs , poly1d ( lin )( xs )) plt . plot ( xs , poly1d ( quad )( xs )) plt . plot ( xs , poly1d ( many )( xs )) plt . ylabel ( 'Y' ) plt . xlabel ( 'X' ) plt . legend ([ 'Underfit' , 'Good fit' , 'Overfit' ]); Errors: Bias and variance \u00b6 Expect error = \\(Bias^2\\) + Variance + Noise Bias: Difference between the average prediction of our model and the correct value which we are trying to predict Variance: The variability of model prediction for a fiven data point. Our goal is to select models that are of optimal complexity. Complex models have low bias and high variance: - Low bias: Complicated models capture a lot of features - High variance: testing set many not have the same feature - Overfitting Simeple models have low variance and high bias. - Underfitting How to reduce variance and keep bias at a low value? \u00b6 Larger training dataset reduces variance Noise is unavoidable on the data Regularization and ensemble learning Selecting good models \u00b6 Validation \u00b6 Split training data into training and validation data Validation data are only used to evaluate the performance of trained model. If model generalize well on validation data, then should also generalize well on testing data. Wasting part of original training data. Cross validation \u00b6 Will make all training data for validation Partition training data into serveral groups repeat: One group as validation set, train new model Performance metric: average error on validation data. k-fold cross validation \u00b6 Equally split data into k folds Each time uses one fold as validation K fold can be used for large dataset Leave-one-out can be used when dataset is small. Use only 1 sample for validation, the rest for training. Select models with cross-validation. Use cross validation to evaluate performance of different models. Select the best model. Improving the models \u00b6 Method Train sequentially or in parallel How to generate different models Reduces bias or variance Bagging Parallel Boostrap data Variance Random Forest Parallel Bootsrap + random subset of features at splitting Variance Boosting Sequential Reweight training data Bias and variance Regularization \u00b6 Prevent overfitting by reducing flexibility of the model. Prevent parameters having too large absolute values. - Reduce variance - Prevent overfitting Ensemeble \u00b6 Standard decision trees can achieve low bias. - Training set error can be zero. You can always train to the last branch - Large variance Early stopping with fixed nodes or fixed depth may incur high bias Averaging \u00b6 For regression: Simply average the results predicted by different trees, can take weighted average For classification: just select the most predicted value. Also called voting Baging \u00b6 Short for Boostrap aggregating. Bootstrap samples B times, each with size N, with replacement. Train B classifiers each with a bootstrap sample. Bagging gets similar bias: data are from resampling. Random Forest \u00b6 Refinement of the bagged trees. Problem: We want the trees to be independent, don't want them to be similar. But bootstrapping data doesn't help that much: still drawn from same dataset with all features. At each tree split, a random sample of m features are drawn. Only these m features are consldered for splitting. Typically, m is \\(\\sqrt{p}\\) pr \\(p/3\\) where p is the total number of features. Boosting \u00b6 Random forest and bagging: trees are trained in parallel Boosting: trees should be trained sequentially - Start with original training sample - In each iteration: - Train a classifier and check wich samples are hard to train - Increase the weight of those mis-classified samples in training data - Repeat this - Final classifier: weighted classifier model.","title":"Lecture 3"},{"location":"MSBD5001/Lecture%203/#overfitting-and-underfitting","text":"","title":"Overfitting and underfitting"},{"location":"MSBD5001/Lecture%203/#overfitting","text":"Even when training data and testing data are i.i.d, generalization may also fail. Is a modeling error which occurs when a function is too closely fit to a limited set of data points.","title":"Overfitting"},{"location":"MSBD5001/Lecture%203/#why-is-overfitting-a-problem","text":"Overfitting leads to low training error yet high testing error. Out goal is to make the testing error small, Not the training error.","title":"Why is overfitting a problem"},{"location":"MSBD5001/Lecture%203/#plotting-a-polynomial","text":"Using a polynomial of degree N to fit \\(y==\\sum^N_{i=1}w_ix_i\\) Higher degree has more complex curve to fit the data. https://github.com/MSBD-5001/Lecture-Materials/blob/master/l3_simulation_lecture.ipynb","title":"Plotting a polynomial"},{"location":"MSBD5001/Lecture%203/#underfitting","text":"Occurs when the model or algorithm doesn't fit the the data well enough.","title":"Underfitting"},{"location":"MSBD5001/Lecture%203/#example","text":"import numpy as np import matplotlib.pyplot as plt import pandas as pd import statsmodels.api as sm from statsmodels import regression from scipy i0mport poly1d /usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm x = np . arange ( 10 ) y = 2 * np . random . randn ( 10 ) + x ** 2 xs = np . linspace ( - 0 . 25 , 9 . 25 , 200 ) lin = np . polyfit ( x , y , 1 ) quad = np . polyfit ( x , y , 2 ) many = np . polyfit ( x , y , 9 ) plt . scatter ( x , y ) plt . plot ( xs , poly1d ( lin )( xs )) plt . plot ( xs , poly1d ( quad )( xs )) plt . plot ( xs , poly1d ( many )( xs )) plt . ylabel ( 'Y' ) plt . xlabel ( 'X' ) plt . legend ([ 'Underfit' , 'Good fit' , 'Overfit' ]);","title":"Example"},{"location":"MSBD5001/Lecture%203/#errors-bias-and-variance","text":"Expect error = \\(Bias^2\\) + Variance + Noise Bias: Difference between the average prediction of our model and the correct value which we are trying to predict Variance: The variability of model prediction for a fiven data point. Our goal is to select models that are of optimal complexity. Complex models have low bias and high variance: - Low bias: Complicated models capture a lot of features - High variance: testing set many not have the same feature - Overfitting Simeple models have low variance and high bias. - Underfitting","title":"Errors: Bias and variance"},{"location":"MSBD5001/Lecture%203/#how-to-reduce-variance-and-keep-bias-at-a-low-value","text":"Larger training dataset reduces variance Noise is unavoidable on the data Regularization and ensemble learning","title":"How to reduce variance and keep bias at a low value?"},{"location":"MSBD5001/Lecture%203/#selecting-good-models","text":"","title":"Selecting good models"},{"location":"MSBD5001/Lecture%203/#validation","text":"Split training data into training and validation data Validation data are only used to evaluate the performance of trained model. If model generalize well on validation data, then should also generalize well on testing data. Wasting part of original training data.","title":"Validation"},{"location":"MSBD5001/Lecture%203/#cross-validation","text":"Will make all training data for validation Partition training data into serveral groups repeat: One group as validation set, train new model Performance metric: average error on validation data.","title":"Cross validation"},{"location":"MSBD5001/Lecture%203/#k-fold-cross-validation","text":"Equally split data into k folds Each time uses one fold as validation K fold can be used for large dataset Leave-one-out can be used when dataset is small. Use only 1 sample for validation, the rest for training. Select models with cross-validation. Use cross validation to evaluate performance of different models. Select the best model.","title":"k-fold cross validation"},{"location":"MSBD5001/Lecture%203/#improving-the-models","text":"Method Train sequentially or in parallel How to generate different models Reduces bias or variance Bagging Parallel Boostrap data Variance Random Forest Parallel Bootsrap + random subset of features at splitting Variance Boosting Sequential Reweight training data Bias and variance","title":"Improving the models"},{"location":"MSBD5001/Lecture%203/#regularization","text":"Prevent overfitting by reducing flexibility of the model. Prevent parameters having too large absolute values. - Reduce variance - Prevent overfitting","title":"Regularization"},{"location":"MSBD5001/Lecture%203/#ensemeble","text":"Standard decision trees can achieve low bias. - Training set error can be zero. You can always train to the last branch - Large variance Early stopping with fixed nodes or fixed depth may incur high bias","title":"Ensemeble"},{"location":"MSBD5001/Lecture%203/#averaging","text":"For regression: Simply average the results predicted by different trees, can take weighted average For classification: just select the most predicted value. Also called voting","title":"Averaging"},{"location":"MSBD5001/Lecture%203/#baging","text":"Short for Boostrap aggregating. Bootstrap samples B times, each with size N, with replacement. Train B classifiers each with a bootstrap sample. Bagging gets similar bias: data are from resampling.","title":"Baging"},{"location":"MSBD5001/Lecture%203/#random-forest","text":"Refinement of the bagged trees. Problem: We want the trees to be independent, don't want them to be similar. But bootstrapping data doesn't help that much: still drawn from same dataset with all features. At each tree split, a random sample of m features are drawn. Only these m features are consldered for splitting. Typically, m is \\(\\sqrt{p}\\) pr \\(p/3\\) where p is the total number of features.","title":"Random Forest"},{"location":"MSBD5001/Lecture%203/#boosting","text":"Random forest and bagging: trees are trained in parallel Boosting: trees should be trained sequentially - Start with original training sample - In each iteration: - Train a classifier and check wich samples are hard to train - Increase the weight of those mis-classified samples in training data - Repeat this - Final classifier: weighted classifier model.","title":"Boosting"},{"location":"MSBD5001/Lecture1/","text":"Lecture 1 \u00b6 Classification - Data to classes Regression - Predicting a numeric value Clustering Different types of problems \u00b6 Classification Problem - MNIST Dataset Regression - Predicting stock value Clustering Automatically identify the data Data integration \u00b6 Data are created independently A higher-level abstraction Statical analysis \u00b6 Collecting data \u00b6 Collecting, exploring and presenting large amounts of data to discover underlying patterns and trends Data come in two types: - Discrete - Continuous We have - barchart - piechart, Stem-and-leaf plot - Scatterplot ( it uses caresian coordinates to display values for two variables for set of data) - Form - Direction Numerical descriptive measures of data (Central tendency) - Mean - Min - Max - Median - Mode A sampling method is a procudure for selecting sample elements from a population. Relationship between variables: \u00b6 Eyeball fit: Fit two points on the plot so that the line passing through them fives a fairly good fit. Least square fit: Fit a line \\(y = a + bX\\) such that it minimaizes the error S Correlation coefficient, denoted as r, measures the degree to which two variables movements are associated. r = 1 means perfect positive relationship r = 1 means a perfect negative relationship r = 0 means no relationship Forecasting \u00b6 An experiment is an action where the result is uncertain A sample space is all the possible outomes of an experiment, denoted as \\(S\\) . A event is a subset of S Probability : is the measure of how likely an event is to occur out of the number of possible outcomes. $p = \\frac{The\\ number\\ of outcomes}{sample space} $ Parameters \u00b6 Sample can be generated by a probability model, where parameters are characteristics of the model Variance \u00b6 Variance is another parameter of probability model It is a measure of how spread out it is Statical analysis \u00b6 Collecting, exploring and presenting large amounts of data to discover underlying patterns and","title":"Lecture1"},{"location":"MSBD5001/Lecture1/#lecture-1","text":"Classification - Data to classes Regression - Predicting a numeric value Clustering","title":"Lecture 1"},{"location":"MSBD5001/Lecture1/#different-types-of-problems","text":"Classification Problem - MNIST Dataset Regression - Predicting stock value Clustering Automatically identify the data","title":"Different types of problems"},{"location":"MSBD5001/Lecture1/#data-integration","text":"Data are created independently A higher-level abstraction","title":"Data integration"},{"location":"MSBD5001/Lecture1/#statical-analysis","text":"","title":"Statical analysis"},{"location":"MSBD5001/Lecture1/#collecting-data","text":"Collecting, exploring and presenting large amounts of data to discover underlying patterns and trends Data come in two types: - Discrete - Continuous We have - barchart - piechart, Stem-and-leaf plot - Scatterplot ( it uses caresian coordinates to display values for two variables for set of data) - Form - Direction Numerical descriptive measures of data (Central tendency) - Mean - Min - Max - Median - Mode A sampling method is a procudure for selecting sample elements from a population.","title":"Collecting data"},{"location":"MSBD5001/Lecture1/#relationship-between-variables","text":"Eyeball fit: Fit two points on the plot so that the line passing through them fives a fairly good fit. Least square fit: Fit a line \\(y = a + bX\\) such that it minimaizes the error S Correlation coefficient, denoted as r, measures the degree to which two variables movements are associated. r = 1 means perfect positive relationship r = 1 means a perfect negative relationship r = 0 means no relationship","title":"Relationship between variables:"},{"location":"MSBD5001/Lecture1/#forecasting","text":"An experiment is an action where the result is uncertain A sample space is all the possible outomes of an experiment, denoted as \\(S\\) . A event is a subset of S Probability : is the measure of how likely an event is to occur out of the number of possible outcomes. $p = \\frac{The\\ number\\ of outcomes}{sample space} $","title":"Forecasting"},{"location":"MSBD5001/Lecture1/#parameters","text":"Sample can be generated by a probability model, where parameters are characteristics of the model","title":"Parameters"},{"location":"MSBD5001/Lecture1/#variance","text":"Variance is another parameter of probability model It is a measure of how spread out it is","title":"Variance"},{"location":"MSBD5001/Lecture1/#statical-analysis_1","text":"Collecting, exploring and presenting large amounts of data to discover underlying patterns and","title":"Statical analysis"},{"location":"MSBD5003/Lecture%202/","text":"!pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 61kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 43.9MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=b5c7cfca9a7d57719e6782127296677cad2ca9705fded3759f518ea214bb4960 Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 Spark \u00b6 Execution is pipelined and parallel. No need to store intermediate results. Lazy execution allows optimization. RDD has enough information about how it was rderived from to compute its partitions from data in stable storage. Example: If a partition of errors is lost, Spark rebuilds it by applying a filter on only the corresponding partition of lines. Partitions can be recomputed in parallel on different nodes, without having to roll back the whole program. Sample spark program \u00b6 from pyspark.context import SparkContext import requests sc = SparkContext.getOrCreate() text_file = sc.textFile('sample_data/README.md') Filter each line which contains T lines = text_file.filter(lambda line: 'T' in line) lines.collect() ['This directory includes a few sample datasets to get you started.', ' [MNIST database](https://en.wikipedia.org/wiki/MNIST_database), which is', ' Statistician. 27 (1): 17-21. JSTOR 2682899.'] Average text_file = sc.textFile('sample_data/README.md', 2) lines.map(lambda line: len(line.split())).reduce(lambda a, b: (a + b) / 2) 6.75 Average with partition 3 text_file = sc.textFile('sample_data/README.md', 5) lines.map(lambda line: len(line.split())).reduce(lambda a, b: (a + b) / 2) 6.75 RDD Operations \u00b6 Download file r = requests.get('https://www.cse.ust.hk/msbd5003/data/fruits.txt') open('fruits.txt', 'wb').write(r.content) 65 print file \u00b6 fruits = sc.textFile('fruits.txt') fruits.collect() ['apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry'] Map \u00b6 fruitsReversed = fruits . map ( lambda fruit : fruit [ ::- 1 ]) fruitsReversed . collect () ['elppa', 'ananab', 'nolem yranac', 'parg', 'nomel', 'egnaro', 'elppaenip', 'yrrebwarts'] Filter \u00b6 shortFruits = fruits.filter(lambda fruit: len(fruit) <= 5) shortFruits.collect() ['apple', 'grap', 'lemon'] FlatMap \u00b6 characters = fruits.flatMap(lambda fruit: list(fruit)) print(characters.collect()) ['a', 'p', 'p', 'l', 'e', 'b', 'a', 'n', 'a', 'n', 'a', 'c', 'a', 'n', 'a', 'r', 'y', ' ', 'm', 'e', 'l', 'o', 'n', 'g', 'r', 'a', 'p', 'l', 'e', 'm', 'o', 'n', 'o', 'r', 'a', 'n', 'g', 'e', 'p', 'i', 'n', 'e', 'a', 'p', 'p', 'l', 'e', 's', 't', 'r', 'a', 'w', 'b', 'e', 'r', 'r', 'y'] Collect \u00b6 new_fruits = fruits.union(fruits) new_fruits.collect() ['apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry', 'apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry'] new_fruits = fruits.intersection(fruits) new_fruits.collect() ['orange', 'pineapple', 'canary melon', 'lemon', 'banana', 'apple', 'grap', 'strawberry'] new_fruits = fruits.union(fruits).distinct() new_fruits.collect() ['orange', 'pineapple', 'canary melon', 'lemon', 'banana', 'apple', 'grap', 'strawberry'] RDD Actions \u00b6 collect \u00b6 take \u00b6 first3Fruits = fruits.take(3) print(first3Fruits) ['apple', 'banana', 'canary melon'] count \u00b6 fruits.count() 8 reduce \u00b6 fruits.map(lambda fruit: set(fruit)).reduce(lambda x, y: x.union(y)) {' ', 'a', 'b', 'c', 'e', 'g', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'w', 'y'}","title":"Lecture 2"},{"location":"MSBD5003/Lecture%202/#spark","text":"Execution is pipelined and parallel. No need to store intermediate results. Lazy execution allows optimization. RDD has enough information about how it was rderived from to compute its partitions from data in stable storage. Example: If a partition of errors is lost, Spark rebuilds it by applying a filter on only the corresponding partition of lines. Partitions can be recomputed in parallel on different nodes, without having to roll back the whole program.","title":"Spark"},{"location":"MSBD5003/Lecture%202/#sample-spark-program","text":"from pyspark.context import SparkContext import requests sc = SparkContext.getOrCreate() text_file = sc.textFile('sample_data/README.md') Filter each line which contains T lines = text_file.filter(lambda line: 'T' in line) lines.collect() ['This directory includes a few sample datasets to get you started.', ' [MNIST database](https://en.wikipedia.org/wiki/MNIST_database), which is', ' Statistician. 27 (1): 17-21. JSTOR 2682899.'] Average text_file = sc.textFile('sample_data/README.md', 2) lines.map(lambda line: len(line.split())).reduce(lambda a, b: (a + b) / 2) 6.75 Average with partition 3 text_file = sc.textFile('sample_data/README.md', 5) lines.map(lambda line: len(line.split())).reduce(lambda a, b: (a + b) / 2) 6.75","title":"Sample spark program"},{"location":"MSBD5003/Lecture%202/#rdd-operations","text":"Download file r = requests.get('https://www.cse.ust.hk/msbd5003/data/fruits.txt') open('fruits.txt', 'wb').write(r.content) 65","title":"RDD Operations"},{"location":"MSBD5003/Lecture%202/#print-file","text":"fruits = sc.textFile('fruits.txt') fruits.collect() ['apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry']","title":"print file"},{"location":"MSBD5003/Lecture%202/#map","text":"fruitsReversed = fruits . map ( lambda fruit : fruit [ ::- 1 ]) fruitsReversed . collect () ['elppa', 'ananab', 'nolem yranac', 'parg', 'nomel', 'egnaro', 'elppaenip', 'yrrebwarts']","title":"Map"},{"location":"MSBD5003/Lecture%202/#filter","text":"shortFruits = fruits.filter(lambda fruit: len(fruit) <= 5) shortFruits.collect() ['apple', 'grap', 'lemon']","title":"Filter"},{"location":"MSBD5003/Lecture%202/#flatmap","text":"characters = fruits.flatMap(lambda fruit: list(fruit)) print(characters.collect()) ['a', 'p', 'p', 'l', 'e', 'b', 'a', 'n', 'a', 'n', 'a', 'c', 'a', 'n', 'a', 'r', 'y', ' ', 'm', 'e', 'l', 'o', 'n', 'g', 'r', 'a', 'p', 'l', 'e', 'm', 'o', 'n', 'o', 'r', 'a', 'n', 'g', 'e', 'p', 'i', 'n', 'e', 'a', 'p', 'p', 'l', 'e', 's', 't', 'r', 'a', 'w', 'b', 'e', 'r', 'r', 'y']","title":"FlatMap"},{"location":"MSBD5003/Lecture%202/#collect","text":"new_fruits = fruits.union(fruits) new_fruits.collect() ['apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry', 'apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry'] new_fruits = fruits.intersection(fruits) new_fruits.collect() ['orange', 'pineapple', 'canary melon', 'lemon', 'banana', 'apple', 'grap', 'strawberry'] new_fruits = fruits.union(fruits).distinct() new_fruits.collect() ['orange', 'pineapple', 'canary melon', 'lemon', 'banana', 'apple', 'grap', 'strawberry']","title":"Collect"},{"location":"MSBD5003/Lecture%202/#rdd-actions","text":"","title":"RDD Actions"},{"location":"MSBD5003/Lecture%202/#collect_1","text":"","title":"collect"},{"location":"MSBD5003/Lecture%202/#take","text":"first3Fruits = fruits.take(3) print(first3Fruits) ['apple', 'banana', 'canary melon']","title":"take"},{"location":"MSBD5003/Lecture%202/#count","text":"fruits.count() 8","title":"count"},{"location":"MSBD5003/Lecture%202/#reduce","text":"fruits.map(lambda fruit: set(fruit)).reduce(lambda x, y: x.union(y)) {' ', 'a', 'b', 'c', 'e', 'g', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'w', 'y'}","title":"reduce"},{"location":"MSBD5003/Lecture1/","text":"Big Data Definition \u00b6 Big Data is data whose scale, complexity, and speed require new architecture, techniques, algorithms, and analytics to manage it and extract value and hidden knowledge from it. Old model: Few companies are generating data, all others are consuming data New model: all of us are generating and consuming data at the same time 3 vs \u00b6 Volume \u00b6 Data are big Velocity ( Streaming data ) \u00b6 Data is being generated fast and need to be processed fast Variety \u00b6 various formats, types, and structures Numerical, text, images, audio, video, sequences, time series, social media data, multi-dim arrays, etc... A single application can be generating/collecting many types of data The structure spectrum \u00b6 Structured (schema-first) Relational database -- Semi-structured(Schema-later) Documents XML -- Unstucture data (MongoDB) Structured data \u00b6 Modify the rows is easy but modify the column costs a lot. The relational data model is the most used data model Every relation has a schema defining each columns' type The programmer must statically specify the schema Semi-structured Data \u00b6 Json object consists of a collection name: value pairs, seperated by commas. Each value can be - A string - A number - A boolean - null - An array - a JSON object How to handle big data \u00b6 Race conditions ( use locks ) The frustration of parallel programming \u00b6 Hard to debug How to migrate from one archetecture to another Cloud comuting \u00b6 Dynamic provisioning Scalability Elasticity Mapreduce \u00b6 Map: Takes raw input and produces a key, value pair Reduce: Takes data with same key and produces outputs Shuffling and sorting - Hidden phase between mappers and reducers - Groups all key value pairs Where do we store data \u00b6 Target environment \u00b6 Files are huge but not many Many reads but few writes I/O bandwidth is more important than latency GFS design decisions \u00b6 Files stored into chunks Reliability through replication Single master to coordinate access, keep metadata HDFS \u00b6 Name node: Maintains metadata info about files Maps a filename to a set of blocks Maps a block to the data nodes where it resides replication engine for blocks Datanod Store data Files are divided into blocks Communicates with name nodes through periodic heartbeat M data cells and n parity cells Storage efficiency = \\(\\frac{m}{m+n}\\) # On worker ailure \u00b6 Detect failure via periodic heartbeats Re-execute completed and in-progress map tasks Re-execute Mapreduce: A major step backwards \u00b6 Mapreduce may be a good idea for writing certain types of computations A giant step backward in the programming paradigm for large-scale data intensive applications A sub-cptimal implementation, in that it uses brute force instead of indexing Missing most of features that are routinely included in current DMBS Not novel at all","title":"Lecture1"},{"location":"MSBD5003/Lecture1/#big-data-definition","text":"Big Data is data whose scale, complexity, and speed require new architecture, techniques, algorithms, and analytics to manage it and extract value and hidden knowledge from it. Old model: Few companies are generating data, all others are consuming data New model: all of us are generating and consuming data at the same time","title":"Big Data Definition"},{"location":"MSBD5003/Lecture1/#3-vs","text":"","title":"3 vs"},{"location":"MSBD5003/Lecture1/#volume","text":"Data are big","title":"Volume"},{"location":"MSBD5003/Lecture1/#velocity-streaming-data","text":"Data is being generated fast and need to be processed fast","title":"Velocity ( Streaming data )"},{"location":"MSBD5003/Lecture1/#variety","text":"various formats, types, and structures Numerical, text, images, audio, video, sequences, time series, social media data, multi-dim arrays, etc... A single application can be generating/collecting many types of data","title":"Variety"},{"location":"MSBD5003/Lecture1/#the-structure-spectrum","text":"Structured (schema-first) Relational database -- Semi-structured(Schema-later) Documents XML -- Unstucture data (MongoDB)","title":"The structure spectrum"},{"location":"MSBD5003/Lecture1/#structured-data","text":"Modify the rows is easy but modify the column costs a lot. The relational data model is the most used data model Every relation has a schema defining each columns' type The programmer must statically specify the schema","title":"Structured data"},{"location":"MSBD5003/Lecture1/#semi-structured-data","text":"Json object consists of a collection name: value pairs, seperated by commas. Each value can be - A string - A number - A boolean - null - An array - a JSON object","title":"Semi-structured Data"},{"location":"MSBD5003/Lecture1/#how-to-handle-big-data","text":"Race conditions ( use locks )","title":"How to handle big data"},{"location":"MSBD5003/Lecture1/#the-frustration-of-parallel-programming","text":"Hard to debug How to migrate from one archetecture to another","title":"The frustration of parallel programming"},{"location":"MSBD5003/Lecture1/#cloud-comuting","text":"Dynamic provisioning Scalability Elasticity","title":"Cloud comuting"},{"location":"MSBD5003/Lecture1/#mapreduce","text":"Map: Takes raw input and produces a key, value pair Reduce: Takes data with same key and produces outputs Shuffling and sorting - Hidden phase between mappers and reducers - Groups all key value pairs","title":"Mapreduce"},{"location":"MSBD5003/Lecture1/#where-do-we-store-data","text":"","title":"Where do we store data"},{"location":"MSBD5003/Lecture1/#target-environment","text":"Files are huge but not many Many reads but few writes I/O bandwidth is more important than latency","title":"Target environment"},{"location":"MSBD5003/Lecture1/#gfs-design-decisions","text":"Files stored into chunks Reliability through replication Single master to coordinate access, keep metadata","title":"GFS design decisions"},{"location":"MSBD5003/Lecture1/#hdfs","text":"Name node: Maintains metadata info about files Maps a filename to a set of blocks Maps a block to the data nodes where it resides replication engine for blocks Datanod Store data Files are divided into blocks Communicates with name nodes through periodic heartbeat M data cells and n parity cells Storage efficiency = \\(\\frac{m}{m+n}\\) #","title":"HDFS"},{"location":"MSBD5003/Lecture1/#on-worker-ailure","text":"Detect failure via periodic heartbeats Re-execute completed and in-progress map tasks Re-execute","title":"On worker ailure"},{"location":"MSBD5003/Lecture1/#mapreduce-a-major-step-backwards","text":"Mapreduce may be a good idea for writing certain types of computations A giant step backward in the programming paradigm for large-scale data intensive applications A sub-cptimal implementation, in that it uses brute force instead of indexing Missing most of features that are routinely included in current DMBS Not novel at all","title":"Mapreduce: A major step backwards"},{"location":"MSBD5006/Lecture%201/","text":"Asset returns \u00b6 Let \\(p_t\\) be the price of an asset at time t, and assume no dividend. One-period simple return or simple net return. \\[R_t = \\frac{P_t}{P_t-1} - 1 = \\frac{P_t - P_{t-1}}{P_{t-1}} \\] Gross return \\[ 1 + P_t = \\frac{P_t}{P_{t - 1}}\\ or P_t = P_{t-1}(1+P_t) \\] Multi-period simple return or the k-period simple net return \\[R_t(k) = \\frac{P_t}{P_{t-k}}-1\\] Gross return \\[1 + R_t(k) = \\sum^{k -1 }_{j=0}(1+R_{t-j}) \\] Continues Compounding \u00b6 from math import exp def pay_interest ( base , interest , payments ): return base * ( base + interest / payments ) ** payments def pay_interest_continue ( base , interest , number_of_years ): return base * exp ( interest * number_of_years ) pay_interest_continue(1, 0.1, 1) 1.1051709180756477 \\[ R_t = log \\] Log return \u00b6 \\[r_{pt} = \\sum_{i = 1}{n}w_ir_{it}\\] Excess return \u00b6 \\[Z_t = R_t - R_{0t}, z_t = r_t - r_{0t}\\] where \\(r_{0t}\\) denotes the log return of a reference asset (e.g. risk-free interest rate) such as shortterm U.S. Treasury bill return, etc..","title":"Lecture 1"},{"location":"MSBD5006/Lecture%201/#asset-returns","text":"Let \\(p_t\\) be the price of an asset at time t, and assume no dividend. One-period simple return or simple net return. \\[R_t = \\frac{P_t}{P_t-1} - 1 = \\frac{P_t - P_{t-1}}{P_{t-1}} \\] Gross return \\[ 1 + P_t = \\frac{P_t}{P_{t - 1}}\\ or P_t = P_{t-1}(1+P_t) \\] Multi-period simple return or the k-period simple net return \\[R_t(k) = \\frac{P_t}{P_{t-k}}-1\\] Gross return \\[1 + R_t(k) = \\sum^{k -1 }_{j=0}(1+R_{t-j}) \\]","title":"Asset returns"},{"location":"MSBD5006/Lecture%201/#continues-compounding","text":"from math import exp def pay_interest ( base , interest , payments ): return base * ( base + interest / payments ) ** payments def pay_interest_continue ( base , interest , number_of_years ): return base * exp ( interest * number_of_years ) pay_interest_continue(1, 0.1, 1) 1.1051709180756477 \\[ R_t = log \\]","title":"Continues Compounding"},{"location":"MSBD5006/Lecture%201/#log-return","text":"\\[r_{pt} = \\sum_{i = 1}{n}w_ir_{it}\\]","title":"Log return"},{"location":"MSBD5006/Lecture%201/#excess-return","text":"\\[Z_t = R_t - R_{0t}, z_t = r_t - r_{0t}\\] where \\(r_{0t}\\) denotes the log return of a reference asset (e.g. risk-free interest rate) such as shortterm U.S. Treasury bill return, etc..","title":"Excess return"},{"location":"MSBD5006/Lecture%202/","text":"Mean and variance: \u03bcx = E(X) and \u03c3x2 = Var(X) = E(X \u2212 \u03bcx)2 Skewness (symmetry) and kurtosis (fat-tails) Kurtosis: How high is the p High kurtosis implies heavy (or long) tails in dis- tribution. Symmetry has important implications in holding short or long financial positions and in risk man- agement. (X \u2212 \u03bcx)3 (X \u2212 \u03bcx)4 S ( x ) = E \u03c3 x3 , K ( x ) = E \u03c3 x4 . Normal distribution \u00b6 E(X) = \u03bc Var(X) = \u03c32 S(X) = 0 K(X) = 3 ml = 0, for l is odd. T-distribution \u00b6 Symmetry at 0 \\[E(x) > 0, v >1\\] Chi-squared distribution \u00b6 \\[E(X) = k$$ $$Var(X) = 2k\\] Joint Distribution \u00b6 \\[F_{X,Y}(x, y) = P(X\\leq x, Y\\leq y)\\] Marginal Distribution \u00b6 The marginal distribution of X is obtained by integrating out Y . A similar definition applies to the marginal distribution of Y .","title":"Lecture 2"},{"location":"MSBD5006/Lecture%202/#normal-distribution","text":"E(X) = \u03bc Var(X) = \u03c32 S(X) = 0 K(X) = 3 ml = 0, for l is odd.","title":"Normal distribution"},{"location":"MSBD5006/Lecture%202/#t-distribution","text":"Symmetry at 0 \\[E(x) > 0, v >1\\]","title":"T-distribution"},{"location":"MSBD5006/Lecture%202/#chi-squared-distribution","text":"\\[E(X) = k$$ $$Var(X) = 2k\\]","title":"Chi-squared distribution"},{"location":"MSBD5006/Lecture%202/#joint-distribution","text":"\\[F_{X,Y}(x, y) = P(X\\leq x, Y\\leq y)\\]","title":"Joint Distribution"},{"location":"MSBD5006/Lecture%202/#marginal-distribution","text":"The marginal distribution of X is obtained by integrating out Y . A similar definition applies to the marginal distribution of Y .","title":"Marginal Distribution"},{"location":"MSBD5012/lectures/Lecture%202/","text":"Divergence \u00b6 ML Setup \\(P(x)\\) -> generate -> Data -> learn \\(Q(x)\\) where Q should as close to P as possible. Relative entropy or Kullback-Leibler divergence \u00b6 Meassure ow much a distribution Q(X) differs from a \"True\" probability distribution P(X) K-L Divergence if Q from P is defined as follows: \\[ KL(P||Q) = \\sum_x{P(X)log(\\frac{P(X)}{Q(X)})} = -log\\sum_x{Q(X)}\\] Minimize cross entropy = Maximizing log likelyhood Supervised learning \u00b6 Unsupervised learning \u00b6 Multual information \u00b6 H(x): Initial uncertainty about x H(X | Y): Expected uncertainty about x if y is tested Linear Regression \u00b6 \\[y = w_0 + w_1x_1\\] Mean Square Error \u00b6 import numpy as np # Given values Y_true = [ 1 , 1 , 2 , 2 , 4 ] # Y_true = Y (original values) # Calculated values Y_pred = [ 0.6 , 1.29 , 1.99 , 2.69 , 3.4 ] # Y_pred = Y' # Mean Squared Error MSE = np . square ( np . subtract ( Y_true , Y_pred )) . mean () MSE 0.21606 Hypothesis space \u00b6 Is the set of functioins that it is allowed to select as being the solution. Thhe size of the hypothesis space is called the capacity of the model. For polynomial regression, the larger the d, the higher the model capacity. Higher model capacity implies better fit to training data. \\(S_1 = \\{y = w_0 + w_1x_1 | w_0, w_1 \\in R\\}\\) \\(S_2 = \\{y=w_0 + w_1x_1 + w_2x_1^2 + w_3x_1^3 | w_0, w_1, w_2, w_3 \\in R\\}\\) Generalization Error \u00b6 Model select: Validation Split training data into two parts. One part for training and second part for validation. This has to be randomly split. Regularization","title":"Lecture 2"},{"location":"MSBD5012/lectures/Lecture%202/#divergence","text":"ML Setup \\(P(x)\\) -> generate -> Data -> learn \\(Q(x)\\) where Q should as close to P as possible.","title":"Divergence"},{"location":"MSBD5012/lectures/Lecture%202/#relative-entropy-or-kullback-leibler-divergence","text":"Meassure ow much a distribution Q(X) differs from a \"True\" probability distribution P(X) K-L Divergence if Q from P is defined as follows: \\[ KL(P||Q) = \\sum_x{P(X)log(\\frac{P(X)}{Q(X)})} = -log\\sum_x{Q(X)}\\] Minimize cross entropy = Maximizing log likelyhood","title":"Relative entropy or Kullback-Leibler divergence"},{"location":"MSBD5012/lectures/Lecture%202/#supervised-learning","text":"","title":"Supervised learning"},{"location":"MSBD5012/lectures/Lecture%202/#unsupervised-learning","text":"","title":"Unsupervised learning"},{"location":"MSBD5012/lectures/Lecture%202/#multual-information","text":"H(x): Initial uncertainty about x H(X | Y): Expected uncertainty about x if y is tested","title":"Multual information"},{"location":"MSBD5012/lectures/Lecture%202/#linear-regression","text":"\\[y = w_0 + w_1x_1\\]","title":"Linear Regression"},{"location":"MSBD5012/lectures/Lecture%202/#mean-square-error","text":"import numpy as np # Given values Y_true = [ 1 , 1 , 2 , 2 , 4 ] # Y_true = Y (original values) # Calculated values Y_pred = [ 0.6 , 1.29 , 1.99 , 2.69 , 3.4 ] # Y_pred = Y' # Mean Squared Error MSE = np . square ( np . subtract ( Y_true , Y_pred )) . mean () MSE 0.21606","title":"Mean Square Error"},{"location":"MSBD5012/lectures/Lecture%202/#hypothesis-space","text":"Is the set of functioins that it is allowed to select as being the solution. Thhe size of the hypothesis space is called the capacity of the model. For polynomial regression, the larger the d, the higher the model capacity. Higher model capacity implies better fit to training data. \\(S_1 = \\{y = w_0 + w_1x_1 | w_0, w_1 \\in R\\}\\) \\(S_2 = \\{y=w_0 + w_1x_1 + w_2x_1^2 + w_3x_1^3 | w_0, w_1, w_2, w_3 \\in R\\}\\)","title":"Hypothesis space"},{"location":"MSBD5012/lectures/Lecture%202/#generalization-error","text":"Model select: Validation Split training data into two parts. One part for training and second part for validation. This has to be randomly split. Regularization","title":"Generalization Error"},{"location":"MSBD5012/lectures/Lecture1/","text":"Likehood possibility \u00b6 \\[L(H|E) = P(E|H) \\]","title":"Lecture1"},{"location":"MSBD5012/lectures/Lecture1/#likehood-possibility","text":"\\[L(H|E) = P(E|H) \\]","title":"Likehood possibility"}]}