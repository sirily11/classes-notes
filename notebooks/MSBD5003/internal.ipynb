{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"internal.ipynb","provenance":[],"authorship_tag":"ABX9TyNFmHT3STmLNUKQk3JeHNNu"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"cHnEmllzosm1"},"source":["!pip install pyspark"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QNkB4iZs9J7u"},"source":["Code at internal.ipynb"]},{"cell_type":"markdown","metadata":{"id":"jVGYhPeV9TqS"},"source":["## Data Partitioning"]},{"cell_type":"markdown","metadata":{"id":"ZNkbm4jj9XLR"},"source":["- RDDs are stored in partitions. When performing computations on RDDs, these partitions can be operated on in parallel. \n","\n","- You get better parallelism when the partitions are balanced.\n","\n","- When RDDs are first created, the partitions are balanced.\n","However, partitions may get out of balance after certain transformations."]},{"cell_type":"markdown","metadata":{"id":"VdUKWm6A_pja"},"source":["### Hash Partitioning\n","\n","We can view the contents of each partition:\n","- e.g., prime.glom().collect()[1][0:4]\n","- We see that it hashed all numbers x such that x mod 8 = 1 to partition #1\n","\n","In general, hash partitioning allocates tuple (k, v) to partition p where \n","- p = k.hashCode() % numPartitions\n","\n","Usually works well but be aware of bad inputs!"]},{"cell_type":"markdown","metadata":{"id":"WnqplDhOAF2n"},"source":["### Shuffle\n","\n","Spark uses shuffles to implement wide dependencies\n","- Examples: reduceByKey, repartition, coalesce, join (on RDDs not partitioned using the same partitioner)\n","\n","Spark generates sets of tasks - map tasks to organize the data, and a set of reduce tasks to group/aggregate it.\n","\n","Internally, Spark builds a hash table within each task to perform the grouping.\n","\n","If the hash table is too large, Spark will spill these tables to disk, incurring the additional overhead of disk I/O\n","\n","RDDs resulting from shuffles are automatically cached."]},{"cell_type":"markdown","metadata":{"id":"95r3b3oTLkYD"},"source":["### Range partitioning\n","\n","For data types that have or ordering defined\n","- Examples: Int, Char, String, …\n","- Internally, Spark samples the data so as to produce more balanced partitions.\n","- Used by default after sorting\n","Example: \n","- An RDD with keys [8, 96, 240, 400, 401, 800], \n","- Number of partitions: 4\n","- In this case, hash partitioning distributes the keys as follows among the partitions:\n","- partition 0: [8, 96, 240, 400, 800]\n","- partition 1: [401]\n","- partition 2: []\n","- partition 3: []\n","\n","Range partitioning would improve the partitioning significantly"]},{"cell_type":"markdown","metadata":{"id":"d8CPzEr6L2XO"},"source":["## Partitioner inheritance\n","\n","Operations on Pair RDDs that hold to (and propagate) a partitioner:\n","\n","- mapValues (if parent has a partitioner)\n","- flatMapValues (if parent has a partitioner)\n","- filter (if parent has a partitioner)"]},{"cell_type":"markdown","metadata":{"id":"2nbQeSWaBpBs"},"source":["## Partitioning Data Using Transformations\n","\n","Map will lose partitioner because it may change the key so that mapValues will enable us to still do map transformations without changing the keys."]},{"cell_type":"markdown","metadata":{"id":"uPhoq7cGCdJl"},"source":["## Job Scheduling\n","\n","- Job: a Spark action (e.g. save, collect) and any tasks that need to run to evaluate that action.\n","\n","- Multiple parallel jobs can run simultaneously if they are submitted from separate threads.\n","\n","- Spark’s scheduler runs jobs in FIFO fashion (default)\n","\n","- Each job consists of multiple stages\n","\n","- A stage can only start after all its parent stages have completed\n","\n","- Each stage has many tasks\n","Spark assigns tasks to machines based on data locality\n","\n","- Different levels of locality are used\n","    - PROCESS_LOCAL data is in the same JVM as the running code\n","    - NODE_LOCAL data is on the same node\n","    - RACK_LOCAL data is on the same rack of servers. \n","    - ANY \n","\n","- Spark will wait a bit for a free executor before switching to the next locality level.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"C6KyC9U-yvgs"},"source":[""],"execution_count":null,"outputs":[]}]}