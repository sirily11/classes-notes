{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"List of notes \u00b6","title":"List of notes"},{"location":"#list-of-notes","text":"","title":"List of notes"},{"location":"MSBD5001/Lecture%202/","text":"Supervised learning \u00b6 Supervised larning Unsupervised learning Reinforcement learning Whats is machine learning \u00b6 A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks. Linear Regression \u00b6 A Part of machine learning Given training set x, y Find a good approximation to f: \\(x \\to y\\) Examples: Spam detection ( Classification) Digit recognition ( Classification) House price prediction (Refression) Terminology \u00b6 Given a data point (x, y), x is called featyre vector, y is called label The dataset given for learning is training data The dataset to be tested is called testing data Machine learning 3 steps \u00b6 Collect data, extract features Determine a model Train the model with the data Loss \u00b6 Loss on traning set We measure the error using a loss function \\(L(y, \\hat{y})\\) For regression, squared error is often used \\( \\(L(y_1, f(x_i)) = (y_i - f(x_i))^2\\) \\) Loss on testing set Empirical loss is measuring the loss on the training set We assume both training set and testing set are i.i.d from the same distribution D - Minimizing loss on training set will make loss on testing set small Minimizing loss functions \u00b6 The minimizers of some loss functions have analytical solutions: an exact solution you can explicitly derive by analyzing the formula. However, most poular supervised learning models use loss functions with no analytical solution We use gradient descent to approximate the minimal value of function. Gradients: A vector, points to the direction where changing value is the fastest. Method \u00b6 For function G, randomly guess an initial value \\(x_0\\) Repeat \\(xi+1 = x_i - r \\times \\nabla G(x)\\) where \\(\\nabla\\) denotes the gradients, r denotes learning rate Until convergence from sympy import symbols , diff r = 0.1 f_i = ( 1 , 1 , 1 ) x , y , z = symbols ( 'x y z' , real = True ) f = ( y + 2 * x ) ** 2 + y + 2 * x g = ( diff ( f , x ), diff ( f , y ), diff ( f , z )) G (8*x + 4*y + 2, 4*x + 2*y + 1, 0) import numpy as np result = np . array ([ 8 , 6 , 3 ]) * r + np . array ([ 1 , 1 , 1 ]) result array([1.8, 1.6, 1.3]) Linear Classification \u00b6 Use a line to separate data points Use \\(x = (x_1, x_2)\\) , \\(w = (w_1, w_2)\\) , i.e., x, w are vectors in 2D space Doesn't work well with classification problem Label y as either 1 or -1 Find f_w(x) = w^Tx that minimizes the loss function \\( \\(L(f_w(x)) = \\frac{1}{n}\\sum_{i=1}^n(w^Tx_i-y_1)^2\\) \\) Find a line that minimizes the distance between red and blue If there is a outlier in the graph, the seperation line will miss classification some points - If the value get very large, the \\(w^TX_i\\) is correct \\(\\to\\) large loss value even if predict value is positive. \u00b6 Solution: We use sigmoid function to minimize the value between 0 and 1 \\( \\(\\sigma(a) = \\frac{1}{1+exp(-a)}\\) \\) Similar to step functions Continuous and easy to compute Some properties of sigmoid function \u00b6 \\(\\sigma(a) = \\frac{1}{1+exp(-a)} \\in (0, 1)\\) symetric Easy to compute gradients Logistic Regression \u00b6 Better approach ( cross-entropy loss function) find w that minimizes loss function If misclassfication happens on i-th data with label 1, \\(log(\\sigma(w^Tx_i))\\) is very large No analytical solution, needs gradient descent SVM \u00b6 A svm performs classification by finding the hyperplane that maximizes the margin between the two classes K-Nearest neighbor methods \u00b6 Learning algorithm: just store training examples Prediction algorithm: Regression: take the average value of k nearest neighbors Classification: assign to the most frequent class of k nearest neighbors Easy to train with high storage requirement, but high-computation cost at prediction --- Linear knn Advantages Easy to fit Strong assumtions on linear relationship Disadvantages Hard to classify the data Takes a lot of computation power Decision Tree \u00b6 Entropy is used to measure how informative is a probability distribution. The more entropy, the more uncertainty. More info Wrap up \u00b6 Collect data, extract features Determine a model Select a good model for your data","title":"Lecture 2"},{"location":"MSBD5001/Lecture%202/#supervised-learning","text":"Supervised larning Unsupervised learning Reinforcement learning","title":"Supervised learning"},{"location":"MSBD5001/Lecture%202/#whats-is-machine-learning","text":"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks.","title":"Whats is machine learning"},{"location":"MSBD5001/Lecture%202/#linear-regression","text":"A Part of machine learning Given training set x, y Find a good approximation to f: \\(x \\to y\\) Examples: Spam detection ( Classification) Digit recognition ( Classification) House price prediction (Refression)","title":"Linear Regression"},{"location":"MSBD5001/Lecture%202/#terminology","text":"Given a data point (x, y), x is called featyre vector, y is called label The dataset given for learning is training data The dataset to be tested is called testing data","title":"Terminology"},{"location":"MSBD5001/Lecture%202/#machine-learning-3-steps","text":"Collect data, extract features Determine a model Train the model with the data","title":"Machine learning 3 steps"},{"location":"MSBD5001/Lecture%202/#loss","text":"Loss on traning set We measure the error using a loss function \\(L(y, \\hat{y})\\) For regression, squared error is often used \\( \\(L(y_1, f(x_i)) = (y_i - f(x_i))^2\\) \\) Loss on testing set Empirical loss is measuring the loss on the training set We assume both training set and testing set are i.i.d from the same distribution D - Minimizing loss on training set will make loss on testing set small","title":"Loss"},{"location":"MSBD5001/Lecture%202/#minimizing-loss-functions","text":"The minimizers of some loss functions have analytical solutions: an exact solution you can explicitly derive by analyzing the formula. However, most poular supervised learning models use loss functions with no analytical solution We use gradient descent to approximate the minimal value of function. Gradients: A vector, points to the direction where changing value is the fastest.","title":"Minimizing loss functions"},{"location":"MSBD5001/Lecture%202/#method","text":"For function G, randomly guess an initial value \\(x_0\\) Repeat \\(xi+1 = x_i - r \\times \\nabla G(x)\\) where \\(\\nabla\\) denotes the gradients, r denotes learning rate Until convergence from sympy import symbols , diff r = 0.1 f_i = ( 1 , 1 , 1 ) x , y , z = symbols ( 'x y z' , real = True ) f = ( y + 2 * x ) ** 2 + y + 2 * x g = ( diff ( f , x ), diff ( f , y ), diff ( f , z )) G (8*x + 4*y + 2, 4*x + 2*y + 1, 0) import numpy as np result = np . array ([ 8 , 6 , 3 ]) * r + np . array ([ 1 , 1 , 1 ]) result array([1.8, 1.6, 1.3])","title":"Method"},{"location":"MSBD5001/Lecture%202/#linear-classification","text":"Use a line to separate data points Use \\(x = (x_1, x_2)\\) , \\(w = (w_1, w_2)\\) , i.e., x, w are vectors in 2D space Doesn't work well with classification problem Label y as either 1 or -1 Find f_w(x) = w^Tx that minimizes the loss function \\( \\(L(f_w(x)) = \\frac{1}{n}\\sum_{i=1}^n(w^Tx_i-y_1)^2\\) \\) Find a line that minimizes the distance between red and blue If there is a outlier in the graph, the seperation line will miss classification some points","title":"Linear Classification"},{"location":"MSBD5001/Lecture%202/#-if-the-value-get-very-large-the-wtx_i-is-correct-to-large-loss-value-even-if-predict-value-is-positive","text":"Solution: We use sigmoid function to minimize the value between 0 and 1 \\( \\(\\sigma(a) = \\frac{1}{1+exp(-a)}\\) \\) Similar to step functions Continuous and easy to compute","title":"- If the value get very large, the \\(w^TX_i\\) is correct \\(\\to\\) large loss value even if predict value is positive."},{"location":"MSBD5001/Lecture%202/#some-properties-of-sigmoid-function","text":"\\(\\sigma(a) = \\frac{1}{1+exp(-a)} \\in (0, 1)\\) symetric Easy to compute gradients","title":"Some properties of sigmoid function"},{"location":"MSBD5001/Lecture%202/#logistic-regression","text":"Better approach ( cross-entropy loss function) find w that minimizes loss function If misclassfication happens on i-th data with label 1, \\(log(\\sigma(w^Tx_i))\\) is very large No analytical solution, needs gradient descent","title":"Logistic Regression"},{"location":"MSBD5001/Lecture%202/#svm","text":"A svm performs classification by finding the hyperplane that maximizes the margin between the two classes","title":"SVM"},{"location":"MSBD5001/Lecture%202/#k-nearest-neighbor-methods","text":"Learning algorithm: just store training examples Prediction algorithm: Regression: take the average value of k nearest neighbors Classification: assign to the most frequent class of k nearest neighbors Easy to train with high storage requirement, but high-computation cost at prediction --- Linear knn Advantages Easy to fit Strong assumtions on linear relationship Disadvantages Hard to classify the data Takes a lot of computation power","title":"K-Nearest neighbor methods"},{"location":"MSBD5001/Lecture%202/#decision-tree","text":"Entropy is used to measure how informative is a probability distribution. The more entropy, the more uncertainty. More info","title":"Decision Tree"},{"location":"MSBD5001/Lecture%202/#wrap-up","text":"Collect data, extract features Determine a model Select a good model for your data","title":"Wrap up"},{"location":"MSBD5001/Lecture%203/","text":"Overfitting and underfitting \u00b6 Overfitting \u00b6 Even when training data and testing data are i.i.d, generalization may also fail. Is a modeling error which occurs when a function is too closely fit to a limited set of data points. Why is overfitting a problem \u00b6 Overfitting leads to low training error yet high testing error. Out goal is to make the testing error small, Not the training error. Plotting a polynomial \u00b6 Using a polynomial of degree N to fit \\(y==\\sum^N_{i=1}w_ix_i\\) Higher degree has more complex curve to fit the data. https://github.com/MSBD-5001/Lecture-Materials/blob/master/l3_simulation_lecture.ipynb Underfitting \u00b6 Occurs when the model or algorithm doesn't fit the the data well enough. Example \u00b6 import numpy as np import matplotlib.pyplot as plt import pandas as pd import statsmodels.api as sm from statsmodels import regression from scipy i0mport poly1d /usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm x = np . arange ( 10 ) y = 2 * np . random . randn ( 10 ) + x ** 2 xs = np . linspace ( - 0 . 25 , 9 . 25 , 200 ) lin = np . polyfit ( x , y , 1 ) quad = np . polyfit ( x , y , 2 ) many = np . polyfit ( x , y , 9 ) plt . scatter ( x , y ) plt . plot ( xs , poly1d ( lin )( xs )) plt . plot ( xs , poly1d ( quad )( xs )) plt . plot ( xs , poly1d ( many )( xs )) plt . ylabel ( 'Y' ) plt . xlabel ( 'X' ) plt . legend ([ 'Underfit' , 'Good fit' , 'Overfit' ]); Errors: Bias and variance \u00b6 Expect error = \\(Bias^2\\) + Variance + Noise Bias: Difference between the average prediction of our model and the correct value which we are trying to predict Variance: The variability of model prediction for a fiven data point. Our goal is to select models that are of optimal complexity. Complex models have low bias and high variance: - Low bias: Complicated models capture a lot of features - High variance: testing set many not have the same feature - Overfitting Simeple models have low variance and high bias. - Underfitting How to reduce variance and keep bias at a low value? \u00b6 Larger training dataset reduces variance Noise is unavoidable on the data Regularization and ensemble learning Selecting good models \u00b6 Validation \u00b6 Split training data into training and validation data Validation data are only used to evaluate the performance of trained model. If model generalize well on validation data, then should also generalize well on testing data. Wasting part of original training data. Cross validation \u00b6 Will make all training data for validation Partition training data into serveral groups repeat: One group as validation set, train new model Performance metric: average error on validation data. k-fold cross validation \u00b6 Equally split data into k folds Each time uses one fold as validation K fold can be used for large dataset Leave-one-out can be used when dataset is small. Use only 1 sample for validation, the rest for training. Select models with cross-validation. Use cross validation to evaluate performance of different models. Select the best model. Improving the models \u00b6 Method Train sequentially or in parallel How to generate different models Reduces bias or variance Bagging Parallel Boostrap data Variance Random Forest Parallel Bootsrap + random subset of features at splitting Variance Boosting Sequential Reweight training data Bias and variance Regularization \u00b6 Prevent overfitting by reducing flexibility of the model. Prevent parameters having too large absolute values. - Reduce variance - Prevent overfitting Ensemeble \u00b6 Standard decision trees can achieve low bias. - Training set error can be zero. You can always train to the last branch - Large variance Early stopping with fixed nodes or fixed depth may incur high bias Averaging \u00b6 For regression: Simply average the results predicted by different trees, can take weighted average For classification: just select the most predicted value. Also called voting Baging \u00b6 Short for Boostrap aggregating. Bootstrap samples B times, each with size N, with replacement. Train B classifiers each with a bootstrap sample. Bagging gets similar bias: data are from resampling. Random Forest \u00b6 Refinement of the bagged trees. Problem: We want the trees to be independent, don't want them to be similar. But bootstrapping data doesn't help that much: still drawn from same dataset with all features. At each tree split, a random sample of m features are drawn. Only these m features are consldered for splitting. Typically, m is \\(\\sqrt{p}\\) pr \\(p/3\\) where p is the total number of features. Boosting \u00b6 Random forest and bagging: trees are trained in parallel Boosting: trees should be trained sequentially - Start with original training sample - In each iteration: - Train a classifier and check wich samples are hard to train - Increase the weight of those mis-classified samples in training data - Repeat this - Final classifier: weighted classifier model.","title":"Lecture 3"},{"location":"MSBD5001/Lecture%203/#overfitting-and-underfitting","text":"","title":"Overfitting and underfitting"},{"location":"MSBD5001/Lecture%203/#overfitting","text":"Even when training data and testing data are i.i.d, generalization may also fail. Is a modeling error which occurs when a function is too closely fit to a limited set of data points.","title":"Overfitting"},{"location":"MSBD5001/Lecture%203/#why-is-overfitting-a-problem","text":"Overfitting leads to low training error yet high testing error. Out goal is to make the testing error small, Not the training error.","title":"Why is overfitting a problem"},{"location":"MSBD5001/Lecture%203/#plotting-a-polynomial","text":"Using a polynomial of degree N to fit \\(y==\\sum^N_{i=1}w_ix_i\\) Higher degree has more complex curve to fit the data. https://github.com/MSBD-5001/Lecture-Materials/blob/master/l3_simulation_lecture.ipynb","title":"Plotting a polynomial"},{"location":"MSBD5001/Lecture%203/#underfitting","text":"Occurs when the model or algorithm doesn't fit the the data well enough.","title":"Underfitting"},{"location":"MSBD5001/Lecture%203/#example","text":"import numpy as np import matplotlib.pyplot as plt import pandas as pd import statsmodels.api as sm from statsmodels import regression from scipy i0mport poly1d /usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm x = np . arange ( 10 ) y = 2 * np . random . randn ( 10 ) + x ** 2 xs = np . linspace ( - 0 . 25 , 9 . 25 , 200 ) lin = np . polyfit ( x , y , 1 ) quad = np . polyfit ( x , y , 2 ) many = np . polyfit ( x , y , 9 ) plt . scatter ( x , y ) plt . plot ( xs , poly1d ( lin )( xs )) plt . plot ( xs , poly1d ( quad )( xs )) plt . plot ( xs , poly1d ( many )( xs )) plt . ylabel ( 'Y' ) plt . xlabel ( 'X' ) plt . legend ([ 'Underfit' , 'Good fit' , 'Overfit' ]);","title":"Example"},{"location":"MSBD5001/Lecture%203/#errors-bias-and-variance","text":"Expect error = \\(Bias^2\\) + Variance + Noise Bias: Difference between the average prediction of our model and the correct value which we are trying to predict Variance: The variability of model prediction for a fiven data point. Our goal is to select models that are of optimal complexity. Complex models have low bias and high variance: - Low bias: Complicated models capture a lot of features - High variance: testing set many not have the same feature - Overfitting Simeple models have low variance and high bias. - Underfitting","title":"Errors: Bias and variance"},{"location":"MSBD5001/Lecture%203/#how-to-reduce-variance-and-keep-bias-at-a-low-value","text":"Larger training dataset reduces variance Noise is unavoidable on the data Regularization and ensemble learning","title":"How to reduce variance and keep bias at a low value?"},{"location":"MSBD5001/Lecture%203/#selecting-good-models","text":"","title":"Selecting good models"},{"location":"MSBD5001/Lecture%203/#validation","text":"Split training data into training and validation data Validation data are only used to evaluate the performance of trained model. If model generalize well on validation data, then should also generalize well on testing data. Wasting part of original training data.","title":"Validation"},{"location":"MSBD5001/Lecture%203/#cross-validation","text":"Will make all training data for validation Partition training data into serveral groups repeat: One group as validation set, train new model Performance metric: average error on validation data.","title":"Cross validation"},{"location":"MSBD5001/Lecture%203/#k-fold-cross-validation","text":"Equally split data into k folds Each time uses one fold as validation K fold can be used for large dataset Leave-one-out can be used when dataset is small. Use only 1 sample for validation, the rest for training. Select models with cross-validation. Use cross validation to evaluate performance of different models. Select the best model.","title":"k-fold cross validation"},{"location":"MSBD5001/Lecture%203/#improving-the-models","text":"Method Train sequentially or in parallel How to generate different models Reduces bias or variance Bagging Parallel Boostrap data Variance Random Forest Parallel Bootsrap + random subset of features at splitting Variance Boosting Sequential Reweight training data Bias and variance","title":"Improving the models"},{"location":"MSBD5001/Lecture%203/#regularization","text":"Prevent overfitting by reducing flexibility of the model. Prevent parameters having too large absolute values. - Reduce variance - Prevent overfitting","title":"Regularization"},{"location":"MSBD5001/Lecture%203/#ensemeble","text":"Standard decision trees can achieve low bias. - Training set error can be zero. You can always train to the last branch - Large variance Early stopping with fixed nodes or fixed depth may incur high bias","title":"Ensemeble"},{"location":"MSBD5001/Lecture%203/#averaging","text":"For regression: Simply average the results predicted by different trees, can take weighted average For classification: just select the most predicted value. Also called voting","title":"Averaging"},{"location":"MSBD5001/Lecture%203/#baging","text":"Short for Boostrap aggregating. Bootstrap samples B times, each with size N, with replacement. Train B classifiers each with a bootstrap sample. Bagging gets similar bias: data are from resampling.","title":"Baging"},{"location":"MSBD5001/Lecture%203/#random-forest","text":"Refinement of the bagged trees. Problem: We want the trees to be independent, don't want them to be similar. But bootstrapping data doesn't help that much: still drawn from same dataset with all features. At each tree split, a random sample of m features are drawn. Only these m features are consldered for splitting. Typically, m is \\(\\sqrt{p}\\) pr \\(p/3\\) where p is the total number of features.","title":"Random Forest"},{"location":"MSBD5001/Lecture%203/#boosting","text":"Random forest and bagging: trees are trained in parallel Boosting: trees should be trained sequentially - Start with original training sample - In each iteration: - Train a classifier and check wich samples are hard to train - Increase the weight of those mis-classified samples in training data - Repeat this - Final classifier: weighted classifier model.","title":"Boosting"},{"location":"MSBD5001/Lecture1/","text":"Lecture 1 \u00b6 Classification - Data to classes Regression - Predicting a numeric value Clustering Different types of problems \u00b6 Classification Problem - MNIST Dataset Regression - Predicting stock value Clustering Automatically identify the data Data integration \u00b6 Data are created independently A higher-level abstraction Statical analysis \u00b6 Collecting data \u00b6 Collecting, exploring and presenting large amounts of data to discover underlying patterns and trends Data come in two types: - Discrete - Continuous We have - barchart - piechart, Stem-and-leaf plot - Scatterplot ( it uses caresian coordinates to display values for two variables for set of data) - Form - Direction Numerical descriptive measures of data (Central tendency) - Mean - Min - Max - Median - Mode A sampling method is a procudure for selecting sample elements from a population. Relationship between variables: \u00b6 Eyeball fit: Fit two points on the plot so that the line passing through them fives a fairly good fit. Least square fit: Fit a line \\(y = a + bX\\) such that it minimaizes the error S Correlation coefficient, denoted as r, measures the degree to which two variables movements are associated. r = 1 means perfect positive relationship r = 1 means a perfect negative relationship r = 0 means no relationship Forecasting \u00b6 An experiment is an action where the result is uncertain A sample space is all the possible outomes of an experiment, denoted as \\(S\\) . A event is a subset of S Probability : is the measure of how likely an event is to occur out of the number of possible outcomes. $p = \\frac{The\\ number\\ of outcomes}{sample space} $ Parameters \u00b6 Sample can be generated by a probability model, where parameters are characteristics of the model Variance \u00b6 Variance is another parameter of probability model It is a measure of how spread out it is Statical analysis \u00b6 Collecting, exploring and presenting large amounts of data to discover underlying patterns and","title":"Lecture1"},{"location":"MSBD5001/Lecture1/#lecture-1","text":"Classification - Data to classes Regression - Predicting a numeric value Clustering","title":"Lecture 1"},{"location":"MSBD5001/Lecture1/#different-types-of-problems","text":"Classification Problem - MNIST Dataset Regression - Predicting stock value Clustering Automatically identify the data","title":"Different types of problems"},{"location":"MSBD5001/Lecture1/#data-integration","text":"Data are created independently A higher-level abstraction","title":"Data integration"},{"location":"MSBD5001/Lecture1/#statical-analysis","text":"","title":"Statical analysis"},{"location":"MSBD5001/Lecture1/#collecting-data","text":"Collecting, exploring and presenting large amounts of data to discover underlying patterns and trends Data come in two types: - Discrete - Continuous We have - barchart - piechart, Stem-and-leaf plot - Scatterplot ( it uses caresian coordinates to display values for two variables for set of data) - Form - Direction Numerical descriptive measures of data (Central tendency) - Mean - Min - Max - Median - Mode A sampling method is a procudure for selecting sample elements from a population.","title":"Collecting data"},{"location":"MSBD5001/Lecture1/#relationship-between-variables","text":"Eyeball fit: Fit two points on the plot so that the line passing through them fives a fairly good fit. Least square fit: Fit a line \\(y = a + bX\\) such that it minimaizes the error S Correlation coefficient, denoted as r, measures the degree to which two variables movements are associated. r = 1 means perfect positive relationship r = 1 means a perfect negative relationship r = 0 means no relationship","title":"Relationship between variables:"},{"location":"MSBD5001/Lecture1/#forecasting","text":"An experiment is an action where the result is uncertain A sample space is all the possible outomes of an experiment, denoted as \\(S\\) . A event is a subset of S Probability : is the measure of how likely an event is to occur out of the number of possible outcomes. $p = \\frac{The\\ number\\ of outcomes}{sample space} $","title":"Forecasting"},{"location":"MSBD5001/Lecture1/#parameters","text":"Sample can be generated by a probability model, where parameters are characteristics of the model","title":"Parameters"},{"location":"MSBD5001/Lecture1/#variance","text":"Variance is another parameter of probability model It is a measure of how spread out it is","title":"Variance"},{"location":"MSBD5001/Lecture1/#statical-analysis_1","text":"Collecting, exploring and presenting large amounts of data to discover underlying patterns and","title":"Statical analysis"},{"location":"MSBD5001/Lecture4/","text":"Unsupervised learning \u00b6 Another important class of machine learning methods Analyze the structure of the data using feature X Supervised: Use features X to predict labels Y Unsupervised: Only requires features, don\u2019t deal with labels Examples: - Clustering: divide a dataset into meaningful groups. Data points in the same group are more similar with each other, compared to those in different groups. Dimensionality Reduction: | have a dataset of extremely high dimension of features (e.g., images), can | represent them with a lower dimension? Ranking: | have a dataset represented as a graph, each data point is a node, and their relationship are edges, e.g., the World Wide Web, can | rank the importance of the data points? Clustering \u00b6 Clustering: the process of grouping a set of objects into classes of similar objects Objects within the same cluster should be more similar. Objects across the different clusters should be less similar. K-means clustering method \u00b6 Given k, the k-means algorithm is implemented in four steps: Partition objects into k nonempty subsets Compute the mean point for every cluster at current partitioning Reassign each object to the cluster with the nearest mean point Go back to Step 2, stop when the assignment does not change k means by python \u00b6 Original Graph import matplotlib.pyplot as plt import seaborn as sns ; sns . set () # for plot styling import numpy as np from sklearn.datasets.samples_generator import make_blobs X , y_true = make_blobs ( n_samples = 300 , centers = 4 , cluster_std = 0.60 , random_state = 0 ) plt . scatter ( X [:, 0 ], X [:, 1 ], s = 50 ); After grouping from sklearn.cluster import KMeans for i in range ( 1 , 10 ): kmeans = KMeans ( n_clusters = i ) kmeans . fit ( X ) y_kmeans = kmeans . predict ( X ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y_kmeans , s = 50 , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 ) plt . title ( f \"k-means cluster: {i}\" ) plt . show () Example 1 \u00b6 Suppose you have 5 points in 1-D: {1,2,4,7,10}. Use k-means to cluster these points with k=2. Start with initial partition {1} and {2,4,7,10}. The distance is difference of coordinate on the axis. Drawbacks \u00b6 Both large K and small K can lead to bad results: left K=4, right K=2. Didn't describe data well Hierarchical clustering \u00b6 A method of cluster analysis which seeks to build a hierarchy of clusters No need to specify the number of clusters, we can generate partitions at different levels of the hierarchy. A dendrogram is a tree diagram that can be used to represent the hierarchical clustering structure between data points. The height of connections in the dendrogram represents the distance between partitions: The higher the connection, the larger distance between the two connected clusters. Cut the dendrogram: Clustering is obtained by cutting the dendrogram at the desired level, then each connected component forms a cluster. Building diagram \u00b6 Two types: bottom-up (agglomerative), and top-down (divisive) Bottom-up: two groups are merged if distance between them is less than a threshold Top-down: one group is split into two if inter- group distance is more than a threshold Dimensionality Reduction \u00b6 Given data points in d dimensions, convert them to data points in r<d dimensions, with minimal loss of information. Used for statistical analysis, data compression, and data visualization Idea of Principle Component Analysis \u00b6 Reduce from n-dimension to k-dimension: Find vectors \\(u^1,u^2,...,u^k\\) onto which to project the data, so as to minimize the projection error. These vectors should represent primary information of data, we call them principle components Identity matrix \u00b6 In linear algebra, the identity matrix (sometimes ambiguously called a unit matrix) of size n is the n \u00d7 n square matrix with ones on the main diagonal and zeros elsewhere. It is denoted by In, or simply by I if the size is immaterial or can be trivially determined by the context. In some fields, such as quantum mechanics, the identity matrix is denoted by a boldface one, 1; otherwise it is identical to I. Less frequently, some mathematics books use U or E to represent the identity matrix, meaning \"unit matrix\" and the German word Einheitsmatrix respectively. Eigenvalues and eigenvectors \u00b6 https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors https://www.khanacademy.org/math/linear-algebra/alternate-bases/eigen-everything/v/linear-algebra-introduction-to-eigenvalues-and-eigenvectors Intuitive Idea of PCA \u00b6 https://dilloncamp.com/projects/pca.html What we DON\u2019T want for projection: Original data has large variance, but projected data has small variance. lt means original data is spread, but projected data is not: A lot of information loss during projection. PCA\u2018s goal: maximize the variance of projected data. In fact, the mathematical definition of principle. components (PC) is the eigenvectors of covariance matrix of data points The order of PCs follows the magnitude of. eigenvalues, e.g., the most Signi icant PC is the eigenvector corresponding to largest eigenvalue Page rating \u00b6 A method for rating the importance of web pages using the link structure of the web Simple Recursive Formulation \u00b6 Each link\u2019s vote is proportional to the importance of its source page If page P with importance x has n out-links, each link gets x/n votes Page P\u2019s own importance is the sum of the votes on its in-links Final PageRank score: Importance=sum of votes from all in-links Page rank in python \u00b6 \\[ r_a = \\sum_{j=1}^n L_{a.j}r_j\\] import numpy as np import matplotlib.pyplot as plt # set plot size plt . rcParams [ 'figure.figsize' ] = [ 20 , 5 ] def page_rank ( matrix : np . array , iter = 3 ): shape = matrix . shape [ 1 ] r = np . full (( shape , 1 ), 1 / shape ) l = matrix total_results = None for i in range ( iter ): r = l . dot ( r ) if i == 0 : total_results = r else : total_results = np . concatenate (( total_results , r ), axis = 1 ) return r , total_results m = np . array ( [[ 0 , 0.5 , 0 , 0 ], [ 1 / 3 , 0 , 0 , 1 / 2 ], [ 1 / 3 , 0 , 0 , 1 / 2 ], [ 1 / 3 , 1 / 2 , 1 , 0 ] ] ) r , total_results = page_rank ( m , 20 ) for i , p in enumerate ( total_results ): plt . plot ( p , label = f \" { i } - line\" ) plt . legend () <matplotlib.legend.Legend at 0x7fc4907065c0> Random walk interpretation \u00b6 An equivalent view of PageRank. Imagine a random web surfer At any time t, surfer is on some page P At time t+1, the surfer follows an outlink from P uniformly at random. Ends up on some page Q linked from P, process repeats indefinitely Let p(t) be a vector whose \\(i^{th}\\) component is the probability that the surfer is at page i at time t * p(t) is a probability distribution on pages Stationary Distribution \u00b6 Where is the surfer at time t+1? - Follows a link uniformly at some probability - p(t+1) = Mp(t) where M is the transition probability Suppose the random walk reaches a state such that p(t+1) = Mp(t) = p(t) Then p(t) is called a stationary distribution for the random walk The PageRank score r is the stationary distribution, can be solved by r=Mr. Normalization by scaling sum of r to 1. Stationary Distribution=PageRank Score \u00b6 Stationary distribution represents PageRank score. PageRank Score: A node\u2019s importance equals to the votes from adjacent nodes. Stationary Distribution: Probability of being at one node equals to sum of the probabilities coming from other nodes Both of them describe the stable state. Spider Traps \u00b6 Agroup of pages is a spider trap if there are no links from pages within the group to pages outside the group Random surfer gets trapped, it continuously walk in the trap. Spider traps violate the conditions needed for the random walk theorem import numpy as np def pagerank ( M , num_iterations : int = 100 , d : float = 0.85 ): \"\"\"PageRank: The trillion dollar algorithm. Parameters ---------- M : numpy array adjacency matrix where M_i,j represents the link from 'j' to 'i', such that for all 'j' sum(i, M_i,j) = 1 num_iterations : int, optional number of iterations, by default 100 d : float, optional damping factor, by default 0.85 Returns ------- numpy array a vector of ranks such that v_i is the i-th rank from [0, 1], v sums to 1 \"\"\" N = M . shape [ 1 ] v = np . random . rand ( N , 1 ) v = v / np . linalg . norm ( v , 1 ) M_hat = ( d * M + ( 1 - d ) / N ) for i in range ( num_iterations ): v = M_hat @ v return v M = np . array ([[ 0.5 , 0.5 , 0 ], [ 0.5 , 0 , 0 ], [ 0 , 0.5 , 1 ], ]) v = pagerank ( M , 100 , 0.8 ) print ( v ) [[0.21212121] [0.15151515] [0.63636364]]","title":"Lecture4"},{"location":"MSBD5001/Lecture4/#unsupervised-learning","text":"Another important class of machine learning methods Analyze the structure of the data using feature X Supervised: Use features X to predict labels Y Unsupervised: Only requires features, don\u2019t deal with labels Examples: - Clustering: divide a dataset into meaningful groups. Data points in the same group are more similar with each other, compared to those in different groups. Dimensionality Reduction: | have a dataset of extremely high dimension of features (e.g., images), can | represent them with a lower dimension? Ranking: | have a dataset represented as a graph, each data point is a node, and their relationship are edges, e.g., the World Wide Web, can | rank the importance of the data points?","title":"Unsupervised learning"},{"location":"MSBD5001/Lecture4/#clustering","text":"Clustering: the process of grouping a set of objects into classes of similar objects Objects within the same cluster should be more similar. Objects across the different clusters should be less similar.","title":"Clustering"},{"location":"MSBD5001/Lecture4/#k-means-clustering-method","text":"Given k, the k-means algorithm is implemented in four steps: Partition objects into k nonempty subsets Compute the mean point for every cluster at current partitioning Reassign each object to the cluster with the nearest mean point Go back to Step 2, stop when the assignment does not change","title":"K-means clustering method"},{"location":"MSBD5001/Lecture4/#k-means-by-python","text":"Original Graph import matplotlib.pyplot as plt import seaborn as sns ; sns . set () # for plot styling import numpy as np from sklearn.datasets.samples_generator import make_blobs X , y_true = make_blobs ( n_samples = 300 , centers = 4 , cluster_std = 0.60 , random_state = 0 ) plt . scatter ( X [:, 0 ], X [:, 1 ], s = 50 ); After grouping from sklearn.cluster import KMeans for i in range ( 1 , 10 ): kmeans = KMeans ( n_clusters = i ) kmeans . fit ( X ) y_kmeans = kmeans . predict ( X ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y_kmeans , s = 50 , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 ) plt . title ( f \"k-means cluster: {i}\" ) plt . show ()","title":"k means by python"},{"location":"MSBD5001/Lecture4/#example-1","text":"Suppose you have 5 points in 1-D: {1,2,4,7,10}. Use k-means to cluster these points with k=2. Start with initial partition {1} and {2,4,7,10}. The distance is difference of coordinate on the axis.","title":"Example 1"},{"location":"MSBD5001/Lecture4/#drawbacks","text":"Both large K and small K can lead to bad results: left K=4, right K=2. Didn't describe data well","title":"Drawbacks"},{"location":"MSBD5001/Lecture4/#hierarchical-clustering","text":"A method of cluster analysis which seeks to build a hierarchy of clusters No need to specify the number of clusters, we can generate partitions at different levels of the hierarchy. A dendrogram is a tree diagram that can be used to represent the hierarchical clustering structure between data points. The height of connections in the dendrogram represents the distance between partitions: The higher the connection, the larger distance between the two connected clusters. Cut the dendrogram: Clustering is obtained by cutting the dendrogram at the desired level, then each connected component forms a cluster.","title":"Hierarchical clustering"},{"location":"MSBD5001/Lecture4/#building-diagram","text":"Two types: bottom-up (agglomerative), and top-down (divisive) Bottom-up: two groups are merged if distance between them is less than a threshold Top-down: one group is split into two if inter- group distance is more than a threshold","title":"Building diagram"},{"location":"MSBD5001/Lecture4/#dimensionality-reduction","text":"Given data points in d dimensions, convert them to data points in r<d dimensions, with minimal loss of information. Used for statistical analysis, data compression, and data visualization","title":"Dimensionality Reduction"},{"location":"MSBD5001/Lecture4/#idea-of-principle-component-analysis","text":"Reduce from n-dimension to k-dimension: Find vectors \\(u^1,u^2,...,u^k\\) onto which to project the data, so as to minimize the projection error. These vectors should represent primary information of data, we call them principle components","title":"Idea of Principle Component Analysis"},{"location":"MSBD5001/Lecture4/#identity-matrix","text":"In linear algebra, the identity matrix (sometimes ambiguously called a unit matrix) of size n is the n \u00d7 n square matrix with ones on the main diagonal and zeros elsewhere. It is denoted by In, or simply by I if the size is immaterial or can be trivially determined by the context. In some fields, such as quantum mechanics, the identity matrix is denoted by a boldface one, 1; otherwise it is identical to I. Less frequently, some mathematics books use U or E to represent the identity matrix, meaning \"unit matrix\" and the German word Einheitsmatrix respectively.","title":"Identity matrix"},{"location":"MSBD5001/Lecture4/#eigenvalues-and-eigenvectors","text":"https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors https://www.khanacademy.org/math/linear-algebra/alternate-bases/eigen-everything/v/linear-algebra-introduction-to-eigenvalues-and-eigenvectors","title":"Eigenvalues and eigenvectors"},{"location":"MSBD5001/Lecture4/#intuitive-idea-of-pca","text":"https://dilloncamp.com/projects/pca.html What we DON\u2019T want for projection: Original data has large variance, but projected data has small variance. lt means original data is spread, but projected data is not: A lot of information loss during projection. PCA\u2018s goal: maximize the variance of projected data. In fact, the mathematical definition of principle. components (PC) is the eigenvectors of covariance matrix of data points The order of PCs follows the magnitude of. eigenvalues, e.g., the most Signi icant PC is the eigenvector corresponding to largest eigenvalue","title":"Intuitive Idea of PCA"},{"location":"MSBD5001/Lecture4/#page-rating","text":"A method for rating the importance of web pages using the link structure of the web","title":"Page rating"},{"location":"MSBD5001/Lecture4/#simple-recursive-formulation","text":"Each link\u2019s vote is proportional to the importance of its source page If page P with importance x has n out-links, each link gets x/n votes Page P\u2019s own importance is the sum of the votes on its in-links Final PageRank score: Importance=sum of votes from all in-links","title":"Simple Recursive Formulation"},{"location":"MSBD5001/Lecture4/#page-rank-in-python","text":"\\[ r_a = \\sum_{j=1}^n L_{a.j}r_j\\] import numpy as np import matplotlib.pyplot as plt # set plot size plt . rcParams [ 'figure.figsize' ] = [ 20 , 5 ] def page_rank ( matrix : np . array , iter = 3 ): shape = matrix . shape [ 1 ] r = np . full (( shape , 1 ), 1 / shape ) l = matrix total_results = None for i in range ( iter ): r = l . dot ( r ) if i == 0 : total_results = r else : total_results = np . concatenate (( total_results , r ), axis = 1 ) return r , total_results m = np . array ( [[ 0 , 0.5 , 0 , 0 ], [ 1 / 3 , 0 , 0 , 1 / 2 ], [ 1 / 3 , 0 , 0 , 1 / 2 ], [ 1 / 3 , 1 / 2 , 1 , 0 ] ] ) r , total_results = page_rank ( m , 20 ) for i , p in enumerate ( total_results ): plt . plot ( p , label = f \" { i } - line\" ) plt . legend () <matplotlib.legend.Legend at 0x7fc4907065c0>","title":"Page rank in python"},{"location":"MSBD5001/Lecture4/#random-walk-interpretation","text":"An equivalent view of PageRank. Imagine a random web surfer At any time t, surfer is on some page P At time t+1, the surfer follows an outlink from P uniformly at random. Ends up on some page Q linked from P, process repeats indefinitely Let p(t) be a vector whose \\(i^{th}\\) component is the probability that the surfer is at page i at time t * p(t) is a probability distribution on pages","title":"Random walk interpretation"},{"location":"MSBD5001/Lecture4/#stationary-distribution","text":"Where is the surfer at time t+1? - Follows a link uniformly at some probability - p(t+1) = Mp(t) where M is the transition probability Suppose the random walk reaches a state such that p(t+1) = Mp(t) = p(t) Then p(t) is called a stationary distribution for the random walk The PageRank score r is the stationary distribution, can be solved by r=Mr. Normalization by scaling sum of r to 1.","title":"Stationary Distribution"},{"location":"MSBD5001/Lecture4/#stationary-distributionpagerank-score","text":"Stationary distribution represents PageRank score. PageRank Score: A node\u2019s importance equals to the votes from adjacent nodes. Stationary Distribution: Probability of being at one node equals to sum of the probabilities coming from other nodes Both of them describe the stable state.","title":"Stationary Distribution=PageRank Score"},{"location":"MSBD5001/Lecture4/#spider-traps","text":"Agroup of pages is a spider trap if there are no links from pages within the group to pages outside the group Random surfer gets trapped, it continuously walk in the trap. Spider traps violate the conditions needed for the random walk theorem import numpy as np def pagerank ( M , num_iterations : int = 100 , d : float = 0.85 ): \"\"\"PageRank: The trillion dollar algorithm. Parameters ---------- M : numpy array adjacency matrix where M_i,j represents the link from 'j' to 'i', such that for all 'j' sum(i, M_i,j) = 1 num_iterations : int, optional number of iterations, by default 100 d : float, optional damping factor, by default 0.85 Returns ------- numpy array a vector of ranks such that v_i is the i-th rank from [0, 1], v sums to 1 \"\"\" N = M . shape [ 1 ] v = np . random . rand ( N , 1 ) v = v / np . linalg . norm ( v , 1 ) M_hat = ( d * M + ( 1 - d ) / N ) for i in range ( num_iterations ): v = M_hat @ v return v M = np . array ([[ 0.5 , 0.5 , 0 ], [ 0.5 , 0 , 0 ], [ 0 , 0.5 , 1 ], ]) v = pagerank ( M , 100 , 0.8 ) print ( v ) [[0.21212121] [0.15151515] [0.63636364]]","title":"Spider Traps"},{"location":"MSBD5003/Lecture1/","text":"Big Data Definition \u00b6 Big Data is data whose scale, complexity, and speed require new architecture, techniques, algorithms, and analytics to manage it and extract value and hidden knowledge from it. Old model: Few companies are generating data, all others are consuming data New model: all of us are generating and consuming data at the same time 3 vs \u00b6 Volume \u00b6 Data are big Velocity ( Streaming data ) \u00b6 Data is being generated fast and need to be processed fast Variety \u00b6 various formats, types, and structures Numerical, text, images, audio, video, sequences, time series, social media data, multi-dim arrays, etc... A single application can be generating/collecting many types of data The structure spectrum \u00b6 Structured (schema-first) Relational database -- Semi-structured(Schema-later) Documents XML -- Unstucture data (MongoDB) Structured data \u00b6 Modify the rows is easy but modify the column costs a lot. The relational data model is the most used data model Every relation has a schema defining each columns' type The programmer must statically specify the schema Semi-structured Data \u00b6 Json object consists of a collection name: value pairs, seperated by commas. Each value can be - A string - A number - A boolean - null - An array - a JSON object How to handle big data \u00b6 Race conditions ( use locks ) The frustration of parallel programming \u00b6 Hard to debug How to migrate from one archetecture to another Cloud comuting \u00b6 Dynamic provisioning Scalability Elasticity Mapreduce \u00b6 Map: Takes raw input and produces a key, value pair Reduce: Takes data with same key and produces outputs Shuffling and sorting - Hidden phase between mappers and reducers - Groups all key value pairs Where do we store data \u00b6 Target environment \u00b6 Files are huge but not many Many reads but few writes I/O bandwidth is more important than latency GFS design decisions \u00b6 Files stored into chunks Reliability through replication Single master to coordinate access, keep metadata HDFS \u00b6 Name node: Maintains metadata info about files Maps a filename to a set of blocks Maps a block to the data nodes where it resides replication engine for blocks Datanod Store data Files are divided into blocks Communicates with name nodes through periodic heartbeat M data cells and n parity cells Storage efficiency = \\(\\frac{m}{m+n}\\) # On worker ailure \u00b6 Detect failure via periodic heartbeats Re-execute completed and in-progress map tasks Re-execute Mapreduce: A major step backwards \u00b6 Mapreduce may be a good idea for writing certain types of computations A giant step backward in the programming paradigm for large-scale data intensive applications A sub-cptimal implementation, in that it uses brute force instead of indexing Missing most of features that are routinely included in current DMBS Not novel at all","title":"Lecture1"},{"location":"MSBD5003/Lecture1/#big-data-definition","text":"Big Data is data whose scale, complexity, and speed require new architecture, techniques, algorithms, and analytics to manage it and extract value and hidden knowledge from it. Old model: Few companies are generating data, all others are consuming data New model: all of us are generating and consuming data at the same time","title":"Big Data Definition"},{"location":"MSBD5003/Lecture1/#3-vs","text":"","title":"3 vs"},{"location":"MSBD5003/Lecture1/#volume","text":"Data are big","title":"Volume"},{"location":"MSBD5003/Lecture1/#velocity-streaming-data","text":"Data is being generated fast and need to be processed fast","title":"Velocity ( Streaming data )"},{"location":"MSBD5003/Lecture1/#variety","text":"various formats, types, and structures Numerical, text, images, audio, video, sequences, time series, social media data, multi-dim arrays, etc... A single application can be generating/collecting many types of data","title":"Variety"},{"location":"MSBD5003/Lecture1/#the-structure-spectrum","text":"Structured (schema-first) Relational database -- Semi-structured(Schema-later) Documents XML -- Unstucture data (MongoDB)","title":"The structure spectrum"},{"location":"MSBD5003/Lecture1/#structured-data","text":"Modify the rows is easy but modify the column costs a lot. The relational data model is the most used data model Every relation has a schema defining each columns' type The programmer must statically specify the schema","title":"Structured data"},{"location":"MSBD5003/Lecture1/#semi-structured-data","text":"Json object consists of a collection name: value pairs, seperated by commas. Each value can be - A string - A number - A boolean - null - An array - a JSON object","title":"Semi-structured Data"},{"location":"MSBD5003/Lecture1/#how-to-handle-big-data","text":"Race conditions ( use locks )","title":"How to handle big data"},{"location":"MSBD5003/Lecture1/#the-frustration-of-parallel-programming","text":"Hard to debug How to migrate from one archetecture to another","title":"The frustration of parallel programming"},{"location":"MSBD5003/Lecture1/#cloud-comuting","text":"Dynamic provisioning Scalability Elasticity","title":"Cloud comuting"},{"location":"MSBD5003/Lecture1/#mapreduce","text":"Map: Takes raw input and produces a key, value pair Reduce: Takes data with same key and produces outputs Shuffling and sorting - Hidden phase between mappers and reducers - Groups all key value pairs","title":"Mapreduce"},{"location":"MSBD5003/Lecture1/#where-do-we-store-data","text":"","title":"Where do we store data"},{"location":"MSBD5003/Lecture1/#target-environment","text":"Files are huge but not many Many reads but few writes I/O bandwidth is more important than latency","title":"Target environment"},{"location":"MSBD5003/Lecture1/#gfs-design-decisions","text":"Files stored into chunks Reliability through replication Single master to coordinate access, keep metadata","title":"GFS design decisions"},{"location":"MSBD5003/Lecture1/#hdfs","text":"Name node: Maintains metadata info about files Maps a filename to a set of blocks Maps a block to the data nodes where it resides replication engine for blocks Datanod Store data Files are divided into blocks Communicates with name nodes through periodic heartbeat M data cells and n parity cells Storage efficiency = \\(\\frac{m}{m+n}\\) #","title":"HDFS"},{"location":"MSBD5003/Lecture1/#on-worker-ailure","text":"Detect failure via periodic heartbeats Re-execute completed and in-progress map tasks Re-execute","title":"On worker ailure"},{"location":"MSBD5003/Lecture1/#mapreduce-a-major-step-backwards","text":"Mapreduce may be a good idea for writing certain types of computations A giant step backward in the programming paradigm for large-scale data intensive applications A sub-cptimal implementation, in that it uses brute force instead of indexing Missing most of features that are routinely included in current DMBS Not novel at all","title":"Mapreduce: A major step backwards"},{"location":"MSBD5003/Lecture2/","text":"!pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 69kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 37.0MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=a6732c6ade26f105c00ba7849e278a010c66b051a7f273b109760a82bbea01a0 Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 Spark \u00b6 Execution is pipelined and parallel. No need to store intermediate results. Lazy execution allows optimization. RDD has enough information about how it was rderived from to compute its partitions from data in stable storage. Example: If a partition of errors is lost, Spark rebuilds it by applying a filter on only the corresponding partition of lines. Partitions can be recomputed in parallel on different nodes, without having to roll back the whole program. Sample spark program \u00b6 from pyspark.context import SparkContext import requests sc = SparkContext.getOrCreate() text_file = sc.textFile('sample_data/README.md') Filter each line which contains T lines = text_file.filter(lambda line: 'T' in line) lines.collect() ['This directory includes a few sample datasets to get you started.', ' [MNIST database](https://en.wikipedia.org/wiki/MNIST_database), which is', ' Statistician. 27 (1): 17-21. JSTOR 2682899.'] Average text_file = sc.textFile('sample_data/README.md', 2) lines.map(lambda line: len(line.split())).reduce(lambda a, b: (a + b) / 2) 6.75 Average with partition 3 text_file = sc.textFile('sample_data/README.md', 5) lines.map(lambda line: len(line.split())).reduce(lambda a, b: (a + b) / 2) 6.75 RDD Operations \u00b6 Download file r = requests.get('https://www.cse.ust.hk/msbd5003/data/fruits.txt') open('fruits.txt', 'wb').write(r.content) 65 print file \u00b6 fruits = sc.textFile('fruits.txt') fruits.collect() ['apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry'] Map \u00b6 fruitsReversed = fruits . map ( lambda fruit : fruit [ ::- 1 ]) fruitsReversed . collect () ['elppa', 'ananab', 'nolem yranac', 'parg', 'nomel', 'egnaro', 'elppaenip', 'yrrebwarts'] Filter \u00b6 shortFruits = fruits.filter(lambda fruit: len(fruit) <= 5) shortFruits.collect() ['apple', 'grap', 'lemon'] FlatMap \u00b6 characters = fruits.flatMap(lambda fruit: list(fruit)) print(characters.collect()) ['a', 'p', 'p', 'l', 'e', 'b', 'a', 'n', 'a', 'n', 'a', 'c', 'a', 'n', 'a', 'r', 'y', ' ', 'm', 'e', 'l', 'o', 'n', 'g', 'r', 'a', 'p', 'l', 'e', 'm', 'o', 'n', 'o', 'r', 'a', 'n', 'g', 'e', 'p', 'i', 'n', 'e', 'a', 'p', 'p', 'l', 'e', 's', 't', 'r', 'a', 'w', 'b', 'e', 'r', 'r', 'y'] Collect \u00b6 new_fruits = fruits.union(fruits) new_fruits.collect() ['apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry', 'apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry'] new_fruits = fruits.intersection(fruits) new_fruits.collect() ['orange', 'pineapple', 'canary melon', 'lemon', 'banana', 'apple', 'grap', 'strawberry'] new_fruits = fruits.union(fruits).distinct() new_fruits.collect() ['orange', 'pineapple', 'canary melon', 'lemon', 'banana', 'apple', 'grap', 'strawberry'] RDD Actions \u00b6 collect \u00b6 take \u00b6 first3Fruits = fruits.take(3) print(first3Fruits) ['apple', 'banana', 'canary melon'] count \u00b6 fruits.count() 8 reduce \u00b6 fruits.map(lambda fruit: set(fruit)).reduce(lambda x, y: x.union(y)) {' ', 'a', 'b', 'c', 'e', 'g', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'w', 'y'}","title":"Lecture2"},{"location":"MSBD5003/Lecture2/#spark","text":"Execution is pipelined and parallel. No need to store intermediate results. Lazy execution allows optimization. RDD has enough information about how it was rderived from to compute its partitions from data in stable storage. Example: If a partition of errors is lost, Spark rebuilds it by applying a filter on only the corresponding partition of lines. Partitions can be recomputed in parallel on different nodes, without having to roll back the whole program.","title":"Spark"},{"location":"MSBD5003/Lecture2/#sample-spark-program","text":"from pyspark.context import SparkContext import requests sc = SparkContext.getOrCreate() text_file = sc.textFile('sample_data/README.md') Filter each line which contains T lines = text_file.filter(lambda line: 'T' in line) lines.collect() ['This directory includes a few sample datasets to get you started.', ' [MNIST database](https://en.wikipedia.org/wiki/MNIST_database), which is', ' Statistician. 27 (1): 17-21. JSTOR 2682899.'] Average text_file = sc.textFile('sample_data/README.md', 2) lines.map(lambda line: len(line.split())).reduce(lambda a, b: (a + b) / 2) 6.75 Average with partition 3 text_file = sc.textFile('sample_data/README.md', 5) lines.map(lambda line: len(line.split())).reduce(lambda a, b: (a + b) / 2) 6.75","title":"Sample spark program"},{"location":"MSBD5003/Lecture2/#rdd-operations","text":"Download file r = requests.get('https://www.cse.ust.hk/msbd5003/data/fruits.txt') open('fruits.txt', 'wb').write(r.content) 65","title":"RDD Operations"},{"location":"MSBD5003/Lecture2/#print-file","text":"fruits = sc.textFile('fruits.txt') fruits.collect() ['apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry']","title":"print file"},{"location":"MSBD5003/Lecture2/#map","text":"fruitsReversed = fruits . map ( lambda fruit : fruit [ ::- 1 ]) fruitsReversed . collect () ['elppa', 'ananab', 'nolem yranac', 'parg', 'nomel', 'egnaro', 'elppaenip', 'yrrebwarts']","title":"Map"},{"location":"MSBD5003/Lecture2/#filter","text":"shortFruits = fruits.filter(lambda fruit: len(fruit) <= 5) shortFruits.collect() ['apple', 'grap', 'lemon']","title":"Filter"},{"location":"MSBD5003/Lecture2/#flatmap","text":"characters = fruits.flatMap(lambda fruit: list(fruit)) print(characters.collect()) ['a', 'p', 'p', 'l', 'e', 'b', 'a', 'n', 'a', 'n', 'a', 'c', 'a', 'n', 'a', 'r', 'y', ' ', 'm', 'e', 'l', 'o', 'n', 'g', 'r', 'a', 'p', 'l', 'e', 'm', 'o', 'n', 'o', 'r', 'a', 'n', 'g', 'e', 'p', 'i', 'n', 'e', 'a', 'p', 'p', 'l', 'e', 's', 't', 'r', 'a', 'w', 'b', 'e', 'r', 'r', 'y']","title":"FlatMap"},{"location":"MSBD5003/Lecture2/#collect","text":"new_fruits = fruits.union(fruits) new_fruits.collect() ['apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry', 'apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry'] new_fruits = fruits.intersection(fruits) new_fruits.collect() ['orange', 'pineapple', 'canary melon', 'lemon', 'banana', 'apple', 'grap', 'strawberry'] new_fruits = fruits.union(fruits).distinct() new_fruits.collect() ['orange', 'pineapple', 'canary melon', 'lemon', 'banana', 'apple', 'grap', 'strawberry']","title":"Collect"},{"location":"MSBD5003/Lecture2/#rdd-actions","text":"","title":"RDD Actions"},{"location":"MSBD5003/Lecture2/#collect_1","text":"","title":"collect"},{"location":"MSBD5003/Lecture2/#take","text":"first3Fruits = fruits.take(3) print(first3Fruits) ['apple', 'banana', 'canary melon']","title":"take"},{"location":"MSBD5003/Lecture2/#count","text":"fruits.count() 8","title":"count"},{"location":"MSBD5003/Lecture2/#reduce","text":"fruits.map(lambda fruit: set(fruit)).reduce(lambda x, y: x.union(y)) {' ', 'a', 'b', 'c', 'e', 'g', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'w', 'y'}","title":"reduce"},{"location":"MSBD5003/Lecture3/","text":"!pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 69kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 40.7MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=afa56cc6a9466c8bd467ecaf305826a28ca15fb5f31ee4f848525e71d4193f60 Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 Example: Linear-time selection \u00b6 Problem: \u00b6 \u2014 Input: an array A of n numbers (unordered), and k \u2014 Output: the k-th smallest number (counting from 0) Algorithm \u00b6 \\(x=A[0]\\) partition A into \\(A[0..mid-1] < A[mid] = x < A[mid+1..n-1]\\) if \\(mid =k\\) then return \\(x\\) if \\(k<mid\\) then \\(A= A[O..mid-1]\\) if k > mid then \\(A = A[mid+1,n-1], k= k\u2014 mid-1\\) gotostep 1 Key-value Pairs \u00b6 While most Spark operations work on RDDs containing any type of objects, a few special operations are only available on RDDs of key-value pairs. In Python, these operations work on RDDs containing built-in Python tuples such as (1, 2). Simply create such tuples and then call your desired operation. For example, the following code uses the reduceByKey operation on key-value pairs to count how many times each line of text occurs in a file: lines = sc . textFile ( \"README.md\" ) pairs = lines . map ( lambda s : ( s , 1 )) counts = pairs . reduceByKey ( lambda a , b : a + b ) We could also use counts.sortByKey() , for example, to sort the pairs alphabetically, and finally counts.collect() to bring them back to the driver program as a list of objects. PMI \u00b6 PMI (pointwise mutual information) is a measure of association used in information theory and statistics. Given a list of pairs (x, y) \\[pmi(x, y) = log\\frac{p(x,y)}{p(x)p(y}\\] where - \\(p(x)\\) : probability of x - \\(p(y)\\) : probability of y - \\(p(x,y)\\) : joint probability Example: p(x=0) = 0.8, p(x=1)=0.2, p(y=0)=0.25, p(y=1)=0.75 pmi(x=0;y=0) = \u22121 pmi(x=0;y=1) = 0.222392 pmi(x=1;y=0) = 1.584963 pmi(x=1;y=1) = -1.584963 Example notebook see: note book in class/PMI","title":"Lecture3"},{"location":"MSBD5003/Lecture3/#example-linear-time-selection","text":"","title":"Example: Linear-time selection"},{"location":"MSBD5003/Lecture3/#problem","text":"\u2014 Input: an array A of n numbers (unordered), and k \u2014 Output: the k-th smallest number (counting from 0)","title":"Problem:"},{"location":"MSBD5003/Lecture3/#algorithm","text":"\\(x=A[0]\\) partition A into \\(A[0..mid-1] < A[mid] = x < A[mid+1..n-1]\\) if \\(mid =k\\) then return \\(x\\) if \\(k<mid\\) then \\(A= A[O..mid-1]\\) if k > mid then \\(A = A[mid+1,n-1], k= k\u2014 mid-1\\) gotostep 1","title":"Algorithm"},{"location":"MSBD5003/Lecture3/#key-value-pairs","text":"While most Spark operations work on RDDs containing any type of objects, a few special operations are only available on RDDs of key-value pairs. In Python, these operations work on RDDs containing built-in Python tuples such as (1, 2). Simply create such tuples and then call your desired operation. For example, the following code uses the reduceByKey operation on key-value pairs to count how many times each line of text occurs in a file: lines = sc . textFile ( \"README.md\" ) pairs = lines . map ( lambda s : ( s , 1 )) counts = pairs . reduceByKey ( lambda a , b : a + b ) We could also use counts.sortByKey() , for example, to sort the pairs alphabetically, and finally counts.collect() to bring them back to the driver program as a list of objects.","title":"Key-value Pairs"},{"location":"MSBD5003/Lecture3/#pmi","text":"PMI (pointwise mutual information) is a measure of association used in information theory and statistics. Given a list of pairs (x, y) \\[pmi(x, y) = log\\frac{p(x,y)}{p(x)p(y}\\] where - \\(p(x)\\) : probability of x - \\(p(y)\\) : probability of y - \\(p(x,y)\\) : joint probability Example: p(x=0) = 0.8, p(x=1)=0.2, p(y=0)=0.25, p(y=1)=0.75 pmi(x=0;y=0) = \u22121 pmi(x=0;y=1) = 0.222392 pmi(x=1;y=0) = 1.584963 pmi(x=1;y=1) = -1.584963 Example notebook see: note book in class/PMI","title":"PMI"},{"location":"MSBD5003/Lecture4/","text":"SQL \u00b6 A tuple = a record = row A table = a set of tuples join \u00b6 support we have a table with following schema drop table if exists product , location ; create table location ( id int primary key auto_increment , location text not null ); create table product ( id int primary key auto_increment , name text , location_id int , foreign key ( location_id ) references location ( id ) ); insert into location ( location ) values ( 'Shenzhen' ); insert into location ( location ) values ( 'Shanghai' ); insert into location ( location ) values ( 'Beijing' ); insert into product ( name , location_id ) values ( 'iPad' , 1 ); insert into product ( name , location_id ) values ( 'iPhone' , 2 ); insert into product ( name , location_id ) values ( 'iMac' , 3 ); First way \u00b6 select name from product inner join location l on product . location_id = l . id where location = 'Shenzhen' ; Second way \u00b6 select name from product , location l where location_id = l . id and location = 'Shenzhen' ; Outter join \u00b6 iPad Shenzhen iPhone Shanghai iMac Beijing Null Hangzhou Inner join \u00b6 iPad Shenzhen iPhone Shanghai iMac Beijing PySpark \u00b6 !pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 61kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 32.1MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=bde33d542b14d579dae0fe882a74c735cdcb1eb1aa95d876737953e41f19d397 Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 from pyspark.sql import Row row = Row(name='Alice', age=11) row.name, row.age ('Alice', 11) row = Row(name='Alice', age=11, count=1) print(f\"Row.count: {row.count}\") print(f\"Row['count']: {row['count']}\") Row.count: <built-in method count of Row object at 0x7fd3091ecaf0> Row['count']: 1","title":"Lecture4"},{"location":"MSBD5003/Lecture4/#sql","text":"A tuple = a record = row A table = a set of tuples","title":"SQL"},{"location":"MSBD5003/Lecture4/#join","text":"support we have a table with following schema drop table if exists product , location ; create table location ( id int primary key auto_increment , location text not null ); create table product ( id int primary key auto_increment , name text , location_id int , foreign key ( location_id ) references location ( id ) ); insert into location ( location ) values ( 'Shenzhen' ); insert into location ( location ) values ( 'Shanghai' ); insert into location ( location ) values ( 'Beijing' ); insert into product ( name , location_id ) values ( 'iPad' , 1 ); insert into product ( name , location_id ) values ( 'iPhone' , 2 ); insert into product ( name , location_id ) values ( 'iMac' , 3 );","title":"join"},{"location":"MSBD5003/Lecture4/#first-way","text":"select name from product inner join location l on product . location_id = l . id where location = 'Shenzhen' ;","title":"First way"},{"location":"MSBD5003/Lecture4/#second-way","text":"select name from product , location l where location_id = l . id and location = 'Shenzhen' ;","title":"Second way"},{"location":"MSBD5003/Lecture4/#outter-join","text":"iPad Shenzhen iPhone Shanghai iMac Beijing Null Hangzhou","title":"Outter join"},{"location":"MSBD5003/Lecture4/#inner-join","text":"iPad Shenzhen iPhone Shanghai iMac Beijing","title":"Inner join"},{"location":"MSBD5003/Lecture4/#pyspark","text":"!pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 61kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 32.1MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=bde33d542b14d579dae0fe882a74c735cdcb1eb1aa95d876737953e41f19d397 Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 from pyspark.sql import Row row = Row(name='Alice', age=11) row.name, row.age ('Alice', 11) row = Row(name='Alice', age=11, count=1) print(f\"Row.count: {row.count}\") print(f\"Row['count']: {row['count']}\") Row.count: <built-in method count of Row object at 0x7fd3091ecaf0> Row['count']: 1","title":"PySpark"},{"location":"MSBD5003/Lecture5/","text":"","title":"Lecture5"},{"location":"MSBD5003/homeworks/hw1/","text":"!pip install pyspar import requests from pyspark.context import SparkContext r = requests . get ( 'https://www.cse.ust.hk/msbd5003/data/fruits.txt' ) open ( 'fruits.txt' , 'wb' ) . write ( r . content ) sc = SparkContext . getOrCreate () Question 1 \u00b6 The following piece of code computes the frequencies of the words in a text file: from operator import add lines = sc . textFile ( 'README.md' ) counts = lines . flatMap ( lambda x : x . split ()) \\ . map ( lambda x : ( x , 1 )) \\ . reduceByKey ( add ) Add one line to find the most frequent word. Output this word and its frequency. Hint: Use sortBy(), reduce(), or max() from operator import add lines = sc . textFile ( 'sample_data/README.md' ) counts = lines . flatMap ( lambda x : x . split ()) \\ . map ( lambda x : ( x , 1 )) \\ . reduceByKey ( add ) \\ . max ( key = lambda x : x [ 1 ]) print ( counts ) ('is', 4) Question 2 \u00b6 Modify the word count example above, so that we only count the frequencies of those words consisting of 5 or more characters. from operator import add lines = sc . textFile ( 'sample_data/README.md' ) counts = lines . flatMap ( lambda x : x . split ()) \\ . map ( lambda x : ( x , 1 )) \\ . reduceByKey ( add ) \\ . filter ( lambda x : len ( x [ 0 ]) >= 5 ) print ( counts . take ( 10 )) [('directory', 1), ('datasets', 1), ('`california_housing_data*.csv`', 1), ('housing', 1), ('https://developers.google.com/machine-learning/crash-course/california-housing-data-description', 1), ('`mnist_*.csv`', 1), (\"[Anscombe's\", 1), ('originally', 1), ('Anscombe,', 1), (\"'Graphs\", 1)] Question 3 \u00b6 Consider the following piece of code: A = sc . parallelize ( range ( 1 , 100 )) t = 50 B = A . filter ( lambda x : x < t ) print ( B . count ()) t = 10 C = B . filter ( lambda x : x > t ) print ( C . count ()) What's its output? (Yes, you can just run it.) A = sc.parallelize(range(1, 100)) t = 50 B = A.filter(lambda x: x < t) print(B.count()) t = 10 C = B.filter(lambda x: x > t) print(C.count()) 49 0 Question 4 \u00b6 The intent of the code above is to get all numbers below 50 from A and put them into B, and then get all numbers above 10 from B and put them into C. Fix the code so that it produces the desired behavior, by adding one line of code. You are not allowed to change the existing code. A = sc.parallelize(range(1, 100)) t = 50 B = A.filter(lambda x: x < t) B.cache() print(B.count()) t = 10 C = B.filter(lambda x: x > t) print(C.count()) 49 39 Question 5 \u00b6 Modify the PMI example by sending a_dict and n_dict inside the closure. Do not use broadcast variables. By changing broadcast variable n_dict = sc.broadcast(n_freqs.collectAsMap()) a_dict = sc.broadcast(a_freqs.collectAsMap()) to global variable n_dict = n_freqs . collectAsMap () a_dict = a_freqs . collectAsMap () Question 6 \u00b6 The following code creates an RDD with 4 partitions: partition 0, 1, 2, and 3. A = sc . parallelize ( range ( 100 ), 4 ) For each item in the RDD, add its partition number to it, and write the results to another RDD, i.e., the resulting RDD should contain: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102] def f ( splitIndex , iterator ): for i in iterator : yield i + splitIndex A = sc . parallelize ( range ( 100 ), 4 ) A . mapPartitionsWithIndex ( f ). collect ()","title":"Hw1"},{"location":"MSBD5003/homeworks/hw1/#question-1","text":"The following piece of code computes the frequencies of the words in a text file: from operator import add lines = sc . textFile ( 'README.md' ) counts = lines . flatMap ( lambda x : x . split ()) \\ . map ( lambda x : ( x , 1 )) \\ . reduceByKey ( add ) Add one line to find the most frequent word. Output this word and its frequency. Hint: Use sortBy(), reduce(), or max() from operator import add lines = sc . textFile ( 'sample_data/README.md' ) counts = lines . flatMap ( lambda x : x . split ()) \\ . map ( lambda x : ( x , 1 )) \\ . reduceByKey ( add ) \\ . max ( key = lambda x : x [ 1 ]) print ( counts ) ('is', 4)","title":"Question 1"},{"location":"MSBD5003/homeworks/hw1/#question-2","text":"Modify the word count example above, so that we only count the frequencies of those words consisting of 5 or more characters. from operator import add lines = sc . textFile ( 'sample_data/README.md' ) counts = lines . flatMap ( lambda x : x . split ()) \\ . map ( lambda x : ( x , 1 )) \\ . reduceByKey ( add ) \\ . filter ( lambda x : len ( x [ 0 ]) >= 5 ) print ( counts . take ( 10 )) [('directory', 1), ('datasets', 1), ('`california_housing_data*.csv`', 1), ('housing', 1), ('https://developers.google.com/machine-learning/crash-course/california-housing-data-description', 1), ('`mnist_*.csv`', 1), (\"[Anscombe's\", 1), ('originally', 1), ('Anscombe,', 1), (\"'Graphs\", 1)]","title":"Question 2"},{"location":"MSBD5003/homeworks/hw1/#question-3","text":"Consider the following piece of code: A = sc . parallelize ( range ( 1 , 100 )) t = 50 B = A . filter ( lambda x : x < t ) print ( B . count ()) t = 10 C = B . filter ( lambda x : x > t ) print ( C . count ()) What's its output? (Yes, you can just run it.) A = sc.parallelize(range(1, 100)) t = 50 B = A.filter(lambda x: x < t) print(B.count()) t = 10 C = B.filter(lambda x: x > t) print(C.count()) 49 0","title":"Question 3"},{"location":"MSBD5003/homeworks/hw1/#question-4","text":"The intent of the code above is to get all numbers below 50 from A and put them into B, and then get all numbers above 10 from B and put them into C. Fix the code so that it produces the desired behavior, by adding one line of code. You are not allowed to change the existing code. A = sc.parallelize(range(1, 100)) t = 50 B = A.filter(lambda x: x < t) B.cache() print(B.count()) t = 10 C = B.filter(lambda x: x > t) print(C.count()) 49 39","title":"Question 4"},{"location":"MSBD5003/homeworks/hw1/#question-5","text":"Modify the PMI example by sending a_dict and n_dict inside the closure. Do not use broadcast variables. By changing broadcast variable n_dict = sc.broadcast(n_freqs.collectAsMap()) a_dict = sc.broadcast(a_freqs.collectAsMap()) to global variable n_dict = n_freqs . collectAsMap () a_dict = a_freqs . collectAsMap ()","title":"Question 5"},{"location":"MSBD5003/homeworks/hw1/#question-6","text":"The following code creates an RDD with 4 partitions: partition 0, 1, 2, and 3. A = sc . parallelize ( range ( 100 ), 4 ) For each item in the RDD, add its partition number to it, and write the results to another RDD, i.e., the resulting RDD should contain: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102] def f ( splitIndex , iterator ): for i in iterator : yield i + splitIndex A = sc . parallelize ( range ( 100 ), 4 ) A . mapPartitionsWithIndex ( f ). collect ()","title":"Question 6"},{"location":"MSBD5003/notebooks%20in%20class/PMI/","text":"PMI \u00b6 PMI (pointwise mutual information) is a measure of association used in information theory and statistics. Given a list of pairs (x, y) \\[pmi(x, y) = log\\frac{p(x,y)}{p(x)p(y}\\] where - \\(p(x)\\) : probability of x - \\(p(y)\\) : probability of y - \\(p(x,y)\\) : joint probability Example: p(x=0) = 0.8, p(x=1)=0.2, p(y=0)=0.25, p(y=1)=0.75 pmi(x=0;y=0) = \u22121 pmi(x=0;y=1) = 0.222392 pmi(x=1;y=0) = 1.584963 pmi(x=1;y=1) = -1.584963 Example notebook see: note book in class/PMI ! pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 70kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 44.6MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=ae3121fc30af19c4ec22b0beb2c7452d103f59dc5ad06c6fa21a5b108cdbf54a Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 import requests from pyspark.context import SparkContext r = requests . get ( 'https://www.cse.ust.hk/msbd5003/data/adj_noun_pairs.txt' ) open ( 'adj_noun_pairs.txt' , 'wb' ) . write ( r . content ) sc = SparkContext . getOrCreate () # Data file at https://www.cse.ust.hk/msbd5003/data lines = sc . textFile ( 'adj_noun_pairs.txt' ) lines . count () 3162692 lines . getNumPartitions () 2 lines . take ( 5 ) ['early radical', 'french revolution', 'pejorative way', 'violent means', 'positive label'] # Converting lines into word pairs. # Data is dirty: some lines have more than 2 words, so filter them out. pairs = lines . map ( lambda l : tuple ( l . split ())) . filter ( lambda p : len ( p ) == 2 ) pairs . cache () PythonRDD[4] at RDD at PythonRDD.scala:53 pairs . take ( 5 ) [('early', 'radical'), ('french', 'revolution'), ('pejorative', 'way'), ('violent', 'means'), ('positive', 'label')] N = pairs . count () N 3162674 # Compute the frequency of each pair. # Ignore pairs that not frequent enough pair_freqs = pairs . map ( lambda p : ( p , 1 )) . reduceByKey ( lambda f1 , f2 : f1 + f2 ) \\ . filter ( lambda pf : pf [ 1 ] >= 100 ) pair_freqs . take ( 5 ) [(('political', 'philosophy'), 160), (('human', 'society'), 154), (('16th', 'century'), 950), (('first', 'man'), 166), (('same', 'time'), 2744)] # Computing the frequencies of the adjectives and the nouns a_freqs = pairs . map ( lambda p : ( p [ 0 ], 1 )) . reduceByKey ( lambda x , y : x + y ) n_freqs = pairs . map ( lambda p : ( p [ 1 ], 1 )) . reduceByKey ( lambda x , y : x + y ) a_freqs . take ( 5 ) [('violent', 1191), ('positive', 2302), ('self-defined', 3), ('political', 15935), ('differ', 381)] n_freqs . count () 106333 # Broadcasting the adjective and noun frequencies. #a_dict = a_freqs.collectAsMap() #a_dict = sc.parallelize(a_dict).map(lambda x: x) n_dict = sc . broadcast ( n_freqs . collectAsMap ()) a_dict = sc . broadcast ( a_freqs . collectAsMap ()) a_dict . value [ 'violent' ] 1191 from math import * # Computing the PMI for a pair. def pmi_score ( pair_freq ): w1 , w2 = pair_freq [ 0 ] f = pair_freq [ 1 ] pmi = log ( float ( f ) * N / ( a_dict . value [ w1 ] * n_dict . value [ w2 ]), 2 ) return pmi , ( w1 , w2 ) # Computing the PMI for all pairs. scored_pairs = pair_freqs . map ( pmi_score ) # Printing the most strongly associated pairs. scored_pairs . top ( 10 ) [(14.41018838546462, ('magna', 'carta')), (13.071365888694997, ('polish-lithuanian', 'Commonwealth')), (12.990597616733414, ('nitrous', 'oxide')), (12.64972604311254, ('latter-day', 'Saints')), (12.50658937509916, ('stainless', 'steel')), (12.482331020687814, ('pave', 'runway')), (12.19140721768055, ('corporal', 'punishment')), (12.183248694293388, ('capital', 'punishment')), (12.147015483562537, ('rush', 'yard')), (12.109945794428935, ('globular', 'cluster'))] Another way n_dict = n_freqs . collectAsMap () a_dict = a_freqs . collectAsMap () from math import * # Computing the PMI for a pair. def pmi_score ( pair_freq ): w1 , w2 = pair_freq [ 0 ] f = pair_freq [ 1 ] pmi = log ( float ( f ) * N / ( a_dict [ w1 ] * n_dict [ w2 ]), 2 ) return pmi , ( w1 , w2 ) scored_pairs = pair_freqs . map ( pmi_score ) scored_pairs . top ( 10 ) [(14.41018838546462, ('magna', 'carta')), (13.071365888694997, ('polish-lithuanian', 'Commonwealth')), (12.990597616733414, ('nitrous', 'oxide')), (12.64972604311254, ('latter-day', 'Saints')), (12.50658937509916, ('stainless', 'steel')), (12.482331020687814, ('pave', 'runway')), (12.19140721768055, ('corporal', 'punishment')), (12.183248694293388, ('capital', 'punishment')), (12.147015483562537, ('rush', 'yard')), (12.109945794428935, ('globular', 'cluster'))]","title":"Pmi"},{"location":"MSBD5003/notebooks%20in%20class/PMI/#pmi","text":"PMI (pointwise mutual information) is a measure of association used in information theory and statistics. Given a list of pairs (x, y) \\[pmi(x, y) = log\\frac{p(x,y)}{p(x)p(y}\\] where - \\(p(x)\\) : probability of x - \\(p(y)\\) : probability of y - \\(p(x,y)\\) : joint probability Example: p(x=0) = 0.8, p(x=1)=0.2, p(y=0)=0.25, p(y=1)=0.75 pmi(x=0;y=0) = \u22121 pmi(x=0;y=1) = 0.222392 pmi(x=1;y=0) = 1.584963 pmi(x=1;y=1) = -1.584963 Example notebook see: note book in class/PMI ! pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 70kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 44.6MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=ae3121fc30af19c4ec22b0beb2c7452d103f59dc5ad06c6fa21a5b108cdbf54a Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 import requests from pyspark.context import SparkContext r = requests . get ( 'https://www.cse.ust.hk/msbd5003/data/adj_noun_pairs.txt' ) open ( 'adj_noun_pairs.txt' , 'wb' ) . write ( r . content ) sc = SparkContext . getOrCreate () # Data file at https://www.cse.ust.hk/msbd5003/data lines = sc . textFile ( 'adj_noun_pairs.txt' ) lines . count () 3162692 lines . getNumPartitions () 2 lines . take ( 5 ) ['early radical', 'french revolution', 'pejorative way', 'violent means', 'positive label'] # Converting lines into word pairs. # Data is dirty: some lines have more than 2 words, so filter them out. pairs = lines . map ( lambda l : tuple ( l . split ())) . filter ( lambda p : len ( p ) == 2 ) pairs . cache () PythonRDD[4] at RDD at PythonRDD.scala:53 pairs . take ( 5 ) [('early', 'radical'), ('french', 'revolution'), ('pejorative', 'way'), ('violent', 'means'), ('positive', 'label')] N = pairs . count () N 3162674 # Compute the frequency of each pair. # Ignore pairs that not frequent enough pair_freqs = pairs . map ( lambda p : ( p , 1 )) . reduceByKey ( lambda f1 , f2 : f1 + f2 ) \\ . filter ( lambda pf : pf [ 1 ] >= 100 ) pair_freqs . take ( 5 ) [(('political', 'philosophy'), 160), (('human', 'society'), 154), (('16th', 'century'), 950), (('first', 'man'), 166), (('same', 'time'), 2744)] # Computing the frequencies of the adjectives and the nouns a_freqs = pairs . map ( lambda p : ( p [ 0 ], 1 )) . reduceByKey ( lambda x , y : x + y ) n_freqs = pairs . map ( lambda p : ( p [ 1 ], 1 )) . reduceByKey ( lambda x , y : x + y ) a_freqs . take ( 5 ) [('violent', 1191), ('positive', 2302), ('self-defined', 3), ('political', 15935), ('differ', 381)] n_freqs . count () 106333 # Broadcasting the adjective and noun frequencies. #a_dict = a_freqs.collectAsMap() #a_dict = sc.parallelize(a_dict).map(lambda x: x) n_dict = sc . broadcast ( n_freqs . collectAsMap ()) a_dict = sc . broadcast ( a_freqs . collectAsMap ()) a_dict . value [ 'violent' ] 1191 from math import * # Computing the PMI for a pair. def pmi_score ( pair_freq ): w1 , w2 = pair_freq [ 0 ] f = pair_freq [ 1 ] pmi = log ( float ( f ) * N / ( a_dict . value [ w1 ] * n_dict . value [ w2 ]), 2 ) return pmi , ( w1 , w2 ) # Computing the PMI for all pairs. scored_pairs = pair_freqs . map ( pmi_score ) # Printing the most strongly associated pairs. scored_pairs . top ( 10 ) [(14.41018838546462, ('magna', 'carta')), (13.071365888694997, ('polish-lithuanian', 'Commonwealth')), (12.990597616733414, ('nitrous', 'oxide')), (12.64972604311254, ('latter-day', 'Saints')), (12.50658937509916, ('stainless', 'steel')), (12.482331020687814, ('pave', 'runway')), (12.19140721768055, ('corporal', 'punishment')), (12.183248694293388, ('capital', 'punishment')), (12.147015483562537, ('rush', 'yard')), (12.109945794428935, ('globular', 'cluster'))] Another way n_dict = n_freqs . collectAsMap () a_dict = a_freqs . collectAsMap () from math import * # Computing the PMI for a pair. def pmi_score ( pair_freq ): w1 , w2 = pair_freq [ 0 ] f = pair_freq [ 1 ] pmi = log ( float ( f ) * N / ( a_dict [ w1 ] * n_dict [ w2 ]), 2 ) return pmi , ( w1 , w2 ) scored_pairs = pair_freqs . map ( pmi_score ) scored_pairs . top ( 10 ) [(14.41018838546462, ('magna', 'carta')), (13.071365888694997, ('polish-lithuanian', 'Commonwealth')), (12.990597616733414, ('nitrous', 'oxide')), (12.64972604311254, ('latter-day', 'Saints')), (12.50658937509916, ('stainless', 'steel')), (12.482331020687814, ('pave', 'runway')), (12.19140721768055, ('corporal', 'punishment')), (12.183248694293388, ('capital', 'punishment')), (12.147015483562537, ('rush', 'yard')), (12.109945794428935, ('globular', 'cluster'))]","title":"PMI"},{"location":"MSBD5003/notebooks%20in%20class/rdd/","text":"How do I make an RDD? \u00b6 RDDs can be created from stable storage or by transforming other RDDs. Run the cells below to create RDDs from files on the local drive. All data files can be downloaded from https://www.cse.ust.hk/msbd5003/data/ For example, https://www.cse.ust.hk/msbd5003/data/fruits.txt # Read data from local file system: print ( sc . version ) fruits = sc . textFile ( '../data/fruits.txt' ) yellowThings = sc . textFile ( '../data/yellowthings.txt' ) print ( fruits . collect ()) print ( yellowThings . collect ()) 3.0.0 ['apple', 'banana', 'canary melon', 'grape', 'lemon', 'orange', 'pineapple', 'strawberry'] ['banana', 'bee', 'butter', 'canary melon', 'gold', 'lemon', 'pineapple', 'sunflower'] # Read data from HDFS : fruits = sc . textFile ( 'hdfs://url:9000/pathname/fruits.txt' ) fruits . collect () RDD operations \u00b6 # map fruitsReversed = fruits . map ( lambda fruit : fruit [:: - 1 ]) # fruitsReversed = fruits.map(lambda fruit: fruit[::-1]) fruitsReversed . persist () # try changing the file and re-execute with and without cache print ( fruitsReversed . collect ()) # What happens when you uncomment the first line and run the whole program again with cache()? ['elppab', 'ananab', 'nolem yranac', 'parg', 'nomel', 'egnaro', 'elppaenip', 'yrrebwarts'] # filter shortFruits = fruits . filter ( lambda fruit : len ( fruit ) <= 5 ) print ( shortFruits . collect ()) ['grap', 'lemon'] # flatMap characters = fruits . flatMap ( lambda fruit : list ( fruit )) print ( characters . collect ()) ['b', 'a', 'p', 'p', 'l', 'e', 'b', 'a', 'n', 'a', 'n', 'a', 'c', 'a', 'n', 'a', 'r', 'y', ' ', 'm', 'e', 'l', 'o', 'n', 'g', 'r', 'a', 'p', 'l', 'e', 'm', 'o', 'n', 'o', 'r', 'a', 'n', 'g', 'e', 'p', 'i', 'n', 'e', 'a', 'p', 'p', 'l', 'e', 's', 't', 'r', 'a', 'w', 'b', 'e', 'r', 'r', 'y'] # union fruitsAndYellowThings = fruits . union ( yellowThings ) print ( fruitsAndYellowThings . collect ()) ['bapple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry', 'banana', 'bee', 'butter', 'canary melon', 'gold', 'lemon', 'pineapple', 'sunflower'] # intersection yellowFruits = fruits . intersection ( yellowThings ) print ( yellowFruits . collect ()) ['pineapple', 'canary melon', 'lemon', 'banana'] # distinct distinctFruitsAndYellowThings = fruitsAndYellowThings . distinct () print ( distinctFruitsAndYellowThings . collect ()) ['orange', 'pineapple', 'canary melon', 'lemon', 'bee', 'banana', 'butter', 'gold', 'sunflower', 'apple', 'grap', 'strawberry'] RDD actions \u00b6 Following are examples of some of the common actions available. For a detailed list, see RDD Actions . Run some transformations below to understand this better. Place the cursor in the cell and press SHIFT + ENTER . # collect fruitsArray = fruits . collect () yellowThingsArray = yellowThings . collect () print ( fruitsArray ) ['apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry'] # count numFruits = fruits . count () print ( numFruits ) 8 # take first3Fruits = fruits . take ( 3 ) print ( first3Fruits ) ['apple', 'banana', 'canary melon'] # reduce letterSet = fruits . map ( lambda fruit : set ( fruit )) . reduce ( lambda x , y : x . union ( y )) print ( letterSet ) {'o', 'r', 'a', 'i', 'p', 'g', 'c', ' ', 'l', 'y', 'e', 'w', 'n', 'b', 'm', 't', 's'} letterSet = fruits . flatMap ( lambda fruit : list ( fruit )) . distinct () . collect () print ( letterSet ) ['p', 'l', 'b', 'c', 'r', 'y', 'g', 'i', 's', 'a', 'e', 'n', ' ', 'm', 'o', 't', 'w'] Closure \u00b6 counter = 0 rdd = sc . parallelize ( range ( 10 )) # Wrong: Don't do this!! def increment_counter ( x ): global counter counter += x print ( rdd . collect ()) rdd . foreach ( increment_counter ) print ( counter ) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 0 rdd = sc . parallelize ( range ( 10 )) accum = sc . accumulator ( 0 ) def g ( x ): global accum accum += x a = rdd . foreach ( g ) print ( accum . value ) -45 rdd = sc . parallelize ( range ( 10 )) accum = sc . accumulator ( 0 ) def g ( x ): global accum accum += x return x * x a = rdd . map ( g ) print ( accum . value ) #print(a.reduce(lambda x, y: x+y)) a . cache () tmp = a . count () print ( accum . value ) print ( rdd . reduce ( lambda x , y : x + y )) tmp = a . count () print ( accum . value ) print ( rdd . reduce ( lambda x , y : x + y )) 0 45 45 45 45 Computing Pi using Monte Carlo simulation \u00b6 # From the official spark examples. import random import time partitions = 1000 n = 1000 * partitions def f ( _ ): x = random . random () y = random . random () return 1 if x ** 2 + y ** 2 < 1 else 0 count = sc . parallelize ( range ( 1 , n + 1 ), partitions ) \\ . map ( f ) . sum () print ( \"Pi is roughly\" , 4.0 * count / n ) Pi is roughly 3.140944 # Example: glom import sys import random def f ( _ ): random . seed ( time . time ()) return random . random () a = sc . parallelize ( range ( 0 , 100 ), 10 ) print ( a . collect ()) print ( a . glom () . collect ()) print ( a . map ( f ) . glom () . collect ()) # Weird behavior: Initially, random numbers are synched across all workers, but will get # out-of-sync after a large (e.g, 1000000) number of random numbers have been generated. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [[0.6608713426170987, 0.698767024318554, 0.1874105777790005, 0.7623702078433652, 0.8851287594440702, 0.31740294580255735, 0.19323310102732394, 0.42450071105921683, 0.5933781451859748, 0.7458943680790939], [0.7261502175930336, 0.22659503054053598, 0.9192074261481535, 0.4774662604141523, 0.7974422880272903, 0.2584976474338707, 0.6055611352765481, 0.5244790798752513, 0.6861813792912159, 0.5652815222674437], [0.27860057141024697, 0.27383515025078553, 0.9176819782462265, 0.417689753313761, 0.6135860183360143, 0.8162090147099693, 0.39224804876974406, 0.543173888187219, 0.3098912544023783, 0.633182881742779], [0.0952563896474653, 0.7477071810186972, 0.5004564582092008, 0.2614834043253954, 0.5982982446751687, 0.8544002333592715, 0.26000819037953216, 0.40177311792144454, 0.03851083747397188, 0.05167636277510712], [0.9726302497724043, 0.42432064255976365, 0.9305610323744404, 0.771694551386715, 0.6789841281422876, 0.9487832709253969, 0.4943030306526911, 0.22888583384514705, 0.6165263440265218, 0.8948635092093183], [0.9816006872849989, 0.3233518004555158, 0.6660672115030636, 0.9921564654020117, 0.9574487554669273, 0.00033642413291157247, 0.5729463981674527, 0.63676146970985, 0.1068707761119706, 0.4974835849045728], [0.6877782810075579, 0.11000878013616322, 0.6630366287015564, 0.0320757478156235, 0.5550374523078817, 0.11429763248899893, 0.7746616174182379, 0.6935564378314162, 0.6081187039755812, 0.3594774747771995], [0.3402744125431225, 0.8533066685831103, 0.18605963113570156, 0.9700428171414653, 0.9046533776474858, 0.4199976147427207, 0.01833313615444565, 0.5003118405702941, 0.9167261953361863, 0.6543553598701435], [0.5463089308369264, 0.19187434980340723, 0.5311179490604816, 0.7210872364087648, 0.25848050944241396, 0.9138829006068386, 0.5015098582184656, 0.9245322749204768, 0.4746635193819774, 0.733561516539988], [0.5924804586325896, 0.44157691425623313, 0.06474182310396659, 0.3705313104712945, 0.218280453275444, 0.911250263493956, 0.4908690024649712, 0.031427016100674665, 0.3749922950484815, 0.29534800562581187]] # Example: mapPartition and mapPartitionWithIndex a = sc . parallelize ( range ( 0 , 20 ), 4 ) print ( a . glom () . collect ()) def f ( it ): s = 0 for i in it : s += i yield s print ( a . mapPartitions ( f ) . collect ()) def f ( index , it ): s = index for i in it : s += i yield s print ( a . mapPartitionsWithIndex ( f ) . collect ()) [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]] [0, 1, 3, 6, 10, 5, 11, 18, 26, 35, 10, 21, 33, 46, 60, 15, 31, 48, 66, 85] [0, 1, 3, 6, 10, 6, 12, 19, 27, 36, 12, 23, 35, 48, 62, 18, 34, 51, 69, 88] # Correct version import random import time partitions = 1000 n = 1000 * partitions seed = time . time () def f ( index , it ): random . seed ( index + seed ) for i in it : x = random . random () y = random . random () yield 1 if x ** 2 + y ** 2 < 1 else 0 count = sc . parallelize ( range ( 1 , n + 1 ), partitions ) \\ . mapPartitionsWithIndex ( f ) . sum () print ( \"Pi is roughly\" , 4.0 * count / n ) Pi is roughly 3.141832 Closure and Persistence \u00b6 # RDD variables are references A = sc . parallelize ( range ( 10 )) B = A . map ( lambda x : x * 2 ) A = B . map ( lambda x : x + 1 ) A . take ( 10 ) [1, 3, 5, 7, 9, 11, 13, 15, 17, 19] # Linear-time selection data = [ 34 , 67 , 21 , 56 , 47 , 89 , 12 , 44 , 74 , 43 , 26 ] A = sc . parallelize ( data , 2 ) k = 4 while True : x = A . first () A1 = A . filter ( lambda z : z < x ) A2 = A . filter ( lambda z : z > x ) A1 . cache () A2 . cache () mid = A1 . count () if mid == k : print ( x ) break if k < mid : A = A1 else : A = A2 k = k - mid - 1 43 sorted ( data ) [12, 21, 26, 34, 43, 44, 47, 56, 67, 74, 89] A = sc . parallelize ( range ( 10 )) x = 5 B = A . filter ( lambda z : z < x ) # B.cache() print ( B . count ()) x = 3 print ( B . count ()) 5 5 A = sc . parallelize ( range ( 10 )) x = 5 B = A . filter ( lambda z : z < x ) # B.cache() B . unpersist () # print(B.take(10)) print ( B . collect ()) x = 3 #print(B.take(10)) print ( B . collect ()) # collect() doesn't always re-collect data - bad design! # Always use take() instead of collect() [0, 1, 2, 3, 4] [0, 1, 2, 3, 4] Key-Value Pairs \u00b6 # reduceByKey numFruitsByLength = fruits . map ( lambda fruit : ( len ( fruit ), 1 )) . reduceByKey ( lambda x , y : x + y ) print ( numFruitsByLength . take ( 10 )) from operator import add lines = sc . textFile ( '../data/course.txt' ) counts = lines . flatMap ( lambda x : x . split ()) \\ . map ( lambda x : ( x , 1 )) \\ . reduceByKey ( add ) print ( counts . sortByKey () . take ( 20 )) print ( counts . sortBy ( lambda x : x [ 1 ], False ) . take ( 20 )) # Join simple example products = sc . parallelize ([( 1 , \"Apple\" ), ( 2 , \"Orange\" ), ( 3 , \"TV\" ), ( 5 , \"Computer\" )]) #trans = sc.parallelize([(1, 134, \"OK\"), (3, 34, \"OK\"), (5, 162, \"Error\"), (1, 135, \"OK\"), (2, 53, \"OK\"), (1, 45, \"OK\")]) trans = sc . parallelize ([( 1 , ( 134 , \"OK\" )), ( 3 , ( 34 , \"OK\" )), ( 5 , ( 162 , \"Error\" )), ( 1 , ( 135 , \"OK\" )), ( 2 , ( 53 , \"OK\" )), ( 1 , ( 45 , \"OK\" ))]) print ( products . join ( trans ) . take ( 20 )) K-means clustering \u00b6 import numpy as np def parseVector ( line ): return np . array ([ float ( x ) for x in line . split ()]) def closestPoint ( p , centers ): bestIndex = 0 closest = float ( \"+inf\" ) for i in range ( len ( centers )): tempDist = np . sum (( p - centers [ i ]) ** 2 ) if tempDist < closest : closest = tempDist bestIndex = i return bestIndex # The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_data.txt lines = sc . textFile ( '../data/kmeans_data.txt' , 5 ) # The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_bigdata.txt # lines = sc.textFile('../data/kmeans_bigdata.txt', 5) # lines is an RDD of strings K = 3 convergeDist = 0.01 # terminate algorithm when the total distance from old center to new centers is less than this value data = lines . map ( parseVector ) . cache () # data is an RDD of arrays kCenters = data . takeSample ( False , K , 1 ) # intial centers as a list of arrays tempDist = 1.0 # total distance from old centers to new centers while tempDist > convergeDist : closest = data . map ( lambda p : ( closestPoint ( p , kCenters ), ( p , 1 ))) # for each point in data, find its closest center # closest is an RDD of tuples (index of closest center, (point, 1)) pointStats = closest . reduceByKey ( lambda p1 , p2 : ( p1 [ 0 ] + p2 [ 0 ], p1 [ 1 ] + p2 [ 1 ])) # pointStats is an RDD of tuples (index of center, # (array of sums of coordinates, total number of points assigned)) newCenters = pointStats . map ( lambda st : ( st [ 0 ], st [ 1 ][ 0 ] / st [ 1 ][ 1 ])) . collect () # compute the new centers tempDist = sum ( np . sum (( kCenters [ i ] - p ) ** 2 ) for ( i , p ) in newCenters ) # compute the total disctance from old centers to new centers for ( i , p ) in newCenters : kCenters [ i ] = p print ( \"Final centers: \" , kCenters ) PageRank \u00b6 import re from operator import add def computeContribs ( urls , rank ): # Calculates URL contributions to the rank of other URLs. num_urls = len ( urls ) for url in urls : yield ( url , rank / num_urls ) def parseNeighbors ( urls ): # Parses a urls pair string into urls pair.\"\"\" parts = urls . split ( ' ' ) return parts [ 0 ], parts [ 1 ] # Loads in input file. It should be in format of: # URL neighbor URL # URL neighbor URL # URL neighbor URL # ... # The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/* lines = sc . textFile ( \"../data/pagerank_data.txt\" , 2 ) # lines = sc.textFile(\"../data/dblp.in\", 5) numOfIterations = 10 # Loads all URLs from input file and initialize their neighbors. links = lines . map ( lambda urls : parseNeighbors ( urls )) \\ . groupByKey () # Loads all URLs with other URL(s) link to from input file # and initialize ranks of them to one. ranks = links . mapValues ( lambda neighbors : 1.0 ) # Calculates and updates URL ranks continuously using PageRank algorithm. for iteration in range ( numOfIterations ): # Calculates URL contributions to the rank of other URLs. contribs = links . join ( ranks ) \\ . flatMap ( lambda url_urls_rank : computeContribs ( url_urls_rank [ 1 ][ 0 ], url_urls_rank [ 1 ][ 1 ])) # After the join, each element in the RDD is of the form # (url, (list of neighbor urls, rank)) # Re-calculates URL ranks based on neighbor contributions. # ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15) ranks = contribs . reduceByKey ( add ) . map ( lambda t : ( t [ 0 ], t [ 1 ] * 0.85 + 0.15 )) print ( ranks . top ( 5 , lambda x : x [ 1 ])) Join vs. Broadcast Variables \u00b6 products = sc . parallelize ([( 1 , \"Apple\" ), ( 2 , \"Orange\" ), ( 3 , \"TV\" ), ( 5 , \"Computer\" )]) trans = sc . parallelize ([( 1 , ( 134 , \"OK\" )), ( 3 , ( 34 , \"OK\" )), ( 5 , ( 162 , \"Error\" )), ( 1 , ( 135 , \"OK\" )), ( 2 , ( 53 , \"OK\" )), ( 1 , ( 45 , \"OK\" ))]) print ( trans . join ( products ) . take ( 20 )) products = { 1 : \"Apple\" , 2 : \"Orange\" , 3 : \"TV\" , 5 : \"Computer\" } trans = sc . parallelize ([( 1 , ( 134 , \"OK\" )), ( 3 , ( 34 , \"OK\" )), ( 5 , ( 162 , \"Error\" )), ( 1 , ( 135 , \"OK\" )), ( 2 , ( 53 , \"OK\" )), ( 1 , ( 45 , \"OK\" ))]) broadcasted_products = sc . broadcast ( products ) results = trans . map ( lambda x : ( x [ 0 ], broadcasted_products . value [ x [ 0 ]], x [ 1 ])) # results = trans.map(lambda x: (x[0], products[x[0]], x[1])) print ( results . take ( 20 ))","title":"Rdd"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#how-do-i-make-an-rdd","text":"RDDs can be created from stable storage or by transforming other RDDs. Run the cells below to create RDDs from files on the local drive. All data files can be downloaded from https://www.cse.ust.hk/msbd5003/data/ For example, https://www.cse.ust.hk/msbd5003/data/fruits.txt # Read data from local file system: print ( sc . version ) fruits = sc . textFile ( '../data/fruits.txt' ) yellowThings = sc . textFile ( '../data/yellowthings.txt' ) print ( fruits . collect ()) print ( yellowThings . collect ()) 3.0.0 ['apple', 'banana', 'canary melon', 'grape', 'lemon', 'orange', 'pineapple', 'strawberry'] ['banana', 'bee', 'butter', 'canary melon', 'gold', 'lemon', 'pineapple', 'sunflower'] # Read data from HDFS : fruits = sc . textFile ( 'hdfs://url:9000/pathname/fruits.txt' ) fruits . collect ()","title":"How do I make an RDD?"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#rdd-operations","text":"# map fruitsReversed = fruits . map ( lambda fruit : fruit [:: - 1 ]) # fruitsReversed = fruits.map(lambda fruit: fruit[::-1]) fruitsReversed . persist () # try changing the file and re-execute with and without cache print ( fruitsReversed . collect ()) # What happens when you uncomment the first line and run the whole program again with cache()? ['elppab', 'ananab', 'nolem yranac', 'parg', 'nomel', 'egnaro', 'elppaenip', 'yrrebwarts'] # filter shortFruits = fruits . filter ( lambda fruit : len ( fruit ) <= 5 ) print ( shortFruits . collect ()) ['grap', 'lemon'] # flatMap characters = fruits . flatMap ( lambda fruit : list ( fruit )) print ( characters . collect ()) ['b', 'a', 'p', 'p', 'l', 'e', 'b', 'a', 'n', 'a', 'n', 'a', 'c', 'a', 'n', 'a', 'r', 'y', ' ', 'm', 'e', 'l', 'o', 'n', 'g', 'r', 'a', 'p', 'l', 'e', 'm', 'o', 'n', 'o', 'r', 'a', 'n', 'g', 'e', 'p', 'i', 'n', 'e', 'a', 'p', 'p', 'l', 'e', 's', 't', 'r', 'a', 'w', 'b', 'e', 'r', 'r', 'y'] # union fruitsAndYellowThings = fruits . union ( yellowThings ) print ( fruitsAndYellowThings . collect ()) ['bapple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry', 'banana', 'bee', 'butter', 'canary melon', 'gold', 'lemon', 'pineapple', 'sunflower'] # intersection yellowFruits = fruits . intersection ( yellowThings ) print ( yellowFruits . collect ()) ['pineapple', 'canary melon', 'lemon', 'banana'] # distinct distinctFruitsAndYellowThings = fruitsAndYellowThings . distinct () print ( distinctFruitsAndYellowThings . collect ()) ['orange', 'pineapple', 'canary melon', 'lemon', 'bee', 'banana', 'butter', 'gold', 'sunflower', 'apple', 'grap', 'strawberry']","title":"RDD operations"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#rdd-actions","text":"Following are examples of some of the common actions available. For a detailed list, see RDD Actions . Run some transformations below to understand this better. Place the cursor in the cell and press SHIFT + ENTER . # collect fruitsArray = fruits . collect () yellowThingsArray = yellowThings . collect () print ( fruitsArray ) ['apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry'] # count numFruits = fruits . count () print ( numFruits ) 8 # take first3Fruits = fruits . take ( 3 ) print ( first3Fruits ) ['apple', 'banana', 'canary melon'] # reduce letterSet = fruits . map ( lambda fruit : set ( fruit )) . reduce ( lambda x , y : x . union ( y )) print ( letterSet ) {'o', 'r', 'a', 'i', 'p', 'g', 'c', ' ', 'l', 'y', 'e', 'w', 'n', 'b', 'm', 't', 's'} letterSet = fruits . flatMap ( lambda fruit : list ( fruit )) . distinct () . collect () print ( letterSet ) ['p', 'l', 'b', 'c', 'r', 'y', 'g', 'i', 's', 'a', 'e', 'n', ' ', 'm', 'o', 't', 'w']","title":"RDD actions"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#closure","text":"counter = 0 rdd = sc . parallelize ( range ( 10 )) # Wrong: Don't do this!! def increment_counter ( x ): global counter counter += x print ( rdd . collect ()) rdd . foreach ( increment_counter ) print ( counter ) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 0 rdd = sc . parallelize ( range ( 10 )) accum = sc . accumulator ( 0 ) def g ( x ): global accum accum += x a = rdd . foreach ( g ) print ( accum . value ) -45 rdd = sc . parallelize ( range ( 10 )) accum = sc . accumulator ( 0 ) def g ( x ): global accum accum += x return x * x a = rdd . map ( g ) print ( accum . value ) #print(a.reduce(lambda x, y: x+y)) a . cache () tmp = a . count () print ( accum . value ) print ( rdd . reduce ( lambda x , y : x + y )) tmp = a . count () print ( accum . value ) print ( rdd . reduce ( lambda x , y : x + y )) 0 45 45 45 45","title":"Closure"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#computing-pi-using-monte-carlo-simulation","text":"# From the official spark examples. import random import time partitions = 1000 n = 1000 * partitions def f ( _ ): x = random . random () y = random . random () return 1 if x ** 2 + y ** 2 < 1 else 0 count = sc . parallelize ( range ( 1 , n + 1 ), partitions ) \\ . map ( f ) . sum () print ( \"Pi is roughly\" , 4.0 * count / n ) Pi is roughly 3.140944 # Example: glom import sys import random def f ( _ ): random . seed ( time . time ()) return random . random () a = sc . parallelize ( range ( 0 , 100 ), 10 ) print ( a . collect ()) print ( a . glom () . collect ()) print ( a . map ( f ) . glom () . collect ()) # Weird behavior: Initially, random numbers are synched across all workers, but will get # out-of-sync after a large (e.g, 1000000) number of random numbers have been generated. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [[0.6608713426170987, 0.698767024318554, 0.1874105777790005, 0.7623702078433652, 0.8851287594440702, 0.31740294580255735, 0.19323310102732394, 0.42450071105921683, 0.5933781451859748, 0.7458943680790939], [0.7261502175930336, 0.22659503054053598, 0.9192074261481535, 0.4774662604141523, 0.7974422880272903, 0.2584976474338707, 0.6055611352765481, 0.5244790798752513, 0.6861813792912159, 0.5652815222674437], [0.27860057141024697, 0.27383515025078553, 0.9176819782462265, 0.417689753313761, 0.6135860183360143, 0.8162090147099693, 0.39224804876974406, 0.543173888187219, 0.3098912544023783, 0.633182881742779], [0.0952563896474653, 0.7477071810186972, 0.5004564582092008, 0.2614834043253954, 0.5982982446751687, 0.8544002333592715, 0.26000819037953216, 0.40177311792144454, 0.03851083747397188, 0.05167636277510712], [0.9726302497724043, 0.42432064255976365, 0.9305610323744404, 0.771694551386715, 0.6789841281422876, 0.9487832709253969, 0.4943030306526911, 0.22888583384514705, 0.6165263440265218, 0.8948635092093183], [0.9816006872849989, 0.3233518004555158, 0.6660672115030636, 0.9921564654020117, 0.9574487554669273, 0.00033642413291157247, 0.5729463981674527, 0.63676146970985, 0.1068707761119706, 0.4974835849045728], [0.6877782810075579, 0.11000878013616322, 0.6630366287015564, 0.0320757478156235, 0.5550374523078817, 0.11429763248899893, 0.7746616174182379, 0.6935564378314162, 0.6081187039755812, 0.3594774747771995], [0.3402744125431225, 0.8533066685831103, 0.18605963113570156, 0.9700428171414653, 0.9046533776474858, 0.4199976147427207, 0.01833313615444565, 0.5003118405702941, 0.9167261953361863, 0.6543553598701435], [0.5463089308369264, 0.19187434980340723, 0.5311179490604816, 0.7210872364087648, 0.25848050944241396, 0.9138829006068386, 0.5015098582184656, 0.9245322749204768, 0.4746635193819774, 0.733561516539988], [0.5924804586325896, 0.44157691425623313, 0.06474182310396659, 0.3705313104712945, 0.218280453275444, 0.911250263493956, 0.4908690024649712, 0.031427016100674665, 0.3749922950484815, 0.29534800562581187]] # Example: mapPartition and mapPartitionWithIndex a = sc . parallelize ( range ( 0 , 20 ), 4 ) print ( a . glom () . collect ()) def f ( it ): s = 0 for i in it : s += i yield s print ( a . mapPartitions ( f ) . collect ()) def f ( index , it ): s = index for i in it : s += i yield s print ( a . mapPartitionsWithIndex ( f ) . collect ()) [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]] [0, 1, 3, 6, 10, 5, 11, 18, 26, 35, 10, 21, 33, 46, 60, 15, 31, 48, 66, 85] [0, 1, 3, 6, 10, 6, 12, 19, 27, 36, 12, 23, 35, 48, 62, 18, 34, 51, 69, 88] # Correct version import random import time partitions = 1000 n = 1000 * partitions seed = time . time () def f ( index , it ): random . seed ( index + seed ) for i in it : x = random . random () y = random . random () yield 1 if x ** 2 + y ** 2 < 1 else 0 count = sc . parallelize ( range ( 1 , n + 1 ), partitions ) \\ . mapPartitionsWithIndex ( f ) . sum () print ( \"Pi is roughly\" , 4.0 * count / n ) Pi is roughly 3.141832","title":"Computing Pi using Monte Carlo simulation"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#closure-and-persistence","text":"# RDD variables are references A = sc . parallelize ( range ( 10 )) B = A . map ( lambda x : x * 2 ) A = B . map ( lambda x : x + 1 ) A . take ( 10 ) [1, 3, 5, 7, 9, 11, 13, 15, 17, 19] # Linear-time selection data = [ 34 , 67 , 21 , 56 , 47 , 89 , 12 , 44 , 74 , 43 , 26 ] A = sc . parallelize ( data , 2 ) k = 4 while True : x = A . first () A1 = A . filter ( lambda z : z < x ) A2 = A . filter ( lambda z : z > x ) A1 . cache () A2 . cache () mid = A1 . count () if mid == k : print ( x ) break if k < mid : A = A1 else : A = A2 k = k - mid - 1 43 sorted ( data ) [12, 21, 26, 34, 43, 44, 47, 56, 67, 74, 89] A = sc . parallelize ( range ( 10 )) x = 5 B = A . filter ( lambda z : z < x ) # B.cache() print ( B . count ()) x = 3 print ( B . count ()) 5 5 A = sc . parallelize ( range ( 10 )) x = 5 B = A . filter ( lambda z : z < x ) # B.cache() B . unpersist () # print(B.take(10)) print ( B . collect ()) x = 3 #print(B.take(10)) print ( B . collect ()) # collect() doesn't always re-collect data - bad design! # Always use take() instead of collect() [0, 1, 2, 3, 4] [0, 1, 2, 3, 4]","title":"Closure and Persistence"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#key-value-pairs","text":"# reduceByKey numFruitsByLength = fruits . map ( lambda fruit : ( len ( fruit ), 1 )) . reduceByKey ( lambda x , y : x + y ) print ( numFruitsByLength . take ( 10 )) from operator import add lines = sc . textFile ( '../data/course.txt' ) counts = lines . flatMap ( lambda x : x . split ()) \\ . map ( lambda x : ( x , 1 )) \\ . reduceByKey ( add ) print ( counts . sortByKey () . take ( 20 )) print ( counts . sortBy ( lambda x : x [ 1 ], False ) . take ( 20 )) # Join simple example products = sc . parallelize ([( 1 , \"Apple\" ), ( 2 , \"Orange\" ), ( 3 , \"TV\" ), ( 5 , \"Computer\" )]) #trans = sc.parallelize([(1, 134, \"OK\"), (3, 34, \"OK\"), (5, 162, \"Error\"), (1, 135, \"OK\"), (2, 53, \"OK\"), (1, 45, \"OK\")]) trans = sc . parallelize ([( 1 , ( 134 , \"OK\" )), ( 3 , ( 34 , \"OK\" )), ( 5 , ( 162 , \"Error\" )), ( 1 , ( 135 , \"OK\" )), ( 2 , ( 53 , \"OK\" )), ( 1 , ( 45 , \"OK\" ))]) print ( products . join ( trans ) . take ( 20 ))","title":"Key-Value Pairs"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#k-means-clustering","text":"import numpy as np def parseVector ( line ): return np . array ([ float ( x ) for x in line . split ()]) def closestPoint ( p , centers ): bestIndex = 0 closest = float ( \"+inf\" ) for i in range ( len ( centers )): tempDist = np . sum (( p - centers [ i ]) ** 2 ) if tempDist < closest : closest = tempDist bestIndex = i return bestIndex # The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_data.txt lines = sc . textFile ( '../data/kmeans_data.txt' , 5 ) # The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_bigdata.txt # lines = sc.textFile('../data/kmeans_bigdata.txt', 5) # lines is an RDD of strings K = 3 convergeDist = 0.01 # terminate algorithm when the total distance from old center to new centers is less than this value data = lines . map ( parseVector ) . cache () # data is an RDD of arrays kCenters = data . takeSample ( False , K , 1 ) # intial centers as a list of arrays tempDist = 1.0 # total distance from old centers to new centers while tempDist > convergeDist : closest = data . map ( lambda p : ( closestPoint ( p , kCenters ), ( p , 1 ))) # for each point in data, find its closest center # closest is an RDD of tuples (index of closest center, (point, 1)) pointStats = closest . reduceByKey ( lambda p1 , p2 : ( p1 [ 0 ] + p2 [ 0 ], p1 [ 1 ] + p2 [ 1 ])) # pointStats is an RDD of tuples (index of center, # (array of sums of coordinates, total number of points assigned)) newCenters = pointStats . map ( lambda st : ( st [ 0 ], st [ 1 ][ 0 ] / st [ 1 ][ 1 ])) . collect () # compute the new centers tempDist = sum ( np . sum (( kCenters [ i ] - p ) ** 2 ) for ( i , p ) in newCenters ) # compute the total disctance from old centers to new centers for ( i , p ) in newCenters : kCenters [ i ] = p print ( \"Final centers: \" , kCenters )","title":"K-means clustering"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#pagerank","text":"import re from operator import add def computeContribs ( urls , rank ): # Calculates URL contributions to the rank of other URLs. num_urls = len ( urls ) for url in urls : yield ( url , rank / num_urls ) def parseNeighbors ( urls ): # Parses a urls pair string into urls pair.\"\"\" parts = urls . split ( ' ' ) return parts [ 0 ], parts [ 1 ] # Loads in input file. It should be in format of: # URL neighbor URL # URL neighbor URL # URL neighbor URL # ... # The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/* lines = sc . textFile ( \"../data/pagerank_data.txt\" , 2 ) # lines = sc.textFile(\"../data/dblp.in\", 5) numOfIterations = 10 # Loads all URLs from input file and initialize their neighbors. links = lines . map ( lambda urls : parseNeighbors ( urls )) \\ . groupByKey () # Loads all URLs with other URL(s) link to from input file # and initialize ranks of them to one. ranks = links . mapValues ( lambda neighbors : 1.0 ) # Calculates and updates URL ranks continuously using PageRank algorithm. for iteration in range ( numOfIterations ): # Calculates URL contributions to the rank of other URLs. contribs = links . join ( ranks ) \\ . flatMap ( lambda url_urls_rank : computeContribs ( url_urls_rank [ 1 ][ 0 ], url_urls_rank [ 1 ][ 1 ])) # After the join, each element in the RDD is of the form # (url, (list of neighbor urls, rank)) # Re-calculates URL ranks based on neighbor contributions. # ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15) ranks = contribs . reduceByKey ( add ) . map ( lambda t : ( t [ 0 ], t [ 1 ] * 0.85 + 0.15 )) print ( ranks . top ( 5 , lambda x : x [ 1 ]))","title":"PageRank"},{"location":"MSBD5003/notebooks%20in%20class/rdd/#join-vs-broadcast-variables","text":"products = sc . parallelize ([( 1 , \"Apple\" ), ( 2 , \"Orange\" ), ( 3 , \"TV\" ), ( 5 , \"Computer\" )]) trans = sc . parallelize ([( 1 , ( 134 , \"OK\" )), ( 3 , ( 34 , \"OK\" )), ( 5 , ( 162 , \"Error\" )), ( 1 , ( 135 , \"OK\" )), ( 2 , ( 53 , \"OK\" )), ( 1 , ( 45 , \"OK\" ))]) print ( trans . join ( products ) . take ( 20 )) products = { 1 : \"Apple\" , 2 : \"Orange\" , 3 : \"TV\" , 5 : \"Computer\" } trans = sc . parallelize ([( 1 , ( 134 , \"OK\" )), ( 3 , ( 34 , \"OK\" )), ( 5 , ( 162 , \"Error\" )), ( 1 , ( 135 , \"OK\" )), ( 2 , ( 53 , \"OK\" )), ( 1 , ( 45 , \"OK\" ))]) broadcasted_products = sc . broadcast ( products ) results = trans . map ( lambda x : ( x [ 0 ], broadcasted_products . value [ x [ 0 ]], x [ 1 ])) # results = trans.map(lambda x: (x[0], products[x[0]], x[1])) print ( results . take ( 20 ))","title":"Join vs. Broadcast Variables"},{"location":"MSBD5003/notebooks%20in%20class/sparksql/","text":"! pip install pyspark Collecting pyspark \u001b[?25l Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204.2MB 56kB/s \u001b[?25hCollecting py4j==0.10.9 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 39.0MB/s \u001b[?25hBuilding wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=2f5ff611b6f601d04626ac58c802aeb82b7faa0f99da467b2288b8fc05a7d419 Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9 pyspark-3.0.1 from pyspark.context import SparkContext from pyspark.sql.session import SparkSession sc = SparkContext . getOrCreate () spark = SparkSession ( sc ) Dataframe operations \u00b6 from pyspark.sql import Row row = Row ( name = \"Alice\" , age = 11 ) print ( row ) print ( row [ 'name' ], row [ 'age' ]) print ( row . name , row . age ) row = Row ( name = \"Alice\" , age = 11 , count = 1 ) print ( row . count ) print ( row [ 'count' ]) Row(name='Alice', age=11) Alice 11 Alice 11 <built-in method count of Row object at 0x7f3384ce6e08> 1 ! wget https : // www . cse . ust . hk / msbd5003 / data / building . csv --2020-10-06 11:49:18-- https://www.cse.ust.hk/msbd5003/data/building.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 544 [text/plain] Saving to: \u2018building.csv.1\u2019 building.csv.1 100%[===================>] 544 --.-KB/s in 0s 2020-10-06 11:49:20 (30.9 MB/s) - \u2018building.csv.1\u2019 saved [544/544] # Data file at https://www.cse.ust.hk/msbd5003/data/building.csv df = spark . read . csv ( 'building.csv' , header = True , inferSchema = True ) # show the content of the dataframe df . show () +----------+-----------+-----------+-----------+------------+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country| +----------+-----------+-----------+-----------+------------+ | 1| M1| 25| AC1000| USA| | 2| M2| 27| FN39TG| France| | 3| M3| 28| JDNS77| Brazil| | 4| M4| 17| GG1919| Finland| | 5| M5| 3| ACMAX22| Hong Kong| | 6| M6| 9| AC1000| Singapore| | 7| M7| 13| FN39TG|South Africa| | 8| M8| 25| JDNS77| Australia| | 9| M9| 11| GG1919| Mexico| | 10| M10| 23| ACMAX22| China| | 11| M11| 14| AC1000| Belgium| | 12| M12| 26| FN39TG| Finland| | 13| M13| 25| JDNS77|Saudi Arabia| | 14| M14| 17| GG1919| Germany| | 15| M15| 19| ACMAX22| Israel| | 16| M16| 23| AC1000| Turkey| | 17| M17| 11| FN39TG| Egypt| | 18| M18| 25| JDNS77| Indonesia| | 19| M19| 14| GG1919| Canada| | 20| M20| 19| ACMAX22| Argentina| +----------+-----------+-----------+-----------+------------+ # Print the dataframe schema in a tree format df . printSchema () root |-- BuildingID: integer (nullable = true) |-- BuildingMgr: string (nullable = true) |-- BuildingAge: integer (nullable = true) |-- HVACproduct: string (nullable = true) |-- Country: string (nullable = true) # Create an RDD from the dataframe dfrdd = df . rdd dfrdd . take ( 3 ) [Row(BuildingID=1, BuildingMgr='M1', BuildingAge=25, HVACproduct='AC1000', Country='USA'), Row(BuildingID=2, BuildingMgr='M2', BuildingAge=27, HVACproduct='FN39TG', Country='France'), Row(BuildingID=3, BuildingMgr='M3', BuildingAge=28, HVACproduct='JDNS77', Country='Brazil')] # Retrieve specific columns from the dataframe df . select ( 'BuildingID' , 'Country' ) . show () +----------+------------+ |BuildingID| Country| +----------+------------+ | 1| USA| | 2| France| | 3| Brazil| | 4| Finland| | 5| Hong Kong| | 6| Singapore| | 7|South Africa| | 8| Australia| | 9| Mexico| | 10| China| | 11| Belgium| | 12| Finland| | 13|Saudi Arabia| | 14| Germany| | 15| Israel| | 16| Turkey| | 17| Egypt| | 18| Indonesia| | 19| Canada| | 20| Argentina| +----------+------------+ from pyspark.sql.functions import * df . where ( \"Country<'USA'\" ) . select ( 'BuildingID' , lit ( 'OK' )) . show () +----------+---+ |BuildingID| OK| +----------+---+ | 2| OK| | 3| OK| | 4| OK| | 5| OK| | 6| OK| | 7| OK| | 8| OK| | 9| OK| | 10| OK| | 11| OK| | 12| OK| | 13| OK| | 14| OK| | 15| OK| | 16| OK| | 17| OK| | 18| OK| | 19| OK| | 20| OK| +----------+---+ # Use GroupBy clause with dataframe df . groupBy ( 'HVACProduct' ) . count () . show () +-----------+-----+ |HVACProduct|count| +-----------+-----+ | ACMAX22| 4| | AC1000| 4| | JDNS77| 4| | FN39TG| 4| | GG1919| 4| +-----------+-----+ ! wget https : // www . cse . ust . hk / msbd5003 / data / Customer . csv ! wget https : // www . cse . ust . hk / msbd5003 / data / Product . csv ! wget https : // www . cse . ust . hk / msbd5003 / data / SalesOrderDetail . csv ! wget https : // www . cse . ust . hk / msbd5003 / data / SalesOrderHeader . csv --2020-10-06 11:51:08-- https://www.cse.ust.hk/msbd5003/data/Customer.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 199491 (195K) [text/plain] Saving to: \u2018Customer.csv\u2019 Customer.csv 100%[===================>] 194.82K 320KB/s in 0.6s 2020-10-06 11:51:11 (320 KB/s) - \u2018Customer.csv\u2019 saved [199491/199491] --2020-10-06 11:51:12-- https://www.cse.ust.hk/msbd5003/data/Product.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1355634 (1.3M) [text/plain] Saving to: \u2018Product.csv\u2019 Product.csv 100%[===================>] 1.29M 1.05MB/s in 1.2s 2020-10-06 11:51:15 (1.05 MB/s) - \u2018Product.csv\u2019 saved [1355634/1355634] --2020-10-06 11:51:15-- https://www.cse.ust.hk/msbd5003/data/SalesOrderDetail.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 56766 (55K) [text/plain] Saving to: \u2018SalesOrderDetail.csv\u2019 SalesOrderDetail.cs 100%[===================>] 55.44K 137KB/s in 0.4s 2020-10-06 11:51:18 (137 KB/s) - \u2018SalesOrderDetail.csv\u2019 saved [56766/56766] --2020-10-06 11:51:18-- https://www.cse.ust.hk/msbd5003/data/SalesOrderHeader.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 8680 (8.5K) [text/plain] Saving to: \u2018SalesOrderHeader.csv\u2019 SalesOrderHeader.cs 100%[===================>] 8.48K --.-KB/s in 0s 2020-10-06 11:51:19 (79.4 MB/s) - \u2018SalesOrderHeader.csv\u2019 saved [8680/8680] Rewriting SQL with DataFrame API \u00b6 # Load data from csv files # Data files at https://www.cse.ust.hk/msbd5003/data dfCustomer = spark . read . csv ( 'Customer.csv' , header = True , inferSchema = True ) dfProduct = spark . read . csv ( 'Product.csv' , header = True , inferSchema = True ) dfDetail = spark . read . csv ( 'SalesOrderDetail.csv' , header = True , inferSchema = True ) dfHeader = spark . read . csv ( 'SalesOrderHeader.csv' , header = True , inferSchema = True ) # SELECT ProductID, Name, ListPrice # FROM Product # WHERE Color = 'black' dfProduct . filter ( \"Color = 'Black'\" ) \\ . select ( 'ProductID' , 'Name' , 'ListPrice' ) \\ . show ( truncate = False ) +---------+-----------------------------+---------+ |ProductID|Name |ListPrice| +---------+-----------------------------+---------+ |680 |HL Road Frame - Black, 58 |1431.5 | |708 |Sport-100 Helmet, Black |34.99 | |722 |LL Road Frame - Black, 58 |337.22 | |723 |LL Road Frame - Black, 60 |337.22 | |724 |LL Road Frame - Black, 62 |337.22 | |736 |LL Road Frame - Black, 44 |337.22 | |737 |LL Road Frame - Black, 48 |337.22 | |738 |LL Road Frame - Black, 52 |337.22 | |743 |HL Mountain Frame - Black, 42|1349.6 | |744 |HL Mountain Frame - Black, 44|1349.6 | |745 |HL Mountain Frame - Black, 48|1349.6 | |746 |HL Mountain Frame - Black, 46|1349.6 | |747 |HL Mountain Frame - Black, 38|1349.6 | |765 |Road-650 Black, 58 |782.99 | |766 |Road-650 Black, 60 |782.99 | |767 |Road-650 Black, 62 |782.99 | |768 |Road-650 Black, 44 |782.99 | |769 |Road-650 Black, 48 |782.99 | |770 |Road-650 Black, 52 |782.99 | |775 |Mountain-100 Black, 38 |3374.99 | +---------+-----------------------------+---------+ only showing top 20 rows dfProduct . where ( dfProduct . Color == 'Black' ) \\ . select ( dfProduct . ProductID , dfProduct [ 'Name' ], ( dfProduct [ 'ListPrice' ] * 2 ) . alias ( 'Double price' )) \\ . show ( truncate = False ) +---------+-----------------------------+------------+ |ProductID|Name |Double price| +---------+-----------------------------+------------+ |680 |HL Road Frame - Black, 58 |2863.0 | |708 |Sport-100 Helmet, Black |69.98 | |722 |LL Road Frame - Black, 58 |674.44 | |723 |LL Road Frame - Black, 60 |674.44 | |724 |LL Road Frame - Black, 62 |674.44 | |736 |LL Road Frame - Black, 44 |674.44 | |737 |LL Road Frame - Black, 48 |674.44 | |738 |LL Road Frame - Black, 52 |674.44 | |743 |HL Mountain Frame - Black, 42|2699.2 | |744 |HL Mountain Frame - Black, 44|2699.2 | |745 |HL Mountain Frame - Black, 48|2699.2 | |746 |HL Mountain Frame - Black, 46|2699.2 | |747 |HL Mountain Frame - Black, 38|2699.2 | |765 |Road-650 Black, 58 |1565.98 | |766 |Road-650 Black, 60 |1565.98 | |767 |Road-650 Black, 62 |1565.98 | |768 |Road-650 Black, 44 |1565.98 | |769 |Road-650 Black, 48 |1565.98 | |770 |Road-650 Black, 52 |1565.98 | |775 |Mountain-100 Black, 38 |6749.98 | +---------+-----------------------------+------------+ only showing top 20 rows dfProduct . where ( dfProduct . ListPrice * 2 > 100 ) \\ . select ( dfProduct . ProductID , dfProduct [ 'Name' ], dfProduct . ListPrice * 2 ) \\ . show ( truncate = False ) +---------+-------------------------+---------------+ |ProductID|Name |(ListPrice * 2)| +---------+-------------------------+---------------+ |680 |HL Road Frame - Black, 58|2863.0 | |706 |HL Road Frame - Red, 58 |2863.0 | |717 |HL Road Frame - Red, 62 |2863.0 | |718 |HL Road Frame - Red, 44 |2863.0 | |719 |HL Road Frame - Red, 48 |2863.0 | |720 |HL Road Frame - Red, 52 |2863.0 | |721 |HL Road Frame - Red, 56 |2863.0 | |722 |LL Road Frame - Black, 58|674.44 | |723 |LL Road Frame - Black, 60|674.44 | |724 |LL Road Frame - Black, 62|674.44 | |725 |LL Road Frame - Red, 44 |674.44 | |726 |LL Road Frame - Red, 48 |674.44 | |727 |LL Road Frame - Red, 52 |674.44 | |728 |LL Road Frame - Red, 58 |674.44 | |729 |LL Road Frame - Red, 60 |674.44 | |730 |LL Road Frame - Red, 62 |674.44 | |731 |ML Road Frame - Red, 44 |1189.66 | |732 |ML Road Frame - Red, 48 |1189.66 | |733 |ML Road Frame - Red, 52 |1189.66 | |734 |ML Road Frame - Red, 58 |1189.66 | +---------+-------------------------+---------------+ only showing top 20 rows # SELECT ProductID, Name, ListPrice # FROM Product # WHERE Color = 'black' # ORDER BY ProductID dfProduct . filter ( \"Color = 'Black'\" ) \\ . select ( 'ProductID' , 'Name' , 'ListPrice' ) \\ . orderBy ( 'ListPrice' ) \\ . show ( truncate = False ) +---------+--------------------------+---------+ |ProductID|Name |ListPrice| +---------+--------------------------+---------+ |860 |Half-Finger Gloves, L |24.49 | |859 |Half-Finger Gloves, M |24.49 | |858 |Half-Finger Gloves, S |24.49 | |708 |Sport-100 Helmet, Black |34.99 | |862 |Full-Finger Gloves, M |37.99 | |861 |Full-Finger Gloves, S |37.99 | |863 |Full-Finger Gloves, L |37.99 | |841 |Men's Sports Shorts, S |59.99 | |849 |Men's Sports Shorts, M |59.99 | |851 |Men's Sports Shorts, XL |59.99 | |850 |Men's Sports Shorts, L |59.99 | |815 |LL Mountain Front Wheel |60.745 | |868 |Women's Mountain Shorts, M|69.99 | |869 |Women's Mountain Shorts, L|69.99 | |867 |Women's Mountain Shorts, S|69.99 | |853 |Women's Tights, M |74.99 | |854 |Women's Tights, L |74.99 | |852 |Women's Tights, S |74.99 | |818 |LL Road Front Wheel |85.565 | |823 |LL Mountain Rear Wheel |87.745 | +---------+--------------------------+---------+ only showing top 20 rows # Find all orders and details on black product, # return the product SalesOrderID, SalesOrderDetailID, Name, UnitPrice, and OrderQty # SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty # FROM SalesLT.SalesOrderDetail, SalesLT.Product # WHERE SalesOrderDetail.ProductID = Product.ProductID AND Color = 'Black' # SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty # FROM SalesLT.SalesOrderDetail # JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID # WHERE Color = 'Black' # Spark SQL supports natural joins dfDetail . join ( dfProduct , 'ProductID' ) \\ . select ( 'SalesOrderID' , 'SalesOrderDetailID' , 'Name' , 'UnitPrice' , 'OrderQty' ) \\ . filter ( \"Color='Black'\" ) \\ . show () # If we move the filter to after select, it still works. Why? +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71797| 111045|LL Road Frame - B...| 202.332| 3| | 71783| 110730|LL Road Frame - B...| 202.332| 6| | 71938| 113297|LL Road Frame - B...| 202.332| 3| | 71915| 113090|LL Road Frame - B...| 202.332| 2| | 71815| 111451|LL Road Frame - B...| 202.332| 1| | 71797| 111044|LL Road Frame - B...| 202.332| 1| | 71783| 110710|LL Road Frame - B...| 202.332| 4| | 71936| 113260|HL Mountain Frame...| 809.76| 4| | 71899| 112937|HL Mountain Frame...| 809.76| 1| | 71845| 112137|HL Mountain Frame...| 809.76| 2| | 71832| 111806|HL Mountain Frame...| 809.76| 4| | 71780| 110622|HL Mountain Frame...| 809.76| 1| | 71936| 113235|HL Mountain Frame...| 809.76| 4| | 71845| 112134|HL Mountain Frame...| 809.76| 3| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows # This also works: d1 = dfDetail . join ( dfProduct , 'ProductID' ) \\ . select ( 'SalesOrderID' , 'SalesOrderDetailID' , 'Name' , 'UnitPrice' , 'OrderQty' ) d1 . show () d2 = d1 . filter ( \"Color = 'Black'\" ) d2 . show () d2 . explain () +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113278|Sport-100 Helmet,...| 20.994| 3| | 71936| 113228|Sport-100 Helmet,...| 20.994| 1| | 71902| 112980|Sport-100 Helmet,...| 20.994| 2| | 71797| 111075|Sport-100 Helmet,...| 20.994| 6| | 71784| 110794|Sport-100 Helmet,...| 20.994| 10| | 71783| 110751|Sport-100 Helmet,...| 20.994| 10| | 71782| 110709|Sport-100 Helmet,...| 20.994| 3| | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71938| 113282|Sport-100 Helmet,...| 20.994| 3| | 71902| 112995|Sport-100 Helmet,...| 20.994| 7| | 71863| 112395|Sport-100 Helmet,...| 20.994| 1| | 71797| 111038|Sport-100 Helmet,...| 20.994| 4| | 71784| 110753|Sport-100 Helmet,...| 20.994| 2| | 71783| 110749|Sport-100 Helmet,...| 19.2445| 15| | 71782| 110708|Sport-100 Helmet,...| 20.994| 6| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71797| 111045|LL Road Frame - B...| 202.332| 3| | 71783| 110730|LL Road Frame - B...| 202.332| 6| | 71938| 113297|LL Road Frame - B...| 202.332| 3| | 71915| 113090|LL Road Frame - B...| 202.332| 2| | 71815| 111451|LL Road Frame - B...| 202.332| 1| | 71797| 111044|LL Road Frame - B...| 202.332| 1| | 71783| 110710|LL Road Frame - B...| 202.332| 4| | 71936| 113260|HL Mountain Frame...| 809.76| 4| | 71899| 112937|HL Mountain Frame...| 809.76| 1| | 71845| 112137|HL Mountain Frame...| 809.76| 2| | 71832| 111806|HL Mountain Frame...| 809.76| 4| | 71780| 110622|HL Mountain Frame...| 809.76| 1| | 71936| 113235|HL Mountain Frame...| 809.76| 4| | 71845| 112134|HL Mountain Frame...| 809.76| 3| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows == Physical Plan == *(2) Project [SalesOrderID#217, SalesOrderDetailID#218, Name#168, UnitPrice#221, OrderQty#219] +- *(2) BroadcastHashJoin [ProductID#220], [ProductID#167], Inner, BuildLeft :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[3, int, true] as bigint))), [id=#380] : +- *(1) Project [SalesOrderID#217, SalesOrderDetailID#218, OrderQty#219, ProductID#220, UnitPrice#221] : +- *(1) Filter isnotnull(ProductID#220) : +- FileScan csv [SalesOrderID#217,SalesOrderDetailID#218,OrderQty#219,ProductID#220,UnitPrice#221] Batched: false, DataFilters: [isnotnull(ProductID#220)], Format: CSV, Location: InMemoryFileIndex[file:/content/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double> +- *(2) Project [ProductID#167, Name#168] +- *(2) Filter ((isnotnull(Color#170) AND (Color#170 = Black)) AND isnotnull(ProductID#167)) +- FileScan csv [ProductID#167,Name#168,Color#170] Batched: false, DataFilters: [isnotnull(Color#170), (Color#170 = Black), isnotnull(ProductID#167)], Format: CSV, Location: InMemoryFileIndex[file:/content/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string> # SparkSQL performs optimization depending on whether intermediate dataframe are cached or not: d1 = dfDetail . join ( dfProduct , 'ProductID' ) \\ . select ( 'SalesOrderID' , 'SalesOrderDetailID' , 'Name' , 'UnitPrice' , 'OrderQty' ) d1 . persist () d1 . show () +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113278|Sport-100 Helmet,...| 20.994| 3| | 71936| 113228|Sport-100 Helmet,...| 20.994| 1| | 71902| 112980|Sport-100 Helmet,...| 20.994| 2| | 71797| 111075|Sport-100 Helmet,...| 20.994| 6| | 71784| 110794|Sport-100 Helmet,...| 20.994| 10| | 71783| 110751|Sport-100 Helmet,...| 20.994| 10| | 71782| 110709|Sport-100 Helmet,...| 20.994| 3| | 71938| 113295|Sport-100 Helmet,...| 20.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71938| 113282|Sport-100 Helmet,...| 20.994| 3| | 71902| 112995|Sport-100 Helmet,...| 20.994| 7| | 71863| 112395|Sport-100 Helmet,...| 20.994| 1| | 71797| 111038|Sport-100 Helmet,...| 20.994| 4| | 71784| 110753|Sport-100 Helmet,...| 20.994| 2| | 71783| 110749|Sport-100 Helmet,...| 19.2445| 15| | 71782| 110708|Sport-100 Helmet,...| 20.994| 6| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows d2 = d1 . filter ( \"Color = 'Black'\" ) #d2 = d1.filter(\"OrderQty >= 10\") d2 . show () d2 . explain () +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71797| 111045|LL Road Frame - B...| 202.332| 3| | 71783| 110730|LL Road Frame - B...| 202.332| 6| | 71938| 113297|LL Road Frame - B...| 202.332| 3| | 71915| 113090|LL Road Frame - B...| 202.332| 2| | 71815| 111451|LL Road Frame - B...| 202.332| 1| | 71797| 111044|LL Road Frame - B...| 202.332| 1| | 71783| 110710|LL Road Frame - B...| 202.332| 4| | 71936| 113260|HL Mountain Frame...| 809.76| 4| | 71899| 112937|HL Mountain Frame...| 809.76| 1| | 71845| 112137|HL Mountain Frame...| 809.76| 2| | 71832| 111806|HL Mountain Frame...| 809.76| 4| | 71780| 110622|HL Mountain Frame...| 809.76| 1| | 71936| 113235|HL Mountain Frame...| 809.76| 4| | 71845| 112134|HL Mountain Frame...| 809.76| 3| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows == Physical Plan == *(2) Project [SalesOrderID#112, SalesOrderDetailID#113, Name#63, UnitPrice#116, OrderQty#114] +- *(2) BroadcastHashJoin [ProductID#115], [ProductID#62], Inner, BuildLeft :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[3, int, true] as bigint))), [id=#280] : +- *(1) Project [SalesOrderID#112, SalesOrderDetailID#113, OrderQty#114, ProductID#115, UnitPrice#116] : +- *(1) Filter isnotnull(ProductID#115) : +- FileScan csv [SalesOrderID#112,SalesOrderDetailID#113,OrderQty#114,ProductID#115,UnitPrice#116] Batched: false, DataFilters: [isnotnull(ProductID#115)], Format: CSV, Location: InMemoryFileIndex[file:/csproject/msbd5003/public_html/data/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double> +- *(2) Project [ProductID#62, Name#63] +- *(2) Filter ((isnotnull(Color#65) AND (Color#65 = Black)) AND isnotnull(ProductID#62)) +- FileScan csv [ProductID#62,Name#63,Color#65] Batched: false, DataFilters: [isnotnull(Color#65), (Color#65 = Black), isnotnull(ProductID#62)], Format: CSV, Location: InMemoryFileIndex[file:/csproject/msbd5003/public_html/data/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string> # This will report an error: d1 = dfDetail . join ( dfProduct , 'ProductID' ) \\ . select ( 'SalesOrderID' , 'SalesOrderDetailID' , 'Name' , 'UnitPrice' , 'OrderQty' ) d1 . write . csv ( 'temp.csv' , mode = 'overwrite' , header = True ) d2 = spark . read . csv ( 'temp.csv' , header = True , inferSchema = True ) d2 . filter ( \"Color = 'Black'\" ) . show () --------------------------------------------------------------------------- AnalysisException Traceback (most recent call last) <ipython-input-18-168595bb0376> in <module> 5 d1.write.csv('temp.csv', mode = 'overwrite', header = True) 6 d2 = spark.read.csv('temp.csv', header = True, inferSchema = True) ----> 7 d2.filter(\"Color = 'Black'\").show() /csproject/msbd5003/python/pyspark/sql/dataframe.py in filter(self, condition) 1457 \"\"\" 1458 if isinstance(condition, basestring): -> 1459 jdf = self._jdf.filter(condition) 1460 elif isinstance(condition, Column): 1461 jdf = self._jdf.filter(condition._jc) /csproject/msbd5003/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args) 1303 answer = self.gateway_client.send_command(command) 1304 return_value = get_return_value( -> 1305 answer, self.gateway_client, self.target_id, self.name) 1306 1307 for temp_arg in temp_args: /csproject/msbd5003/python/pyspark/sql/utils.py in deco(*a, **kw) 135 # Hide where the exception came from that shows a non-Pythonic 136 # JVM exception message. --> 137 raise_from(converted) 138 else: 139 raise /csproject/msbd5003/python/pyspark/sql/utils.py in raise_from(e) AnalysisException: cannot resolve '`Color`' given input columns: [Name, OrderQty, SalesOrderDetailID, SalesOrderID, UnitPrice]; line 1 pos 0; 'Filter ('Color = Black) +- Relation[SalesOrderID#580,SalesOrderDetailID#581,Name#582,UnitPrice#583,OrderQty#584] csv # Find all orders that include at least one black product, # return the product SalesOrderID, Name, UnitPrice, and OrderQty # SELECT DISTINCT SalesOrderID # FROM SalesLT.SalesOrderDetail # JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID # WHERE Color = 'Black' dfDetail . join ( dfProduct . filter ( \"Color='Black'\" ), 'ProductID' ) \\ . select ( 'SalesOrderID' ) \\ . distinct () \\ . show () +------------+ |SalesOrderID| +------------+ | 71902| | 71832| | 71915| | 71831| | 71898| | 71935| | 71938| | 71845| | 71783| | 71815| | 71936| | 71863| | 71780| | 71782| | 71899| | 71784| | 71797| +------------+ # How many colors in the products? # SELECT COUNT(DISTINCT Color) # FROM SalesLT.Product dfProduct . select ( 'Color' ) . distinct () . count () # It's 1 more than standard SQL. In standard SQL, COUNT() does not count NULLs. 10 # Find the total price of each order, # return SalesOrderID and total price (column name should be \u2018totalprice\u2019) # SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice # FROM SalesLT.SalesOrderDetail # GROUP BY SalesOrderID dfDetail . select ( '*' , ( dfDetail . UnitPrice * dfDetail . OrderQty * ( 1 - dfDetail . UnitPriceDiscount )) . alias ( 'netprice' )) \\ . groupBy ( 'SalesOrderID' ) . sum ( 'netprice' ) \\ . withColumnRenamed ( 'sum(netprice)' , 'TotalPrice' ) \\ . show () +------------+------------------+ |SalesOrderID| sum(netprice)| +------------+------------------+ | 71867| 858.9| | 71902|59894.209199999976| | 71832| 28950.678108| | 71915|1732.8899999999999| | 71946| 31.584| | 71895|221.25600000000003| | 71816|2847.4079999999994| | 71831| 1712.946| | 71923| 96.108824| | 71858|11528.844000000001| | 71917| 37.758| | 71897| 10585.05| | 71885| 524.664| | 71856|500.30400000000003| | 71898| 53248.69200000002| | 71774| 713.796| | 71796| 47848.02600000001| | 71935|5533.8689079999995| | 71938| 74160.228| | 71845| 34118.5356| +------------+------------------+ only showing top 20 rows # Find the total price of each order where the total price > 10000 # SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice # FROM SalesLT.SalesOrderDetail # GROUP BY SalesOrderID # HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000 dfDetail . select ( '*' , ( dfDetail . UnitPrice * dfDetail . OrderQty * ( 1 - dfDetail . UnitPriceDiscount )) . alias ( 'netprice' )) \\ . groupBy ( 'SalesOrderID' ) . sum ( 'netprice' ) \\ . withColumnRenamed ( 'sum(netprice)' , 'TotalPrice' ) \\ . where ( 'TotalPrice > 10000' ) \\ . show () +------------+------------------+ |SalesOrderID| TotalPrice| +------------+------------------+ | 71902|59894.209199999976| | 71832| 28950.678108| | 71858|11528.844000000001| | 71897| 10585.05| | 71898| 53248.69200000002| | 71796| 47848.02600000001| | 71938| 74160.228| | 71845| 34118.5356| | 71783| 65683.367986| | 71936| 79589.61602399996| | 71780|29923.007999999998| | 71782| 33319.98600000001| | 71784| 89869.27631400003| | 71797| 65123.46341800001| +------------+------------------+ # Find the total price on the black products of each order where the total price > 10000 # SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice # FROM SalesLT.SalesOrderDetail, SalesLT.Product # WHERE SalesLT.SalesOrderDetail.ProductID = SalesLT.Product.ProductID AND Color = 'Black' # GROUP BY SalesOrderID # HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000 dfDetail . select ( '*' , ( dfDetail . UnitPrice * dfDetail . OrderQty * ( 1 - dfDetail . UnitPriceDiscount )) . alias ( 'netprice' )) \\ . join ( dfProduct , 'ProductID' ) \\ . where ( \"Color = 'Black'\" ) \\ . groupBy ( 'SalesOrderID' ) . sum ( 'netprice' ) \\ . withColumnRenamed ( 'sum(netprice)' , 'TotalPrice' ) \\ . where ( 'TotalPrice > 10000' ) \\ . show () +------------+------------------+ |SalesOrderID| TotalPrice| +------------+------------------+ | 71902|26677.883999999995| | 71832| 16883.748108| | 71938| 33779.448| | 71845| 18109.836| | 71783|15524.117476000003| | 71936| 44490.290424| | 71780| 16964.322| | 71797| 27581.613792| +------------+------------------+ # For each customer, find the total quantity of black products bought. # Report CustomerID, FirstName, LastName, and total quantity # select saleslt.customer.customerid, FirstName, LastName, sum(orderqty) # from saleslt.customer # left outer join # ( # saleslt.salesorderheader # join saleslt.salesorderdetail # on saleslt.salesorderdetail.salesorderid = saleslt.salesorderheader.salesorderid # join saleslt.product # on saleslt.product.productid = saleslt.salesorderdetail.productid and color = 'black' # ) # on saleslt.customer.customerid = saleslt.salesorderheader.customerid # group by saleslt.customer.customerid, FirstName, LastName # order by sum(orderqty) desc d1 = dfDetail . join ( dfProduct , 'ProductID' ) \\ . where ( 'Color = \"Black\"' ) \\ . join ( dfHeader , 'SalesOrderID' ) \\ . groupBy ( 'CustomerID' ) . sum ( 'OrderQty' ) dfCustomer . join ( d1 , 'CustomerID' , 'left_outer' ) \\ . select ( 'CustomerID' , 'FirstName' , 'LastName' , 'sum(OrderQty)' ) \\ . orderBy ( 'sum(OrderQty)' , ascending = False ) \\ . show () +----------+------------+------------+-------------+ |CustomerID| FirstName| LastName|sum(OrderQty)| +----------+------------+------------+-------------+ | 30050| Krishna|Sunkammurali| 89| | 29796| Jon| Grande| 65| | 29957| Kevin| Liu| 62| | 29929| Jeffrey| Kurtz| 46| | 29546| Christopher| Beck| 45| | 29922| Pamala| Kotc| 34| | 30113| Raja| Venugopal| 34| | 29938| Frank| Campbell| 29| | 29736| Terry| Eminhizer| 23| | 29485| Catherine| Abel| 10| | 30019| Matthew| Miller| 9| | 29932| Rebecca| Laszlo| 7| | 29975| Walter| Mays| 5| | 29638| Rosmarie| Carroll| 2| | 29531| Cory| Booth| 1| | 30089|Michael John| Troyer| 1| | 29568| Donald| Blanton| 1| | 29868| Denean| Ison| null| | 29646| Stacey| Cereghino| null| | 29905| Elizabeth| Keyser| null| +----------+------------+------------+-------------+ only showing top 20 rows Embed SQL queries \u00b6 You can also run SQL queries over dataframes once you register them as temporary tables within the SparkSession. # Register the dataframe as a temporary view called HVAC df . createOrReplaceTempView ( 'HVAC' ) spark . sql ( 'SELECT * FROM HVAC WHERE BuildingAge >= 10' ) . show () +----------+-----------+-----------+-----------+------------+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country| +----------+-----------+-----------+-----------+------------+ | 1| M1| 25| AC1000| USA| | 2| M2| 27| FN39TG| France| | 3| M3| 28| JDNS77| Brazil| | 4| M4| 17| GG1919| Finland| | 7| M7| 13| FN39TG|South Africa| | 8| M8| 25| JDNS77| Australia| | 9| M9| 11| GG1919| Mexico| | 10| M10| 23| ACMAX22| China| | 11| M11| 14| AC1000| Belgium| | 12| M12| 26| FN39TG| Finland| | 13| M13| 25| JDNS77|Saudi Arabia| | 14| M14| 17| GG1919| Germany| | 15| M15| 19| ACMAX22| Israel| | 16| M16| 23| AC1000| Turkey| | 17| M17| 11| FN39TG| Egypt| | 18| M18| 25| JDNS77| Indonesia| | 19| M19| 14| GG1919| Canada| | 20| M20| 19| ACMAX22| Argentina| +----------+-----------+-----------+-----------+------------+ # Can even mix DataFrame API with SQL: df . where ( 'BuildingAge >= 10' ) . createOrReplaceTempView ( 'OldBuildings' ) spark . sql ( 'SELECT HVACproduct, COUNT(*) FROM OldBuildings GROUP BY HVACproduct' ) . show () +-----------+--------+ |HVACproduct|count(1)| +-----------+--------+ | ACMAX22| 3| | AC1000| 3| | JDNS77| 4| | FN39TG| 4| | GG1919| 4| +-----------+--------+ d1 = spark . sql ( 'SELECT * FROM HVAC WHERE BuildingAge >= 10' ) d1 . groupBy ( 'HVACproduct' ) . count () . show () +-----------+-----+ |HVACproduct|count| +-----------+-----+ | ACMAX22| 3| | AC1000| 3| | JDNS77| 4| | FN39TG| 4| | GG1919| 4| +-----------+-----+ # UDF from pyspark.sql.functions import udf from pyspark.sql.types import IntegerType slen = udf ( lambda s : len ( s ) + 2 , IntegerType ()) df . select ( '*' , slen ( df [ 'Country' ]) . alias ( 'slen' )) . show () +----------+-----------+-----------+-----------+------------+----+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country|slen| +----------+-----------+-----------+-----------+------------+----+ | 1| M1| 25| AC1000| USA| 5| | 2| M2| 27| FN39TG| France| 8| | 3| M3| 28| JDNS77| Brazil| 8| | 4| M4| 17| GG1919| Finland| 9| | 5| M5| 3| ACMAX22| Hong Kong| 11| | 6| M6| 9| AC1000| Singapore| 11| | 7| M7| 13| FN39TG|South Africa| 14| | 8| M8| 25| JDNS77| Australia| 11| | 9| M9| 11| GG1919| Mexico| 8| | 10| M10| 23| ACMAX22| China| 7| | 11| M11| 14| AC1000| Belgium| 9| | 12| M12| 26| FN39TG| Finland| 9| | 13| M13| 25| JDNS77|Saudi Arabia| 14| | 14| M14| 17| GG1919| Germany| 9| | 15| M15| 19| ACMAX22| Israel| 8| | 16| M16| 23| AC1000| Turkey| 8| | 17| M17| 11| FN39TG| Egypt| 7| | 18| M18| 25| JDNS77| Indonesia| 11| | 19| M19| 14| GG1919| Canada| 8| | 20| M20| 19| ACMAX22| Argentina| 11| +----------+-----------+-----------+-----------+------------+----+ spark . udf . register ( 'slen' , lambda s : len ( s ), IntegerType ()) spark . sql ( 'SELECT *, slen(Country) AS slen FROM HVAC' ) . show () +----------+-----------+-----------+-----------+------------+----+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country|slen| +----------+-----------+-----------+-----------+------------+----+ | 1| M1| 25| AC1000| USA| 3| | 2| M2| 27| FN39TG| France| 6| | 3| M3| 28| JDNS77| Brazil| 6| | 4| M4| 17| GG1919| Finland| 7| | 5| M5| 3| ACMAX22| Hong Kong| 9| | 6| M6| 9| AC1000| Singapore| 9| | 7| M7| 13| FN39TG|South Africa| 12| | 8| M8| 25| JDNS77| Australia| 9| | 9| M9| 11| GG1919| Mexico| 6| | 10| M10| 23| ACMAX22| China| 5| | 11| M11| 14| AC1000| Belgium| 7| | 12| M12| 26| FN39TG| Finland| 7| | 13| M13| 25| JDNS77|Saudi Arabia| 12| | 14| M14| 17| GG1919| Germany| 7| | 15| M15| 19| ACMAX22| Israel| 6| | 16| M16| 23| AC1000| Turkey| 6| | 17| M17| 11| FN39TG| Egypt| 5| | 18| M18| 25| JDNS77| Indonesia| 9| | 19| M19| 14| GG1919| Canada| 6| | 20| M20| 19| ACMAX22| Argentina| 9| +----------+-----------+-----------+-----------+------------+----+ Flexible Data Model \u00b6 Sample data file at https://www.cse.ust.hk/msbd5003/data/products.json df = spark . read . json ( '../data/products.json' ) df . printSchema () root |-- dimensions: struct (nullable = true) | |-- height: double (nullable = true) | |-- length: double (nullable = true) | |-- width: double (nullable = true) |-- id: long (nullable = true) |-- name: string (nullable = true) |-- price: double (nullable = true) |-- tags: array (nullable = true) | |-- element: string (containsNull = true) |-- warehouseLocation: struct (nullable = true) | |-- latitude: double (nullable = true) | |-- longitude: double (nullable = true) df . show () +----------------+---+----------------+-----+-----------+-----------------+ | dimensions| id| name|price| tags|warehouseLocation| +----------------+---+----------------+-----+-----------+-----------------+ |[9.5, 7.0, 12.0]| 2|An ice sculpture| 12.5|[cold, ice]| [-78.75, 20.4]| | [1.0, 3.1, 1.0]| 3| A blue mouse| 25.5| null| [54.4, -32.7]| +----------------+---+----------------+-----+-----------+-----------------+ # Accessing nested fields df . select ( df [ 'dimensions.height' ]) . show () +------+ |height| +------+ | 9.5| | 1.0| +------+ df . select ( 'dimensions.height' ) . show () +------+ |height| +------+ | 9.5| | 1.0| +------+ df . select ( 'dimensions.height' ) \\ . filter ( \"tags[0] = 'cold' AND warehouseLocation.latitude < 0\" ) \\ . show () +------+ |height| +------+ | 9.5| +------+ df . rdd . take ( 3 ) [Row(dimensions=Row(height=9.5, length=7.0, width=12.0), id=2, name='An ice sculpture', price=12.5, tags=['cold', 'ice'], warehouseLocation=Row(latitude=-78.75, longitude=20.4)), Row(dimensions=Row(height=1.0, length=3.1, width=1.0), id=3, name='A blue mouse', price=25.5, tags=None, warehouseLocation=Row(latitude=54.4, longitude=-32.7))] Converting between RDD and DataFrame \u00b6 Sample data file at: https://www.cse.ust.hk/msbd5003/data/people.txt # Load a text file and convert each line to a Row. lines = sc . textFile ( \"../data/people.txt\" ) def parse ( l ): a = l . split ( ',' ) return ( a [ 0 ], int ( a [ 1 ])) rdd = lines . map ( parse ) rdd . collect () [('Michael', 29), ('Andy', 30), ('Justin', 19)] # Create the DataFrame from an RDD of tuples, schema is inferred df = spark . createDataFrame ( rdd ) df . printSchema () df . show () root |-- _1: string (nullable = true) |-- _2: long (nullable = true) +-------+---+ | _1| _2| +-------+---+ |Michael| 29| | Andy| 30| | Justin| 19| +-------+---+ # Create the DataFrame from an RDD of tuples with column names, type is inferred df = spark . createDataFrame ( rdd , [ 'name' , 'age' ]) df . printSchema () df . show () root |-- name: string (nullable = true) |-- age: long (nullable = true) +-------+---+ | name|age| +-------+---+ |Michael| 29| | Andy| 30| | Justin| 19| +-------+---+ # Create the DataFrame from an RDD of Rows, type is given in the Row objects from pyspark.sql import Row rdd_rows = rdd . map ( lambda p : Row ( name = p [ 0 ], age = p [ 1 ])) df = spark . createDataFrame ( rdd_rows ) df . printSchema () df . show () root |-- name: string (nullable = true) |-- age: long (nullable = true) +-------+---+ | name|age| +-------+---+ |Michael| 29| | Andy| 30| | Justin| 19| +-------+---+ # Row fields with types incompatible with that of previous rows will be turned into nulls row1 = Row ( name = \"Alice\" , age = 11 ) row2 = Row ( name = \"Bob\" , age = '12' ) rdd_rows = sc . parallelize ([ row1 , row2 ]) df1 = spark . createDataFrame ( rdd_rows ) df1 . show () +-----+----+ | name| age| +-----+----+ |Alice| 11| | Bob|null| +-----+----+ # rdd returns the content as an RDD of Rows teenagers = df . filter ( 'age >= 13 and age <= 19' ) teenNames = teenagers . rdd . map ( lambda p : \"Name: \" + p . name ) teenNames . collect () ['Name: Justin'] Note: \u00b6 DataFrames are stored using columnar storage with compression RDDs are stored using row storage without compression The RDD view of DataFrame just provides an interface, the Row objects are constructed on the fly and do not necessarily represent the internal storage format of the data Closure in DataFrames \u00b6 data = range ( 10 ) df = spark . createDataFrame ( zip ( data , data )) df . printSchema () df . show () root |-- _1: long (nullable = true) |-- _2: long (nullable = true) +---+---+ | _1| _2| +---+---+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| | 5| 5| | 6| 6| | 7| 7| | 8| 8| | 9| 9| +---+---+ # The 'closure' behaviour in RDD doesn't seem to exist for DataFrames x = 5 df1 = df . filter ( df . _1 < x ) df1 . show () x = 3 df1 . show () +---+---+ | _1| _2| +---+---+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| +---+---+ +---+---+ | _1| _2| +---+---+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| +---+---+ # Because of the Catalyst optimizer ! df1 . explain () == Physical Plan == *(1) Filter (isnotnull(_1#1265L) AND (_1#1265L < 5)) +- *(1) Scan ExistingRDD[_1#1265L,_2#1266L] def f (): return x / 2 x = 5 df1 = df . select ( df . _1 * 2 + f () + 1 + 1 ) df1 . explain () df1 . show () == Physical Plan == *(1) Project [(((cast((_1#1265L * 2) as double) + 2.5) + 1.0) + 1.0) AS ((((_1 * 2) + 2.5) + 1) + 1)#1296] +- *(1) Scan ExistingRDD[_1#1265L,_2#1266L] +----------------------------+ |((((_1 * 2) + 2.5) + 1) + 1)| +----------------------------+ | 4.5| | 6.5| | 8.5| | 10.5| | 12.5| | 14.5| | 16.5| | 18.5| | 20.5| | 22.5| +----------------------------+ rdd = sc . parallelize ( range ( 10 )) x = 5 a = rdd . filter ( lambda z : z < x ) print ( a . take ( 10 )) x = 3 print ( a . take ( 10 )) [0, 1, 2, 3, 4] [0, 1, 2] counter = 0 def increment_counter ( x ): global counter counter += 1 df . foreach ( increment_counter ) print ( counter ) 0","title":"Sparksql"},{"location":"MSBD5003/notebooks%20in%20class/sparksql/#dataframe-operations","text":"from pyspark.sql import Row row = Row ( name = \"Alice\" , age = 11 ) print ( row ) print ( row [ 'name' ], row [ 'age' ]) print ( row . name , row . age ) row = Row ( name = \"Alice\" , age = 11 , count = 1 ) print ( row . count ) print ( row [ 'count' ]) Row(name='Alice', age=11) Alice 11 Alice 11 <built-in method count of Row object at 0x7f3384ce6e08> 1 ! wget https : // www . cse . ust . hk / msbd5003 / data / building . csv --2020-10-06 11:49:18-- https://www.cse.ust.hk/msbd5003/data/building.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 544 [text/plain] Saving to: \u2018building.csv.1\u2019 building.csv.1 100%[===================>] 544 --.-KB/s in 0s 2020-10-06 11:49:20 (30.9 MB/s) - \u2018building.csv.1\u2019 saved [544/544] # Data file at https://www.cse.ust.hk/msbd5003/data/building.csv df = spark . read . csv ( 'building.csv' , header = True , inferSchema = True ) # show the content of the dataframe df . show () +----------+-----------+-----------+-----------+------------+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country| +----------+-----------+-----------+-----------+------------+ | 1| M1| 25| AC1000| USA| | 2| M2| 27| FN39TG| France| | 3| M3| 28| JDNS77| Brazil| | 4| M4| 17| GG1919| Finland| | 5| M5| 3| ACMAX22| Hong Kong| | 6| M6| 9| AC1000| Singapore| | 7| M7| 13| FN39TG|South Africa| | 8| M8| 25| JDNS77| Australia| | 9| M9| 11| GG1919| Mexico| | 10| M10| 23| ACMAX22| China| | 11| M11| 14| AC1000| Belgium| | 12| M12| 26| FN39TG| Finland| | 13| M13| 25| JDNS77|Saudi Arabia| | 14| M14| 17| GG1919| Germany| | 15| M15| 19| ACMAX22| Israel| | 16| M16| 23| AC1000| Turkey| | 17| M17| 11| FN39TG| Egypt| | 18| M18| 25| JDNS77| Indonesia| | 19| M19| 14| GG1919| Canada| | 20| M20| 19| ACMAX22| Argentina| +----------+-----------+-----------+-----------+------------+ # Print the dataframe schema in a tree format df . printSchema () root |-- BuildingID: integer (nullable = true) |-- BuildingMgr: string (nullable = true) |-- BuildingAge: integer (nullable = true) |-- HVACproduct: string (nullable = true) |-- Country: string (nullable = true) # Create an RDD from the dataframe dfrdd = df . rdd dfrdd . take ( 3 ) [Row(BuildingID=1, BuildingMgr='M1', BuildingAge=25, HVACproduct='AC1000', Country='USA'), Row(BuildingID=2, BuildingMgr='M2', BuildingAge=27, HVACproduct='FN39TG', Country='France'), Row(BuildingID=3, BuildingMgr='M3', BuildingAge=28, HVACproduct='JDNS77', Country='Brazil')] # Retrieve specific columns from the dataframe df . select ( 'BuildingID' , 'Country' ) . show () +----------+------------+ |BuildingID| Country| +----------+------------+ | 1| USA| | 2| France| | 3| Brazil| | 4| Finland| | 5| Hong Kong| | 6| Singapore| | 7|South Africa| | 8| Australia| | 9| Mexico| | 10| China| | 11| Belgium| | 12| Finland| | 13|Saudi Arabia| | 14| Germany| | 15| Israel| | 16| Turkey| | 17| Egypt| | 18| Indonesia| | 19| Canada| | 20| Argentina| +----------+------------+ from pyspark.sql.functions import * df . where ( \"Country<'USA'\" ) . select ( 'BuildingID' , lit ( 'OK' )) . show () +----------+---+ |BuildingID| OK| +----------+---+ | 2| OK| | 3| OK| | 4| OK| | 5| OK| | 6| OK| | 7| OK| | 8| OK| | 9| OK| | 10| OK| | 11| OK| | 12| OK| | 13| OK| | 14| OK| | 15| OK| | 16| OK| | 17| OK| | 18| OK| | 19| OK| | 20| OK| +----------+---+ # Use GroupBy clause with dataframe df . groupBy ( 'HVACProduct' ) . count () . show () +-----------+-----+ |HVACProduct|count| +-----------+-----+ | ACMAX22| 4| | AC1000| 4| | JDNS77| 4| | FN39TG| 4| | GG1919| 4| +-----------+-----+ ! wget https : // www . cse . ust . hk / msbd5003 / data / Customer . csv ! wget https : // www . cse . ust . hk / msbd5003 / data / Product . csv ! wget https : // www . cse . ust . hk / msbd5003 / data / SalesOrderDetail . csv ! wget https : // www . cse . ust . hk / msbd5003 / data / SalesOrderHeader . csv --2020-10-06 11:51:08-- https://www.cse.ust.hk/msbd5003/data/Customer.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 199491 (195K) [text/plain] Saving to: \u2018Customer.csv\u2019 Customer.csv 100%[===================>] 194.82K 320KB/s in 0.6s 2020-10-06 11:51:11 (320 KB/s) - \u2018Customer.csv\u2019 saved [199491/199491] --2020-10-06 11:51:12-- https://www.cse.ust.hk/msbd5003/data/Product.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1355634 (1.3M) [text/plain] Saving to: \u2018Product.csv\u2019 Product.csv 100%[===================>] 1.29M 1.05MB/s in 1.2s 2020-10-06 11:51:15 (1.05 MB/s) - \u2018Product.csv\u2019 saved [1355634/1355634] --2020-10-06 11:51:15-- https://www.cse.ust.hk/msbd5003/data/SalesOrderDetail.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 56766 (55K) [text/plain] Saving to: \u2018SalesOrderDetail.csv\u2019 SalesOrderDetail.cs 100%[===================>] 55.44K 137KB/s in 0.4s 2020-10-06 11:51:18 (137 KB/s) - \u2018SalesOrderDetail.csv\u2019 saved [56766/56766] --2020-10-06 11:51:18-- https://www.cse.ust.hk/msbd5003/data/SalesOrderHeader.csv Resolving www.cse.ust.hk (www.cse.ust.hk)... 143.89.40.27 Connecting to www.cse.ust.hk (www.cse.ust.hk)|143.89.40.27|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 8680 (8.5K) [text/plain] Saving to: \u2018SalesOrderHeader.csv\u2019 SalesOrderHeader.cs 100%[===================>] 8.48K --.-KB/s in 0s 2020-10-06 11:51:19 (79.4 MB/s) - \u2018SalesOrderHeader.csv\u2019 saved [8680/8680]","title":"Dataframe operations"},{"location":"MSBD5003/notebooks%20in%20class/sparksql/#rewriting-sql-with-dataframe-api","text":"# Load data from csv files # Data files at https://www.cse.ust.hk/msbd5003/data dfCustomer = spark . read . csv ( 'Customer.csv' , header = True , inferSchema = True ) dfProduct = spark . read . csv ( 'Product.csv' , header = True , inferSchema = True ) dfDetail = spark . read . csv ( 'SalesOrderDetail.csv' , header = True , inferSchema = True ) dfHeader = spark . read . csv ( 'SalesOrderHeader.csv' , header = True , inferSchema = True ) # SELECT ProductID, Name, ListPrice # FROM Product # WHERE Color = 'black' dfProduct . filter ( \"Color = 'Black'\" ) \\ . select ( 'ProductID' , 'Name' , 'ListPrice' ) \\ . show ( truncate = False ) +---------+-----------------------------+---------+ |ProductID|Name |ListPrice| +---------+-----------------------------+---------+ |680 |HL Road Frame - Black, 58 |1431.5 | |708 |Sport-100 Helmet, Black |34.99 | |722 |LL Road Frame - Black, 58 |337.22 | |723 |LL Road Frame - Black, 60 |337.22 | |724 |LL Road Frame - Black, 62 |337.22 | |736 |LL Road Frame - Black, 44 |337.22 | |737 |LL Road Frame - Black, 48 |337.22 | |738 |LL Road Frame - Black, 52 |337.22 | |743 |HL Mountain Frame - Black, 42|1349.6 | |744 |HL Mountain Frame - Black, 44|1349.6 | |745 |HL Mountain Frame - Black, 48|1349.6 | |746 |HL Mountain Frame - Black, 46|1349.6 | |747 |HL Mountain Frame - Black, 38|1349.6 | |765 |Road-650 Black, 58 |782.99 | |766 |Road-650 Black, 60 |782.99 | |767 |Road-650 Black, 62 |782.99 | |768 |Road-650 Black, 44 |782.99 | |769 |Road-650 Black, 48 |782.99 | |770 |Road-650 Black, 52 |782.99 | |775 |Mountain-100 Black, 38 |3374.99 | +---------+-----------------------------+---------+ only showing top 20 rows dfProduct . where ( dfProduct . Color == 'Black' ) \\ . select ( dfProduct . ProductID , dfProduct [ 'Name' ], ( dfProduct [ 'ListPrice' ] * 2 ) . alias ( 'Double price' )) \\ . show ( truncate = False ) +---------+-----------------------------+------------+ |ProductID|Name |Double price| +---------+-----------------------------+------------+ |680 |HL Road Frame - Black, 58 |2863.0 | |708 |Sport-100 Helmet, Black |69.98 | |722 |LL Road Frame - Black, 58 |674.44 | |723 |LL Road Frame - Black, 60 |674.44 | |724 |LL Road Frame - Black, 62 |674.44 | |736 |LL Road Frame - Black, 44 |674.44 | |737 |LL Road Frame - Black, 48 |674.44 | |738 |LL Road Frame - Black, 52 |674.44 | |743 |HL Mountain Frame - Black, 42|2699.2 | |744 |HL Mountain Frame - Black, 44|2699.2 | |745 |HL Mountain Frame - Black, 48|2699.2 | |746 |HL Mountain Frame - Black, 46|2699.2 | |747 |HL Mountain Frame - Black, 38|2699.2 | |765 |Road-650 Black, 58 |1565.98 | |766 |Road-650 Black, 60 |1565.98 | |767 |Road-650 Black, 62 |1565.98 | |768 |Road-650 Black, 44 |1565.98 | |769 |Road-650 Black, 48 |1565.98 | |770 |Road-650 Black, 52 |1565.98 | |775 |Mountain-100 Black, 38 |6749.98 | +---------+-----------------------------+------------+ only showing top 20 rows dfProduct . where ( dfProduct . ListPrice * 2 > 100 ) \\ . select ( dfProduct . ProductID , dfProduct [ 'Name' ], dfProduct . ListPrice * 2 ) \\ . show ( truncate = False ) +---------+-------------------------+---------------+ |ProductID|Name |(ListPrice * 2)| +---------+-------------------------+---------------+ |680 |HL Road Frame - Black, 58|2863.0 | |706 |HL Road Frame - Red, 58 |2863.0 | |717 |HL Road Frame - Red, 62 |2863.0 | |718 |HL Road Frame - Red, 44 |2863.0 | |719 |HL Road Frame - Red, 48 |2863.0 | |720 |HL Road Frame - Red, 52 |2863.0 | |721 |HL Road Frame - Red, 56 |2863.0 | |722 |LL Road Frame - Black, 58|674.44 | |723 |LL Road Frame - Black, 60|674.44 | |724 |LL Road Frame - Black, 62|674.44 | |725 |LL Road Frame - Red, 44 |674.44 | |726 |LL Road Frame - Red, 48 |674.44 | |727 |LL Road Frame - Red, 52 |674.44 | |728 |LL Road Frame - Red, 58 |674.44 | |729 |LL Road Frame - Red, 60 |674.44 | |730 |LL Road Frame - Red, 62 |674.44 | |731 |ML Road Frame - Red, 44 |1189.66 | |732 |ML Road Frame - Red, 48 |1189.66 | |733 |ML Road Frame - Red, 52 |1189.66 | |734 |ML Road Frame - Red, 58 |1189.66 | +---------+-------------------------+---------------+ only showing top 20 rows # SELECT ProductID, Name, ListPrice # FROM Product # WHERE Color = 'black' # ORDER BY ProductID dfProduct . filter ( \"Color = 'Black'\" ) \\ . select ( 'ProductID' , 'Name' , 'ListPrice' ) \\ . orderBy ( 'ListPrice' ) \\ . show ( truncate = False ) +---------+--------------------------+---------+ |ProductID|Name |ListPrice| +---------+--------------------------+---------+ |860 |Half-Finger Gloves, L |24.49 | |859 |Half-Finger Gloves, M |24.49 | |858 |Half-Finger Gloves, S |24.49 | |708 |Sport-100 Helmet, Black |34.99 | |862 |Full-Finger Gloves, M |37.99 | |861 |Full-Finger Gloves, S |37.99 | |863 |Full-Finger Gloves, L |37.99 | |841 |Men's Sports Shorts, S |59.99 | |849 |Men's Sports Shorts, M |59.99 | |851 |Men's Sports Shorts, XL |59.99 | |850 |Men's Sports Shorts, L |59.99 | |815 |LL Mountain Front Wheel |60.745 | |868 |Women's Mountain Shorts, M|69.99 | |869 |Women's Mountain Shorts, L|69.99 | |867 |Women's Mountain Shorts, S|69.99 | |853 |Women's Tights, M |74.99 | |854 |Women's Tights, L |74.99 | |852 |Women's Tights, S |74.99 | |818 |LL Road Front Wheel |85.565 | |823 |LL Mountain Rear Wheel |87.745 | +---------+--------------------------+---------+ only showing top 20 rows # Find all orders and details on black product, # return the product SalesOrderID, SalesOrderDetailID, Name, UnitPrice, and OrderQty # SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty # FROM SalesLT.SalesOrderDetail, SalesLT.Product # WHERE SalesOrderDetail.ProductID = Product.ProductID AND Color = 'Black' # SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty # FROM SalesLT.SalesOrderDetail # JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID # WHERE Color = 'Black' # Spark SQL supports natural joins dfDetail . join ( dfProduct , 'ProductID' ) \\ . select ( 'SalesOrderID' , 'SalesOrderDetailID' , 'Name' , 'UnitPrice' , 'OrderQty' ) \\ . filter ( \"Color='Black'\" ) \\ . show () # If we move the filter to after select, it still works. Why? +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71797| 111045|LL Road Frame - B...| 202.332| 3| | 71783| 110730|LL Road Frame - B...| 202.332| 6| | 71938| 113297|LL Road Frame - B...| 202.332| 3| | 71915| 113090|LL Road Frame - B...| 202.332| 2| | 71815| 111451|LL Road Frame - B...| 202.332| 1| | 71797| 111044|LL Road Frame - B...| 202.332| 1| | 71783| 110710|LL Road Frame - B...| 202.332| 4| | 71936| 113260|HL Mountain Frame...| 809.76| 4| | 71899| 112937|HL Mountain Frame...| 809.76| 1| | 71845| 112137|HL Mountain Frame...| 809.76| 2| | 71832| 111806|HL Mountain Frame...| 809.76| 4| | 71780| 110622|HL Mountain Frame...| 809.76| 1| | 71936| 113235|HL Mountain Frame...| 809.76| 4| | 71845| 112134|HL Mountain Frame...| 809.76| 3| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows # This also works: d1 = dfDetail . join ( dfProduct , 'ProductID' ) \\ . select ( 'SalesOrderID' , 'SalesOrderDetailID' , 'Name' , 'UnitPrice' , 'OrderQty' ) d1 . show () d2 = d1 . filter ( \"Color = 'Black'\" ) d2 . show () d2 . explain () +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113278|Sport-100 Helmet,...| 20.994| 3| | 71936| 113228|Sport-100 Helmet,...| 20.994| 1| | 71902| 112980|Sport-100 Helmet,...| 20.994| 2| | 71797| 111075|Sport-100 Helmet,...| 20.994| 6| | 71784| 110794|Sport-100 Helmet,...| 20.994| 10| | 71783| 110751|Sport-100 Helmet,...| 20.994| 10| | 71782| 110709|Sport-100 Helmet,...| 20.994| 3| | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71938| 113282|Sport-100 Helmet,...| 20.994| 3| | 71902| 112995|Sport-100 Helmet,...| 20.994| 7| | 71863| 112395|Sport-100 Helmet,...| 20.994| 1| | 71797| 111038|Sport-100 Helmet,...| 20.994| 4| | 71784| 110753|Sport-100 Helmet,...| 20.994| 2| | 71783| 110749|Sport-100 Helmet,...| 19.2445| 15| | 71782| 110708|Sport-100 Helmet,...| 20.994| 6| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71797| 111045|LL Road Frame - B...| 202.332| 3| | 71783| 110730|LL Road Frame - B...| 202.332| 6| | 71938| 113297|LL Road Frame - B...| 202.332| 3| | 71915| 113090|LL Road Frame - B...| 202.332| 2| | 71815| 111451|LL Road Frame - B...| 202.332| 1| | 71797| 111044|LL Road Frame - B...| 202.332| 1| | 71783| 110710|LL Road Frame - B...| 202.332| 4| | 71936| 113260|HL Mountain Frame...| 809.76| 4| | 71899| 112937|HL Mountain Frame...| 809.76| 1| | 71845| 112137|HL Mountain Frame...| 809.76| 2| | 71832| 111806|HL Mountain Frame...| 809.76| 4| | 71780| 110622|HL Mountain Frame...| 809.76| 1| | 71936| 113235|HL Mountain Frame...| 809.76| 4| | 71845| 112134|HL Mountain Frame...| 809.76| 3| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows == Physical Plan == *(2) Project [SalesOrderID#217, SalesOrderDetailID#218, Name#168, UnitPrice#221, OrderQty#219] +- *(2) BroadcastHashJoin [ProductID#220], [ProductID#167], Inner, BuildLeft :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[3, int, true] as bigint))), [id=#380] : +- *(1) Project [SalesOrderID#217, SalesOrderDetailID#218, OrderQty#219, ProductID#220, UnitPrice#221] : +- *(1) Filter isnotnull(ProductID#220) : +- FileScan csv [SalesOrderID#217,SalesOrderDetailID#218,OrderQty#219,ProductID#220,UnitPrice#221] Batched: false, DataFilters: [isnotnull(ProductID#220)], Format: CSV, Location: InMemoryFileIndex[file:/content/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double> +- *(2) Project [ProductID#167, Name#168] +- *(2) Filter ((isnotnull(Color#170) AND (Color#170 = Black)) AND isnotnull(ProductID#167)) +- FileScan csv [ProductID#167,Name#168,Color#170] Batched: false, DataFilters: [isnotnull(Color#170), (Color#170 = Black), isnotnull(ProductID#167)], Format: CSV, Location: InMemoryFileIndex[file:/content/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string> # SparkSQL performs optimization depending on whether intermediate dataframe are cached or not: d1 = dfDetail . join ( dfProduct , 'ProductID' ) \\ . select ( 'SalesOrderID' , 'SalesOrderDetailID' , 'Name' , 'UnitPrice' , 'OrderQty' ) d1 . persist () d1 . show () +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113278|Sport-100 Helmet,...| 20.994| 3| | 71936| 113228|Sport-100 Helmet,...| 20.994| 1| | 71902| 112980|Sport-100 Helmet,...| 20.994| 2| | 71797| 111075|Sport-100 Helmet,...| 20.994| 6| | 71784| 110794|Sport-100 Helmet,...| 20.994| 10| | 71783| 110751|Sport-100 Helmet,...| 20.994| 10| | 71782| 110709|Sport-100 Helmet,...| 20.994| 3| | 71938| 113295|Sport-100 Helmet,...| 20.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71938| 113282|Sport-100 Helmet,...| 20.994| 3| | 71902| 112995|Sport-100 Helmet,...| 20.994| 7| | 71863| 112395|Sport-100 Helmet,...| 20.994| 1| | 71797| 111038|Sport-100 Helmet,...| 20.994| 4| | 71784| 110753|Sport-100 Helmet,...| 20.994| 2| | 71783| 110749|Sport-100 Helmet,...| 19.2445| 15| | 71782| 110708|Sport-100 Helmet,...| 20.994| 6| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows d2 = d1 . filter ( \"Color = 'Black'\" ) #d2 = d1.filter(\"OrderQty >= 10\") d2 . show () d2 . explain () +------------+------------------+--------------------+---------+--------+ |SalesOrderID|SalesOrderDetailID| Name|UnitPrice|OrderQty| +------------+------------------+--------------------+---------+--------+ | 71938| 113295|Sport-100 Helmet,...| 29.994| 5| | 71902| 112988|Sport-100 Helmet,...| 20.994| 4| | 71797| 111082|Sport-100 Helmet,...| 20.2942| 12| | 71784| 110795|Sport-100 Helmet,...| 20.2942| 12| | 71783| 110752|Sport-100 Helmet,...| 20.2942| 11| | 71782| 110690|Sport-100 Helmet,...| 20.994| 7| | 71797| 111045|LL Road Frame - B...| 202.332| 3| | 71783| 110730|LL Road Frame - B...| 202.332| 6| | 71938| 113297|LL Road Frame - B...| 202.332| 3| | 71915| 113090|LL Road Frame - B...| 202.332| 2| | 71815| 111451|LL Road Frame - B...| 202.332| 1| | 71797| 111044|LL Road Frame - B...| 202.332| 1| | 71783| 110710|LL Road Frame - B...| 202.332| 4| | 71936| 113260|HL Mountain Frame...| 809.76| 4| | 71899| 112937|HL Mountain Frame...| 809.76| 1| | 71845| 112137|HL Mountain Frame...| 809.76| 2| | 71832| 111806|HL Mountain Frame...| 809.76| 4| | 71780| 110622|HL Mountain Frame...| 809.76| 1| | 71936| 113235|HL Mountain Frame...| 809.76| 4| | 71845| 112134|HL Mountain Frame...| 809.76| 3| +------------+------------------+--------------------+---------+--------+ only showing top 20 rows == Physical Plan == *(2) Project [SalesOrderID#112, SalesOrderDetailID#113, Name#63, UnitPrice#116, OrderQty#114] +- *(2) BroadcastHashJoin [ProductID#115], [ProductID#62], Inner, BuildLeft :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[3, int, true] as bigint))), [id=#280] : +- *(1) Project [SalesOrderID#112, SalesOrderDetailID#113, OrderQty#114, ProductID#115, UnitPrice#116] : +- *(1) Filter isnotnull(ProductID#115) : +- FileScan csv [SalesOrderID#112,SalesOrderDetailID#113,OrderQty#114,ProductID#115,UnitPrice#116] Batched: false, DataFilters: [isnotnull(ProductID#115)], Format: CSV, Location: InMemoryFileIndex[file:/csproject/msbd5003/public_html/data/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double> +- *(2) Project [ProductID#62, Name#63] +- *(2) Filter ((isnotnull(Color#65) AND (Color#65 = Black)) AND isnotnull(ProductID#62)) +- FileScan csv [ProductID#62,Name#63,Color#65] Batched: false, DataFilters: [isnotnull(Color#65), (Color#65 = Black), isnotnull(ProductID#62)], Format: CSV, Location: InMemoryFileIndex[file:/csproject/msbd5003/public_html/data/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string> # This will report an error: d1 = dfDetail . join ( dfProduct , 'ProductID' ) \\ . select ( 'SalesOrderID' , 'SalesOrderDetailID' , 'Name' , 'UnitPrice' , 'OrderQty' ) d1 . write . csv ( 'temp.csv' , mode = 'overwrite' , header = True ) d2 = spark . read . csv ( 'temp.csv' , header = True , inferSchema = True ) d2 . filter ( \"Color = 'Black'\" ) . show () --------------------------------------------------------------------------- AnalysisException Traceback (most recent call last) <ipython-input-18-168595bb0376> in <module> 5 d1.write.csv('temp.csv', mode = 'overwrite', header = True) 6 d2 = spark.read.csv('temp.csv', header = True, inferSchema = True) ----> 7 d2.filter(\"Color = 'Black'\").show() /csproject/msbd5003/python/pyspark/sql/dataframe.py in filter(self, condition) 1457 \"\"\" 1458 if isinstance(condition, basestring): -> 1459 jdf = self._jdf.filter(condition) 1460 elif isinstance(condition, Column): 1461 jdf = self._jdf.filter(condition._jc) /csproject/msbd5003/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args) 1303 answer = self.gateway_client.send_command(command) 1304 return_value = get_return_value( -> 1305 answer, self.gateway_client, self.target_id, self.name) 1306 1307 for temp_arg in temp_args: /csproject/msbd5003/python/pyspark/sql/utils.py in deco(*a, **kw) 135 # Hide where the exception came from that shows a non-Pythonic 136 # JVM exception message. --> 137 raise_from(converted) 138 else: 139 raise /csproject/msbd5003/python/pyspark/sql/utils.py in raise_from(e) AnalysisException: cannot resolve '`Color`' given input columns: [Name, OrderQty, SalesOrderDetailID, SalesOrderID, UnitPrice]; line 1 pos 0; 'Filter ('Color = Black) +- Relation[SalesOrderID#580,SalesOrderDetailID#581,Name#582,UnitPrice#583,OrderQty#584] csv # Find all orders that include at least one black product, # return the product SalesOrderID, Name, UnitPrice, and OrderQty # SELECT DISTINCT SalesOrderID # FROM SalesLT.SalesOrderDetail # JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID # WHERE Color = 'Black' dfDetail . join ( dfProduct . filter ( \"Color='Black'\" ), 'ProductID' ) \\ . select ( 'SalesOrderID' ) \\ . distinct () \\ . show () +------------+ |SalesOrderID| +------------+ | 71902| | 71832| | 71915| | 71831| | 71898| | 71935| | 71938| | 71845| | 71783| | 71815| | 71936| | 71863| | 71780| | 71782| | 71899| | 71784| | 71797| +------------+ # How many colors in the products? # SELECT COUNT(DISTINCT Color) # FROM SalesLT.Product dfProduct . select ( 'Color' ) . distinct () . count () # It's 1 more than standard SQL. In standard SQL, COUNT() does not count NULLs. 10 # Find the total price of each order, # return SalesOrderID and total price (column name should be \u2018totalprice\u2019) # SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice # FROM SalesLT.SalesOrderDetail # GROUP BY SalesOrderID dfDetail . select ( '*' , ( dfDetail . UnitPrice * dfDetail . OrderQty * ( 1 - dfDetail . UnitPriceDiscount )) . alias ( 'netprice' )) \\ . groupBy ( 'SalesOrderID' ) . sum ( 'netprice' ) \\ . withColumnRenamed ( 'sum(netprice)' , 'TotalPrice' ) \\ . show () +------------+------------------+ |SalesOrderID| sum(netprice)| +------------+------------------+ | 71867| 858.9| | 71902|59894.209199999976| | 71832| 28950.678108| | 71915|1732.8899999999999| | 71946| 31.584| | 71895|221.25600000000003| | 71816|2847.4079999999994| | 71831| 1712.946| | 71923| 96.108824| | 71858|11528.844000000001| | 71917| 37.758| | 71897| 10585.05| | 71885| 524.664| | 71856|500.30400000000003| | 71898| 53248.69200000002| | 71774| 713.796| | 71796| 47848.02600000001| | 71935|5533.8689079999995| | 71938| 74160.228| | 71845| 34118.5356| +------------+------------------+ only showing top 20 rows # Find the total price of each order where the total price > 10000 # SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice # FROM SalesLT.SalesOrderDetail # GROUP BY SalesOrderID # HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000 dfDetail . select ( '*' , ( dfDetail . UnitPrice * dfDetail . OrderQty * ( 1 - dfDetail . UnitPriceDiscount )) . alias ( 'netprice' )) \\ . groupBy ( 'SalesOrderID' ) . sum ( 'netprice' ) \\ . withColumnRenamed ( 'sum(netprice)' , 'TotalPrice' ) \\ . where ( 'TotalPrice > 10000' ) \\ . show () +------------+------------------+ |SalesOrderID| TotalPrice| +------------+------------------+ | 71902|59894.209199999976| | 71832| 28950.678108| | 71858|11528.844000000001| | 71897| 10585.05| | 71898| 53248.69200000002| | 71796| 47848.02600000001| | 71938| 74160.228| | 71845| 34118.5356| | 71783| 65683.367986| | 71936| 79589.61602399996| | 71780|29923.007999999998| | 71782| 33319.98600000001| | 71784| 89869.27631400003| | 71797| 65123.46341800001| +------------+------------------+ # Find the total price on the black products of each order where the total price > 10000 # SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice # FROM SalesLT.SalesOrderDetail, SalesLT.Product # WHERE SalesLT.SalesOrderDetail.ProductID = SalesLT.Product.ProductID AND Color = 'Black' # GROUP BY SalesOrderID # HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000 dfDetail . select ( '*' , ( dfDetail . UnitPrice * dfDetail . OrderQty * ( 1 - dfDetail . UnitPriceDiscount )) . alias ( 'netprice' )) \\ . join ( dfProduct , 'ProductID' ) \\ . where ( \"Color = 'Black'\" ) \\ . groupBy ( 'SalesOrderID' ) . sum ( 'netprice' ) \\ . withColumnRenamed ( 'sum(netprice)' , 'TotalPrice' ) \\ . where ( 'TotalPrice > 10000' ) \\ . show () +------------+------------------+ |SalesOrderID| TotalPrice| +------------+------------------+ | 71902|26677.883999999995| | 71832| 16883.748108| | 71938| 33779.448| | 71845| 18109.836| | 71783|15524.117476000003| | 71936| 44490.290424| | 71780| 16964.322| | 71797| 27581.613792| +------------+------------------+ # For each customer, find the total quantity of black products bought. # Report CustomerID, FirstName, LastName, and total quantity # select saleslt.customer.customerid, FirstName, LastName, sum(orderqty) # from saleslt.customer # left outer join # ( # saleslt.salesorderheader # join saleslt.salesorderdetail # on saleslt.salesorderdetail.salesorderid = saleslt.salesorderheader.salesorderid # join saleslt.product # on saleslt.product.productid = saleslt.salesorderdetail.productid and color = 'black' # ) # on saleslt.customer.customerid = saleslt.salesorderheader.customerid # group by saleslt.customer.customerid, FirstName, LastName # order by sum(orderqty) desc d1 = dfDetail . join ( dfProduct , 'ProductID' ) \\ . where ( 'Color = \"Black\"' ) \\ . join ( dfHeader , 'SalesOrderID' ) \\ . groupBy ( 'CustomerID' ) . sum ( 'OrderQty' ) dfCustomer . join ( d1 , 'CustomerID' , 'left_outer' ) \\ . select ( 'CustomerID' , 'FirstName' , 'LastName' , 'sum(OrderQty)' ) \\ . orderBy ( 'sum(OrderQty)' , ascending = False ) \\ . show () +----------+------------+------------+-------------+ |CustomerID| FirstName| LastName|sum(OrderQty)| +----------+------------+------------+-------------+ | 30050| Krishna|Sunkammurali| 89| | 29796| Jon| Grande| 65| | 29957| Kevin| Liu| 62| | 29929| Jeffrey| Kurtz| 46| | 29546| Christopher| Beck| 45| | 29922| Pamala| Kotc| 34| | 30113| Raja| Venugopal| 34| | 29938| Frank| Campbell| 29| | 29736| Terry| Eminhizer| 23| | 29485| Catherine| Abel| 10| | 30019| Matthew| Miller| 9| | 29932| Rebecca| Laszlo| 7| | 29975| Walter| Mays| 5| | 29638| Rosmarie| Carroll| 2| | 29531| Cory| Booth| 1| | 30089|Michael John| Troyer| 1| | 29568| Donald| Blanton| 1| | 29868| Denean| Ison| null| | 29646| Stacey| Cereghino| null| | 29905| Elizabeth| Keyser| null| +----------+------------+------------+-------------+ only showing top 20 rows","title":"Rewriting SQL with DataFrame API"},{"location":"MSBD5003/notebooks%20in%20class/sparksql/#embed-sql-queries","text":"You can also run SQL queries over dataframes once you register them as temporary tables within the SparkSession. # Register the dataframe as a temporary view called HVAC df . createOrReplaceTempView ( 'HVAC' ) spark . sql ( 'SELECT * FROM HVAC WHERE BuildingAge >= 10' ) . show () +----------+-----------+-----------+-----------+------------+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country| +----------+-----------+-----------+-----------+------------+ | 1| M1| 25| AC1000| USA| | 2| M2| 27| FN39TG| France| | 3| M3| 28| JDNS77| Brazil| | 4| M4| 17| GG1919| Finland| | 7| M7| 13| FN39TG|South Africa| | 8| M8| 25| JDNS77| Australia| | 9| M9| 11| GG1919| Mexico| | 10| M10| 23| ACMAX22| China| | 11| M11| 14| AC1000| Belgium| | 12| M12| 26| FN39TG| Finland| | 13| M13| 25| JDNS77|Saudi Arabia| | 14| M14| 17| GG1919| Germany| | 15| M15| 19| ACMAX22| Israel| | 16| M16| 23| AC1000| Turkey| | 17| M17| 11| FN39TG| Egypt| | 18| M18| 25| JDNS77| Indonesia| | 19| M19| 14| GG1919| Canada| | 20| M20| 19| ACMAX22| Argentina| +----------+-----------+-----------+-----------+------------+ # Can even mix DataFrame API with SQL: df . where ( 'BuildingAge >= 10' ) . createOrReplaceTempView ( 'OldBuildings' ) spark . sql ( 'SELECT HVACproduct, COUNT(*) FROM OldBuildings GROUP BY HVACproduct' ) . show () +-----------+--------+ |HVACproduct|count(1)| +-----------+--------+ | ACMAX22| 3| | AC1000| 3| | JDNS77| 4| | FN39TG| 4| | GG1919| 4| +-----------+--------+ d1 = spark . sql ( 'SELECT * FROM HVAC WHERE BuildingAge >= 10' ) d1 . groupBy ( 'HVACproduct' ) . count () . show () +-----------+-----+ |HVACproduct|count| +-----------+-----+ | ACMAX22| 3| | AC1000| 3| | JDNS77| 4| | FN39TG| 4| | GG1919| 4| +-----------+-----+ # UDF from pyspark.sql.functions import udf from pyspark.sql.types import IntegerType slen = udf ( lambda s : len ( s ) + 2 , IntegerType ()) df . select ( '*' , slen ( df [ 'Country' ]) . alias ( 'slen' )) . show () +----------+-----------+-----------+-----------+------------+----+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country|slen| +----------+-----------+-----------+-----------+------------+----+ | 1| M1| 25| AC1000| USA| 5| | 2| M2| 27| FN39TG| France| 8| | 3| M3| 28| JDNS77| Brazil| 8| | 4| M4| 17| GG1919| Finland| 9| | 5| M5| 3| ACMAX22| Hong Kong| 11| | 6| M6| 9| AC1000| Singapore| 11| | 7| M7| 13| FN39TG|South Africa| 14| | 8| M8| 25| JDNS77| Australia| 11| | 9| M9| 11| GG1919| Mexico| 8| | 10| M10| 23| ACMAX22| China| 7| | 11| M11| 14| AC1000| Belgium| 9| | 12| M12| 26| FN39TG| Finland| 9| | 13| M13| 25| JDNS77|Saudi Arabia| 14| | 14| M14| 17| GG1919| Germany| 9| | 15| M15| 19| ACMAX22| Israel| 8| | 16| M16| 23| AC1000| Turkey| 8| | 17| M17| 11| FN39TG| Egypt| 7| | 18| M18| 25| JDNS77| Indonesia| 11| | 19| M19| 14| GG1919| Canada| 8| | 20| M20| 19| ACMAX22| Argentina| 11| +----------+-----------+-----------+-----------+------------+----+ spark . udf . register ( 'slen' , lambda s : len ( s ), IntegerType ()) spark . sql ( 'SELECT *, slen(Country) AS slen FROM HVAC' ) . show () +----------+-----------+-----------+-----------+------------+----+ |BuildingID|BuildingMgr|BuildingAge|HVACproduct| Country|slen| +----------+-----------+-----------+-----------+------------+----+ | 1| M1| 25| AC1000| USA| 3| | 2| M2| 27| FN39TG| France| 6| | 3| M3| 28| JDNS77| Brazil| 6| | 4| M4| 17| GG1919| Finland| 7| | 5| M5| 3| ACMAX22| Hong Kong| 9| | 6| M6| 9| AC1000| Singapore| 9| | 7| M7| 13| FN39TG|South Africa| 12| | 8| M8| 25| JDNS77| Australia| 9| | 9| M9| 11| GG1919| Mexico| 6| | 10| M10| 23| ACMAX22| China| 5| | 11| M11| 14| AC1000| Belgium| 7| | 12| M12| 26| FN39TG| Finland| 7| | 13| M13| 25| JDNS77|Saudi Arabia| 12| | 14| M14| 17| GG1919| Germany| 7| | 15| M15| 19| ACMAX22| Israel| 6| | 16| M16| 23| AC1000| Turkey| 6| | 17| M17| 11| FN39TG| Egypt| 5| | 18| M18| 25| JDNS77| Indonesia| 9| | 19| M19| 14| GG1919| Canada| 6| | 20| M20| 19| ACMAX22| Argentina| 9| +----------+-----------+-----------+-----------+------------+----+","title":"Embed SQL queries"},{"location":"MSBD5003/notebooks%20in%20class/sparksql/#flexible-data-model","text":"Sample data file at https://www.cse.ust.hk/msbd5003/data/products.json df = spark . read . json ( '../data/products.json' ) df . printSchema () root |-- dimensions: struct (nullable = true) | |-- height: double (nullable = true) | |-- length: double (nullable = true) | |-- width: double (nullable = true) |-- id: long (nullable = true) |-- name: string (nullable = true) |-- price: double (nullable = true) |-- tags: array (nullable = true) | |-- element: string (containsNull = true) |-- warehouseLocation: struct (nullable = true) | |-- latitude: double (nullable = true) | |-- longitude: double (nullable = true) df . show () +----------------+---+----------------+-----+-----------+-----------------+ | dimensions| id| name|price| tags|warehouseLocation| +----------------+---+----------------+-----+-----------+-----------------+ |[9.5, 7.0, 12.0]| 2|An ice sculpture| 12.5|[cold, ice]| [-78.75, 20.4]| | [1.0, 3.1, 1.0]| 3| A blue mouse| 25.5| null| [54.4, -32.7]| +----------------+---+----------------+-----+-----------+-----------------+ # Accessing nested fields df . select ( df [ 'dimensions.height' ]) . show () +------+ |height| +------+ | 9.5| | 1.0| +------+ df . select ( 'dimensions.height' ) . show () +------+ |height| +------+ | 9.5| | 1.0| +------+ df . select ( 'dimensions.height' ) \\ . filter ( \"tags[0] = 'cold' AND warehouseLocation.latitude < 0\" ) \\ . show () +------+ |height| +------+ | 9.5| +------+ df . rdd . take ( 3 ) [Row(dimensions=Row(height=9.5, length=7.0, width=12.0), id=2, name='An ice sculpture', price=12.5, tags=['cold', 'ice'], warehouseLocation=Row(latitude=-78.75, longitude=20.4)), Row(dimensions=Row(height=1.0, length=3.1, width=1.0), id=3, name='A blue mouse', price=25.5, tags=None, warehouseLocation=Row(latitude=54.4, longitude=-32.7))]","title":"Flexible Data Model"},{"location":"MSBD5003/notebooks%20in%20class/sparksql/#converting-between-rdd-and-dataframe","text":"Sample data file at: https://www.cse.ust.hk/msbd5003/data/people.txt # Load a text file and convert each line to a Row. lines = sc . textFile ( \"../data/people.txt\" ) def parse ( l ): a = l . split ( ',' ) return ( a [ 0 ], int ( a [ 1 ])) rdd = lines . map ( parse ) rdd . collect () [('Michael', 29), ('Andy', 30), ('Justin', 19)] # Create the DataFrame from an RDD of tuples, schema is inferred df = spark . createDataFrame ( rdd ) df . printSchema () df . show () root |-- _1: string (nullable = true) |-- _2: long (nullable = true) +-------+---+ | _1| _2| +-------+---+ |Michael| 29| | Andy| 30| | Justin| 19| +-------+---+ # Create the DataFrame from an RDD of tuples with column names, type is inferred df = spark . createDataFrame ( rdd , [ 'name' , 'age' ]) df . printSchema () df . show () root |-- name: string (nullable = true) |-- age: long (nullable = true) +-------+---+ | name|age| +-------+---+ |Michael| 29| | Andy| 30| | Justin| 19| +-------+---+ # Create the DataFrame from an RDD of Rows, type is given in the Row objects from pyspark.sql import Row rdd_rows = rdd . map ( lambda p : Row ( name = p [ 0 ], age = p [ 1 ])) df = spark . createDataFrame ( rdd_rows ) df . printSchema () df . show () root |-- name: string (nullable = true) |-- age: long (nullable = true) +-------+---+ | name|age| +-------+---+ |Michael| 29| | Andy| 30| | Justin| 19| +-------+---+ # Row fields with types incompatible with that of previous rows will be turned into nulls row1 = Row ( name = \"Alice\" , age = 11 ) row2 = Row ( name = \"Bob\" , age = '12' ) rdd_rows = sc . parallelize ([ row1 , row2 ]) df1 = spark . createDataFrame ( rdd_rows ) df1 . show () +-----+----+ | name| age| +-----+----+ |Alice| 11| | Bob|null| +-----+----+ # rdd returns the content as an RDD of Rows teenagers = df . filter ( 'age >= 13 and age <= 19' ) teenNames = teenagers . rdd . map ( lambda p : \"Name: \" + p . name ) teenNames . collect () ['Name: Justin']","title":"Converting between RDD and DataFrame"},{"location":"MSBD5003/notebooks%20in%20class/sparksql/#note","text":"DataFrames are stored using columnar storage with compression RDDs are stored using row storage without compression The RDD view of DataFrame just provides an interface, the Row objects are constructed on the fly and do not necessarily represent the internal storage format of the data","title":"Note:"},{"location":"MSBD5003/notebooks%20in%20class/sparksql/#closure-in-dataframes","text":"data = range ( 10 ) df = spark . createDataFrame ( zip ( data , data )) df . printSchema () df . show () root |-- _1: long (nullable = true) |-- _2: long (nullable = true) +---+---+ | _1| _2| +---+---+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| | 5| 5| | 6| 6| | 7| 7| | 8| 8| | 9| 9| +---+---+ # The 'closure' behaviour in RDD doesn't seem to exist for DataFrames x = 5 df1 = df . filter ( df . _1 < x ) df1 . show () x = 3 df1 . show () +---+---+ | _1| _2| +---+---+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| +---+---+ +---+---+ | _1| _2| +---+---+ | 0| 0| | 1| 1| | 2| 2| | 3| 3| | 4| 4| +---+---+ # Because of the Catalyst optimizer ! df1 . explain () == Physical Plan == *(1) Filter (isnotnull(_1#1265L) AND (_1#1265L < 5)) +- *(1) Scan ExistingRDD[_1#1265L,_2#1266L] def f (): return x / 2 x = 5 df1 = df . select ( df . _1 * 2 + f () + 1 + 1 ) df1 . explain () df1 . show () == Physical Plan == *(1) Project [(((cast((_1#1265L * 2) as double) + 2.5) + 1.0) + 1.0) AS ((((_1 * 2) + 2.5) + 1) + 1)#1296] +- *(1) Scan ExistingRDD[_1#1265L,_2#1266L] +----------------------------+ |((((_1 * 2) + 2.5) + 1) + 1)| +----------------------------+ | 4.5| | 6.5| | 8.5| | 10.5| | 12.5| | 14.5| | 16.5| | 18.5| | 20.5| | 22.5| +----------------------------+ rdd = sc . parallelize ( range ( 10 )) x = 5 a = rdd . filter ( lambda z : z < x ) print ( a . take ( 10 )) x = 3 print ( a . take ( 10 )) [0, 1, 2, 3, 4] [0, 1, 2] counter = 0 def increment_counter ( x ): global counter counter += 1 df . foreach ( increment_counter ) print ( counter ) 0","title":"Closure in DataFrames"},{"location":"MSBD5006/Lecture%201/","text":"Asset returns \u00b6 Let \\(p_t\\) be the price of an asset at time t, and assume no dividend. One-period simple return or simple net return. \\[R_t = \\frac{P_t}{P_t-1} - 1 = \\frac{P_t - P_{t-1}}{P_{t-1}} \\] Gross return \\[ 1 + P_t = \\frac{P_t}{P_{t - 1}}\\ or P_t = P_{t-1}(1+P_t) \\] Multi-period simple return or the k-period simple net return \\[R_t(k) = \\frac{P_t}{P_{t-k}}-1\\] Gross return \\[1 + R_t(k) = \\sum^{k -1 }_{j=0}(1+R_{t-j}) \\] Continues Compounding \u00b6 from math import exp def pay_interest ( base , interest , payments ): return base * ( base + interest / payments ) ** payments def pay_interest_continue ( base , interest , number_of_years ): return base * exp ( interest * number_of_years ) pay_interest_continue(1, 0.1, 1) 1.1051709180756477 \\[ R_t = log \\] Log return \u00b6 \\[r_{pt} = \\sum_{i = 1}{n}w_ir_{it}\\] Excess return \u00b6 \\[Z_t = R_t - R_{0t}, z_t = r_t - r_{0t}\\] where \\(r_{0t}\\) denotes the log return of a reference asset (e.g. risk-free interest rate) such as shortterm U.S. Treasury bill return, etc..","title":"Lecture 1"},{"location":"MSBD5006/Lecture%201/#asset-returns","text":"Let \\(p_t\\) be the price of an asset at time t, and assume no dividend. One-period simple return or simple net return. \\[R_t = \\frac{P_t}{P_t-1} - 1 = \\frac{P_t - P_{t-1}}{P_{t-1}} \\] Gross return \\[ 1 + P_t = \\frac{P_t}{P_{t - 1}}\\ or P_t = P_{t-1}(1+P_t) \\] Multi-period simple return or the k-period simple net return \\[R_t(k) = \\frac{P_t}{P_{t-k}}-1\\] Gross return \\[1 + R_t(k) = \\sum^{k -1 }_{j=0}(1+R_{t-j}) \\]","title":"Asset returns"},{"location":"MSBD5006/Lecture%201/#continues-compounding","text":"from math import exp def pay_interest ( base , interest , payments ): return base * ( base + interest / payments ) ** payments def pay_interest_continue ( base , interest , number_of_years ): return base * exp ( interest * number_of_years ) pay_interest_continue(1, 0.1, 1) 1.1051709180756477 \\[ R_t = log \\]","title":"Continues Compounding"},{"location":"MSBD5006/Lecture%201/#log-return","text":"\\[r_{pt} = \\sum_{i = 1}{n}w_ir_{it}\\]","title":"Log return"},{"location":"MSBD5006/Lecture%201/#excess-return","text":"\\[Z_t = R_t - R_{0t}, z_t = r_t - r_{0t}\\] where \\(r_{0t}\\) denotes the log return of a reference asset (e.g. risk-free interest rate) such as shortterm U.S. Treasury bill return, etc..","title":"Excess return"},{"location":"MSBD5006/Lecture%202/","text":"Mean and variance: \u03bcx = E(X) and \u03c3x2 = Var(X) = E(X \u2212 \u03bcx)2 Skewness (symmetry) and kurtosis (fat-tails) Kurtosis: How high is the p High kurtosis implies heavy (or long) tails in dis- tribution. Symmetry has important implications in holding short or long financial positions and in risk man- agement. (X \u2212 \u03bcx)3 (X \u2212 \u03bcx)4 S ( x ) = E \u03c3 x3 , K ( x ) = E \u03c3 x4 . Normal distribution \u00b6 E(X) = \u03bc Var(X) = \u03c32 S(X) = 0 K(X) = 3 ml = 0, for l is odd. T-distribution \u00b6 Symmetry at 0 \\[E(x) > 0, v >1\\] Chi-squared distribution \u00b6 \\[E(X) = k$$ $$Var(X) = 2k\\] Joint Distribution \u00b6 \\[F_{X,Y}(x, y) = P(X\\leq x, Y\\leq y)\\] Marginal Distribution \u00b6 The marginal distribution of X is obtained by integrating out Y . A similar definition applies to the marginal distribution of Y .","title":"Lecture 2"},{"location":"MSBD5006/Lecture%202/#normal-distribution","text":"E(X) = \u03bc Var(X) = \u03c32 S(X) = 0 K(X) = 3 ml = 0, for l is odd.","title":"Normal distribution"},{"location":"MSBD5006/Lecture%202/#t-distribution","text":"Symmetry at 0 \\[E(x) > 0, v >1\\]","title":"T-distribution"},{"location":"MSBD5006/Lecture%202/#chi-squared-distribution","text":"\\[E(X) = k$$ $$Var(X) = 2k\\]","title":"Chi-squared distribution"},{"location":"MSBD5006/Lecture%202/#joint-distribution","text":"\\[F_{X,Y}(x, y) = P(X\\leq x, Y\\leq y)\\]","title":"Joint Distribution"},{"location":"MSBD5006/Lecture%202/#marginal-distribution","text":"The marginal distribution of X is obtained by integrating out Y . A similar definition applies to the marginal distribution of Y .","title":"Marginal Distribution"},{"location":"MSBD5008/Lecture2/","text":"Undirected vs Directed Networks \u00b6 Undirected graph Links: undirected Example Collaborations Friendship on Facebook Directed Links: Directed Example Phone calls (If you want to represent the relationship between caller and callee, then you can use undirected graph) Following on Twitter Connectivity of graphs \u00b6 Undirected graph \u00b6 Connected: Any two vertices can be joined by a path A disconnected graph is made up of two or more connected components. Bridge edge: If we erase it, the graph becomes disconnected. Articulation point: If we erase it, the graph becomes disconnected Isolated node: Node without any connected neighbors. Directed graph \u00b6 Strongly connected directed graph: Has a path from each node to every other node and vice versa Weakly connected directed graph: Is connected if we disregard the edge directions Directed Acyclic Graph: Has no cyclesL if u can reach v, the v cannot reach u. Strongly connected components: is a set of nodes S so that: Every pair of nodes in S can reach each other There is no larger set containing S with this property Node degree: The number of edges adjacent to node i Avg. degree: \\(\\hat{k} = \\frac{1}{N}\\) , \\(K_i = \\frac{2E}{N}\\) In directed networks we define an in-degree and out-degree . The total degree of a node is the sum of in-and out-degrees. Complete Graph \u00b6 Maximum number edges in undirected graph on N nodes. \\[ E_{max} = \\frac{N(N-1)}{2} \\] A graph with the number of edges $E = E_{max} $ is a complete graph and its average degree is N - 1 Unweighted graph \u00b6 Graph without weight. For example, Friendship and sex. Weighted graph \u00b6 Graph with weight. For example collaboration with internet roads. Self-edges (Self-loops) \u00b6 Examples: Proteins, Hyperlinks (A web page link which points to itself) Multigraph \u00b6 Examples: Communication, collaboration Examples \u00b6 Example Type www Directed Facebook friendships Undirected, unweighted Citation networks Unweighted, directed, acyclic Collaboration networks Undirected multigraph or weighted graph Mobile phone calls directed Protein interactions Undirected, unweighted with self-interactions Bipartite Graph \u00b6 A graph whose nodes can be divided into two disjoint sets U and V such that every link connects a node in U to one in V; that is U and V are independent sets. Examples \u00b6 Authors-to-papers Actors-to-movies Degree distribution P(k): \u00b6 Probability that a randomly chosen node has degree k. Normalized histogram: \u00b6 \\(P(k) = N_k / N\\) Distance \u00b6 Between a pair of nodes is defined as the number of edges along the shortest path connecting the nodes In directed graphs paths need to follow the direction of the arrows. So that distance is not symmetric \\(h_{a,c} \\neq h_{c,a}\\) Diameter \u00b6 The maximum distance (shortest graph) between any pair of nodes in a graph. Clustering coefficient \u00b6 \\( \\(C_i = \\frac{2e_i}{k_i(k_i - 1)}\\) \\) where \\(e_i\\) is the number of edges between the neighbors of node i. Web as a graph \u00b6 Nodes = Web pages Edges = hyperlinks Side issues \u00b6 Dynamic pages created on the fly Dark matter - inaccessible database generated pages","title":"Lecture2"},{"location":"MSBD5008/Lecture2/#undirected-vs-directed-networks","text":"Undirected graph Links: undirected Example Collaborations Friendship on Facebook Directed Links: Directed Example Phone calls (If you want to represent the relationship between caller and callee, then you can use undirected graph) Following on Twitter","title":"Undirected vs Directed Networks"},{"location":"MSBD5008/Lecture2/#connectivity-of-graphs","text":"","title":"Connectivity of graphs"},{"location":"MSBD5008/Lecture2/#undirected-graph","text":"Connected: Any two vertices can be joined by a path A disconnected graph is made up of two or more connected components. Bridge edge: If we erase it, the graph becomes disconnected. Articulation point: If we erase it, the graph becomes disconnected Isolated node: Node without any connected neighbors.","title":"Undirected graph"},{"location":"MSBD5008/Lecture2/#directed-graph","text":"Strongly connected directed graph: Has a path from each node to every other node and vice versa Weakly connected directed graph: Is connected if we disregard the edge directions Directed Acyclic Graph: Has no cyclesL if u can reach v, the v cannot reach u. Strongly connected components: is a set of nodes S so that: Every pair of nodes in S can reach each other There is no larger set containing S with this property Node degree: The number of edges adjacent to node i Avg. degree: \\(\\hat{k} = \\frac{1}{N}\\) , \\(K_i = \\frac{2E}{N}\\) In directed networks we define an in-degree and out-degree . The total degree of a node is the sum of in-and out-degrees.","title":"Directed graph"},{"location":"MSBD5008/Lecture2/#complete-graph","text":"Maximum number edges in undirected graph on N nodes. \\[ E_{max} = \\frac{N(N-1)}{2} \\] A graph with the number of edges $E = E_{max} $ is a complete graph and its average degree is N - 1","title":"Complete Graph"},{"location":"MSBD5008/Lecture2/#unweighted-graph","text":"Graph without weight. For example, Friendship and sex.","title":"Unweighted graph"},{"location":"MSBD5008/Lecture2/#weighted-graph","text":"Graph with weight. For example collaboration with internet roads.","title":"Weighted graph"},{"location":"MSBD5008/Lecture2/#self-edges-self-loops","text":"Examples: Proteins, Hyperlinks (A web page link which points to itself)","title":"Self-edges (Self-loops)"},{"location":"MSBD5008/Lecture2/#multigraph","text":"Examples: Communication, collaboration","title":"Multigraph"},{"location":"MSBD5008/Lecture2/#examples","text":"Example Type www Directed Facebook friendships Undirected, unweighted Citation networks Unweighted, directed, acyclic Collaboration networks Undirected multigraph or weighted graph Mobile phone calls directed Protein interactions Undirected, unweighted with self-interactions","title":"Examples"},{"location":"MSBD5008/Lecture2/#bipartite-graph","text":"A graph whose nodes can be divided into two disjoint sets U and V such that every link connects a node in U to one in V; that is U and V are independent sets.","title":"Bipartite Graph"},{"location":"MSBD5008/Lecture2/#examples_1","text":"Authors-to-papers Actors-to-movies","title":"Examples"},{"location":"MSBD5008/Lecture2/#degree-distribution-pk","text":"Probability that a randomly chosen node has degree k.","title":"Degree distribution P(k):"},{"location":"MSBD5008/Lecture2/#normalized-histogram","text":"\\(P(k) = N_k / N\\)","title":"Normalized histogram:"},{"location":"MSBD5008/Lecture2/#distance","text":"Between a pair of nodes is defined as the number of edges along the shortest path connecting the nodes In directed graphs paths need to follow the direction of the arrows. So that distance is not symmetric \\(h_{a,c} \\neq h_{c,a}\\)","title":"Distance"},{"location":"MSBD5008/Lecture2/#diameter","text":"The maximum distance (shortest graph) between any pair of nodes in a graph.","title":"Diameter"},{"location":"MSBD5008/Lecture2/#clustering-coefficient","text":"\\( \\(C_i = \\frac{2e_i}{k_i(k_i - 1)}\\) \\) where \\(e_i\\) is the number of edges between the neighbors of node i.","title":"Clustering coefficient"},{"location":"MSBD5008/Lecture2/#web-as-a-graph","text":"Nodes = Web pages Edges = hyperlinks","title":"Web as a graph"},{"location":"MSBD5008/Lecture2/#side-issues","text":"Dynamic pages created on the fly Dark matter - inaccessible database generated pages","title":"Side issues"},{"location":"MSBD5008/Lecture3/","text":"Representation Learning on networks \u00b6 Why Is It Hard? \u00b6 Modern deep learning toolbox is designed for simple sequences or grids. But networks are far more complex! Complex topographical structure (i.e., NO Spatial locality like grids) Node embeddings \u00b6 Goal is to encode nodes so that similarity in the embedding space (e.g., dot product) approximates similarity in the original network. Setup \u00b6 Assume we have a graph G: V is the vertex set. A is the adjacency matrix (assume binary). No node features or extra information is used! \u201cShallow\u201d Encoding \u00b6 Simplest encoding approach: each node is assigned a unique embedding vector. We will focus on shallow encoding in this section... After the break we will discuss more encoders based on deep neural networks. How to Define Node Similarity? \u00b6 Key distinction between \u201cshallow\u201d methods is how they define node similarity. E.g., should two nodes have similar embeddings if they - are connected? - share neighbors - have similar \u201cstructural roles\u201d? Adjacency-based Similarity \u00b6 Similarity function is just the edge weight between u and v in the original network. Intuition: Dot products between node embeddings approximate edge existence. Drawbacks \u00b6 \\(O(V^2)\\) runtime \\(O(V)\\) paramters Only sonsiders direct, local connections Multihop Similarity \u00b6 Idea: Consider k-hop node neighbors Issues \u00b6 Expensive: Generally \\(O(|V^2|)\\) , since we need to iterate over all pairs of nodes. Brittle: Must hand-design deterministic node similarity measures. Massive parameter space: \\(O(|V|)\\) parameters Randome walk Method \u00b6 Estimate probability of visiting node \\(v\\) on a random walk starting from node \\(u\\) using some random walk strategy \\(R\\) . Optimize embeddings to encode these random walk statistics. Why Random Walks? \u00b6 Expressivity: Flexible stochastic definition of node similarity that incorporates both local and higher- order neighborhood information. Efficiency: Do not need to consider all node pairs when training; only need to consider pairs that co-occur on random walks. Random Walk Optimization \u00b6 Run short random walks starting from each node on the graph using some strategy For each node wu collect \\(N_R(U)\\) , the multiset\u201d of nodes visited on random walks starting rom \\(uw\\) . Optimize embeddings to according to \\[L = \\sum_{u\\in V}\\sum_{v \\in NR(U)} - log(P(v |Z_u))\\] (1) Sum over all nodes u (2) sum over nodes v seen on random walks starting from u (3) Predicted probability if u and v co-occuring on random walk Optimizing random walk embeddings = Finding embeddings \\(Z_u\\) that minimize L Graph neural networks \u00b6 From Shallow to Deep \u00b6 Limitations of shallow encoding: O(|V|) parameters are needed: there no parameter sharing and every node has its own unique embedding vector. Inherently \u201ctransductive\u2019: It is impossible to generate embeddings for nodes that were not seen during training. Do not incorporate node features: Many graphs have features that we can and should leverage. Setup \u00b6 Assume we have a graph G: V is the vertex set. A is the adjacency matrix (assume binary). \\(X\\in R^{m \\times |v|}\\) is a matrix of node features Categorical attributes, text, image data \u2014 E.g., profile information in a social network. Node degrees, clustering coefficients, etc. \" Indicator vectors (i.e., one-hot encoding of each node) Neighborhood aggregation \u00b6 Generate node embeddings based on local neighborhoods NetworkX \u00b6 !pip install networkx Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (2.5) Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx) (4.4.2)","title":"Lecture3"},{"location":"MSBD5008/Lecture3/#representation-learning-on-networks","text":"","title":"Representation Learning on networks"},{"location":"MSBD5008/Lecture3/#why-is-it-hard","text":"Modern deep learning toolbox is designed for simple sequences or grids. But networks are far more complex! Complex topographical structure (i.e., NO Spatial locality like grids)","title":"Why Is It Hard?"},{"location":"MSBD5008/Lecture3/#node-embeddings","text":"Goal is to encode nodes so that similarity in the embedding space (e.g., dot product) approximates similarity in the original network.","title":"Node embeddings"},{"location":"MSBD5008/Lecture3/#setup","text":"Assume we have a graph G: V is the vertex set. A is the adjacency matrix (assume binary). No node features or extra information is used!","title":"Setup"},{"location":"MSBD5008/Lecture3/#shallow-encoding","text":"Simplest encoding approach: each node is assigned a unique embedding vector. We will focus on shallow encoding in this section... After the break we will discuss more encoders based on deep neural networks.","title":"\u201cShallow\u201d Encoding"},{"location":"MSBD5008/Lecture3/#how-to-define-node-similarity","text":"Key distinction between \u201cshallow\u201d methods is how they define node similarity. E.g., should two nodes have similar embeddings if they - are connected? - share neighbors - have similar \u201cstructural roles\u201d?","title":"How to Define Node Similarity?"},{"location":"MSBD5008/Lecture3/#adjacency-based-similarity","text":"Similarity function is just the edge weight between u and v in the original network. Intuition: Dot products between node embeddings approximate edge existence.","title":"Adjacency-based Similarity"},{"location":"MSBD5008/Lecture3/#drawbacks","text":"\\(O(V^2)\\) runtime \\(O(V)\\) paramters Only sonsiders direct, local connections","title":"Drawbacks"},{"location":"MSBD5008/Lecture3/#multihop-similarity","text":"Idea: Consider k-hop node neighbors","title":"Multihop Similarity"},{"location":"MSBD5008/Lecture3/#issues","text":"Expensive: Generally \\(O(|V^2|)\\) , since we need to iterate over all pairs of nodes. Brittle: Must hand-design deterministic node similarity measures. Massive parameter space: \\(O(|V|)\\) parameters","title":"Issues"},{"location":"MSBD5008/Lecture3/#randome-walk-method","text":"Estimate probability of visiting node \\(v\\) on a random walk starting from node \\(u\\) using some random walk strategy \\(R\\) . Optimize embeddings to encode these random walk statistics.","title":"Randome walk Method"},{"location":"MSBD5008/Lecture3/#why-random-walks","text":"Expressivity: Flexible stochastic definition of node similarity that incorporates both local and higher- order neighborhood information. Efficiency: Do not need to consider all node pairs when training; only need to consider pairs that co-occur on random walks.","title":"Why Random Walks?"},{"location":"MSBD5008/Lecture3/#random-walk-optimization","text":"Run short random walks starting from each node on the graph using some strategy For each node wu collect \\(N_R(U)\\) , the multiset\u201d of nodes visited on random walks starting rom \\(uw\\) . Optimize embeddings to according to \\[L = \\sum_{u\\in V}\\sum_{v \\in NR(U)} - log(P(v |Z_u))\\] (1) Sum over all nodes u (2) sum over nodes v seen on random walks starting from u (3) Predicted probability if u and v co-occuring on random walk Optimizing random walk embeddings = Finding embeddings \\(Z_u\\) that minimize L","title":"Random Walk Optimization"},{"location":"MSBD5008/Lecture3/#graph-neural-networks","text":"","title":"Graph neural networks"},{"location":"MSBD5008/Lecture3/#from-shallow-to-deep","text":"Limitations of shallow encoding: O(|V|) parameters are needed: there no parameter sharing and every node has its own unique embedding vector. Inherently \u201ctransductive\u2019: It is impossible to generate embeddings for nodes that were not seen during training. Do not incorporate node features: Many graphs have features that we can and should leverage.","title":"From Shallow to Deep"},{"location":"MSBD5008/Lecture3/#setup_1","text":"Assume we have a graph G: V is the vertex set. A is the adjacency matrix (assume binary). \\(X\\in R^{m \\times |v|}\\) is a matrix of node features Categorical attributes, text, image data \u2014 E.g., profile information in a social network. Node degrees, clustering coefficients, etc. \" Indicator vectors (i.e., one-hot encoding of each node)","title":"Setup"},{"location":"MSBD5008/Lecture3/#neighborhood-aggregation","text":"Generate node embeddings based on local neighborhoods","title":"Neighborhood aggregation"},{"location":"MSBD5008/Lecture3/#networkx","text":"!pip install networkx Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (2.5) Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx) (4.4.2)","title":"NetworkX"},{"location":"MSBD5012/lectures/Lecture%202/","text":"Divergence \u00b6 ML Setup \\(P(x)\\) -> generate -> Data -> learn \\(Q(x)\\) where Q should as close to P as possible. Entropy, cross entropy, and KL divergence \u00b6 Entropy \u00b6 \\[H(p) = -\\sum_{i}p_{i}log(p_i)\\] Cross Entropy \u00b6 p = true distribution q = predicted distribution \\[H(p, q) = -\\sum_ip_ilog(q_i)\\] Relative entropy or Kullback-Leibler divergence \u00b6 Meassure how much a distribution Q(X) differs from a \"True\" probability distribution P(X) K-L Divergence if Q from P is defined as follows: \\[ KL(P||Q) = \\sum_x{P(X)log(\\frac{P(X)}{Q(X)})} = -log\\sum_x{Q(X)}\\] Relationship between entropy, cross-entropy, and kl divergence $$cross-entropy = entropy + kl divergence $$ \\[or\\] \\[D_{kl}(p||q) = H(p, q) - H(p)\\] Minimize cross entropy = Maximizing log likelyhood Suppose we have likelihood of the training set is \\[\\sum_{i}(probability\\ of\\ i)^{number\\ of\\ occurrences\\ of\\ i} = \\sum_{i}q_i^{Np_i}\\] where N is number of conditionally independent samples in training set So the log-likelihood divided by N is \\[\\frac{1}{N}log\\sum_iq_i^{Np_i} = \\sum_ip_ilog(q_i) = -H(p, q)\\] Supervised learning \u00b6 Unsupervised learning \u00b6 Multual information \u00b6 H(x): Initial uncertainty about x H(X | Y): Expected uncertainty about x if y is tested Linear Regression \u00b6 \\[y = w_0 + w_1x_1\\] Least Square Regression \u00b6 import matplotlib.pyplot as plt import numpy as np x1 = [ 0 , 0 , 1 , 1 ] x2 = [ 0 , 1 , 0 , 1 ] y = [ 1 , 0 , 0 , 1 ] fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( x1 , x2 , y ) <mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f2ec3a9ae80> using sklearn from sklearn import linear_model X = np . column_stack (( x1 , x2 )) lm = linear_model . LinearRegression () model = lm . fit ( X , y ) print ( model . coef_ ) print ( model . intercept_ ) print ( model . score ( X , y )) [0.00000000e+00 2.22044605e-16] 0.4999999999999999 0.0 using numpy \\( \\(A = (X^TX)^{-1}X^TY\\) \\) ones = [1 for i in range(len(x1))] X = np.column_stack((ones, x1, x2)) X_T = X.transpose() print(X) print(X_T) [[1 0 1] [1 0 1] [1 1 1] [1 1 0] [1 1 0]] [[1 1 1 1 1] [0 0 1 1 1] [1 1 1 0 0]] dot = np.dot(X_T, X) inverse = np.linalg.inv(dot) print(dot) print(inverse) [[5 3 3] [3 3 1] [3 1 3]] [[ 2. -1.5 -1.5] [-1.5 1.5 1. ] [-1.5 1. 1.5]] dot2 = np.dot(inverse, X_T).dot(y) dot2 array([ 2. , 1.5, -1.5]) Mean Square Error \u00b6 import numpy as np # Given values Y_true = [ 1 , 1 , 2 , 2 , 4 ] # Y_true = Y (original values) # Calculated values Y_pred = [ 0.6 , 1.29 , 1.99 , 2.69 , 3.4 ] # Y_pred = Y' # Mean Squared Error MSE = np . square ( np . subtract ( Y_true , Y_pred )) . mean () MSE 0.21606 Hypothesis space \u00b6 Is the set of functioins that it is allowed to select as being the solution. Thhe size of the hypothesis space is called the capacity of the model. For polynomial regression, the larger the d, the higher the model capacity. Higher model capacity implies better fit to training data. \\(S_1 = \\{y = w_0 + w_1x_1 | w_0, w_1 \\in R\\}\\) \\(S_2 = \\{y=w_0 + w_1x_1 + w_2x_1^2 + w_3x_1^3 | w_0, w_1, w_2, w_3 \\in R\\}\\) Generalization Error \u00b6 Model select: Validation Split training data into two parts. One part for training and second part for validation. This has to be randomly split. Regularization Regilarization \u00b6 ridge regression \u00b6 The larger the regularization constant \\(\\lambda\\) , the smaller the weights","title":"Lecture 2"},{"location":"MSBD5012/lectures/Lecture%202/#divergence","text":"ML Setup \\(P(x)\\) -> generate -> Data -> learn \\(Q(x)\\) where Q should as close to P as possible.","title":"Divergence"},{"location":"MSBD5012/lectures/Lecture%202/#entropy-cross-entropy-and-kl-divergence","text":"","title":"Entropy, cross entropy, and KL divergence"},{"location":"MSBD5012/lectures/Lecture%202/#entropy","text":"\\[H(p) = -\\sum_{i}p_{i}log(p_i)\\]","title":"Entropy"},{"location":"MSBD5012/lectures/Lecture%202/#cross-entropy","text":"p = true distribution q = predicted distribution \\[H(p, q) = -\\sum_ip_ilog(q_i)\\]","title":"Cross Entropy"},{"location":"MSBD5012/lectures/Lecture%202/#relative-entropy-or-kullback-leibler-divergence","text":"Meassure how much a distribution Q(X) differs from a \"True\" probability distribution P(X) K-L Divergence if Q from P is defined as follows: \\[ KL(P||Q) = \\sum_x{P(X)log(\\frac{P(X)}{Q(X)})} = -log\\sum_x{Q(X)}\\] Relationship between entropy, cross-entropy, and kl divergence $$cross-entropy = entropy + kl divergence $$ \\[or\\] \\[D_{kl}(p||q) = H(p, q) - H(p)\\] Minimize cross entropy = Maximizing log likelyhood Suppose we have likelihood of the training set is \\[\\sum_{i}(probability\\ of\\ i)^{number\\ of\\ occurrences\\ of\\ i} = \\sum_{i}q_i^{Np_i}\\] where N is number of conditionally independent samples in training set So the log-likelihood divided by N is \\[\\frac{1}{N}log\\sum_iq_i^{Np_i} = \\sum_ip_ilog(q_i) = -H(p, q)\\]","title":"Relative entropy or Kullback-Leibler divergence"},{"location":"MSBD5012/lectures/Lecture%202/#supervised-learning","text":"","title":"Supervised learning"},{"location":"MSBD5012/lectures/Lecture%202/#unsupervised-learning","text":"","title":"Unsupervised learning"},{"location":"MSBD5012/lectures/Lecture%202/#multual-information","text":"H(x): Initial uncertainty about x H(X | Y): Expected uncertainty about x if y is tested","title":"Multual information"},{"location":"MSBD5012/lectures/Lecture%202/#linear-regression","text":"\\[y = w_0 + w_1x_1\\]","title":"Linear Regression"},{"location":"MSBD5012/lectures/Lecture%202/#least-square-regression","text":"import matplotlib.pyplot as plt import numpy as np x1 = [ 0 , 0 , 1 , 1 ] x2 = [ 0 , 1 , 0 , 1 ] y = [ 1 , 0 , 0 , 1 ] fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( x1 , x2 , y ) <mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f2ec3a9ae80> using sklearn from sklearn import linear_model X = np . column_stack (( x1 , x2 )) lm = linear_model . LinearRegression () model = lm . fit ( X , y ) print ( model . coef_ ) print ( model . intercept_ ) print ( model . score ( X , y )) [0.00000000e+00 2.22044605e-16] 0.4999999999999999 0.0 using numpy \\( \\(A = (X^TX)^{-1}X^TY\\) \\) ones = [1 for i in range(len(x1))] X = np.column_stack((ones, x1, x2)) X_T = X.transpose() print(X) print(X_T) [[1 0 1] [1 0 1] [1 1 1] [1 1 0] [1 1 0]] [[1 1 1 1 1] [0 0 1 1 1] [1 1 1 0 0]] dot = np.dot(X_T, X) inverse = np.linalg.inv(dot) print(dot) print(inverse) [[5 3 3] [3 3 1] [3 1 3]] [[ 2. -1.5 -1.5] [-1.5 1.5 1. ] [-1.5 1. 1.5]] dot2 = np.dot(inverse, X_T).dot(y) dot2 array([ 2. , 1.5, -1.5])","title":"Least Square Regression"},{"location":"MSBD5012/lectures/Lecture%202/#mean-square-error","text":"import numpy as np # Given values Y_true = [ 1 , 1 , 2 , 2 , 4 ] # Y_true = Y (original values) # Calculated values Y_pred = [ 0.6 , 1.29 , 1.99 , 2.69 , 3.4 ] # Y_pred = Y' # Mean Squared Error MSE = np . square ( np . subtract ( Y_true , Y_pred )) . mean () MSE 0.21606","title":"Mean Square Error"},{"location":"MSBD5012/lectures/Lecture%202/#hypothesis-space","text":"Is the set of functioins that it is allowed to select as being the solution. Thhe size of the hypothesis space is called the capacity of the model. For polynomial regression, the larger the d, the higher the model capacity. Higher model capacity implies better fit to training data. \\(S_1 = \\{y = w_0 + w_1x_1 | w_0, w_1 \\in R\\}\\) \\(S_2 = \\{y=w_0 + w_1x_1 + w_2x_1^2 + w_3x_1^3 | w_0, w_1, w_2, w_3 \\in R\\}\\)","title":"Hypothesis space"},{"location":"MSBD5012/lectures/Lecture%202/#generalization-error","text":"Model select: Validation Split training data into two parts. One part for training and second part for validation. This has to be randomly split. Regularization","title":"Generalization Error"},{"location":"MSBD5012/lectures/Lecture%202/#regilarization","text":"","title":"Regilarization"},{"location":"MSBD5012/lectures/Lecture%202/#ridge-regression","text":"The larger the regularization constant \\(\\lambda\\) , the smaller the weights","title":"ridge regression"},{"location":"MSBD5012/lectures/Lecture1/","text":"Likehood possibility \u00b6 \\[L(H|E) = P(E|H) \\] import matplotlib.pyplot as plt import numpy as np import scipy.stats mu = 3.0 sigma = 0.5 data = np . random . randn ( 100000 ) * sigma + mu hx , hy , _ = plt . hist ( data , bins = 50 , color = \"lightblue\" ) plt . ylim ( 0 . 0 , max ( hx ) + 0 . 05 ) plt . title ( r 'Normal distribution $\\mu_0 = 3$ and $\\sigma_0 = 0.5$' ) plt . grid () Calculate the log-likelihood \u00b6 scipy . stats . norm . pdf ( 6 , 2 . 0 , 1 . 0 ) print ( np . log ( scipy . stats . norm . pdf ( data , 2 . 0 , 1 . 0 )). sum () ) x = np . linspace ( - 10 , 10 , 1000 , endpoint = True ) y = [] for i in x : y . append ( np . log ( scipy . stats . norm . pdf ( data , i , 0 . 5 )). sum ()) plt . plot ( x , y ) plt . title ( r 'Log-Likelihood' ) plt . xlabel ( r '$\\mu$' ) plt . grid () -154314.14596206427 print('mean ---> ', np.mean(data)) print('std deviation ---> ', np.std(data)) mean ---> 2.999606326069087 std deviation ---> 0.4991923934863224 y_min = y . index ( max ( y )) print ( 'mean (from max log likelohood) ---> ' , x [ y_min ] ) mean (from max log likelohood) ---> 2.9929929929929937","title":"Lecture1"},{"location":"MSBD5012/lectures/Lecture1/#likehood-possibility","text":"\\[L(H|E) = P(E|H) \\] import matplotlib.pyplot as plt import numpy as np import scipy.stats mu = 3.0 sigma = 0.5 data = np . random . randn ( 100000 ) * sigma + mu hx , hy , _ = plt . hist ( data , bins = 50 , color = \"lightblue\" ) plt . ylim ( 0 . 0 , max ( hx ) + 0 . 05 ) plt . title ( r 'Normal distribution $\\mu_0 = 3$ and $\\sigma_0 = 0.5$' ) plt . grid ()","title":"Likehood possibility"},{"location":"MSBD5012/lectures/Lecture1/#calculate-the-log-likelihood","text":"scipy . stats . norm . pdf ( 6 , 2 . 0 , 1 . 0 ) print ( np . log ( scipy . stats . norm . pdf ( data , 2 . 0 , 1 . 0 )). sum () ) x = np . linspace ( - 10 , 10 , 1000 , endpoint = True ) y = [] for i in x : y . append ( np . log ( scipy . stats . norm . pdf ( data , i , 0 . 5 )). sum ()) plt . plot ( x , y ) plt . title ( r 'Log-Likelihood' ) plt . xlabel ( r '$\\mu$' ) plt . grid () -154314.14596206427 print('mean ---> ', np.mean(data)) print('std deviation ---> ', np.std(data)) mean ---> 2.999606326069087 std deviation ---> 0.4991923934863224 y_min = y . index ( max ( y )) print ( 'mean (from max log likelohood) ---> ' , x [ y_min ] ) mean (from max log likelohood) ---> 2.9929929929929937","title":"Calculate the log-likelihood"},{"location":"MSBD5012/lectures/Lecture3/","text":"Homework \u00b6 http://home.cse.ust.hk/~lzhang/teach/msbd5012/ Programming assignment (2020.10.10) Writting assignment (2020.10.03) Logestic regression \u00b6 Gradient Descent \u00b6 https://en.wikipedia.org/wiki/Gradient_descent#:~:text=Gradient%20descent%20is%20a%20first,function%20at%20the%20current%20point. \\[J'(w) = \\frac{dJ(w)}{dw} = lim_{e \\to 0}\\frac{J(w+e) - l(w)}{E}$$ $$J(w + e) = J(w) + eJ'(w)\\] from sklearn.linear_model import SGDRegressor import matplotlib.pyplot as plt import numpy as np x0 = np . array ([ 1 , 1 , 1 , 1 ]) x1 = np . array ([ 0 , 0 , 1 , 1 ]) x2 = np . array ([ 0 , 1 , 0 , 1 ]) x3 = x1 * x2 y = [ 1 , 0 , 0 , 1 ] colors = [ 'red' if i == 0 else 'blue' for i in y ] X = np . column_stack (( x1 , x2 )) # X = np.column_stack((x0, x1, x2, x3)) plt . scatter ( x1 , x2 , color = colors ) <matplotlib.collections.PathCollection at 0x7faa3604e198> import statsmodels.api as sm print ( X ) logit_model = sm . Logit ( y , X ) result = logit_model . fit () print ( result . summary2 ()) [[0 0] [0 1] [1 0] [1 1]] Optimization terminated successfully. Current function value: 0.693147 Iterations 1 Results: Logit ============================================================== Model: Logit Pseudo R-squared: 0.000 Dependent Variable: y AIC: 9.5452 Date: 2020-10-03 07:41 BIC: 8.3178 No. Observations: 4 Log-Likelihood: -2.7726 Df Model: 1 LL-Null: -2.7726 Df Residuals: 2 LLR p-value: 1.0000 Converged: 1.0000 Scale: 1.0000 No. Iterations: 1.0000 ----------------------------------------------------------------- Coef. Std.Err. z P>|z| [0.025 0.975] ----------------------------------------------------------------- x1 0.0000 1.6330 0.0000 1.0000 -3.2006 3.2006 x2 0.0000 1.6330 0.0000 1.0000 -3.2006 3.2006 ============================================================== predictions = model.predict(X) predictions array([0, 0, 0, 0]) Batch gradient descent \u00b6 https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/#:~:text=Batch%20gradient%20descent%20is%20a,is%20called%20a%20training%20epoch. \\[w_j \\leftarrow w_j + \\alpha \\frac{1}{N}\\sum^{N}_{i=1}[y_i - \\sigma (w^Tx_i)]x_{i,j} \\] \\(y_i\\) : Observed output \\(\\sigma (w^Tx_i)\\) Predicted output \\(x_{i, j}\\) input import math import numpy as np def sigmoid ( x ): return 1 / ( 1 + math . exp ( - x )) a = 0.1 x_1 = [ 1, 1, 1, 1 ] x_2 = [ 0, 0, 1, 1 ] x_3 = [ 0, 1, 0 ,1 ] w_1 = - 2 w_2 = 1 w_3 = 1 y = [ 1, 0, 0, 1 ] def compute ( x , w_i ) : sum_i = 0 n = len ( x ) for i in range ( n ) : predicted_output = w_1 + w_2 * x_2 [ i ] + w_3 * x_3 [ i ] temp = ( y [ i ] - sigmoid ( predicted_output )) * x [ i ] sum_i += temp result = w_i + a * sum_i / n return result print ( compute ( x_1 , w_1 )) print ( compute ( x_2 , w_2 )) print ( compute ( x_3 , w_3 )) -1.9789271441190528 1.00577646446575 1.00577646446575 for i in range ( 100 ): w_1 = compute ( x_1 , w_1 ) w_2 = compute ( x_2 , w_2 ) w_3 = compute ( x_3 , w_3 ) err1 = w_1 + w_2 * x_2 [ 0 ] + w_3 * x_3 [ 0 ] err2 = w_1 + w_2 * x_2 [ 1 ] + w_3 * x_3 [ 1 ] err3 = w_1 + w_2 * x_2 [ 2 ] + w_3 * x_3 [ 2 ] err4 = w_1 + w_2 * x_2 [ 3 ] + w_3 * x_3 [ 3 ] ( err1 + err2 + err3 + err4 ) / 4 -0.07952691518172225 Stochastic Gradient descent \u00b6 https://en.wikipedia.org/wiki/Stochastic_gradient_descent Newton's method \u00b6 Adavantages \u00b6 If we only do the 1st order, then we will get overestimate, but with second order estimate, we will correct the overestimate. Softmax Regression \u00b6","title":"Lecture3"},{"location":"MSBD5012/lectures/Lecture3/#homework","text":"http://home.cse.ust.hk/~lzhang/teach/msbd5012/ Programming assignment (2020.10.10) Writting assignment (2020.10.03)","title":"Homework"},{"location":"MSBD5012/lectures/Lecture3/#logestic-regression","text":"","title":"Logestic regression"},{"location":"MSBD5012/lectures/Lecture3/#gradient-descent","text":"https://en.wikipedia.org/wiki/Gradient_descent#:~:text=Gradient%20descent%20is%20a%20first,function%20at%20the%20current%20point. \\[J'(w) = \\frac{dJ(w)}{dw} = lim_{e \\to 0}\\frac{J(w+e) - l(w)}{E}$$ $$J(w + e) = J(w) + eJ'(w)\\] from sklearn.linear_model import SGDRegressor import matplotlib.pyplot as plt import numpy as np x0 = np . array ([ 1 , 1 , 1 , 1 ]) x1 = np . array ([ 0 , 0 , 1 , 1 ]) x2 = np . array ([ 0 , 1 , 0 , 1 ]) x3 = x1 * x2 y = [ 1 , 0 , 0 , 1 ] colors = [ 'red' if i == 0 else 'blue' for i in y ] X = np . column_stack (( x1 , x2 )) # X = np.column_stack((x0, x1, x2, x3)) plt . scatter ( x1 , x2 , color = colors ) <matplotlib.collections.PathCollection at 0x7faa3604e198> import statsmodels.api as sm print ( X ) logit_model = sm . Logit ( y , X ) result = logit_model . fit () print ( result . summary2 ()) [[0 0] [0 1] [1 0] [1 1]] Optimization terminated successfully. Current function value: 0.693147 Iterations 1 Results: Logit ============================================================== Model: Logit Pseudo R-squared: 0.000 Dependent Variable: y AIC: 9.5452 Date: 2020-10-03 07:41 BIC: 8.3178 No. Observations: 4 Log-Likelihood: -2.7726 Df Model: 1 LL-Null: -2.7726 Df Residuals: 2 LLR p-value: 1.0000 Converged: 1.0000 Scale: 1.0000 No. Iterations: 1.0000 ----------------------------------------------------------------- Coef. Std.Err. z P>|z| [0.025 0.975] ----------------------------------------------------------------- x1 0.0000 1.6330 0.0000 1.0000 -3.2006 3.2006 x2 0.0000 1.6330 0.0000 1.0000 -3.2006 3.2006 ============================================================== predictions = model.predict(X) predictions array([0, 0, 0, 0])","title":"Gradient Descent"},{"location":"MSBD5012/lectures/Lecture3/#batch-gradient-descent","text":"https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/#:~:text=Batch%20gradient%20descent%20is%20a,is%20called%20a%20training%20epoch. \\[w_j \\leftarrow w_j + \\alpha \\frac{1}{N}\\sum^{N}_{i=1}[y_i - \\sigma (w^Tx_i)]x_{i,j} \\] \\(y_i\\) : Observed output \\(\\sigma (w^Tx_i)\\) Predicted output \\(x_{i, j}\\) input import math import numpy as np def sigmoid ( x ): return 1 / ( 1 + math . exp ( - x )) a = 0.1 x_1 = [ 1, 1, 1, 1 ] x_2 = [ 0, 0, 1, 1 ] x_3 = [ 0, 1, 0 ,1 ] w_1 = - 2 w_2 = 1 w_3 = 1 y = [ 1, 0, 0, 1 ] def compute ( x , w_i ) : sum_i = 0 n = len ( x ) for i in range ( n ) : predicted_output = w_1 + w_2 * x_2 [ i ] + w_3 * x_3 [ i ] temp = ( y [ i ] - sigmoid ( predicted_output )) * x [ i ] sum_i += temp result = w_i + a * sum_i / n return result print ( compute ( x_1 , w_1 )) print ( compute ( x_2 , w_2 )) print ( compute ( x_3 , w_3 )) -1.9789271441190528 1.00577646446575 1.00577646446575 for i in range ( 100 ): w_1 = compute ( x_1 , w_1 ) w_2 = compute ( x_2 , w_2 ) w_3 = compute ( x_3 , w_3 ) err1 = w_1 + w_2 * x_2 [ 0 ] + w_3 * x_3 [ 0 ] err2 = w_1 + w_2 * x_2 [ 1 ] + w_3 * x_3 [ 1 ] err3 = w_1 + w_2 * x_2 [ 2 ] + w_3 * x_3 [ 2 ] err4 = w_1 + w_2 * x_2 [ 3 ] + w_3 * x_3 [ 3 ] ( err1 + err2 + err3 + err4 ) / 4 -0.07952691518172225","title":"Batch gradient descent"},{"location":"MSBD5012/lectures/Lecture3/#stochastic-gradient-descent","text":"https://en.wikipedia.org/wiki/Stochastic_gradient_descent","title":"Stochastic Gradient descent"},{"location":"MSBD5012/lectures/Lecture3/#newtons-method","text":"","title":"Newton's method"},{"location":"MSBD5012/lectures/Lecture3/#adavantages","text":"If we only do the 1st order, then we will get overestimate, but with second order estimate, we will correct the overestimate.","title":"Adavantages"},{"location":"MSBD5012/lectures/Lecture3/#softmax-regression","text":"","title":"Softmax Regression"},{"location":"MSBD5012/lectures/Lecture4/","text":"Empirecal Error \u00b6 Generalization Error \u00b6","title":"Lecture4"},{"location":"MSBD5012/lectures/Lecture4/#empirecal-error","text":"","title":"Empirecal Error"},{"location":"MSBD5012/lectures/Lecture4/#generalization-error","text":"","title":"Generalization Error"}]}