{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lecture 2.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyM6sMYwN/ZnKmof/Pt+kSz0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"m6H4pZyjY7Gf","colab_type":"text"},"source":["# Supervised learning\n","\n","- Supervised larning\n","\n","- Unsupervised learning\n","\n","- Reinforcement learning\n","\n","## Whats is machine learning\n","\n","> A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks."]},{"cell_type":"markdown","metadata":{"id":"tYL2s4wvY-k7","colab_type":"text"},"source":["## Linear Regression\n","\n","- A Part of machine learning\n","\n","- Given training set x, y\n","\n","- Find a good approximation to f: $x \\to y$\n","\n","- Examples:\n","    - Spam detection ( Classification)\n","\n","    - Digit recognition ( Classification)\n","\n","    - House price prediction (Refression)"]},{"cell_type":"markdown","metadata":{"id":"8ZZ1wcKfbiMi","colab_type":"text"},"source":["### Terminology\n","\n","- Given a data point (x, y), x is called featyre vector, y is called label\n","\n","- The dataset given for learning is training data\n","\n","- The dataset to be tested is called testing data"]},{"cell_type":"markdown","metadata":{"id":"E-wD03K7b24P","colab_type":"text"},"source":["### Machine learning 3 steps\n","\n","1. Collect data, extract features\n","2. Determine a model\n","\n","3. Train the model with the data\n"]},{"cell_type":"markdown","metadata":{"id":"WtuyhmU2crQ9","colab_type":"text"},"source":["### Loss\n","\n","> Loss on traning set\n","\n","We measure the error using a loss function $L(y, \\hat{y})$\n","\n","For regression, squared error is often used $$L(y_1, f(x_i)) = (y_i - f(x_i))^2$$\n","\n","> Loss on testing set\n","\n","**Empirical loss** is measuring the loss on the training set\n","\n","We assume both training set and testing set are i.i.d from the same distribution D\n","- Minimizing loss on training set will make loss on testing set small"]},{"cell_type":"markdown","metadata":{"id":"5Tr0-rmcdumn","colab_type":"text"},"source":["### Minimizing loss functions\n","\n","- The minimizers of some loss functions have analytical solutions: an exact solution you can explicitly derive by analyzing the formula.\n","\n","- However, most poular supervised learning models use loss functions with no analytical solution\n","\n","- We use gradient descent to approximate the minimal value of function.\n","\n","- Gradients: A vector, points to the direction where changing value is the fastest.\n","\n","### Method\n","\n","1. For function G, randomly guess an initial value $x_0$\n","\n","2. **Repeat $xi+1 = x_i - r \\times \\nabla G(x)$ where $\\nabla$ denotes the gradients, r denotes learning rate**\n","\n","3. Until convergence"]},{"cell_type":"code","metadata":{"id":"b-uUst4iY56A","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600086783453,"user_tz":-480,"elapsed":1184,"user":{"displayName":"Qiwei Li","photoUrl":"","userId":"09519962397524608991"}},"outputId":"18225429-c6c9-4fb4-d86d-12e516cc87ba"},"source":["from sympy import symbols, diff\n","\n","r = 0.1\n","f_i = (1, 1, 1)\n","x, y, z = symbols('x y z', real=True)\n","f = (y + 2 * x)**2 + y + 2*x\n","g = (diff(f, x), diff(f, y), diff(f, z))\n","G "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8*x + 4*y + 2, 4*x + 2*y + 1, 0)"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"s2zRGR7WkGIb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600086505267,"user_tz":-480,"elapsed":1047,"user":{"displayName":"Qiwei Li","photoUrl":"","userId":"09519962397524608991"}},"outputId":"3545d925-3c14-4451-954e-969483ace8b7"},"source":["import numpy as np\n","result = np.array([8, 6, 3]) * r +np.array([1, 1, 1])\n","result"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.8, 1.6, 1.3])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"qSS5ubHRmtTS","colab_type":"text"},"source":["### Linear Classification\n","\n","- Use a line to separate data points\n","\n","- Use $x = (x_1, x_2)$,  $w = (w_1, w_2)$, i.e., x, w are vectors in 2D space\n","\n","---\n","\n","**Doesn't work well with classification problem**\n","\n","\n","- Label y as either 1 or -1\n","- Find f_w(x) = w^Tx that minimizes the loss function\n","$$L(f_w(x)) = \\frac{1}{n}\\sum_{i=1}^n(w^Tx_i-y_1)^2$$\n","\n","- Find a line that minimizes the distance between red and blue\n","\n","**If there is a outlier in the graph, the seperation line will miss classification some points**\n","\n","- If the value get very large, the $w^TX_i$ is correct $\\to$ large loss value even if predict value is positive.\n","---\n","\n","**Solution:**\n","\n","We use sigmoid function to minimize the value between 0 and 1\n","$$\\sigma(a) = \\frac{1}{1+exp(-a)}$$\n","\n","1. Similar to step functions\n","\n","2. Continuous and easy to compute\n"]},{"cell_type":"markdown","metadata":{"id":"YnEYnlxYrGBk","colab_type":"text"},"source":["#### Some properties of sigmoid function\n","\n"," \n","1. $\\sigma(a) = \\frac{1}{1+exp(-a)} \\in (0, 1)$\n","\n","2. symetric\n","\n","3. Easy to compute gradients"]},{"cell_type":"markdown","metadata":{"id":"qrPQ7h2At8bq","colab_type":"text"},"source":["#### Logistic Regression\n","\n","- Better approach ( cross-entropy loss function) find w that minimizes loss function\n","\n","- If misclassfication happens on i-th data with label 1, $log(\\sigma(w^Tx_i))$ is very large\n","\n","- No analytical solution, needs gradient descent"]},{"cell_type":"markdown","metadata":{"id":"OGUiWWApvXUs","colab_type":"text"},"source":["#### SVM\n","\n","A svm performs classification by finding the hyperplane that maximizes the margin between the two classes"]},{"cell_type":"markdown","metadata":{"id":"NOJW_DITv_ea","colab_type":"text"},"source":["### K-Nearest neighbor methods\n","\n","- Learning algorithm: just store training examples\n","\n","- Prediction algorithm:\n","    - Regression: take the average value of k nearest neighbors\n","    - Classification: assign to the most frequent class of k nearest neighbors\n","\n","- Easy to train with high storage requirement, but high-computation cost at prediction"]},{"cell_type":"markdown","metadata":{"id":"wpaw2lP_w-TP","colab_type":"text"},"source":["---| Linear | knn\n","---| --- | ---\n","Advantages  | Easy to fit | Strong assumtions on linear relationship\n","Disadvantages | Hard to classify the data | Takes a lot of computation power"]},{"cell_type":"markdown","metadata":{"id":"VKXWN2LL0SRk","colab_type":"text"},"source":["### Decision Tree\n","\n","Entropy is used to measure how informative is a probability distribution. The more entropy, the more uncertainty.\n","\n","More [info](https://medium.com/@rishabhjain_22692/decision-trees-it-begins-here-93ff54ef134#:~:text=Entropy%20%3A%20A%20decision%20tree%20is,the%20homogeneity%20of%20a%20sample.&text=The%20entropy%20for%20each%20branch%20is%20calculated.)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2jp_8ICP2sjx","colab_type":"text"},"source":["# Wrap up\n","\n","1. Collect data, extract features\n","\n","2. Determine a model\n","    - Select a good model for your data\n","\n","    \n"]},{"cell_type":"code","metadata":{"id":"bogOav0hliax","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}