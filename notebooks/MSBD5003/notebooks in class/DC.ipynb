{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"DC.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"GhoFnVrA0AZh","executionInfo":{"status":"ok","timestamp":1604888600889,"user_tz":-480,"elapsed":36052,"user":{"displayName":"Qiwei Li","photoUrl":"","userId":"09519962397524608991"}},"outputId":"9bda6b59-2560-4e6b-8a60-46f4e512e050","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install pyspark"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting pyspark\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n","\u001b[K     |████████████████████████████████| 204.2MB 63kB/s \n","\u001b[?25hCollecting py4j==0.10.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n","\u001b[K     |████████████████████████████████| 204kB 47.6MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=bd505a7a219c64c9cf329fc493b951b33584b8631e2c6893141869ec65dcc4d0\n","  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9 pyspark-3.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OEZuuN9-0CMF","executionInfo":{"status":"ok","timestamp":1604888606785,"user_tz":-480,"elapsed":33714,"user":{"displayName":"Qiwei Li","photoUrl":"","userId":"09519962397524608991"}}},"source":["from pyspark.context import SparkContext\n","from pyspark.sql.session import SparkSession\n","sc = SparkContext.getOrCreate()\n","spark = SparkSession(sc)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pwxFvcqLzNqh"},"source":["### Prefix Sums"]},{"cell_type":"code","metadata":{"id":"w4onun53zNqi","outputId":"82c2fb1f-c850-4ba9-8753-0b44f2a9381f"},"source":["x = [1, 4, 3, 5, 6, 7, 0, 1]\n","\n","rdd = sc.parallelize(x, 4).cache()\n","\n","def f(iterator):\n","    yield sum(iterator)\n","\n","sums = rdd.mapPartitions(f).collect()\n","\n","print(sums)\n","\n","for i in range(1, len(sums)):\n","    sums[i] += sums[i-1]\n","\n","print(sums)\n","\n","def g(index, iterator):\n","    global sums\n","    if index == 0:\n","        s = 0\n","    else:\n","        s = sums[index-1]\n","    for i in iterator:\n","        s += i\n","        yield s\n","\n","prefix_sums = rdd.mapPartitionsWithIndex(g)\n","print(prefix_sums.collect())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[5, 8, 13, 1]\n","[5, 13, 26, 27]\n","[1, 5, 8, 13, 19, 26, 26, 27]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sArmqjCRzNql"},"source":["### Monotocity checking"]},{"cell_type":"code","metadata":{"id":"Dp9d3fF3zNqm","outputId":"e31beeaa-2aae-49d4-909d-3103370e052d"},"source":["x = [1, 3, 4, 5, 7, 3, 10, 14, 16, 20, 21, 24, 24, 26, 27, 30]\n","\n","rdd = sc.parallelize(x, 4).cache()\n","\n","def f(it):\n","    first = next(it)\n","    last = first\n","    increasing = True\n","    for i in it:\n","        if i < last:\n","            increasing = False\n","        last = i\n","    yield increasing, first, last\n","\n","results = rdd.mapPartitions(f).collect()\n","\n","print(results)\n","\n","increasing = True\n","if results[0][0] == False:\n","    increasing = False\n","else:\n","    for i in range(1, len(results)):\n","        if results[i][0] == False or results[i][1] < results[i-1][2]:\n","            increasing = False\n","if increasing:\n","    print(\"Monotone\")\n","else:\n","    print(\"Not monotone\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[(True, 1, 5), (False, 7, 14), (True, 16, 24), (True, 24, 30)]\n","Not monotone\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cGHlbeOuzNqo"},"source":["### Maximum Subarray Problem"]},{"cell_type":"code","metadata":{"id":"BSqLdtzyzNqo","outputId":"1c7c1d22-dd3a-4807-dbe9-2cc0a4aa3225"},"source":["# Classical divide and conquer algorithm\n","\n","A = [-3, 2, 1, -4, 5, 2, -1, 3, -1]\n","\n","def MaxSubarray(A, p, r):\n","    if p == r:\n","        return A[p]\n","    q = (p+r)//2\n","    M1 = MaxSubarray(A, p, q)\n","    M2 = MaxSubarray(A, q+1, r)\n","    Lm = -float('inf')\n","    Rm = Lm\n","    V = 0\n","    for i in range(q, p-1, -1):\n","        V += A[i]\n","        if V > Lm:\n","            Lm = V\n","    V = 0\n","    for i in range(q+1, r+1):\n","        V += A[i]\n","        if V > Rm:\n","            Rm = V\n","    return max(M1, M2, Lm+Rm)\n","\n","print(MaxSubarray(A, 0, len(A)-1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vb4CgintzNqq","outputId":"f8a64b3b-1d26-414f-daa4-ba910f084ec4"},"source":["# Linear-time algorithm\n","# Written in a way so that we can call it for each partition\n","\n","def linear_time(it):\n","    Vmax = -float('inf')\n","    V = 0\n","    for Ai in it:\n","        V += Ai\n","        if V > Vmax:\n","            Vmax = V\n","        if V < 0:\n","            V = 0\n","    yield Vmax\n","    \n","print(next(linear_time(A)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YPXeKwlGzNqs","outputId":"a3b410c5-6efb-42f6-ba13-5ea5862180e8"},"source":["# The Spark algorithm:\n","\n","def compute_sum(it):\n","    yield sum(it)\n","\n","def compute_LmRm(index, it):\n","    Lm = -float('inf')\n","    Rm = -float('inf')\n","    L = sums[index]\n","    R = 0\n","    for Ai in it:\n","        L -= Ai\n","        R += Ai\n","        if L > Lm:\n","            Lm = L\n","        if R > Rm:\n","            Rm = R\n","    yield (Lm, Rm)\n","\n","num_partitions = 4\n","rdd = sc.parallelize(A, num_partitions).cache()\n","sums = rdd.mapPartitions(compute_sum).collect()\n","print(sums)\n","LmRms = rdd.mapPartitionsWithIndex(compute_LmRm).collect()\n","print(LmRms)\n","best = max(rdd.mapPartitions(linear_time).collect())\n","\n","for i in range(num_partitions-1):\n","    for j in range(i+1, num_partitions):\n","        x = LmRms[i][0] + sum(sums[i+1:j]) + LmRms[j][1]\n","        if x > best:\n","            best = x\n","\n","print(best)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-1, -3, 7, 1]\n","[(2, -1), (0, 1), (2, 7), (2, 2)]\n","9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IejfZcE3zNqu"},"source":[""],"execution_count":null,"outputs":[]}]}